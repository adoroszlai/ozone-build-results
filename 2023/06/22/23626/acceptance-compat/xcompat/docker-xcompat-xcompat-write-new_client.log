Attaching to xcompat_scm_1, xcompat_old_client_1_1_0_1, xcompat_s3g_1, xcompat_old_client_1_0_0_1, xcompat_datanode_2, xcompat_recon_1, xcompat_new_client_1, xcompat_datanode_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_datanode_3, xcompat_old_client_1_2_1_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:28:59,083 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = c5fd1eebc244/172.19.0.4
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:28:59,172 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:28:59,427 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:29:00,124 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:29:00,978 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:29:00,979 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:29:01,748 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c5fd1eebc244 ip:172.19.0.4
datanode_1          | 2023-06-22 08:29:02,891 [main] INFO reflections.Reflections: Reflections took 796 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_1          | 2023-06-22 08:29:06,073 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-22 08:29:06,424 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-22 08:29:07,679 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:29:07,820 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-22 08:29:07,823 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:29:07,842 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:29:08,105 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:29:08,224 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:29:08,240 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-22 08:29:08,244 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-22 08:29:08,248 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-22 08:29:08,249 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-22 08:29:08,529 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:29:08,530 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:29:18,745 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-22 08:29:19,506 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:29:19,795 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:29:20,950 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:29:21,012 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-22 08:29:21,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:29:21,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-22 08:29:21,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-22 08:29:21,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-22 08:29:21,046 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:29:21,064 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:29:21,065 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:29:21,066 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:29:21,205 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:29:21,239 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-22 08:29:21,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-22 08:29:23,291 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-22 08:29:23,320 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-22 08:29:23,321 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-22 08:29:23,324 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:29:23,342 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:29:23,355 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:29:23,646 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-22 08:29:24,095 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-22 08:29:25,218 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:29:25,302 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:29:25,488 [main] INFO util.log: Logging initialized @36798ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:29:26,097 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-22 08:29:26,146 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:29:26,206 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:29:26,230 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:29:26,230 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:29:26,231 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:29:26,525 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-22 08:29:26,550 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-22 08:29:26,552 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-22 08:29:26,797 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:29:26,797 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:29:26,799 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-22 08:29:26,863 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@744199bb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:29:26,877 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7dc963be{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:29:27,489 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5746609e{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4323885541222244916/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:29:27,538 [main] INFO server.AbstractConnector: Started ServerConnector@27e656e6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:29:27,539 [main] INFO server.Server: Started @38849ms
datanode_1          | 2023-06-22 08:29:27,566 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-22 08:29:27,566 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-22 08:29:27,569 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:29:27,838 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-22 08:29:28,015 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-22 08:29:29,122 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_1          | 2023-06-22 08:29:29,191 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-22 08:29:29,195 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-22 08:29:29,196 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-22 08:29:29,251 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-22 08:29:29,279 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-22 08:29:29,800 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.9:9891
datanode_1          | 2023-06-22 08:29:30,123 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:29:32,744 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:32,761 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:33,745 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:33,761 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:34,746 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:34,769 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:35,770 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:36,771 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:37,772 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:38,773 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:39,774 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:39,871 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From c5fd1eebc244/172.19.0.4 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:57412 remote=recon/172.19.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:57412 remote=recon/172.19.0.9:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_1          | 2023-06-22 08:29:40,775 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:41,776 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:42,777 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:43,778 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:29:48,793 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From c5fd1eebc244/172.19.0.4 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:37700 remote=scm/172.19.0.13:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.4:37700 remote=scm/172.19.0.13:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_1          | 2023-06-22 08:29:49,803 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-e94cc996-b837-4a4d-a19d-0294a0fdecb7/container.db to cache
datanode_1          | 2023-06-22 08:29:49,804 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-e94cc996-b837-4a4d-a19d-0294a0fdecb7/container.db for volume DS-e94cc996-b837-4a4d-a19d-0294a0fdecb7
datanode_1          | 2023-06-22 08:29:49,812 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:29:49,835 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-22 08:29:50,164 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-22 08:29:50,167 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:29:50,349 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.RaftServer: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start RPC server
datanode_1          | 2023-06-22 08:29:50,366 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: GrpcService started, listening on 9858
datanode_1          | 2023-06-22 08:29:50,375 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: GrpcService started, listening on 9856
datanode_1          | 2023-06-22 08:29:50,384 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: GrpcService started, listening on 9857
datanode_1          | 2023-06-22 08:29:50,396 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 is started using port 9858 for RATIS
datanode_1          | 2023-06-22 08:29:50,397 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-22 08:29:50,399 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-22 08:29:50,399 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7b56dd5f-5763-4a54-b62e-fd435ccd0c95: Started
datanode_1          | 2023-06-22 08:29:50,535 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:29:54,730 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: addNew group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-1D6EFDD8A424:java.util.concurrent.CompletableFuture@3744789b[Not completed]
datanode_1          | 2023-06-22 08:29:54,798 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: new RaftServerImpl for group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:29:54,806 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:29:54,812 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:29:54,812 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:29:54,814 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:29:54,815 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:29:54,815 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:29:54,872 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:29:54,874 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:29:54,890 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:29:54,894 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:29:54,948 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:29:54,951 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:29:54,979 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:29:54,980 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:29:55,027 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:29:55,098 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:29:55,108 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:29:55,110 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:29:55,115 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:29:55,115 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:29:55,116 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:29:55,117 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 does not exist. Creating ...
datanode_1          | 2023-06-22 08:29:55,133 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/in_use.lock acquired by nodename 6@c5fd1eebc244
datanode_1          | 2023-06-22 08:29:55,151 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 has been successfully formatted.
datanode_1          | 2023-06-22 08:29:55,176 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO ratis.ContainerStateMachine: group-1D6EFDD8A424: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:29:55,190 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:29:55,219 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:29:55,226 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:29:55,229 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:29:55,235 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:29:55,269 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:29:55,281 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:29:55,289 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:29:55,291 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:29:55,334 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424
datanode_1          | 2023-06-22 08:29:55,339 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:29:55,340 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:29:55,346 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:29:55,351 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:29:55,356 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:29:55,364 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:29:55,364 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:29:55,365 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:29:55,451 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:29:55,457 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:29:55,610 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:29:55,614 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:29:55,614 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:29:55,653 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:29:55,656 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:29:55,662 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:29:55,666 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:29:55,671 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState
datanode_1          | 2023-06-22 08:29:55,675 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:29:55,685 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:29:55,714 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D6EFDD8A424,id=7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:29:55,722 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:29:55,723 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:29:55,728 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:29:55,728 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:29:55,861 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424
datanode_1          | 2023-06-22 08:30:00,748 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO impl.FollowerState: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5077676857ns, electionTimeout:5062ms
datanode_1          | 2023-06-22 08:30:00,749 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState
datanode_1          | 2023-06-22 08:30:00,751 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:30:00,762 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-22 08:30:00,763 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1
datanode_1          | 2023-06-22 08:30:00,783 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:00,819 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:30:00,821 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:30:00,832 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:29:00,206 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 37f0a4af5edd/172.19.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-06-22 08:30:00,833 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_1          | 2023-06-22 08:30:00,967 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:30:00,968 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection:   Response 0: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0
datanode_1          | 2023-06-22 08:30:00,968 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-06-22 08:30:00,975 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:01,000 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:30:01,000 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:30:01,169 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:30:01,169 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection:   Response 0: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t1
datanode_1          | 2023-06-22 08:30:01,169 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1 ELECTION round 0: result PASSED
datanode_1          | 2023-06-22 08:30:01,169 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1
datanode_1          | 2023-06-22 08:30:01,170 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:30:01,170 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D6EFDD8A424 with new leaderId: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:30:01,170 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: change Leader from null to 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 at term 1 for becomeLeader, leader elected after 6222ms
datanode_1          | 2023-06-22 08:30:01,230 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:30:01,271 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:01,280 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:30:01,305 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:30:01,305 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:30:01,310 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:30:01,375 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:01,386 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:30:01,445 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:30:01,445 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:01,445 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:30:01,459 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-22 08:30:01,464 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:30:01,473 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:30:01,474 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-22 08:30:01,474 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-22 08:30:01,474 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:30:01,480 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:30:01,493 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:30:01,493 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:29:00,279 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:29:00,587 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:29:01,372 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:29:02,356 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:29:02,356 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:29:03,215 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:37f0a4af5edd ip:172.19.0.10
datanode_2          | 2023-06-22 08:29:04,274 [main] INFO reflections.Reflections: Reflections took 700 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_2          | 2023-06-22 08:29:07,335 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-22 08:29:07,794 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-22 08:29:09,087 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:29:09,297 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-22 08:29:09,311 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:29:09,325 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:29:09,547 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:29:09,692 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:29:09,700 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-22 08:29:09,715 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-22 08:29:09,722 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-22 08:29:09,723 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-22 08:29:09,938 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:29:09,952 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:29:19,680 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-22 08:29:20,209 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:29:20,532 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:29:21,069 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:29:21,071 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-22 08:29:21,076 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:29:21,077 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-22 08:29:21,077 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-22 08:29:21,077 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-22 08:29:21,078 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:29:21,079 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:29:21,086 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:29:21,087 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:29:21,579 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-22 08:29:21,645 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-22 08:30:01,493 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:30:01,493 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-22 08:30:01,494 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:30:01,495 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:30:01,498 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-22 08:30:01,503 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-22 08:30:01,505 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:30:01,506 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:30:01,513 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderStateImpl
datanode_1          | 2023-06-22 08:30:01,577 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:30:01,704 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-LeaderElection1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:02,559 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/current/log_inprogress_0
datanode_1          | 2023-06-22 08:30:02,794 [grpc-default-executor-0] WARN server.GrpcLogAppender: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424 is not in [STARTING, RUNNING]: current state is NEW
datanode_1          | 2023-06-22 08:30:02,803 [grpc-default-executor-0] INFO leader.FollowerInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_1          | 2023-06-22 08:30:02,961 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_1          | 2023-06-22 08:30:02,962 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_1          | 2023-06-22 08:30:03,150 [grpc-default-executor-1] WARN server.GrpcLogAppender: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_1          | 2023-06-22 08:30:03,150 [grpc-default-executor-1] INFO leader.FollowerInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-1D6EFDD8A424->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_1          | 2023-06-22 08:30:03,390 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424.
datanode_1          | 2023-06-22 08:30:03,391 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: addNew group-269E8D473FC7:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER] returns group-269E8D473FC7:java.util.concurrent.CompletableFuture@5e220b01[Not completed]
datanode_1          | 2023-06-22 08:30:03,393 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: new RaftServerImpl for group-269E8D473FC7:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:30:03,393 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: ConfigurationManager, init=-1: peers:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:30:03,394 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:30:03,395 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:30:03,395 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:30:03,401 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:30:03,411 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:30:03,412 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1de0afd2-c845-4655-b03a-269e8d473fc7 does not exist. Creating ...
datanode_1          | 2023-06-22 08:30:03,419 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1de0afd2-c845-4655-b03a-269e8d473fc7/in_use.lock acquired by nodename 6@c5fd1eebc244
datanode_1          | 2023-06-22 08:30:03,425 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1de0afd2-c845-4655-b03a-269e8d473fc7 has been successfully formatted.
datanode_1          | 2023-06-22 08:30:03,430 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO ratis.ContainerStateMachine: group-269E8D473FC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:30:03,443 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1de0afd2-c845-4655-b03a-269e8d473fc7
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:30:03,444 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:03,464 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,467 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:30:03,467 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:30:03,467 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:30:03,467 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:30:03,467 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:30:03,470 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,471 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,538 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:30:03,539 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:30:03,539 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:30:03,539 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:30:03,540 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:30:03,541 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: start as a follower, conf=-1: peers:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:29:21,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-22 08:29:23,937 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-22 08:29:23,968 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-22 08:29:23,976 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-22 08:29:23,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:29:23,985 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:29:24,030 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:29:24,488 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-22 08:29:24,965 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-22 08:29:26,080 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:29:26,174 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:29:26,335 [main] INFO util.log: Logging initialized @36501ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:29:27,151 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-22 08:29:27,184 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:29:27,241 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:29:27,270 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:29:27,270 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:29:27,271 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:29:27,549 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-22 08:29:27,571 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-22 08:29:27,578 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-22 08:29:27,788 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:29:27,788 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:29:27,790 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-06-22 08:29:27,872 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ac0fd1d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:29:27,899 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1c15a6aa{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:29:28,501 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@178c4480{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-6564046405819389202/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-22 08:29:28,565 [main] INFO server.AbstractConnector: Started ServerConnector@3751baf6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:29:28,570 [main] INFO server.Server: Started @38735ms
datanode_2          | 2023-06-22 08:29:28,588 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-22 08:29:28,588 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-22 08:29:28,600 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:29:28,857 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-22 08:29:29,057 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-22 08:29:30,355 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_2          | 2023-06-22 08:29:30,418 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-22 08:29:30,418 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-22 08:29:30,435 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-22 08:29:30,468 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-22 08:29:30,505 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-22 08:29:30,974 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.9:9891
datanode_2          | 2023-06-22 08:29:31,260 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:29:33,847 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:30:03,541 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:30:03,542 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState
datanode_1          | 2023-06-22 08:30:03,543 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-269E8D473FC7,id=7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:30:03,544 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:30:03,544 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:30:03,544 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:30:03,545 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:30:03,545 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:30:03,545 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:30:03,549 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=1de0afd2-c845-4655-b03a-269e8d473fc7
datanode_1          | 2023-06-22 08:30:03,549 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=1de0afd2-c845-4655-b03a-269e8d473fc7.
datanode_1          | 2023-06-22 08:30:03,550 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: addNew group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-0437B3489A53:java.util.concurrent.CompletableFuture@16aa7479[Not completed]
datanode_1          | 2023-06-22 08:30:03,560 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: new RaftServerImpl for group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:30:03,562 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:30:03,568 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:30:03,569 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:30:03,571 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:30:03,572 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:30:03,573 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:30:03,573 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:30:03,574 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:30:03,577 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:30:03,577 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:30:03,577 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:30:03,578 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:30:03,578 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:30:03,578 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:30:03,580 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:30:03,590 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:30:03,591 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:30:03,591 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:30:03,592 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:30:03,592 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:30:03,592 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:30:03,593 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 does not exist. Creating ...
datanode_1          | 2023-06-22 08:30:03,600 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/in_use.lock acquired by nodename 6@c5fd1eebc244
datanode_1          | 2023-06-22 08:30:03,603 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 has been successfully formatted.
datanode_1          | 2023-06-22 08:30:03,605 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO ratis.ContainerStateMachine: group-0437B3489A53: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:30:03,606 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:30:03,606 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:30:03,607 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,609 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:30:03,609 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:30:03,613 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,614 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:30:03,614 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:30:03,614 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,615 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53
datanode_1          | 2023-06-22 08:30:03,615 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:30:03,615 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:03,616 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,616 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:30:03,617 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:30:03,617 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:30:03,617 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:30:03,617 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:30:03,620 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:30:03,622 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:30:03,659 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:30:03,659 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:30:03,660 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:30:03,660 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:30:03,663 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:30:03,664 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:03,665 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:30:03,665 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState
datanode_1          | 2023-06-22 08:30:03,666 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0437B3489A53,id=7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:30:03,667 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:30:03,667 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:30:03,667 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:30:03,667 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:30:03,669 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:28:57,751 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 2fbf73961aee/172.19.0.5
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_1          | 2023-06-22 08:30:03,676 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:30:03,677 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53
datanode_1          | 2023-06-22 08:30:05,411 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53.
datanode_1          | 2023-06-22 08:30:08,620 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO impl.FollowerState: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5078754449ns, electionTimeout:5076ms
datanode_1          | 2023-06-22 08:30:08,621 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState
datanode_1          | 2023-06-22 08:30:08,621 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:30:08,622 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-22 08:30:08,622 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2
datanode_1          | 2023-06-22 08:30:08,625 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:08,626 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-22 08:30:08,632 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:08,632 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-22 08:30:08,632 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2
datanode_1          | 2023-06-22 08:30:08,632 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:30:08,633 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-269E8D473FC7 with new leaderId: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_1          | 2023-06-22 08:30:08,634 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: change Leader from null to 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 at term 1 for becomeLeader, leader elected after 5238ms
datanode_1          | 2023-06-22 08:30:08,635 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:30:08,635 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:08,635 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:30:08,637 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderStateImpl
datanode_1          | 2023-06-22 08:30:08,638 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:30:08,647 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1de0afd2-c845-4655-b03a-269e8d473fc7/current/log_inprogress_0
datanode_1          | 2023-06-22 08:30:08,669 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7-LeaderElection2] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-269E8D473FC7: set configuration 0: peers:[7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:08,753 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO impl.FollowerState: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5087603388ns, electionTimeout:5076ms
datanode_1          | 2023-06-22 08:30:08,754 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState
datanode_1          | 2023-06-22 08:30:08,754 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:30:08,754 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:28:57,836 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:28:58,110 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:28:58,780 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:28:59,838 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:28:59,839 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:29:00,649 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2fbf73961aee ip:172.19.0.5
datanode_3          | 2023-06-22 08:29:01,878 [main] INFO reflections.Reflections: Reflections took 812 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_3          | 2023-06-22 08:29:04,905 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-22 08:29:05,278 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-22 08:29:06,649 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:29:06,771 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-22 08:29:06,806 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:29:06,837 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:29:07,119 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:29:07,271 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:29:07,277 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-22 08:29:07,296 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-22 08:29:07,297 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-22 08:29:07,297 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-22 08:29:07,565 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:29:07,568 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:29:17,154 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-22 08:29:17,821 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:29:18,391 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:29:18,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:29:18,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-22 08:29:18,926 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:29:18,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-22 08:29:18,936 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-22 08:29:18,938 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-22 08:29:18,941 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:29:18,953 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:29:18,964 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:29:18,989 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:29:19,062 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:29:19,111 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-22 08:29:19,112 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-22 08:29:21,976 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-22 08:29:21,995 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-22 08:29:21,997 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-22 08:29:22,004 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:29:22,004 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:29:22,014 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:29:22,418 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-22 08:29:23,127 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-22 08:29:24,076 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:29:24,173 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:29:24,409 [main] INFO util.log: Logging initialized @37129ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:29:25,275 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-22 08:29:25,315 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:29:25,380 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:29:25,385 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:29:25,394 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:29:25,394 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-22 08:29:25,728 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-22 08:29:25,778 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-22 08:29:25,780 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-22 08:29:26,010 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:29:26,011 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:29:26,025 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-06-22 08:29:26,085 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ac0fd1d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-22 08:29:26,086 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1c15a6aa{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:29:26,837 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@178c4480{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-3948500605869070011/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:29:26,913 [main] INFO server.AbstractConnector: Started ServerConnector@3751baf6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:29:26,920 [main] INFO server.Server: Started @39639ms
datanode_3          | 2023-06-22 08:29:26,941 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-22 08:29:26,942 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-22 08:29:26,949 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:29:27,188 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-22 08:29:27,416 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-22 08:29:28,678 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_3          | 2023-06-22 08:29:28,775 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-22 08:29:28,779 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-22 08:29:28,792 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-22 08:29:28,832 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-22 08:29:28,873 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-22 08:29:29,281 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.9:9891
datanode_3          | 2023-06-22 08:29:29,619 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:29:32,318 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:30:08,755 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3
datanode_1          | 2023-06-22 08:30:08,764 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:08,774 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:30:08,774 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:30:08,803 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:30:08,803 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.LeaderElection:   Response 0: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#0:FAIL-t0
datanode_1          | 2023-06-22 08:30:08,803 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.LeaderElection:   Response 1: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0
datanode_1          | 2023-06-22 08:30:08,803 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.LeaderElection: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3 PRE_VOTE round 0: result REJECTED
datanode_1          | 2023-06-22 08:30:08,805 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_1          | 2023-06-22 08:30:08,805 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3
datanode_1          | 2023-06-22 08:30:08,805 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-LeaderElection3] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState
datanode_1          | 2023-06-22 08:30:11,233 [grpc-default-executor-1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: receive requestVote(PRE_VOTE, f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, group-0437B3489A53, 0, (t:0, i:0))
datanode_1          | 2023-06-22 08:30:11,239 [grpc-default-executor-0] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: receive requestVote(ELECTION, f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, group-0437B3489A53, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:30:11,240 [grpc-default-executor-1] INFO impl.VoteContext: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FOLLOWER: accept PRE_VOTE from f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:30:11,252 [grpc-default-executor-1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53 replies to PRE_VOTE vote request: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-7b56dd5f-5763-4a54-b62e-fd435ccd0c95#0:OK-t0. Peer's state: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53:t0, leader=null, voted=, raftlog=Memoized:7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:11,255 [grpc-default-executor-0] INFO impl.VoteContext: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FOLLOWER: accept ELECTION from f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:30:11,256 [grpc-default-executor-0] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_1          | 2023-06-22 08:30:11,256 [grpc-default-executor-0] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: shutdown 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState
datanode_1          | 2023-06-22 08:30:11,256 [grpc-default-executor-0] INFO impl.RoleInfo: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: start 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState
datanode_1          | 2023-06-22 08:30:11,256 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState] INFO impl.FollowerState: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-FollowerState was interrupted
datanode_1          | 2023-06-22 08:30:11,276 [grpc-default-executor-0] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53 replies to ELECTION vote request: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-7b56dd5f-5763-4a54-b62e-fd435ccd0c95#0:OK-t1. Peer's state: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53:t1, leader=null, voted=f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, raftlog=Memoized:7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:11,562 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0437B3489A53 with new leaderId: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_1          | 2023-06-22 08:30:11,562 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-server-thread1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: change Leader from null to f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 at term 1 for appendEntries, leader elected after 7984ms
datanode_1          | 2023-06-22 08:30:11,565 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-server-thread1] INFO server.RaftServer$Division: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:30:11,566 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95-server-thread1] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:30:11,570 [7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95@group-0437B3489A53-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/current/log_inprogress_0
datanode_1          | 2023-06-22 08:30:50,535 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:31:50,537 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:32:50,537 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:33:50,538 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:34:50,538 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:35:50,539 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:36:50,539 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:29:32,331 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:33,319 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:33,332 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:34,319 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:34,333 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:35,320 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:36,323 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:37,324 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:38,325 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:39,327 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:39,365 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 2fbf73961aee/172.19.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:33926 remote=recon/172.19.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:33926 remote=recon/172.19.0.9:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_3          | 2023-06-22 08:29:40,328 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:41,329 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:42,329 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:33,855 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:34,848 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.9:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:34,857 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:35,858 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:36,859 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:37,860 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:38,863 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:39,864 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:39,895 [EndpointStateMachine task thread for recon/172.19.0.9:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 37f0a4af5edd/172.19.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:44098 remote=recon/172.19.0.9:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:44098 remote=recon/172.19.0.9:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_2          | 2023-06-22 08:29:40,865 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:41,866 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:42,867 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:43,868 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:29:48,879 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | 2023-06-22 08:29:43,331 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.13:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:29:48,342 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 2fbf73961aee/172.19.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:44782 remote=scm/172.19.0.13:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.5:44782 remote=scm/172.19.0.13:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_3          | 2023-06-22 08:29:50,017 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-9e171479-6769-4e55-b0fa-8803bdc26889/container.db to cache
datanode_3          | 2023-06-22 08:29:50,018 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-9e171479-6769-4e55-b0fa-8803bdc26889/container.db for volume DS-9e171479-6769-4e55-b0fa-8803bdc26889
datanode_3          | 2023-06-22 08:29:50,033 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-22 08:29:50,049 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-22 08:29:50,287 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-22 08:29:50,288 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:29:50,339 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.RaftServer: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start RPC server
datanode_3          | 2023-06-22 08:29:50,346 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: GrpcService started, listening on 9858
datanode_3          | 2023-06-22 08:29:50,351 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: GrpcService started, listening on 9856
datanode_3          | 2023-06-22 08:29:50,355 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: GrpcService started, listening on 9857
datanode_3          | 2023-06-22 08:29:50,366 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 is started using port 9858 for RATIS
datanode_3          | 2023-06-22 08:29:50,367 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-22 08:29:50,367 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-22 08:29:50,367 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Started
datanode_3          | 2023-06-22 08:29:50,439 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:29:51,095 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | java.net.SocketTimeoutException: Call From 37f0a4af5edd/172.19.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:45196 remote=scm/172.19.0.13:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.10:45196 remote=scm/172.19.0.13:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_2          | 2023-06-22 08:29:49,856 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-906e0417-f213-4045-a1f7-44dd7185a319/container.db to cache
datanode_2          | 2023-06-22 08:29:49,861 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/DS-906e0417-f213-4045-a1f7-44dd7185a319/container.db for volume DS-906e0417-f213-4045-a1f7-44dd7185a319
datanode_2          | 2023-06-22 08:29:49,877 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-22 08:29:49,889 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-22 08:29:50,186 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-22 08:29:50,186 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_2          | 2023-06-22 08:29:50,335 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.RaftServer: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start RPC server
datanode_2          | 2023-06-22 08:29:50,354 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: d1e52d63-e5d8-4bcf-98c5-65421cfda140: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:29:50,358 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: d1e52d63-e5d8-4bcf-98c5-65421cfda140: GrpcService started, listening on 9856
datanode_2          | 2023-06-22 08:29:50,367 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO server.GrpcService: d1e52d63-e5d8-4bcf-98c5-65421cfda140: GrpcService started, listening on 9857
datanode_2          | 2023-06-22 08:29:50,389 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d1e52d63-e5d8-4bcf-98c5-65421cfda140 is started using port 9858 for RATIS
datanode_2          | 2023-06-22 08:29:50,390 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d1e52d63-e5d8-4bcf-98c5-65421cfda140 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-22 08:29:50,394 [EndpointStateMachine task thread for scm/172.19.0.13:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d1e52d63-e5d8-4bcf-98c5-65421cfda140 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-22 08:29:50,390 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d1e52d63-e5d8-4bcf-98c5-65421cfda140: Started
datanode_2          | 2023-06-22 08:29:50,502 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:29:58,423 [grpc-default-executor-0] INFO server.RaftServer: d1e52d63-e5d8-4bcf-98c5-65421cfda140: addNew group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-1D6EFDD8A424:java.util.concurrent.CompletableFuture@5a4c8d73[Not completed]
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-06-22 08:30:01,130 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Failed requestVote 7b56dd5f-5763-4a54-b62e-fd435ccd0c95->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#0
datanode_3          | org.apache.ratis.protocol.exceptions.GroupMismatchException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: group-1D6EFDD8A424 not found.
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_3          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_3          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | 2023-06-22 08:30:01,130 [grpc-default-executor-2] WARN server.GrpcServerProtocolService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Failed requestVote 7b56dd5f-5763-4a54-b62e-fd435ccd0c95->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#0
datanode_3          | org.apache.ratis.protocol.exceptions.GroupMismatchException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: group-1D6EFDD8A424 not found.
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_3          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_3          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | 2023-06-22 08:30:01,425 [grpc-default-executor-0] INFO server.RaftServer: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: addNew group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-1D6EFDD8A424:java.util.concurrent.CompletableFuture@7324f722[Not completed]
datanode_3          | 2023-06-22 08:30:01,578 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: new RaftServerImpl for group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:30:01,582 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:30:01,586 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:30:01,586 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:30:01,587 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:01,588 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:30:01,589 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:30:01,637 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:30:01,639 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:30:01,660 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:30:01,662 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:30:01,732 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:01,762 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:01,789 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:30:01,790 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:30:01,956 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:30:02,098 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:30:02,126 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:30:02,131 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:30:02,132 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:30:02,140 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:30:02,141 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:30:02,710 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] WARN server.GrpcServerProtocolService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Failed APPEND_ENTRIES request 7b56dd5f-5763-4a54-b62e-fd435ccd0c95->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "f91e12cd-1de9-431d-8f3a-a5eb5b4a5293"
datanode_3          | address: "172.19.0.5:9856"
datanode_3          | dataStreamAddress: "172.19.0.5:9858"
datanode_3          | clientAddress: "172.19.0.5:9858"
datanode_3          | adminAddress: "172.19.0.5:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "7b56dd5f-5763-4a54-b62e-fd435ccd0c95"
datanode_3          | address: "172.19.0.4:9856"
datanode_3          | priority: 1
datanode_3          | dataStreamAddress: "172.19.0.4:9858"
datanode_3          | clientAddress: "172.19.0.4:9858"
datanode_3          | adminAddress: "172.19.0.4:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "d1e52d63-e5d8-4bcf-98c5-65421cfda140"
datanode_3          | address: "172.19.0.10:9856"
datanode_3          | dataStreamAddress: "172.19.0.10:9858"
datanode_3          | clientAddress: "172.19.0.10:9858"
datanode_3          | adminAddress: "172.19.0.10:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | , old:)
datanode_3          | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424 is not in [STARTING, RUNNING]: current state is NEW
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:121)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitServerRequestAsync$11(RaftServerImpl.java:825)
datanode_3          | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424 is not in [STARTING, RUNNING]: current state is NEW
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$assertLifeCycleState$9(RaftServerImpl.java:749)
datanode_3          | 	at org.apache.ratis.util.LifeCycle.assertCurrentState(LifeCycle.java:253)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.assertLifeCycleState(RaftServerImpl.java:748)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.preAppendEntriesAsync(RaftServerImpl.java:1449)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.appendEntriesAsync(RaftServerImpl.java:1393)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$25(RaftServerProxy.java:637)
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
datanode_3          | 	... 5 more
datanode_3          | 2023-06-22 08:30:02,725 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 does not exist. Creating ...
datanode_3          | 2023-06-22 08:30:02,750 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/in_use.lock acquired by nodename 7@2fbf73961aee
datanode_3          | 2023-06-22 08:30:02,773 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 has been successfully formatted.
datanode_3          | 2023-06-22 08:30:02,816 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO ratis.ContainerStateMachine: group-1D6EFDD8A424: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:30:02,818 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:30:02,848 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:30:02,848 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:29:58,466 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140: new RaftServerImpl for group-1D6EFDD8A424:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:29:58,468 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:29:58,470 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:29:58,471 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:29:58,471 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:29:58,471 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:29:58,471 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:29:58,490 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:29:58,490 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:29:58,519 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:29:58,526 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:29:58,608 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:29:58,621 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:29:58,636 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:29:58,636 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:29:58,738 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:29:58,771 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:29:58,779 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:29:58,780 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:29:58,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:29:58,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:29:58,782 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:29:58,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 does not exist. Creating ...
datanode_2          | 2023-06-22 08:29:58,799 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/in_use.lock acquired by nodename 6@37f0a4af5edd
datanode_2          | 2023-06-22 08:29:58,811 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424 has been successfully formatted.
datanode_2          | 2023-06-22 08:29:58,873 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO ratis.ContainerStateMachine: group-1D6EFDD8A424: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:29:58,908 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:29:59,001 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:29:59,004 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:29:59,009 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:29:59,021 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:29:59,040 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:29:59,102 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:29:59,102 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:29:59,102 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:29:59,141 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424
datanode_2          | 2023-06-22 08:29:59,141 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:29:59,147 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:29:59,153 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:29:59,153 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:29:59,153 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:29:59,157 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:29:59,158 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:29:59,163 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:29:59,226 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:29:59,226 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:29:59,330 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:29:59,331 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:29:59,342 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:29:59,381 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:29:59,383 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:29:59,390 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:29:59,391 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:29:59,397 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState
datanode_2          | 2023-06-22 08:29:59,408 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:29:59,412 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:29:59,423 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D6EFDD8A424,id=d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_2          | 2023-06-22 08:29:59,429 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:29:59,429 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:29:59,431 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:29:59,431 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:30:00,935 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: receive requestVote(PRE_VOTE, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95, group-1D6EFDD8A424, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:30:00,944 [grpc-default-executor-0] INFO impl.VoteContext: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FOLLOWER: accept PRE_VOTE from 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:30:00,953 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424 replies to PRE_VOTE vote request: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0. Peer's state: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424:t0, leader=null, voted=, raftlog=Memoized:d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:01,151 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: receive requestVote(ELECTION, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95, group-1D6EFDD8A424, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:30:01,152 [grpc-default-executor-0] INFO impl.VoteContext: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FOLLOWER: accept ELECTION from 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:30:01,153 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_2          | 2023-06-22 08:30:01,153 [grpc-default-executor-0] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: shutdown d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState
datanode_2          | 2023-06-22 08:30:01,154 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState] INFO impl.FollowerState: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState was interrupted
datanode_2          | 2023-06-22 08:30:01,155 [grpc-default-executor-0] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-FollowerState
datanode_3          | 2023-06-22 08:30:02,851 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:30:02,853 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:30:02,861 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:02,882 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:30:02,883 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:30:02,883 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:02,925 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Failed APPEND_ENTRIES request 7b56dd5f-5763-4a54-b62e-fd435ccd0c95->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#2-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "f91e12cd-1de9-431d-8f3a-a5eb5b4a5293"
datanode_3          | address: "172.19.0.5:9856"
datanode_3          | dataStreamAddress: "172.19.0.5:9858"
datanode_3          | clientAddress: "172.19.0.5:9858"
datanode_3          | adminAddress: "172.19.0.5:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "7b56dd5f-5763-4a54-b62e-fd435ccd0c95"
datanode_3          | address: "172.19.0.4:9856"
datanode_3          | priority: 1
datanode_3          | dataStreamAddress: "172.19.0.4:9858"
datanode_3          | clientAddress: "172.19.0.4:9858"
datanode_3          | adminAddress: "172.19.0.4:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "d1e52d63-e5d8-4bcf-98c5-65421cfda140"
datanode_3          | address: "172.19.0.10:9856"
datanode_3          | dataStreamAddress: "172.19.0.10:9858"
datanode_3          | clientAddress: "172.19.0.10:9858"
datanode_3          | adminAddress: "172.19.0.10:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | , old:)
datanode_3          | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:121)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitServerRequestAsync$11(RaftServerImpl.java:825)
datanode_3          | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.preAppendEntriesAsync(RaftServerImpl.java:1451)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.appendEntriesAsync(RaftServerImpl.java:1393)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$25(RaftServerProxy.java:637)
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
datanode_3          | 	... 5 more
datanode_3          | 2023-06-22 08:30:02,925 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424
datanode_3          | 2023-06-22 08:30:02,972 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:30:02,973 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:02,982 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:02,983 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:30:02,986 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:30:02,992 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:30:02,994 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:02,995 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:30:03,046 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:03,052 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:03,127 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Failed APPEND_ENTRIES request 7b56dd5f-5763-4a54-b62e-fd435ccd0c95->f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#4-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "f91e12cd-1de9-431d-8f3a-a5eb5b4a5293"
datanode_3          | address: "172.19.0.5:9856"
datanode_3          | dataStreamAddress: "172.19.0.5:9858"
datanode_3          | clientAddress: "172.19.0.5:9858"
datanode_3          | adminAddress: "172.19.0.5:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "7b56dd5f-5763-4a54-b62e-fd435ccd0c95"
datanode_3          | address: "172.19.0.4:9856"
datanode_3          | priority: 1
datanode_3          | dataStreamAddress: "172.19.0.4:9858"
datanode_3          | clientAddress: "172.19.0.4:9858"
datanode_3          | adminAddress: "172.19.0.4:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | ,id: "d1e52d63-e5d8-4bcf-98c5-65421cfda140"
datanode_3          | address: "172.19.0.10:9856"
datanode_3          | dataStreamAddress: "172.19.0.10:9858"
datanode_3          | clientAddress: "172.19.0.10:9858"
datanode_3          | adminAddress: "172.19.0.10:9857"
datanode_3          | startupRole: FOLLOWER
datanode_3          | , old:)
datanode_3          | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:121)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitServerRequestAsync$11(RaftServerImpl.java:825)
datanode_3          | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: org.apache.ratis.protocol.exceptions.ServerNotReadyException: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: The server role is not yet initialized.
datanode_2          | 2023-06-22 08:30:01,160 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424 replies to ELECTION vote request: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t1. Peer's state: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424:t1, leader=null, voted=7b56dd5f-5763-4a54-b62e-fd435ccd0c95, raftlog=Memoized:d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:01,934 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D6EFDD8A424 with new leaderId: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_2          | 2023-06-22 08:30:01,936 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: change Leader from null to 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 at term 1 for appendEntries, leader elected after 3330ms
datanode_2          | 2023-06-22 08:30:02,170 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread2] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:02,235 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread2] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:30:02,819 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-1D6EFDD8A424-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/current/log_inprogress_0
datanode_2          | 2023-06-22 08:30:03,730 [grpc-default-executor-0] INFO server.RaftServer: d1e52d63-e5d8-4bcf-98c5-65421cfda140: addNew group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-0437B3489A53:java.util.concurrent.CompletableFuture@21028cb7[Not completed]
datanode_2          | 2023-06-22 08:30:03,740 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140: new RaftServerImpl for group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:30:03,740 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:30:03,741 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:30:03,741 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:30:03,742 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:30:03,743 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:30:03,747 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:30:03,748 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:30:03,748 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:30:03,749 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:30:03,750 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:30:03,751 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:30:03,751 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:30:03,751 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:30:03,751 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:30:03,756 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:30:03,758 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:30:03,759 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:30:03,761 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:30:03,762 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:30:03,762 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:30:03,763 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.preAppendEntriesAsync(RaftServerImpl.java:1451)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerImpl.appendEntriesAsync(RaftServerImpl.java:1393)
datanode_3          | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$25(RaftServerProxy.java:637)
datanode_3          | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
datanode_3          | 	... 5 more
datanode_3          | 2023-06-22 08:30:03,141 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:03,146 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:03,147 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:03,167 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:30:03,167 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:30:03,169 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:03,170 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:30:03,171 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState
datanode_3          | 2023-06-22 08:30:03,175 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:03,178 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:03,180 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1D6EFDD8A424,id=f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:30:03,183 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:03,183 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:30:03,183 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:30:03,184 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:30:04,385 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1D6EFDD8A424 with new leaderId: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_3          | 2023-06-22 08:30:04,400 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-server-thread1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: change Leader from null to 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 at term 1 for appendEntries, leader elected after 2657ms
datanode_3          | 2023-06-22 08:30:04,417 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-server-thread1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:0|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:1|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:04,518 [grpc-default-executor-0] INFO server.RaftServer: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: addNew group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] returns group-0437B3489A53:java.util.concurrent.CompletableFuture@32ab2577[Not completed]
datanode_3          | 2023-06-22 08:30:04,534 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: new RaftServerImpl for group-0437B3489A53:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:30:04,514 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-server-thread1] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:30:04,538 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:30:04,597 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:30:04,600 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:30:04,601 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:04,601 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:30:04,601 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:30:04,601 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:30:04,603 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:30:04,605 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:30:04,607 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:30:04,609 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:04,610 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:04,610 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:30:04,611 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:30:04,612 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:30:04,614 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:30:04,616 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:30:04,617 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:30:04,618 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:30:04,619 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:30:04,620 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:30:04,621 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 does not exist. Creating ...
datanode_3          | 2023-06-22 08:30:04,627 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/in_use.lock acquired by nodename 7@2fbf73961aee
datanode_3          | 2023-06-22 08:30:04,635 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 has been successfully formatted.
datanode_3          | 2023-06-22 08:30:04,638 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO ratis.ContainerStateMachine: group-0437B3489A53: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:30:04,644 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:30:04,649 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:30:04,650 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:04,651 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:30:04,663 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:30:04,665 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:04,666 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:30:04,666 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:30:04,667 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:04,669 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53
datanode_3          | 2023-06-22 08:30:04,670 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:30:04,670 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:04,682 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:04,683 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:30:04,683 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:30:04,684 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:30:04,685 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:04,688 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:30:04,704 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:04,712 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:03,763 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 does not exist. Creating ...
datanode_2          | 2023-06-22 08:30:03,769 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/in_use.lock acquired by nodename 6@37f0a4af5edd
datanode_2          | 2023-06-22 08:30:03,774 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53 has been successfully formatted.
datanode_2          | 2023-06-22 08:30:03,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO ratis.ContainerStateMachine: group-0437B3489A53: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:30:03,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:30:03,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:30:03,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:03,784 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:30:03,784 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:30:03,785 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:03,786 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:30:03,787 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:30:03,794 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:03,796 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53
datanode_2          | 2023-06-22 08:30:03,796 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:30:03,799 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:30:03,799 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:03,799 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:30:03,800 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:30:03,803 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:30:03,804 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:30:03,806 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:30:03,807 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:03,810 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:04,331 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d1e52d63-e5d8-4bcf-98c5-65421cfda140: Detected pause in JVM or host machine approximately 0.344s with 0.514s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=88ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=426ms
datanode_2          | 2023-06-22 08:30:04,339 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:30:04,340 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:30:04,341 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:30:04,343 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:30:04,343 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:30:04,343 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:04,344 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:30:04,344 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState
datanode_2          | 2023-06-22 08:30:04,345 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0437B3489A53,id=d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_2          | 2023-06-22 08:30:04,346 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:30:04,346 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:30:04,346 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:30:04,346 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:30:04,347 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:30:04,359 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:30:08,780 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: receive requestVote(PRE_VOTE, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95, group-0437B3489A53, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:30:08,780 [grpc-default-executor-0] INFO impl.VoteContext: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FOLLOWER: accept PRE_VOTE from 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:30:08,780 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53 replies to PRE_VOTE vote request: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0. Peer's state: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53:t0, leader=null, voted=, raftlog=Memoized:d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:09,501 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:30:09,502 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:30:11,045 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: receive requestVote(PRE_VOTE, f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, group-0437B3489A53, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:30:11,045 [grpc-default-executor-0] INFO impl.VoteContext: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FOLLOWER: accept PRE_VOTE from f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:30:11,046 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53 replies to PRE_VOTE vote request: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0. Peer's state: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53:t0, leader=null, voted=, raftlog=Memoized:d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:11,105 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: receive requestVote(ELECTION, f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, group-0437B3489A53, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:30:11,106 [grpc-default-executor-0] INFO impl.VoteContext: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FOLLOWER: accept ELECTION from f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:30:11,106 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_2          | 2023-06-22 08:30:11,106 [grpc-default-executor-0] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: shutdown d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState
datanode_2          | 2023-06-22 08:30:11,106 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState] INFO impl.FollowerState: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState was interrupted
datanode_2          | 2023-06-22 08:30:11,107 [grpc-default-executor-0] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-FollowerState
datanode_2          | 2023-06-22 08:30:11,109 [grpc-default-executor-0] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53 replies to ELECTION vote request: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t1. Peer's state: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53:t1, leader=null, voted=f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, raftlog=Memoized:d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:11,501 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0437B3489A53 with new leaderId: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_2          | 2023-06-22 08:30:11,501 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: change Leader from null to f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 at term 1 for appendEntries, leader elected after 7750ms
datanode_2          | 2023-06-22 08:30:11,502 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:11,503 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-server-thread1] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:29:00,031 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 91d86f49847e/172.19.0.9
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:29:00,123 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-22 08:29:04,713 [main] INFO reflections.Reflections: Reflections took 454 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-06-22 08:29:09,237 [main] INFO reflections.Reflections: Reflections took 565 ms to scan 3 urls, producing 131 keys and 286 values 
recon_1             | 2023-06-22 08:29:09,782 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-22 08:29:12,715 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:29:19,317 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:29:21,148 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:29:21,151 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.002 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:29:21,535 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:29:21,807 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:29:21,818 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-22 08:29:25,207 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:29:25,296 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-22 08:29:25,411 [main] INFO util.log: Logging initialized @35359ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:29:26,105 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-22 08:29:26,160 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-22 08:29:26,219 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:29:26,235 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:29:26,244 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:29:26,245 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:29:26,622 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-22 08:29:26,647 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:29:28,570 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:29:28,628 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-22 08:29:28,693 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-22 08:29:28,923 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:29:28,923 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:29:31,883 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:29:32,518 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:29:32,712 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:29:32,718 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:29:33,085 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:29:33,301 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-22 08:29:33,334 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-22 08:29:33,387 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-22 08:29:33,395 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:29:33,406 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-22 08:29:33,776 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-22 08:29:33,817 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:29:33,866 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:29:33,897 [Listener at 0.0.0.0/9891] INFO hdds.HddsUtils: Restoring thread name: main
recon_1             | 2023-06-22 08:29:33,911 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-22 08:29:34,042 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-06-22 08:29:34,048 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-22 08:29:34,048 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-22 08:29:34,148 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:29:34,164 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:29:34,164 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-22 08:29:34,608 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-22 08:29:34,612 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-06-22 08:29:34,689 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:29:34,692 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:29:34,695 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:29:34,719 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1ac68548{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:29:34,720 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4c12f54a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:30:05,296 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Detected pause in JVM or host machine approximately 0.296s with 0.573s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=48ms
datanode_3          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=525ms
datanode_3          | 2023-06-22 08:30:05,310 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:05,326 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:05,327 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:05,327 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:30:05,328 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:30:05,330 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:05,331 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:30:05,331 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState
datanode_3          | 2023-06-22 08:30:05,335 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0437B3489A53,id=f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:30:05,336 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:05,338 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:30:05,342 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:30:05,343 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:30:05,356 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:05,369 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:05,509 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/de793908-0ed7-45c2-af21-1d6efdd8a424/current/log_inprogress_0
datanode_3          | 2023-06-22 08:30:08,219 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:08,219 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:08,792 [grpc-default-executor-1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: receive requestVote(PRE_VOTE, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95, group-0437B3489A53, 0, (t:0, i:0))
datanode_3          | 2023-06-22 08:30:08,795 [grpc-default-executor-1] INFO impl.VoteContext: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FOLLOWER: reject PRE_VOTE from 7b56dd5f-5763-4a54-b62e-fd435ccd0c95: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-22 08:30:08,799 [grpc-default-executor-1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53 replies to PRE_VOTE vote request: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95<-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293#0:FAIL-t0. Peer's state: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53:t0, leader=null, voted=, raftlog=Memoized:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:10,531 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO impl.FollowerState: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5199602865ns, electionTimeout:5161ms
datanode_3          | 2023-06-22 08:30:10,532 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: shutdown f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState
datanode_3          | 2023-06-22 08:30:10,533 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:30:10,537 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-22 08:30:10,537 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-FollowerState] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1
recon_1             | 2023-06-22 08:29:38,633 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8cf705e{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-13614347128122944951/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-22 08:29:38,656 [main] INFO server.AbstractConnector: Started ServerConnector@38e052b2{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:29:38,656 [main] INFO server.Server: Started @48607ms
recon_1             | 2023-06-22 08:29:38,660 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-22 08:29:38,660 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-22 08:29:38,665 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:29:38,665 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:29:38,681 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:29:38,704 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:29:38,705 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:29:38,705 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:29:38,705 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-22 08:29:38,713 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:29:41,020 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d86f49847e/172.19.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-22 08:29:43,021 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d86f49847e/172.19.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-22 08:29:45,664 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fb78c014-7100-41f0-9790-f8c68fb7a7e2 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1             | 2023-06-22 08:29:47,669 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fb78c014-7100-41f0-9790-f8c68fb7a7e2 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1             | 2023-06-22 08:29:51,863 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-22 08:29:51,864 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:29:51,864 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-22 08:29:51,865 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:29:51,892 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:29:51,884 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-22 08:29:53,399 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.10:42170: output error
recon_1             | 2023-06-22 08:29:53,405 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.5:40148: output error
recon_1             | 2023-06-22 08:29:53,405 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.10:42176: output error
recon_1             | 2023-06-22 08:29:53,404 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.4:57754: output error
recon_1             | 2023-06-22 08:29:53,404 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.4:57770: output error
recon_1             | 2023-06-22 08:29:53,401 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.5:33926: output error
recon_1             | 2023-06-22 08:29:53,401 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.5:40146: output error
recon_1             | 2023-06-22 08:29:53,401 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.4:57412: output error
recon_1             | 2023-06-22 08:29:53,420 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.19.0.10:44098: output error
recon_1             | 2023-06-22 08:29:53,421 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,437 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,438 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,438 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,439 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,439 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,448 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,448 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,454 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
datanode_2          | 2023-06-22 08:30:11,506 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-0437B3489A53-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/current/log_inprogress_0
datanode_2          | 2023-06-22 08:30:23,758 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d1e52d63-e5d8-4bcf-98c5-65421cfda140: addNew group-D34F13B58203:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-D34F13B58203:java.util.concurrent.CompletableFuture@7ef65a31[Not completed]
datanode_2          | 2023-06-22 08:30:23,770 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140: new RaftServerImpl for group-D34F13B58203:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:30:23,776 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: ConfigurationManager, init=-1: peers:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:30:23,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:30:23,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:30:23,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:30:23,781 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:30:23,782 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:30:23,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:30:23,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:30:23,783 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:30:23,784 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:30:23,785 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:30:23,785 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:30:23,791 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:30:23,791 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:30:23,791 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:30:23,791 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/62067dd6-a943-4c28-af6f-d34f13b58203 does not exist. Creating ...
datanode_2          | 2023-06-22 08:30:23,793 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/62067dd6-a943-4c28-af6f-d34f13b58203/in_use.lock acquired by nodename 6@37f0a4af5edd
datanode_2          | 2023-06-22 08:30:23,795 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/62067dd6-a943-4c28-af6f-d34f13b58203 has been successfully formatted.
datanode_2          | 2023-06-22 08:30:23,819 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO ratis.ContainerStateMachine: group-D34F13B58203: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:30:23,820 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:30:23,820 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:30:23,821 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:23,822 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:30:23,822 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:30:23,822 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:23,823 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:30:23,824 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:30:23,825 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:23,825 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/62067dd6-a943-4c28-af6f-d34f13b58203
datanode_2          | 2023-06-22 08:30:23,826 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:30:10,541 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:10,547 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 7b56dd5f-5763-4a54-b62e-fd435ccd0c95
datanode_3          | 2023-06-22 08:30:10,576 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:10,578 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:10,578 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_3          | 2023-06-22 08:30:11,082 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:30:11,083 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection:   Response 0: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t0
datanode_3          | 2023-06-22 08:30:11,084 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-06-22 08:30:11,091 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:11,095 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:11,096 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:11,118 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:30:11,118 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection:   Response 0: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293<-d1e52d63-e5d8-4bcf-98c5-65421cfda140#0:OK-t1
datanode_3          | 2023-06-22 08:30:11,119 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1 ELECTION round 0: result PASSED
datanode_3          | 2023-06-22 08:30:11,119 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: shutdown f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1
datanode_3          | 2023-06-22 08:30:11,120 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:30:11,122 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0437B3489A53 with new leaderId: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:30:11,122 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: change Leader from null to f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 at term 1 for becomeLeader, leader elected after 6512ms
datanode_3          | 2023-06-22 08:30:11,155 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:30:11,205 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:11,211 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:30:11,225 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:30:11,229 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:30:11,231 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:30:11,255 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:11,259 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:30:11,327 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:30:11,329 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:11,330 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:30:11,337 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:30:11,340 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:30:11,341 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:30:11,342 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:30:11,343 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:30:11,343 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:30:11,344 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:30:11,363 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:30:11,363 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:11,363 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:30:11,363 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:30:11,364 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:30:11,366 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:30:11,366 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:30:11,368 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:30:11,368 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:30:11,369 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:30:11,373 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderStateImpl
datanode_3          | 2023-06-22 08:30:11,378 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:30:11,385 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef1cc5c3-3caa-4c79-bf70-0437b3489a53/current/log_inprogress_0
datanode_3          | 2023-06-22 08:30:11,405 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53-LeaderElection1] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-0437B3489A53: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER, 7b56dd5f-5763-4a54-b62e-fd435ccd0c95|rpc:172.19.0.4:9856|admin:172.19.0.4:9857|client:172.19.0.4:9858|dataStream:172.19.0.4:9858|priority:0|startupRole:FOLLOWER, d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:13,247 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:13,247 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:18,302 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:18,302 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:23,429 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:23,430 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:24,156 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: addNew group-5335D8082A4D:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-5335D8082A4D:java.util.concurrent.CompletableFuture@113ffe40[Not completed]
datanode_3          | 2023-06-22 08:30:24,159 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: new RaftServerImpl for group-5335D8082A4D:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: ConfigurationManager, init=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:30:24,179 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:30:24,180 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:30:24,180 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:30:24,180 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:30:24,181 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:24,181 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:30:24,181 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:30:24,184 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:30:24,186 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:30:24,192 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:30:24,192 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:30:24,193 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:30:24,193 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:30:24,193 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:30:24,194 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4a2a2e46-0e99-4727-85e9-5335d8082a4d does not exist. Creating ...
datanode_3          | 2023-06-22 08:30:24,195 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4a2a2e46-0e99-4727-85e9-5335d8082a4d/in_use.lock acquired by nodename 7@2fbf73961aee
datanode_3          | 2023-06-22 08:30:24,212 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4a2a2e46-0e99-4727-85e9-5335d8082a4d has been successfully formatted.
datanode_3          | 2023-06-22 08:30:24,213 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO ratis.ContainerStateMachine: group-5335D8082A4D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:30:24,220 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:30:24,220 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:30:24,222 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:24,222 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:30:24,222 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:30:24,222 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4a2a2e46-0e99-4727-85e9-5335d8082a4d
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:30:24,223 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:30:24,224 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:30:24,224 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:30:24,674 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Detected pause in JVM or host machine approximately 0.350s with 0.425s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=425ms
datanode_3          | 2023-06-22 08:30:24,690 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:24,690 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:30:24,691 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:24,691 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:30:24,691 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:30:23,826 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:30:23,826 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:23,826 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:30:23,827 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:30:23,838 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:30:23,838 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:30:23,838 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:30:23,839 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:30:23,840 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:30:24,358 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:30:24,363 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:30:24,363 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:30:24,364 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:30:24,364 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:30:24,367 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: start as a follower, conf=-1: peers:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:24,367 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:30:24,367 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState
datanode_2          | 2023-06-22 08:30:24,392 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D34F13B58203,id=d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_2          | 2023-06-22 08:30:24,392 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:30:24,393 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:30:24,394 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:30:24,394 [d1e52d63-e5d8-4bcf-98c5-65421cfda140-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:30:24,398 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:30:24,400 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:30:24,405 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=62067dd6-a943-4c28-af6f-d34f13b58203
datanode_2          | 2023-06-22 08:30:24,406 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=62067dd6-a943-4c28-af6f-d34f13b58203.
datanode_2          | 2023-06-22 08:30:29,583 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO impl.FollowerState: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5215330280ns, electionTimeout:5182ms
datanode_2          | 2023-06-22 08:30:29,585 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: shutdown d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState
datanode_2          | 2023-06-22 08:30:29,585 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:30:29,606 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-22 08:30:29,607 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-FollowerState] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1
datanode_2          | 2023-06-22 08:30:29,625 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.LeaderElection: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:29,628 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.LeaderElection: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-22 08:30:29,811 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.LeaderElection: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:29,811 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.LeaderElection: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1 ELECTION round 0: result PASSED (term=1)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-22 08:29:00,088 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-22 08:29:00,128 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-22 08:29:00,370 [main] INFO util.log: Logging initialized @10694ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-22 08:29:01,481 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-22 08:29:01,617 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-22 08:29:01,672 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-22 08:29:01,685 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-22 08:29:01,688 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-22 08:29:01,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-22 08:29:02,001 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir9751182739081244946
s3g_1               | 2023-06-22 08:29:02,737 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 86158c78629e/172.19.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:29:00,787 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 76a2231da7dd/172.19.0.7
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | 2023-06-22 08:30:29,812 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: shutdown d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1
datanode_2          | 2023-06-22 08:30:29,813 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:30:29,818 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D34F13B58203 with new leaderId: d1e52d63-e5d8-4bcf-98c5-65421cfda140
datanode_2          | 2023-06-22 08:30:29,819 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: change Leader from null to d1e52d63-e5d8-4bcf-98c5-65421cfda140 at term 1 for becomeLeader, leader elected after 6036ms
datanode_2          | 2023-06-22 08:30:29,847 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:30:29,906 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:30:29,916 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:30:29,967 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:30:29,969 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:30:30,494 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d1e52d63-e5d8-4bcf-98c5-65421cfda140: Detected pause in JVM or host machine approximately 0.125s with 0.514s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=514ms
datanode_2          | 2023-06-22 08:30:30,503 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:30:30,525 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:30:30,527 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:30:30,529 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO impl.RoleInfo: d1e52d63-e5d8-4bcf-98c5-65421cfda140: start d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderStateImpl
datanode_2          | 2023-06-22 08:30:30,535 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:30:30,540 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/62067dd6-a943-4c28-af6f-d34f13b58203/current/log_inprogress_0
datanode_2          | 2023-06-22 08:30:30,554 [d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203-LeaderElection1] INFO server.RaftServer$Division: d1e52d63-e5d8-4bcf-98c5-65421cfda140@group-D34F13B58203: set configuration 0: peers:[d1e52d63-e5d8-4bcf-98c5-65421cfda140|rpc:172.19.0.10:9856|admin:172.19.0.10:9857|client:172.19.0.10:9858|dataStream:172.19.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:30:50,505 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:31:50,506 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:32:50,507 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:33:50,507 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:34:50,508 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:35:50,509 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:36:50,509 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir9751182739081244946, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-06-22 08:29:02,777 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-22 08:29:02,905 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-22 08:29:03,473 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-22 08:29:04,429 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-22 08:29:04,429 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-22 08:29:04,696 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-06-22 08:29:04,757 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-22 08:29:04,758 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-06-22 08:29:04,903 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-22 08:29:04,903 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-22 08:29:04,929 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-22 08:29:05,025 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3113a37{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-22 08:29:05,026 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52eacb4b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-06-22 08:29:27,493 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2dc3ece8{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir9751182739081244946/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-13579938109166982023/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-06-22 08:29:27,548 [main] INFO server.AbstractConnector: Started ServerConnector@2c282004{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-22 08:29:27,558 [main] INFO server.Server: Started @37882ms
s3g_1               | 2023-06-22 08:29:27,566 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-22 08:29:27,571 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-22 08:29:27,572 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-22 08:29:00,864 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:29:08,303 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:29:11,221 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:29:11,600 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.7:9862
om_1                | 2023-06-22 08:29:11,601 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:29:11,601 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:29:11,926 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:29:13,324 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863]
om_1                | 2023-06-22 08:29:17,045 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-22 08:29:19,047 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-22 08:29:21,049 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-22 08:29:23,050 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-22 08:29:25,052 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-22 08:29:27,054 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:29:53,635 [IPC Server handler 22 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7b56dd5f-5763-4a54-b62e-fd435ccd0c95
recon_1             | 2023-06-22 08:29:53,651 [IPC Server handler 22 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 7b56dd5f-5763-4a54-b62e-fd435ccd0c95{ip: 172.19.0.4, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:29:54,942 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 7b56dd5f-5763-4a54-b62e-fd435ccd0c95 to Node DB.
recon_1             | 2023-06-22 08:29:55,310 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424. Trying to get from SCM.
recon_1             | 2023-06-22 08:29:55,445 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: de793908-0ed7-45c2-af21-1d6efdd8a424, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.516Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:29:55,775 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: de793908-0ed7-45c2-af21-1d6efdd8a424, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.516Z[UTC]].
recon_1             | 2023-06-22 08:29:55,821 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 reported by 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)
recon_1             | 2023-06-22 08:29:58,884 [IPC Server handler 16 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d1e52d63-e5d8-4bcf-98c5-65421cfda140
recon_1             | 2023-06-22 08:29:58,886 [IPC Server handler 16 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d1e52d63-e5d8-4bcf-98c5-65421cfda140{ip: 172.19.0.10, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:29:58,888 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 reported by d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)
recon_1             | 2023-06-22 08:29:58,893 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d1e52d63-e5d8-4bcf-98c5-65421cfda140 to Node DB.
recon_1             | 2023-06-22 08:30:01,183 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 reported by 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)
recon_1             | 2023-06-22 08:30:02,862 [IPC Server handler 16 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
recon_1             | 2023-06-22 08:30:02,862 [IPC Server handler 16 on default port 9891] INFO node.SCMNodeManager: Registered Data node : f91e12cd-1de9-431d-8f3a-a5eb5b4a5293{ip: 172.19.0.5, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:30:02,864 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 to Node DB.
recon_1             | 2023-06-22 08:30:02,864 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 reported by f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)
recon_1             | 2023-06-22 08:30:02,864 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: de793908-0ed7-45c2-af21-1d6efdd8a424, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7b56dd5f-5763-4a54-b62e-fd435ccd0c95, CreationTimestamp2023-06-22T08:29:52.516Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:30:03,441 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=1de0afd2-c845-4655-b03a-269e8d473fc7. Trying to get from SCM.
recon_1             | 2023-06-22 08:30:03,452 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 1de0afd2-c845-4655-b03a-269e8d473fc7, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.612Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:30:03,454 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 1de0afd2-c845-4655-b03a-269e8d473fc7, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.612Z[UTC]].
recon_1             | 2023-06-22 08:30:03,454 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=1de0afd2-c845-4655-b03a-269e8d473fc7 reported by 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)
recon_1             | 2023-06-22 08:30:03,454 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 1de0afd2-c845-4655-b03a-269e8d473fc7, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7b56dd5f-5763-4a54-b62e-fd435ccd0c95, CreationTimestamp2023-06-22T08:29:52.612Z[UTC]] moved to OPEN state
datanode_3          | 2023-06-22 08:30:24,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: start as a follower, conf=-1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:24,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:30:24,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState
datanode_3          | 2023-06-22 08:30:24,694 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5335D8082A4D,id=f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:30:24,695 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:30:24,695 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:30:24,695 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:30:24,695 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:30:24,696 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:24,696 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:24,697 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=4a2a2e46-0e99-4727-85e9-5335d8082a4d
datanode_3          | 2023-06-22 08:30:24,705 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=4a2a2e46-0e99-4727-85e9-5335d8082a4d.
datanode_3          | 2023-06-22 08:30:28,568 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:28,568 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:29,569 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: Detected pause in JVM or host machine approximately 0.389s with 0.508s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=508ms
datanode_3          | 2023-06-22 08:30:29,776 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO impl.FollowerState: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5083632394ns, electionTimeout:5078ms
datanode_3          | 2023-06-22 08:30:29,779 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: shutdown f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState
datanode_3          | 2023-06-22 08:30:29,780 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:30:29,781 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-22 08:30:29,781 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-FollowerState] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2
datanode_3          | 2023-06-22 08:30:29,839 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:29,840 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-22 08:30:29,862 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:29,863 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.LeaderElection: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-22 08:30:29,865 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: shutdown f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2
datanode_3          | 2023-06-22 08:30:29,866 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:30:29,866 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5335D8082A4D with new leaderId: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
datanode_3          | 2023-06-22 08:30:29,888 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: change Leader from null to f91e12cd-1de9-431d-8f3a-a5eb5b4a5293 at term 1 for becomeLeader, leader elected after 5685ms
datanode_3          | 2023-06-22 08:30:29,903 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:30:29,903 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:29,903 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 2023-06-22 08:30:03,610 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53. Trying to get from SCM.
recon_1             | 2023-06-22 08:30:03,627 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ef1cc5c3-3caa-4c79-bf70-0437b3489a53, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.640Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:30:03,628 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef1cc5c3-3caa-4c79-bf70-0437b3489a53, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.640Z[UTC]].
recon_1             | 2023-06-22 08:30:03,636 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 reported by 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)
recon_1             | 2023-06-22 08:30:03,793 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 reported by d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)
recon_1             | 2023-06-22 08:30:04,687 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 reported by f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)
recon_1             | 2023-06-22 08:30:08,641 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 reported by 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)
recon_1             | 2023-06-22 08:30:11,143 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 reported by f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)
recon_1             | 2023-06-22 08:30:11,143 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ef1cc5c3-3caa-4c79-bf70-0437b3489a53, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, CreationTimestamp2023-06-22T08:29:52.640Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:30:24,327 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=62067dd6-a943-4c28-af6f-d34f13b58203. Trying to get from SCM.
recon_1             | 2023-06-22 08:30:24,383 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 62067dd6-a943-4c28-af6f-d34f13b58203, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d1e52d63-e5d8-4bcf-98c5-65421cfda140, CreationTimestamp2023-06-22T08:29:51.154Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:30:24,393 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 62067dd6-a943-4c28-af6f-d34f13b58203, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d1e52d63-e5d8-4bcf-98c5-65421cfda140, CreationTimestamp2023-06-22T08:29:51.154Z[UTC]].
recon_1             | 2023-06-22 08:30:24,717 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=4a2a2e46-0e99-4727-85e9-5335d8082a4d. Trying to get from SCM.
recon_1             | 2023-06-22 08:30:24,731 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 4a2a2e46-0e99-4727-85e9-5335d8082a4d, Nodes: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.671Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:30:24,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4a2a2e46-0e99-4727-85e9-5335d8082a4d, Nodes: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.671Z[UTC]].
recon_1             | 2023-06-22 08:30:24,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=4a2a2e46-0e99-4727-85e9-5335d8082a4d reported by f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)
recon_1             | 2023-06-22 08:30:24,733 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4a2a2e46-0e99-4727-85e9-5335d8082a4d, Nodes: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, CreationTimestamp2023-06-22T08:29:52.671Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:30:29,014 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:30:29,270 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-22 08:30:38,707 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:30:38,708 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:30:40,481 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687422638708
recon_1             | 2023-06-22 08:30:40,503 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-22 08:30:41,496 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687422638708.
recon_1             | 2023-06-22 08:30:41,700 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:30:42,214 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-06-22 08:30:42,224 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-22 08:30:42,234 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-22 08:30:42,235 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
om_1                | 2023-06-22 08:29:29,056 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-22 08:29:31,058 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-22 08:29:33,060 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-22 08:29:35,062 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-22 08:29:37,064 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-22 08:29:39,066 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-22 08:29:41,068 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-22 08:29:43,072 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 76a2231da7dd/172.19.0.7 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-06-22 08:29:45,501 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fb78c014-7100-41f0-9790-f8c68fb7a7e2 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-06-22 08:29:47,507 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fb78c014-7100-41f0-9790-f8c68fb7a7e2 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5;layoutVersion=6
recon_1             | 2023-06-22 08:30:42,235 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:30:42,237 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:30:42,291 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:30:42,291 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.055 seconds to process 1 keys.
recon_1             | 2023-06-22 08:30:42,320 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-22 08:30:42,348 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-22 08:30:43,143 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-22 08:30:43,165 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:30:52,309 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-06-22 08:30:52,311 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-06-22 08:30:52,319 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-22 08:30:52,320 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-22 08:30:52,343 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-22 08:30:52,345 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-22 08:30:52,382 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-22 08:30:52,386 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-06-22 08:30:52,536 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 175 milliseconds.
recon_1             | 2023-06-22 08:30:52,565 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 236 milliseconds to process 0 existing database records.
recon_1             | 2023-06-22 08:30:52,622 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 58 milliseconds for processing 2 containers.
recon_1             | 2023-06-22 08:31:52,371 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-06-22 08:31:52,420 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:31:52,420 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 48
recon_1             | 2023-06-22 08:32:52,420 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:32:52,421 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-22 08:33:52,421 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:33:52,422 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-22 08:34:52,422 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:34:52,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-22 08:35:52,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:35:52,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-22 08:35:52,582 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-22 08:35:52,586 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 21 milliseconds.
recon_1             | 2023-06-22 08:35:52,624 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
recon_1             | 2023-06-22 08:35:52,628 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 5 milliseconds for processing 2 containers.
recon_1             | 2023-06-22 08:36:52,424 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:36:52,424 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
om_1                | 2023-06-22 08:29:50,162 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 76a2231da7dd/172.19.0.7
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:29:57,409 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 76a2231da7dd/172.19.0.7
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:30:29,905 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO impl.RoleInfo: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293: start f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderStateImpl
datanode_3          | 2023-06-22 08:30:29,906 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:30:29,913 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4a2a2e46-0e99-4727-85e9-5335d8082a4d/current/log_inprogress_0
datanode_3          | 2023-06-22 08:30:29,946 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D-LeaderElection2] INFO server.RaftServer$Division: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-5335D8082A4D: set configuration 0: peers:[f91e12cd-1de9-431d-8f3a-a5eb5b4a5293|rpc:172.19.0.5:9856|admin:172.19.0.5:9857|client:172.19.0.5:9858|dataStream:172.19.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:30:33,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:33,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:38,715 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:38,715 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:43,813 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:43,814 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:48,826 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:48,827 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:50,439 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:30:53,844 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:53,844 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:30:58,980 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:30:58,980 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:04,051 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:04,052 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:09,244 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:09,246 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:14,295 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:14,295 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:19,456 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:19,460 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:24,594 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:24,595 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:29,595 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:29,595 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:34,779 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:34,779 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:29:09,388 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = e69db5f5ff78/172.19.0.13
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
datanode_3          | 2023-06-22 08:31:39,858 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:39,858 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:44,884 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:44,884 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:50,035 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:50,035 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:31:50,440 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:31:55,227 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:31:55,228 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:00,304 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:00,304 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:05,413 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:05,413 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:10,429 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:10,429 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:15,445 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:15,445 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:20,612 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:20,612 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:25,627 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:25,628 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:30,647 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:30,647 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:35,734 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:35,734 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:40,769 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:40,769 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:45,909 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:45,909 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:50,441 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:32:51,070 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:51,071 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:32:56,248 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:32:56,249 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:01,342 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:29:09,537 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:29:10,610 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:29:12,997 [main] INFO reflections.Reflections: Reflections took 1778 ms to scan 3 urls, producing 131 keys and 286 values 
scm_1               | 2023-06-22 08:29:14,679 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:29:14,812 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:29:17,232 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-22 08:29:18,477 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:29:18,509 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:29:18,509 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:29:18,510 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:29:18,510 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-22 08:29:18,534 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-22 08:29:57,452 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:30:01,124 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:30:02,825 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:30:03,193 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.7:9862
om_1                | 2023-06-22 08:30:03,194 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:30:03,200 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:30:03,383 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:30:03,671 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-22 08:30:04,729 [main] INFO reflections.Reflections: Reflections took 741 ms to scan 1 urls, producing 137 keys and 395 values [using 2 cores]
om_1                | 2023-06-22 08:30:04,810 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-22 08:30:04,925 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:30:06,138 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863]
om_1                | 2023-06-22 08:30:06,338 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.13:9863]
om_1                | 2023-06-22 08:30:07,699 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-22 08:30:07,766 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:30:08,362 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-22 08:30:09,445 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-22 08:30:09,594 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-22 08:30:09,615 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:30:09,715 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-22 08:30:09,717 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-22 08:30:10,112 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:30:10,359 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:30:10,361 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-22 08:30:10,450 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:30:10,468 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:30:10,542 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-22 08:30:10,619 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-22 08:30:10,757 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:33:01,342 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:06,496 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:06,497 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:11,547 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:11,547 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:16,716 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:16,716 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:21,755 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:21,756 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:26,872 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:26,872 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:31,904 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:31,904 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:36,910 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:36,910 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:41,928 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:41,928 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:47,000 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:47,000 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:50,441 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:33:52,076 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:52,076 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:33:57,082 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:33:57,082 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:02,103 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:02,103 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:07,299 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:07,299 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:12,450 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:12,450 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:17,460 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:17,460 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:22,657 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:22,658 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:27,839 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-22 08:30:10,789 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:30:10,792 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:30:10,793 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:30:10,795 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:30:10,798 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-22 08:30:10,798 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:30:10,801 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-22 08:30:10,806 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:30:10,807 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-22 08:30:10,809 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:30:10,849 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-22 08:30:10,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-22 08:30:10,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-22 08:30:11,971 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-22 08:30:11,981 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-22 08:30:11,983 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-22 08:30:11,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:30:11,985 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:30:11,992 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:30:12,028 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@5901757[Not completed]
om_1                | 2023-06-22 08:30:12,028 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-22 08:30:12,038 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-22 08:30:12,090 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-22 08:30:12,114 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-22 08:30:12,115 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-22 08:30:12,115 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-22 08:30:12,119 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:30:12,119 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:30:12,119 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-22 08:30:12,147 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-22 08:30:12,149 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:30:12,181 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-22 08:30:12,186 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-22 08:30:12,225 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-22 08:30:12,260 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-22 08:30:12,281 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-22 08:30:12,281 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-22 08:30:12,492 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-22 08:30:12,765 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:30:12,781 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-22 08:30:12,786 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-22 08:30:12,789 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-22 08:30:12,789 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-22 08:30:12,789 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-22 08:30:13,627 [main] INFO reflections.Reflections: Reflections took 1315 ms to scan 8 urls, producing 24 keys and 632 values [using 2 cores]
om_1                | 2023-06-22 08:30:14,239 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:30:14,263 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:30:14,321 [Listener at om/9862] INFO hdds.HddsUtils: Restoring thread name: main
om_1                | 2023-06-22 08:30:15,685 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:30:15,708 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:30:15,709 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-22 08:30:15,835 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.19.0.7:9862
om_1                | 2023-06-22 08:30:15,836 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-22 08:30:15,843 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-22 08:30:15,856 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@76a2231da7dd
om_1                | 2023-06-22 08:30:15,871 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
scm_1               | 2023-06-22 08:29:18,557 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-22 08:29:18,574 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:18,581 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-22 08:29:18,582 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:29:18,764 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-22 08:29:18,803 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-22 08:29:18,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-22 08:29:22,065 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-22 08:29:22,107 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-22 08:29:22,108 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-22 08:29:22,140 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:29:22,152 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:29:22,223 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:29:22,318 [main] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: addNew group-0B1D67CA50C5:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER] returns group-0B1D67CA50C5:java.util.concurrent.CompletableFuture@10f7c76[Not completed]
scm_1               | 2023-06-22 08:29:22,607 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2: new RaftServerImpl for group-0B1D67CA50C5:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-22 08:29:22,686 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-22 08:29:22,686 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-22 08:29:22,689 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-22 08:29:22,695 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:29:22,696 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:29:22,700 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-22 08:29:22,801 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: ConfigurationManager, init=-1: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-22 08:29:22,804 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:29:22,915 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-22 08:29:22,932 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:29:23,244 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-22 08:29:23,317 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:23,354 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-22 08:29:23,388 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-22 08:29:23,681 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-22 08:29:23,787 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-22 08:29:25,337 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:29:25,387 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-22 08:29:25,393 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-22 08:29:25,408 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-22 08:29:25,409 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-22 08:29:25,415 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-22 08:29:25,421 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5 does not exist. Creating ...
scm_1               | 2023-06-22 08:29:25,488 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/in_use.lock acquired by nodename 12@e69db5f5ff78
scm_1               | 2023-06-22 08:29:25,704 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5 has been successfully formatted.
scm_1               | 2023-06-22 08:29:25,776 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-22 08:29:25,910 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-22 08:29:25,954 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:25,966 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-22 08:29:26,064 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:34:27,839 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:33,024 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:33,024 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:38,161 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:38,162 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:43,347 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:43,348 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:48,417 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:48,417 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:50,441 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:34:53,582 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:53,582 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:34:58,692 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:34:58,693 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:03,701 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:03,701 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:08,757 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:08,757 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:13,886 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:13,887 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:19,072 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:19,074 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:24,134 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:24,134 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:29,318 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:29,318 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:34,351 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:34,352 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:39,427 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:39,427 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:44,576 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:44,576 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:49,729 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:49,730 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:50,442 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm_1               | 2023-06-22 08:29:26,104 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:29:26,175 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-22 08:29:26,192 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:29:26,205 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:26,321 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5
scm_1               | 2023-06-22 08:29:26,329 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:29:26,341 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:26,344 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:29:26,356 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-22 08:29:26,362 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-22 08:29:26,381 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-22 08:29:26,391 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:26,391 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:29:26,594 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:29:26,604 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:26,921 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:29:26,931 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:29:26,944 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:29:27,028 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:29:27,080 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:29:27,106 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: start as a follower, conf=-1: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:27,145 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-22 08:29:27,150 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState
scm_1               | 2023-06-22 08:29:27,181 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-22 08:29:27,196 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-22 08:29:27,299 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0B1D67CA50C5,id=fb78c014-7100-41f0-9790-f8c68fb7a7e2
scm_1               | 2023-06-22 08:29:27,329 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-22 08:29:27,336 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-22 08:29:27,349 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:29:27,361 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-22 08:29:27,442 [main] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start RPC server
scm_1               | 2023-06-22 08:29:28,150 [main] INFO server.GrpcService: fb78c014-7100-41f0-9790-f8c68fb7a7e2: GrpcService started, listening on 9894
scm_1               | 2023-06-22 08:29:28,208 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fb78c014-7100-41f0-9790-f8c68fb7a7e2: Started
scm_1               | 2023-06-22 08:29:32,257 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.FollowerState: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107748879ns, electionTimeout:5036ms
scm_1               | 2023-06-22 08:29:32,258 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState
scm_1               | 2023-06-22 08:29:32,259 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-22 08:29:32,281 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-22 08:29:32,281 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1
scm_1               | 2023-06-22 08:29:32,304 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:35:54,734 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:54,734 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:35:59,893 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:35:59,894 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:05,007 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:05,007 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:10,044 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:10,044 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:15,225 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:15,225 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:20,354 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:20,355 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:25,513 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:25,513 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:30,544 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:30,544 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:35,615 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:35,615 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:40,699 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:40,700 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:45,835 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:45,836 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:36:50,443 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:36:51,002 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:36:51,002 [f91e12cd-1de9-431d-8f3a-a5eb5b4a5293@group-1D6EFDD8A424-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-22 08:30:15,879 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-22 08:30:15,899 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-22 08:30:15,899 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:30:15,904 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-22 08:30:15,905 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-22 08:30:15,913 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:30:15,927 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-22 08:30:15,928 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-22 08:30:15,928 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:30:15,944 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-22 08:30:15,945 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:30:15,947 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-22 08:30:15,949 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:30:15,950 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-22 08:30:15,950 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-22 08:30:15,952 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-22 08:30:15,952 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-22 08:30:15,953 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-22 08:30:15,976 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-22 08:30:15,977 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:30:16,035 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-22 08:30:16,036 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-22 08:30:16,037 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-22 08:30:16,049 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:30:16,049 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:30:16,052 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:30:16,054 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-22 08:30:16,056 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:30:16,058 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-22 08:30:16,058 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-22 08:30:16,064 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-22 08:30:16,066 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-22 08:30:16,067 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-22 08:30:16,067 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-22 08:30:16,068 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-22 08:30:16,072 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-22 08:30:16,167 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-22 08:30:16,171 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-22 08:30:16,175 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-22 08:30:16,265 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:30:16,266 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:30:16,329 [main] INFO util.log: Logging initialized @25080ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:30:16,756 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-22 08:30:16,786 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:30:16,809 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:30:16,815 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:30:16,815 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:30:16,815 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:30:16,921 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-06-22 08:30:16,937 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-22 08:30:16,939 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-06-22 08:30:17,027 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:29:32,305 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-06-22 08:29:32,316 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:32,317 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-22 08:29:32,317 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1
scm_1               | 2023-06-22 08:29:32,319 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-22 08:29:32,320 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: change Leader from null to fb78c014-7100-41f0-9790-f8c68fb7a7e2 at term 1 for becomeLeader, leader elected after 9102ms
scm_1               | 2023-06-22 08:29:32,336 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-22 08:29:32,355 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:32,359 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:29:32,408 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:32,416 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-22 08:29:32,425 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-22 08:29:32,451 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:32,496 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-22 08:29:32,502 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderStateImpl
scm_1               | 2023-06-22 08:29:32,716 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-22 08:29:32,993 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: set configuration 0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:33,025 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/log_inprogress_0
scm_1               | 2023-06-22 08:29:34,225 [main] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: close
scm_1               | 2023-06-22 08:29:34,226 [main] INFO server.GrpcService: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-22 08:29:34,227 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: shutdown
scm_1               | 2023-06-22 08:29:34,227 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0B1D67CA50C5,id=fb78c014-7100-41f0-9790-f8c68fb7a7e2
scm_1               | 2023-06-22 08:29:34,227 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderStateImpl
scm_1               | 2023-06-22 08:29:34,239 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO impl.PendingRequests: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-22 08:29:34,268 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO impl.StateMachineUpdater: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-22 08:29:34,275 [main] INFO server.GrpcService: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-22 08:29:34,277 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO impl.StateMachineUpdater: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-22 08:29:34,277 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO impl.StateMachineUpdater: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-22 08:29:34,284 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: closes. applyIndex: 0
scm_1               | 2023-06-22 08:29:35,035 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker close()
scm_1               | 2023-06-22 08:29:35,037 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fb78c014-7100-41f0-9790-f8c68fb7a7e2: Stopped
scm_1               | 2023-06-22 08:29:35,038 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:29:35,042 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-6b4c5003-8ed2-4998-9e26-0b1d67ca50c5; layoutVersion=7; scmId=fb78c014-7100-41f0-9790-f8c68fb7a7e2
scm_1               | 2023-06-22 08:29:35,053 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at e69db5f5ff78/172.19.0.13
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:29:38,845 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = e69db5f5ff78/172.19.0.13
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:29:38,868 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:29:38,957 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:29:39,186 [main] INFO reflections.Reflections: Reflections took 176 ms to scan 3 urls, producing 131 keys and 286 values 
scm_1               | 2023-06-22 08:29:39,361 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:29:39,371 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:29:40,209 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:29:40,415 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:29:40,781 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-06-22 08:29:40,784 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-22 08:29:40,873 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:30:17,027 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:30:17,031 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-22 08:30:17,074 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@74ccbafd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:30:17,076 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6083be44{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:30:17,418 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3cb78e36{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-9617562474831060204/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:30:17,442 [main] INFO server.AbstractConnector: Started ServerConnector@2d048228{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-22 08:30:17,442 [main] INFO server.Server: Started @26195ms
om_1                | 2023-06-22 08:30:17,453 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-22 08:30:17,453 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-22 08:30:17,456 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:30:17,465 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-22 08:30:17,495 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:30:17,914 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-22 08:30:18,178 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-06-22 08:30:21,075 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5019182292ns, electionTimeout:5015ms
om_1                | 2023-06-22 08:30:21,076 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:30:21,079 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-22 08:30:21,082 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-06-22 08:30:21,082 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:30:21,103 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:30:21,107 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-22 08:30:21,117 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:30:21,117 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-22 08:30:21,118 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:30:21,118 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-22 08:30:21,130 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 8894ms
om_1                | 2023-06-22 08:30:21,155 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-22 08:30:21,172 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:30:21,176 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:30:21,196 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-22 08:30:21,197 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-22 08:30:21,199 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-22 08:30:21,225 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:30:21,230 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-22 08:30:21,236 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-22 08:30:21,268 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-22 08:30:21,385 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:30:21,497 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-22 08:30:21,635 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-22 08:30:25,035 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-22 08:30:25,185 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-22 08:30:39,771 [qtp1381118175-53] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-06-22 08:30:39,831 [qtp1381118175-53] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1687422639776 in 53 milliseconds
om_1                | 2023-06-22 08:30:39,905 [qtp1381118175-53] INFO db.RDBCheckpointUtils: Waited for 72 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1687422639776 availability.
om_1                | 2023-06-22 08:30:40,349 [qtp1381118175-53] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 441 milliseconds
om_1                | 2023-06-22 08:30:40,352 [qtp1381118175-53] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1                | 2023-06-22 08:30:40,352 [qtp1381118175-53] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687422639776
scm_1               | 2023-06-22 08:29:41,084 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:fb78c014-7100-41f0-9790-f8c68fb7a7e2
scm_1               | 2023-06-22 08:29:41,209 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-22 08:29:41,221 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:29:41,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:29:41,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:29:41,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:29:41,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-22 08:29:41,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-22 08:29:41,227 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-22 08:29:41,231 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:41,232 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-22 08:29:41,232 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:29:41,246 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-22 08:29:41,252 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-22 08:29:41,253 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-22 08:29:41,478 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-22 08:29:41,481 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-22 08:29:41,482 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-22 08:29:41,482 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:29:41,483 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:29:41,488 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:29:41,495 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: found a subdirectory /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5
scm_1               | 2023-06-22 08:29:41,508 [main] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: addNew group-0B1D67CA50C5:[] returns group-0B1D67CA50C5:java.util.concurrent.CompletableFuture@6eeb29c0[Not completed]
scm_1               | 2023-06-22 08:29:41,542 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2: new RaftServerImpl for group-0B1D67CA50C5:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-22 08:29:41,545 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-22 08:29:41,545 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-22 08:29:41,546 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-22 08:29:41,546 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:29:41,547 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:29:41,547 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-22 08:29:41,556 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-22 08:29:41,557 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:29:41,561 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-22 08:29:41,562 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:29:41,589 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-22 08:29:41,593 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:41,597 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-22 08:29:41,598 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-22 08:29:41,621 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-22 08:29:41,763 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:29:41,766 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-22 08:29:41,767 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-22 08:29:41,768 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-22 08:29:41,768 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-22 08:29:41,769 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-22 08:29:41,771 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-22 08:29:41,771 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-22 08:29:41,772 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-22 08:29:41,812 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-06-22 08:29:41,858 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-22 08:29:41,859 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-22 08:29:41,865 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-22 08:29:41,869 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-22 08:29:41,957 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-22 08:29:41,976 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-06-22 08:29:41,980 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-22 08:29:41,991 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-22 08:29:42,011 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-22 08:29:42,012 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-22 08:29:42,019 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-22 08:29:42,019 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:29:42,023 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-22 08:29:42,024 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-22 08:29:42,032 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-22 08:29:42,033 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-22 08:29:42,094 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-22 08:29:42,119 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-22 08:29:42,211 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-22 08:29:42,243 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-22 08:29:42,248 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-22 08:29:42,265 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-22 08:29:42,270 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:42,274 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:29:42,340 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-22 08:29:43,102 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:29:43,134 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:29:43,173 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-22 08:29:43,201 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:29:43,268 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:29:43,282 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:29:43,287 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-22 08:29:43,294 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:29:43,344 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:29:43,358 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:29:43,359 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-22 08:29:43,368 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:29:43,487 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-22 08:29:43,488 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-22 08:29:43,488 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-22 08:29:43,492 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:29:43,516 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-22 08:29:43,525 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/in_use.lock acquired by nodename 6@e69db5f5ff78
scm_1               | 2023-06-22 08:29:43,536 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=fb78c014-7100-41f0-9790-f8c68fb7a7e2} from /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/raft-meta
scm_1               | 2023-06-22 08:29:43,588 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: set configuration 0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:43,592 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-22 08:29:43,605 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-22 08:29:43,605 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:43,610 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-22 08:29:43,612 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-22 08:29:43,619 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:29:43,638 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-22 08:29:43,638 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:29:43,639 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:43,646 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5
scm_1               | 2023-06-22 08:29:43,646 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:29:43,649 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:43,653 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:29:43,653 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-22 08:29:43,654 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-22 08:29:43,655 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-22 08:29:43,655 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:43,656 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:29:43,667 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:29:43,668 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:29:43,698 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:29:43,702 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:29:43,705 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:29:43,752 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: set configuration 0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:43,753 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/log_inprogress_0
scm_1               | 2023-06-22 08:29:43,756 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:29:43,819 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: start as a follower, conf=0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:43,820 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-22 08:29:43,822 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState
scm_1               | 2023-06-22 08:29:43,823 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-22 08:29:43,823 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-22 08:29:43,825 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0B1D67CA50C5,id=fb78c014-7100-41f0-9790-f8c68fb7a7e2
scm_1               | 2023-06-22 08:29:43,830 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-22 08:29:43,831 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-22 08:29:43,831 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:29:43,832 [fb78c014-7100-41f0-9790-f8c68fb7a7e2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-22 08:29:43,836 [main] INFO server.RaftServer: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start RPC server
scm_1               | 2023-06-22 08:29:43,982 [main] INFO server.GrpcService: fb78c014-7100-41f0-9790-f8c68fb7a7e2: GrpcService started, listening on 9894
scm_1               | 2023-06-22 08:29:43,985 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fb78c014-7100-41f0-9790-f8c68fb7a7e2: Started
scm_1               | 2023-06-22 08:29:43,995 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-22 08:29:43,995 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-22 08:29:43,997 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-22 08:29:43,998 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-22 08:29:43,998 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-22 08:29:44,076 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:29:44,088 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:29:44,088 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:29:44,577 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:29:44,578 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:29:44,613 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:29:44,747 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:29:44,749 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:29:44,750 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:29:44,780 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:29:44,931 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:29:44,931 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:29:45,040 [main] INFO util.log: Logging initialized @9194ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:29:45,429 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-22 08:29:45,447 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:29:45,462 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:29:45,465 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:29:45,465 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:29:45,467 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:29:45,613 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-22 08:29:45,616 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-22 08:29:45,619 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-22 08:29:45,714 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:29:45,715 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:29:45,717 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-22 08:29:45,742 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11636d43{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:29:45,743 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@322f6283{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:29:45,860 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5824b485{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7875985607902215488/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-22 08:29:45,871 [main] INFO server.AbstractConnector: Started ServerConnector@4bd29a01{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:29:45,871 [main] INFO server.Server: Started @10024ms
scm_1               | 2023-06-22 08:29:45,873 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-22 08:29:45,873 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-22 08:29:45,874 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:29:48,894 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.FollowerState: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072417678ns, electionTimeout:5069ms
scm_1               | 2023-06-22 08:29:48,896 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState
scm_1               | 2023-06-22 08:29:48,897 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-22 08:29:48,900 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-22 08:29:48,900 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-FollowerState] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1
scm_1               | 2023-06-22 08:29:48,908 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:48,909 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-22 08:29:48,935 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:48,935 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.LeaderElection: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-22 08:29:48,935 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: shutdown fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1
scm_1               | 2023-06-22 08:29:48,936 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-22 08:29:48,936 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-22 08:29:48,937 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-22 08:29:48,941 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: change Leader from null to fb78c014-7100-41f0-9790-f8c68fb7a7e2 at term 2 for becomeLeader, leader elected after 7348ms
scm_1               | 2023-06-22 08:29:48,952 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-22 08:29:48,958 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:48,959 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:29:48,965 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-22 08:29:48,970 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-22 08:29:48,971 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-22 08:29:48,978 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:29:48,980 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-22 08:29:48,982 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO impl.RoleInfo: fb78c014-7100-41f0-9790-f8c68fb7a7e2: start fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderStateImpl
scm_1               | 2023-06-22 08:29:48,988 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-22 08:29:48,995 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/log_inprogress_0 to /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/log_0-0
scm_1               | 2023-06-22 08:29:49,008 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-LeaderElection1] INFO server.RaftServer$Division: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5: set configuration 1: peers:[fb78c014-7100-41f0-9790-f8c68fb7a7e2|rpc:e69db5f5ff78:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:29:49,026 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/6b4c5003-8ed2-4998-9e26-0b1d67ca50c5/current/log_inprogress_1
scm_1               | 2023-06-22 08:29:49,033 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-22 08:29:49,035 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-22 08:29:49,040 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:49,041 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-22 08:29:49,042 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:29:49,043 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:29:49,058 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:29:49,075 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:29:49,348 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.19.0.4:37700: output error
scm_1               | 2023-06-22 08:29:49,348 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.19.0.10:45196: output error
scm_1               | 2023-06-22 08:29:49,354 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:29:49,354 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:29:49,373 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.19.0.5:44782: output error
scm_1               | 2023-06-22 08:29:49,373 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:29:51,049 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d1e52d63-e5d8-4bcf-98c5-65421cfda140
scm_1               | 2023-06-22 08:29:51,070 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d1e52d63-e5d8-4bcf-98c5-65421cfda140{ip: 172.19.0.10, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:29:51,119 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:29:51,167 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=62067dd6-a943-4c28-af6f-d34f13b58203 to datanode:d1e52d63-e5d8-4bcf-98c5-65421cfda140
scm_1               | 2023-06-22 08:29:51,215 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:29:51,261 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:29:51,329 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:29:51,308 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
scm_1               | 2023-06-22 08:29:51,341 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f91e12cd-1de9-431d-8f3a-a5eb5b4a5293{ip: 172.19.0.5, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:29:51,342 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:29:51,345 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:29:51,869 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7b56dd5f-5763-4a54-b62e-fd435ccd0c95
scm_1               | 2023-06-22 08:29:51,872 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7b56dd5f-5763-4a54-b62e-fd435ccd0c95{ip: 172.19.0.4, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:29:51,872 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:29:51,876 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:29:51,885 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:29:51,886 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:29:51,886 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-22 08:29:51,886 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:29:52,433 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 62067dd6-a943-4c28-af6f-d34f13b58203, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:51.154Z[UTC]].
scm_1               | 2023-06-22 08:29:52,455 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:52,516 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 to datanode:d1e52d63-e5d8-4bcf-98c5-65421cfda140
scm_1               | 2023-06-22 08:29:52,531 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 to datanode:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
scm_1               | 2023-06-22 08:29:52,550 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 to datanode:7b56dd5f-5763-4a54-b62e-fd435ccd0c95
scm_1               | 2023-06-22 08:29:52,582 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: de793908-0ed7-45c2-af21-1d6efdd8a424, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.516Z[UTC]].
scm_1               | 2023-06-22 08:29:52,595 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:52,613 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1de0afd2-c845-4655-b03a-269e8d473fc7 to datanode:7b56dd5f-5763-4a54-b62e-fd435ccd0c95
scm_1               | 2023-06-22 08:29:52,633 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 1de0afd2-c845-4655-b03a-269e8d473fc7, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.612Z[UTC]].
scm_1               | 2023-06-22 08:29:52,637 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:52,640 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 to datanode:7b56dd5f-5763-4a54-b62e-fd435ccd0c95
scm_1               | 2023-06-22 08:29:52,645 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 to datanode:d1e52d63-e5d8-4bcf-98c5-65421cfda140
scm_1               | 2023-06-22 08:29:52,647 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 to datanode:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
scm_1               | 2023-06-22 08:29:52,659 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef1cc5c3-3caa-4c79-bf70-0437b3489a53, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.640Z[UTC]].
scm_1               | 2023-06-22 08:29:52,660 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:52,661 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=ef1cc5c3-3caa-4c79-bf70-0437b3489a53 contains same datanodes as previous pipelines: PipelineID=de793908-0ed7-45c2-af21-1d6efdd8a424 nodeIds: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95, d1e52d63-e5d8-4bcf-98c5-65421cfda140, f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
scm_1               | 2023-06-22 08:29:52,671 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4a2a2e46-0e99-4727-85e9-5335d8082a4d to datanode:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293
scm_1               | 2023-06-22 08:29:52,686 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4a2a2e46-0e99-4727-85e9-5335d8082a4d, Nodes: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:29:52.671Z[UTC]].
scm_1               | 2023-06-22 08:29:52,686 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:29:52,696 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-22 08:29:52,724 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-22 08:30:03,002 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: de793908-0ed7-45c2-af21-1d6efdd8a424, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5)7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7b56dd5f-5763-4a54-b62e-fd435ccd0c95, CreationTimestamp2023-06-22T08:29:52.516Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:30:03,031 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:30:03,042 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:30:03,048 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:30:03,048 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:30:03,048 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-22 08:30:03,048 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-22 08:30:03,048 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:30:03,050 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:30:03,050 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-22 08:30:03,086 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-22 08:30:03,088 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-22 08:30:03,466 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 1de0afd2-c845-4655-b03a-269e8d473fc7, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7b56dd5f-5763-4a54-b62e-fd435ccd0c95, CreationTimestamp2023-06-22T08:29:52.612Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:30:11,136 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ef1cc5c3-3caa-4c79-bf70-0437b3489a53, Nodes: 7b56dd5f-5763-4a54-b62e-fd435ccd0c95(xcompat_datanode_1.xcompat_default/172.19.0.4)d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10)f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, CreationTimestamp2023-06-22T08:29:52.640Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:30:23,858 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 62067dd6-a943-4c28-af6f-d34f13b58203, Nodes: d1e52d63-e5d8-4bcf-98c5-65421cfda140(xcompat_datanode_2.xcompat_default/172.19.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d1e52d63-e5d8-4bcf-98c5-65421cfda140, CreationTimestamp2023-06-22T08:29:51.154Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:30:24,724 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4a2a2e46-0e99-4727-85e9-5335d8082a4d, Nodes: f91e12cd-1de9-431d-8f3a-a5eb5b4a5293(xcompat_datanode_3.xcompat_default/172.19.0.5), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f91e12cd-1de9-431d-8f3a-a5eb5b4a5293, CreationTimestamp2023-06-22T08:29:52.671Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:30:25,499 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-22 08:30:25,560 [fb78c014-7100-41f0-9790-f8c68fb7a7e2@group-0B1D67CA50C5-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-22 08:30:25,582 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-22 08:31:10,429 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:31:22,320 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:31:52,730 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-22 08:32:28,263 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:32:39,375 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:33:46,987 [IPC Server handler 54 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:33:52,733 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-22 08:33:58,518 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:34:42,252 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-22 08:35:09,798 [IPC Server handler 24 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:35:21,575 [IPC Server handler 24 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:35:52,734 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-22 08:36:35,479 [IPC Server handler 17 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
scm_1               | 2023-06-22 08:36:47,275 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.6
Attaching to xcompat_s3g_1, xcompat_old_client_1_0_0_1, xcompat_datanode_2, xcompat_om_1, xcompat_datanode_1, xcompat_datanode_3, xcompat_old_client_1_2_1_1, xcompat_scm_1, xcompat_recon_1, xcompat_old_client_1_3_0_1, xcompat_old_client_1_1_0_1, xcompat_new_client_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:37:18 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = b189cdb37b8f/172.20.0.9
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.0.0
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:37:18 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = f2bea0ee412a/172.20.0.11
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.0.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_1          | STARTUP_MSG:   java = 11.0.3
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:37:18 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:37:19 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:37:20 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:37:21 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:37:21 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:37:22 INFO  HddsDatanodeService:209 - HddsDatanodeService host:b189cdb37b8f ip:172.20.0.9
datanode_1          | 2023-06-22 08:37:22 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:37:22 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-22 08:37:22 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:37:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:37:23 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:37:23 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:37:23 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:37:23 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:37:28 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:37:28 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:37:29 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_1          | 2023-06-22 08:37:29 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:37:29 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:37:29 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:37:29 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:37:30 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:37:30 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:37:31 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1          | 2023-06-22 08:37:31 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:37:31 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:37:31 INFO  log:169 - Logging initialized @19049ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:37:32 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-22 08:37:32 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:37:32 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:37:32 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:37:32 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:37:32 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:37:32 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_1          | 2023-06-22 08:37:32 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1          | 2023-06-22 08:37:32 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:37:32 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:37:32 INFO  session:140 - node0 Scavenging every 600000ms
datanode_1          | 2023-06-22 08:37:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a451491{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:37:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@a92be4f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:37:34 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@5981f4a6{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-9406118601102104288.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:37:34 INFO  AbstractConnector:330 - Started ServerConnector@56dfab87{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:37:34 INFO  Server:399 - Started @21778ms
datanode_1          | 2023-06-22 08:37:34 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1          | 2023-06-22 08:37:34 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1          | 2023-06-22 08:37:34 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:37:34 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1          | 2023-06-22 08:37:35 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_1          | 2023-06-22 08:37:35 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:37:37 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:37:38 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:37:39 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From b189cdb37b8f/172.20.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.9:49304 remote=scm/172.20.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_2          | STARTUP_MSG:   java = 11.0.3
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:37:18 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:37:20 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:37:20 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:37:22 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:37:22 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:37:22 INFO  HddsDatanodeService:209 - HddsDatanodeService host:f2bea0ee412a ip:172.20.0.11
datanode_2          | 2023-06-22 08:37:23 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:37:23 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-22 08:37:23 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:37:23 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:37:23 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:37:24 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:37:24 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:37:24 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:37:29 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:37:29 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:37:30 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_2          | 2023-06-22 08:37:30 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:37:30 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:37:30 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:37:30 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:37:31 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:37:31 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:37:32 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_2          | 2023-06-22 08:37:32 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:37:32 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:37:33 INFO  log:169 - Logging initialized @19952ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:37:34 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-22 08:37:34 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:37:34 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:37:34 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:37:34 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:37:34 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:37:34 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_2          | 2023-06-22 08:37:34 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2          | 2023-06-22 08:37:34 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:37:34 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:37:34 INFO  session:140 - node0 Scavenging every 600000ms
datanode_2          | 2023-06-22 08:37:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:37:34 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:37:35 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-12390830660918979123.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-22 08:37:35 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:37:35 INFO  Server:399 - Started @22667ms
datanode_2          | 2023-06-22 08:37:35 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2          | 2023-06-22 08:37:35 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2          | 2023-06-22 08:37:35 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:37:36 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2          | 2023-06-22 08:37:36 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_2          | 2023-06-22 08:37:36 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:37:39 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:37:40 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From f2bea0ee412a/172.20.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:39116 remote=scm/172.20.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.9:49304 remote=scm/172.20.0.7:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_1          | 2023-06-22 08:37:40 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-06-22 08:37:40 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_1          | 2023-06-22 08:37:40 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_1          | 2023-06-22 08:37:40 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis b89efc6d-7472-423a-976a-f1525a889aa3 at port 9858
datanode_1          | 2023-06-22 08:37:41 INFO  RaftServerProxy:304 - b89efc6d-7472-423a-976a-f1525a889aa3: start RPC server
datanode_1          | 2023-06-22 08:37:41 INFO  GrpcService:160 - b89efc6d-7472-423a-976a-f1525a889aa3: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1          | 2023-06-22 08:37:44 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-06-22 08:38:04 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_1          | 2023-06-22 08:38:24 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_1          | 2023-06-22 08:38:36 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerProxy:89 - b89efc6d-7472-423a-976a-f1525a889aa3: addNew group-DF01E0140DE8:[b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] returns group-DF01E0140DE8:java.util.concurrent.CompletableFuture@2f1883b1[Not completed]
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerImpl:107 - b89efc6d-7472-423a-976a-f1525a889aa3: new RaftServerImpl for group-DF01E0140DE8:[b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerImpl:103 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: ConfigurationManager, init=-1: [b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/11dccb04-a62e-47fe-9b28-df01e0140de8 does not exist. Creating ...
datanode_1          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/11dccb04-a62e-47fe-9b28-df01e0140de8/in_use.lock acquired by nodename 7@b189cdb37b8f
datanode_1          | 2023-06-22 08:38:39 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/11dccb04-a62e-47fe-9b28-df01e0140de8 has been successfully formatted.
datanode_1          | 2023-06-22 08:38:39 INFO  ContainerStateMachine:225 - group-DF01E0140DE8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:180 - new b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/11dccb04-a62e-47fe-9b28-df01e0140de8
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.11:39116 remote=scm/172.20.0.7:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_2          | 2023-06-22 08:37:40 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_2          | 2023-06-22 08:37:40 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_2          | 2023-06-22 08:37:40 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 387acfa3-4677-4ee6-a8f7-8806d41030cc at port 9858
datanode_2          | 2023-06-22 08:37:41 INFO  RaftServerProxy:304 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start RPC server
datanode_2          | 2023-06-22 08:37:41 INFO  GrpcService:160 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2          | 2023-06-22 08:37:44 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_2          | 2023-06-22 08:38:04 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_2          | 2023-06-22 08:38:24 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_2          | 2023-06-22 08:38:38 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerProxy:89 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: addNew group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] returns group-96D39EEFA696:java.util.concurrent.CompletableFuture@262b9c9a[Not completed]
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerProxy:89 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: addNew group-426D7B30E994:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858] returns group-426D7B30E994:java.util.concurrent.CompletableFuture@41e10e49[Not completed]
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:107 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: new RaftServerImpl for group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:103 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: ConfigurationManager, init=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 does not exist. Creating ...
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/in_use.lock acquired by nodename 6@f2bea0ee412a
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 has been successfully formatted.
datanode_2          | 2023-06-22 08:38:41 INFO  ContainerStateMachine:225 - group-96D39EEFA696: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:180 - new 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:129 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:129 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:196 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: start as a follower, conf=-1: [b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:185 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:38:40 INFO  RoleInfo:143 - b89efc6d-7472-423a-976a-f1525a889aa3: start FollowerState
datanode_1          | 2023-06-22 08:38:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DF01E0140DE8,id=b89efc6d-7472-423a-976a-f1525a889aa3
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8
datanode_1          | 2023-06-22 08:38:40 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "11dccb04-a62e-47fe-9b28-df01e0140de8"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 1287126814349477886
datanode_1          |   leastSigBits: -7266312799614923288
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerProxy:89 - b89efc6d-7472-423a-976a-f1525a889aa3: addNew group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] returns group-96D39EEFA696:java.util.concurrent.CompletableFuture@1980aab5[Not completed]
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:107 - b89efc6d-7472-423a-976a-f1525a889aa3: new RaftServerImpl for group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:103 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: ConfigurationManager, init=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 does not exist. Creating ...
datanode_1          | 2023-06-22 08:38:40 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/in_use.lock acquired by nodename 7@b189cdb37b8f
datanode_1          | 2023-06-22 08:38:40 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 has been successfully formatted.
datanode_1          | 2023-06-22 08:38:40 INFO  ContainerStateMachine:225 - group-96D39EEFA696: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:180 - new b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:129 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:129 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:107 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: new RaftServerImpl for group-426D7B30E994:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:103 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: ConfigurationManager, init=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/214dfd40-4b1d-42e9-915f-426d7b30e994 does not exist. Creating ...
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/214dfd40-4b1d-42e9-915f-426d7b30e994/in_use.lock acquired by nodename 6@f2bea0ee412a
datanode_2          | 2023-06-22 08:38:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/214dfd40-4b1d-42e9-915f-426d7b30e994 has been successfully formatted.
datanode_2          | 2023-06-22 08:38:41 INFO  ContainerStateMachine:225 - group-426D7B30E994: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:180 - new 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/214dfd40-4b1d-42e9-915f-426d7b30e994
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:129 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:38:41 INFO  SegmentedRaftLogWorker:129 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:196 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: start as a follower, conf=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:185 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:38:41 INFO  RoleInfo:143 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start FollowerState
datanode_2          | 2023-06-22 08:38:41 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96D39EEFA696,id=387acfa3-4677-4ee6-a8f7-8806d41030cc
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:196 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: start as a follower, conf=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858], old=null
datanode_2          | 2023-06-22 08:38:41 INFO  RaftServerImpl:185 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:38:41 INFO  RoleInfo:143 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start FollowerState
datanode_2          | 2023-06-22 08:38:41 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-426D7B30E994,id=387acfa3-4677-4ee6-a8f7-8806d41030cc
datanode_2          | 2023-06-22 08:38:41 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994
datanode_2          | 2023-06-22 08:38:41 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "214dfd40-4b1d-42e9-915f-426d7b30e994"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 2399852629038285545
datanode_2          |   leastSigBits: -7971579777436817004
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:7359d79c-a308-41a2-b591-63e1620e32e7
datanode_2          | 2023-06-22 08:38:44 INFO  RoleInfo:121 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: shutdown FollowerState
datanode_2          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start FollowerState
datanode_2          | 2023-06-22 08:38:44 INFO  FollowerState:117 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2          | 2023-06-22 08:38:44 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-96D39EEFA696 with new leaderId: 7359d79c-a308-41a2-b591-63e1620e32e7
datanode_2          | 2023-06-22 08:38:44 INFO  RaftServerImpl:255 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: change Leader from null to 7359d79c-a308-41a2-b591-63e1620e32e7 at term 1 for appendEntries, leader elected after 3556ms
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:129 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:38:40 INFO  SegmentedRaftLogWorker:129 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:196 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: start as a follower, conf=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_1          | 2023-06-22 08:38:40 INFO  RaftServerImpl:185 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:38:40 INFO  RoleInfo:143 - b89efc6d-7472-423a-976a-f1525a889aa3: start FollowerState
datanode_1          | 2023-06-22 08:38:40 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96D39EEFA696,id=b89efc6d-7472-423a-976a-f1525a889aa3
datanode_1          | 2023-06-22 08:38:40 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696
datanode_1          | 2023-06-22 08:38:42 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "04a23e87-0fa9-48c2-9a0a-96d39eefa696"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 333898072183097538
datanode_1          |   leastSigBits: -7346894006452771178
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:7359d79c-a308-41a2-b591-63e1620e32e7
datanode_1          | 2023-06-22 08:38:44 INFO  RoleInfo:121 - b89efc6d-7472-423a-976a-f1525a889aa3: shutdown FollowerState
datanode_1          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - b89efc6d-7472-423a-976a-f1525a889aa3: start FollowerState
datanode_1          | 2023-06-22 08:38:44 INFO  FollowerState:117 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1          | 2023-06-22 08:38:44 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-96D39EEFA696 with new leaderId: 7359d79c-a308-41a2-b591-63e1620e32e7
datanode_1          | 2023-06-22 08:38:44 INFO  RaftServerImpl:255 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: change Leader from null to 7359d79c-a308-41a2-b591-63e1620e32e7 at term 1 for appendEntries, leader elected after 4033ms
datanode_1          | 2023-06-22 08:38:44 INFO  RaftServerImpl:356 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696: set configuration 0: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null at 0
datanode_1          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:397 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:596 - b89efc6d-7472-423a-976a-f1525a889aa3@group-96D39EEFA696-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/current/log_inprogress_0
datanode_1          | 2023-06-22 08:38:45 INFO  FollowerState:108 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-FollowerState: change to CANDIDATE, lastRpcTime:5173ms, electionTimeout:5132ms
datanode_1          | 2023-06-22 08:38:45 INFO  RoleInfo:121 - b89efc6d-7472-423a-976a-f1525a889aa3: shutdown FollowerState
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerImpl:185 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:38:45 INFO  RoleInfo:143 - b89efc6d-7472-423a-976a-f1525a889aa3: start LeaderElection
datanode_1          | 2023-06-22 08:38:45 INFO  LeaderElection:209 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-LeaderElection1: begin an election at term 1 for -1: [b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_1          | 2023-06-22 08:38:45 INFO  RoleInfo:134 - b89efc6d-7472-423a-976a-f1525a889aa3: shutdown LeaderElection
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerImpl:185 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:38:45 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-DF01E0140DE8 with new leaderId: b89efc6d-7472-423a-976a-f1525a889aa3
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerImpl:255 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: change Leader from null to b89efc6d-7472-423a-976a-f1525a889aa3 at term 1 for becomeLeader, leader elected after 5861ms
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:38:45 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:38:45 INFO  RoleInfo:143 - b89efc6d-7472-423a-976a-f1525a889aa3: start LeaderState
datanode_1          | 2023-06-22 08:38:45 INFO  SegmentedRaftLogWorker:397 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:38:45 INFO  SegmentedRaftLogWorker:596 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/11dccb04-a62e-47fe-9b28-df01e0140de8/current/log_inprogress_0
datanode_1          | 2023-06-22 08:38:45 INFO  RaftServerImpl:356 - b89efc6d-7472-423a-976a-f1525a889aa3@group-DF01E0140DE8: set configuration 0: [b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null at 0
datanode_2          | 2023-06-22 08:38:44 INFO  RaftServerImpl:356 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696: set configuration 0: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null at 0
datanode_2          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:397 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:596 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-96D39EEFA696-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/current/log_inprogress_0
datanode_2          | 2023-06-22 08:38:46 INFO  FollowerState:108 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-FollowerState: change to CANDIDATE, lastRpcTime:5037ms, electionTimeout:5036ms
datanode_2          | 2023-06-22 08:38:46 INFO  RoleInfo:121 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: shutdown FollowerState
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerImpl:185 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:38:46 INFO  RoleInfo:143 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start LeaderElection
datanode_2          | 2023-06-22 08:38:46 INFO  LeaderElection:209 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-LeaderElection1: begin an election at term 1 for -1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858], old=null
datanode_2          | 2023-06-22 08:38:46 INFO  RoleInfo:134 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: shutdown LeaderElection
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerImpl:185 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:38:46 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-426D7B30E994 with new leaderId: 387acfa3-4677-4ee6-a8f7-8806d41030cc
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerImpl:255 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: change Leader from null to 387acfa3-4677-4ee6-a8f7-8806d41030cc at term 1 for becomeLeader, leader elected after 5145ms
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:38:46 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:38:46 INFO  RoleInfo:143 - 387acfa3-4677-4ee6-a8f7-8806d41030cc: start LeaderState
datanode_2          | 2023-06-22 08:38:46 INFO  SegmentedRaftLogWorker:397 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:38:46 INFO  SegmentedRaftLogWorker:596 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/214dfd40-4b1d-42e9-915f-426d7b30e994/current/log_inprogress_0
datanode_2          | 2023-06-22 08:38:46 INFO  RaftServerImpl:356 - 387acfa3-4677-4ee6-a8f7-8806d41030cc@group-426D7B30E994: set configuration 0: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858], old=null at 0
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:37:16 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 12b3ece59ca8/172.20.0.5
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.0.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_3          | STARTUP_MSG:   java = 11.0.3
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:37:16 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:37:18 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:37:19 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:37:20 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:37:20 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:37:20 INFO  HddsDatanodeService:209 - HddsDatanodeService host:12b3ece59ca8 ip:172.20.0.5
datanode_3          | 2023-06-22 08:37:21 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:37:21 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-22 08:37:21 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:37:21 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:37:21 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:37:22 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:37:22 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:37:22 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:37:28 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:37:28 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:37:28 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_3          | 2023-06-22 08:37:28 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:37:28 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:37:28 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:37:28 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:37:29 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:37:29 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:37:30 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_3          | 2023-06-22 08:37:30 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:37:30 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:37:31 INFO  log:169 - Logging initialized @19987ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:37:31 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-22 08:37:31 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:37:31 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:37:31 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:37:31 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:37:31 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-22 08:37:32 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_3          | 2023-06-22 08:37:32 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_3          | 2023-06-22 08:37:32 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:37:32 INFO  session:338 - No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:37:32 INFO  session:140 - node0 Scavenging every 600000ms
datanode_3          | 2023-06-22 08:37:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-22 08:37:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:37:33 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-15830646438394165551.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:37:33 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:37:33 INFO  Server:399 - Started @22899ms
datanode_3          | 2023-06-22 08:37:33 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3          | 2023-06-22 08:37:33 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3          | 2023-06-22 08:37:33 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:37:34 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3          | 2023-06-22 08:37:34 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_3          | 2023-06-22 08:37:34 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:37:37 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:37:38 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:37:39 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:37:40 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-06-22 08:37:40 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 12b3ece59ca8/172.20.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.5:37540 remote=scm/172.20.0.7:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.5:37540 remote=scm/172.20.0.7:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_3          | 2023-06-22 08:37:40 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_3          | 2023-06-22 08:37:40 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_3          | 2023-06-22 08:37:40 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 7359d79c-a308-41a2-b591-63e1620e32e7 at port 9858
datanode_3          | 2023-06-22 08:37:41 INFO  RaftServerProxy:304 - 7359d79c-a308-41a2-b591-63e1620e32e7: start RPC server
datanode_3          | 2023-06-22 08:37:41 INFO  GrpcService:160 - 7359d79c-a308-41a2-b591-63e1620e32e7: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3          | 2023-06-22 08:37:44 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-06-22 08:38:04 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_3          | 2023-06-22 08:38:24 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_3          | 2023-06-22 08:38:36 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerProxy:89 - 7359d79c-a308-41a2-b591-63e1620e32e7: addNew group-27E665E9FAFB:[7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858] returns group-27E665E9FAFB:java.util.concurrent.CompletableFuture@1fcba463[Not completed]
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:107 - 7359d79c-a308-41a2-b591-63e1620e32e7: new RaftServerImpl for group-27E665E9FAFB:[7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:103 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: ConfigurationManager, init=-1: [7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/75e5d1a3-5198-45a8-8c57-27e665e9fafb does not exist. Creating ...
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/75e5d1a3-5198-45a8-8c57-27e665e9fafb/in_use.lock acquired by nodename 7@12b3ece59ca8
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/75e5d1a3-5198-45a8-8c57-27e665e9fafb has been successfully formatted.
datanode_3          | 2023-06-22 08:38:39 INFO  ContainerStateMachine:225 - group-27E665E9FAFB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:180 - new 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/75e5d1a3-5198-45a8-8c57-27e665e9fafb
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:129 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:129 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:196 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: start as a follower, conf=-1: [7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858], old=null
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:38:39 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start FollowerState
datanode_3          | 2023-06-22 08:38:39 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-27E665E9FAFB,id=7359d79c-a308-41a2-b591-63e1620e32e7
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB
datanode_3          | 2023-06-22 08:38:39 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "75e5d1a3-5198-45a8-8c57-27e665e9fafb"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 8495426771483116968
datanode_3          |   leastSigBits: -8334148714920019205
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerProxy:89 - 7359d79c-a308-41a2-b591-63e1620e32e7: addNew group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] returns group-96D39EEFA696:java.util.concurrent.CompletableFuture@7d47c095[Not completed]
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:107 - 7359d79c-a308-41a2-b591-63e1620e32e7: new RaftServerImpl for group-96D39EEFA696:[387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:103 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: ConfigurationManager, init=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 does not exist. Creating ...
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/in_use.lock acquired by nodename 7@12b3ece59ca8
datanode_3          | 2023-06-22 08:38:39 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696 has been successfully formatted.
datanode_3          | 2023-06-22 08:38:39 INFO  ContainerStateMachine:225 - group-96D39EEFA696: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:180 - new 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:129 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:38:39 INFO  SegmentedRaftLogWorker:129 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:196 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: start as a follower, conf=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_3          | 2023-06-22 08:38:39 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:38:39 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start FollowerState
datanode_3          | 2023-06-22 08:38:39 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-96D39EEFA696,id=7359d79c-a308-41a2-b591-63e1620e32e7
datanode_3          | 2023-06-22 08:38:39 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:42 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "04a23e87-0fa9-48c2-9a0a-96d39eefa696"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 333898072183097538
datanode_3          |   leastSigBits: -7346894006452771178
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-22 08:38:44 INFO  FollowerState:108 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-FollowerState: change to CANDIDATE, lastRpcTime:5110ms, electionTimeout:5109ms
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:121 - 7359d79c-a308-41a2-b591-63e1620e32e7: shutdown FollowerState
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start LeaderElection
datanode_3          | 2023-06-22 08:38:44 INFO  LeaderElection:209 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-LeaderElection1: begin an election at term 1 for -1: [7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858], old=null
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:134 - 7359d79c-a308-41a2-b591-63e1620e32e7: shutdown LeaderElection
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:38:44 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-27E665E9FAFB with new leaderId: 7359d79c-a308-41a2-b591-63e1620e32e7
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:255 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: change Leader from null to 7359d79c-a308-41a2-b591-63e1620e32e7 at term 1 for becomeLeader, leader elected after 5260ms
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start LeaderState
datanode_3          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:397 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:356 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB: set configuration 0: [7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858], old=null at 0
datanode_3          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:596 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-27E665E9FAFB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/75e5d1a3-5198-45a8-8c57-27e665e9fafb/current/log_inprogress_0
datanode_3          | 2023-06-22 08:38:44 INFO  FollowerState:108 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-FollowerState: change to CANDIDATE, lastRpcTime:5183ms, electionTimeout:5182ms
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:121 - 7359d79c-a308-41a2-b591-63e1620e32e7: shutdown FollowerState
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start LeaderElection
datanode_3          | 2023-06-22 08:38:44 INFO  LeaderElection:209 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-LeaderElection2: begin an election at term 1 for -1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_3          | 2023-06-22 08:38:44 INFO  LeaderElection:61 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-LeaderElection2: Election PASSED; received 1 response(s) [7359d79c-a308-41a2-b591-63e1620e32e7<-387acfa3-4677-4ee6-a8f7-8806d41030cc#0:OK-t1] and 0 exception(s); 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696:t1, leader=null, voted=7359d79c-a308-41a2-b591-63e1620e32e7, raftlog=7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:134 - 7359d79c-a308-41a2-b591-63e1620e32e7: shutdown LeaderElection
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:185 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:38:44 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-96D39EEFA696 with new leaderId: 7359d79c-a308-41a2-b591-63e1620e32e7
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:255 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: change Leader from null to 7359d79c-a308-41a2-b591-63e1620e32e7 at term 1 for becomeLeader, leader elected after 5321ms
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis_grpc.log_appender.7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:38:44 INFO  RoleInfo:143 - 7359d79c-a308-41a2-b591-63e1620e32e7: start LeaderState
datanode_3          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:397 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:38:44 INFO  RaftServerImpl:356 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696: set configuration 0: [387acfa3-4677-4ee6-a8f7-8806d41030cc:172.20.0.11:9858, 7359d79c-a308-41a2-b591-63e1620e32e7:172.20.0.5:9858, b89efc6d-7472-423a-976a-f1525a889aa3:172.20.0.9:9858], old=null at 0
datanode_3          | 2023-06-22 08:38:44 INFO  SegmentedRaftLogWorker:596 - 7359d79c-a308-41a2-b591-63e1620e32e7@group-96D39EEFA696-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/04a23e87-0fa9-48c2-9a0a-96d39eefa696/current/log_inprogress_0
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:37:15 INFO  ReconServer:112 - STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = e3e1be92aea1/172.20.0.6
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.0.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
recon_1             | STARTUP_MSG:   java = 11.0.3
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:37:15 INFO  ReconServer:90 - registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:37:19 INFO  ReconRestServletModule:75 - rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-22 08:37:21 INFO  ReconServer:93 - Initializing Recon server...
recon_1             | 2023-06-22 08:37:23 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:37:29 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:37:31 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:37:32 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:37:32 INFO  ReconServer:101 - Creating Recon Schema.
recon_1             | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1             | 2023-06-22 08:37:36 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
recon_1             | 2023-06-22 08:37:36 INFO  BaseHttpServer:207 - Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:37:36 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-22 08:37:36 INFO  log:169 - Logging initialized @25640ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:37:37 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-22 08:37:37 WARN  HttpRequestLog:103 - Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-22 08:37:37 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:37:37 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:37:37 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:37:37 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:37:37 INFO  ReconTaskControllerImpl:79 - Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:37:38 INFO  ReconTaskControllerImpl:79 - Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:37:38 INFO  OmUtils:550 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:37:38 INFO  OmUtils:569 - No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:37:38 INFO  deprecation:1395 - No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-22 08:37:38 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:37:39 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:37:39 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@66451058
recon_1             | 2023-06-22 08:37:39 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
recon_1             | 2023-06-22 08:37:39 WARN  DBStoreBuilder:277 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:37:39 INFO  SCMNodeManager:116 - Entering startup safe mode.
recon_1             | 2023-06-22 08:37:39 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:37:39 INFO  ReconNodeManager:100 - Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:37:39 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-22 08:37:39 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:37:39 INFO  Server:1219 - Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:37:39 INFO  SCMPipelineManager:161 - No pipeline exists in current db
recon_1             | 2023-06-22 08:37:39 INFO  ReconServer:109 - Recon server initialized successfully!
recon_1             | 2023-06-22 08:37:39 INFO  ReconServer:134 - Starting Recon server
recon_1             | 2023-06-22 08:37:39 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:37:39 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:37:39 INFO  MetricsSystemImpl:191 - Recon metrics system started
recon_1             | 2023-06-22 08:37:40 INFO  HttpServer2:1237 - Jetty bound to port 9888
recon_1             | 2023-06-22 08:37:40 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
recon_1             | 2023-06-22 08:37:40 INFO  session:333 - DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:37:40 INFO  session:338 - No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:37:40 INFO  session:140 - node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:37:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@723877dd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:37:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@6d229b1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-22 08:37:43 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@aa633e6{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_0_0_jar-_-any-501861690921197560.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/recon}
recon_1             | 2023-06-22 08:37:43 INFO  AbstractConnector:330 - Started ServerConnector@62765aec{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:37:43 INFO  Server:399 - Started @32051ms
recon_1             | 2023-06-22 08:37:43 INFO  MetricsSinkAdapter:204 - Sink prometheus started
recon_1             | 2023-06-22 08:37:43 INFO  MetricsSystemImpl:301 - Registered sink prometheus
recon_1             | 2023-06-22 08:37:43 INFO  BaseHttpServer:327 - HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:37:43 INFO  OzoneManagerServiceProviderImpl:198 - Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:37:43 INFO  OzoneManagerServiceProviderImpl:176 - Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:37:43 INFO  OzoneManagerServiceProviderImpl:186 - Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:37:43 INFO  ReconOmMetadataManagerImpl:65 - Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:37:43 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:37:43 INFO  ReconTaskControllerImpl:221 - Starting Recon Task Controller.
recon_1             | 2023-06-22 08:37:43 INFO  ReconStorageContainerManagerFacade:206 - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:37:43 INFO  ReconStorageContainerManagerFacade:256 - Obtained 0 pipelines from SCM.
recon_1             | 2023-06-22 08:37:43 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:37:43 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:37:43 INFO  Server:1460 - IPC Server Responder: starting
recon_1             | 2023-06-22 08:37:43 INFO  Server:1298 - IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:37:43 INFO  ReconScmTask:46 - Registered PipelineSyncTask task 
recon_1             | 2023-06-22 08:37:43 INFO  ReconScmTask:56 - Starting PipelineSyncTask Thread.
recon_1             | 2023-06-22 08:37:43 INFO  ReconScmTask:46 - Registered ContainerHealthTask task 
recon_1             | 2023-06-22 08:37:43 INFO  ReconScmTask:56 - Starting ContainerHealthTask Thread.
recon_1             | 2023-06-22 08:37:43 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:37:43 INFO  PipelineSyncTask:61 - Pipeline sync Thread took 39 milliseconds.
recon_1             | 2023-06-22 08:37:44 INFO  ContainerHealthTask:77 - Container Health task thread took 181 milliseconds to process 0 existing database records.
recon_1             | 2023-06-22 08:37:44 INFO  ContainerHealthTask:86 - Container Health task thread took 42 milliseconds for processing 0 containers.
recon_1             | 2023-06-22 08:38:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/7359d79c-a308-41a2-b591-63e1620e32e7
recon_1             | 2023-06-22 08:38:38 INFO  SCMNodeManager:273 - Registered Data node : 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:38 INFO  ReconNodeManager:116 - Adding new node 7359d79c-a308-41a2-b591-63e1620e32e7 to Node DB.
recon_1             | 2023-06-22 08:38:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/b89efc6d-7472-423a-976a-f1525a889aa3
recon_1             | 2023-06-22 08:38:38 INFO  SCMNodeManager:273 - Registered Data node : b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:38 INFO  ReconNodeManager:116 - Adding new node b89efc6d-7472-423a-976a-f1525a889aa3 to Node DB.
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=75e5d1a3-5198-45a8-8c57-27e665e9fafb. Trying to get from SCM.
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 75e5d1a3-5198-45a8-8c57-27e665e9fafb, Nodes: 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:7359d79c-a308-41a2-b591-63e1620e32e7, CreationTimestamp2023-06-22T08:38:36.591Z] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:38:39 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 75e5d1a3-5198-45a8-8c57-27e665e9fafb, Nodes: 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:7359d79c-a308-41a2-b591-63e1620e32e7, CreationTimestamp2023-06-22T08:38:36.591Z]
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696. Trying to get from SCM.
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 04a23e87-0fa9-48c2-9a0a-96d39eefa696, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:38.221Z] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:38:39 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 04a23e87-0fa9-48c2-9a0a-96d39eefa696, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:38.221Z]
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:39 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=11dccb04-a62e-47fe-9b28-df01e0140de8. Trying to get from SCM.
recon_1             | 2023-06-22 08:38:40 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 11dccb04-a62e-47fe-9b28-df01e0140de8, Nodes: b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b89efc6d-7472-423a-976a-f1525a889aa3, CreationTimestamp2023-06-22T08:38:37.030Z] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:38:40 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 11dccb04-a62e-47fe-9b28-df01e0140de8, Nodes: b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b89efc6d-7472-423a-976a-f1525a889aa3, CreationTimestamp2023-06-22T08:38:37.030Z]
recon_1             | 2023-06-22 08:38:40 INFO  NetworkTopology:111 - Added a new node: /default-rack/387acfa3-4677-4ee6-a8f7-8806d41030cc
recon_1             | 2023-06-22 08:38:40 INFO  SCMNodeManager:273 - Registered Data node : 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:40 INFO  ReconNodeManager:116 - Adding new node 387acfa3-4677-4ee6-a8f7-8806d41030cc to Node DB.
recon_1             | 2023-06-22 08:38:40 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:41 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:41 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=214dfd40-4b1d-42e9-915f-426d7b30e994. Trying to get from SCM.
recon_1             | 2023-06-22 08:38:41 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 214dfd40-4b1d-42e9-915f-426d7b30e994, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:387acfa3-4677-4ee6-a8f7-8806d41030cc, CreationTimestamp2023-06-22T08:38:38.209Z] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:38:41 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 214dfd40-4b1d-42e9-915f-426d7b30e994, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:387acfa3-4677-4ee6-a8f7-8806d41030cc, CreationTimestamp2023-06-22T08:38:38.209Z]
recon_1             | 2023-06-22 08:38:41 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:43 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:38:43 INFO  OzoneManagerServiceProviderImpl:409 - Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:38:43 INFO  OzoneManagerServiceProviderImpl:316 - Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687423123377
recon_1             | 2023-06-22 08:38:43 INFO  ReconOmMetadataManagerImpl:91 - Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687423123377.
recon_1             | 2023-06-22 08:38:43 INFO  OzoneManagerServiceProviderImpl:421 - Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:38:43 INFO  ContainerKeyMapperTask:73 - Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:38:43 INFO  ContainerDBServiceProviderImpl:117 - Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1687423123780
recon_1             | 2023-06-22 08:38:43 INFO  ContainerDBServiceProviderImpl:122 - Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1687423042169.
recon_1             | 2023-06-22 08:38:43 INFO  ContainerKeyMapperTask:89 - Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:38:43 INFO  ContainerKeyMapperTask:92 - It took me 0.074 seconds to process 0 keys.
recon_1             | 2023-06-22 08:38:43 INFO  FileSizeCountTask:102 - Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-22 08:38:44 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:44 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 reported by 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-22 08:38:44 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 04a23e87-0fa9-48c2-9a0a-96d39eefa696, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:7359d79c-a308-41a2-b591-63e1620e32e7, CreationTimestamp2023-06-22T08:38:38.221Z] moved to OPEN state
recon_1             | 2023-06-22 08:38:53 INFO  ReconContainerManager:89 - New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-22 08:38:53 INFO  ReconContainerManager:157 - Successfully added container #1 to Recon.
recon_1             | 2023-06-22 08:39:03 INFO  ReconContainerManager:89 - New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-22 08:39:03 INFO  ReconContainerManager:157 - Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:39:43 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:39:43 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-06-22 08:39:43 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 10
recon_1             | 2023-06-22 08:39:44 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 6 OM DB update event(s).
recon_1             | 2023-06-22 08:39:44 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
recon_1             | 2023-06-22 08:40:44 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:40:44 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-06-22 08:40:44 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 6
recon_1             | 2023-06-22 08:40:44 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 5 OM DB update event(s).
recon_1             | 2023-06-22 08:40:44 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-22 08:37:19 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
s3g_1               | 2023-06-22 08:37:19 INFO  BaseHttpServer:207 - Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-22 08:37:19 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-22 08:37:19 INFO  log:169 - Logging initialized @6975ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-22 08:37:20 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-22 08:37:20 INFO  HttpRequestLog:86 - Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-22 08:37:20 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-22 08:37:20 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-22 08:37:20 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-22 08:37:20 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-22 08:37:21 INFO  Gateway:112 - STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 671172977fdb/172.20.0.12
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.0.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.10.3.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
s3g_1               | STARTUP_MSG:   java = 11.0.3
s3g_1               | ************************************************************/
s3g_1               | 2023-06-22 08:37:21 INFO  Gateway:90 - registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-22 08:37:21 INFO  Gateway:68 - Starting Ozone S3 gateway
s3g_1               | 2023-06-22 08:37:21 INFO  HttpServer2:1237 - Jetty bound to port 9878
s3g_1               | 2023-06-22 08:37:21 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
s3g_1               | 2023-06-22 08:37:22 INFO  session:333 - DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-22 08:37:22 INFO  session:338 - No SessionScavenger set, using defaults
s3g_1               | 2023-06-22 08:37:22 INFO  session:140 - node0 Scavenging every 660000ms
s3g_1               | 2023-06-22 08:37:22 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@24c1b2d2{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-22 08:37:22 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@1df8da7a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/static,AVAILABLE}
s3g_1               | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 22, 2023 8:37:38 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-22 08:37:38 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@11d4d979{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_0_0_jar-_-any-1641745746397292085.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-22 08:37:38 INFO  AbstractConnector:330 - Started ServerConnector@b86de0d{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1               | 2023-06-22 08:37:38 INFO  Server:399 - Started @25372ms
s3g_1               | 2023-06-22 08:37:38 INFO  BaseHttpServer:327 - HTTP server of s3gateway listening at http://0.0.0.0:9878
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:37:18 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 938b19f2ab40/172.20.0.10
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-22 08:37:18 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:37:24 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:37:24 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.10:9862
om_1                | 2023-06-22 08:37:24 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:37:25 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:37:25 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-22 08:37:27 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:28 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:29 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:30 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:31 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:32 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:33 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:34 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:35 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:36 INFO  Client:958 - Retrying connect to server: scm/172.20.0.7:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:37:36 INFO  RetriableTask:62 - Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2efc32fe-20b6-4973-83a8-7d56971997f8;layoutVersion=0
om_1                | 2023-06-22 08:37:41 INFO  OzoneManagerStarter:124 - SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 938b19f2ab40/172.20.0.10
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:37:43 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 938b19f2ab40/172.20.0.10
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-22 08:37:43 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:37:45 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:37:45 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.10:9862
om_1                | 2023-06-22 08:37:45 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:37:45 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:37:45 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:37:45 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-22 08:37:47 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:37:47 INFO  OzoneManager:3574 - Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:37:47 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:37:47 INFO  Server:1219 - Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:37:47 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:37:47 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:37:47 INFO  MetricsSystemImpl:191 - OzoneManager metrics system started
om_1                | 2023-06-22 08:37:47 INFO  OzoneManager:1114 - OzoneManager RPC server is listening at om/172.20.0.10:9862
om_1                | 2023-06-22 08:37:48 INFO  BaseHttpServer:207 - Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:37:48 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:37:48 INFO  log:169 - Logging initialized @5917ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:37:48 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-22 08:37:48 INFO  HttpRequestLog:86 - Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:37:48 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:37:48 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:37:48 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:37:48 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:37:48 INFO  HttpServer2:1237 - Jetty bound to port 9874
om_1                | 2023-06-22 08:37:48 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
om_1                | 2023-06-22 08:37:48 INFO  session:333 - DefaultSessionIdManager workerName=node0
om_1                | 2023-06-22 08:37:48 INFO  session:338 - No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:37:48 INFO  session:140 - node0 Scavenging every 600000ms
om_1                | 2023-06-22 08:37:48 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@2100d047{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:37:48 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@47be0f9b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:37:48 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@1d247525{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_0_0_jar-_-any-3282525889677480299.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:37:48 INFO  AbstractConnector:330 - Started ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1                | 2023-06-22 08:37:48 INFO  Server:399 - Started @6375ms
om_1                | 2023-06-22 08:37:48 INFO  MetricsSinkAdapter:204 - Sink prometheus started
om_1                | 2023-06-22 08:37:48 INFO  MetricsSystemImpl:301 - Registered sink prometheus
om_1                | 2023-06-22 08:37:48 INFO  BaseHttpServer:327 - HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:37:48 INFO  Server:1460 - IPC Server Responder: starting
om_1                | 2023-06-22 08:37:48 INFO  Server:1298 - IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:37:48 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
om_1                | 2023-06-22 08:38:43 INFO  OMDBCheckpointServlet:101 - Received request to obtain OM DB checkpoint snapshot
om_1                | 2023-06-22 08:38:43 INFO  RDBCheckpointManager:86 - Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1687423123525 in 35 milliseconds
om_1                | 2023-06-22 08:38:43 INFO  OMDBCheckpointServlet:144 - Time taken to write the checkpoint to response output stream: 64 milliseconds
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:37:20 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = ff488ad866e6/172.20.0.7
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.0.0
om_1                | 2023-06-22 08:38:43 INFO  RocksDBCheckpoint:78 - Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1687423123525
om_1                | 2023-06-22 08:38:51 INFO  OMVolumeCreateRequest:195 - created volume:vol1 for user:hadoop
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:37:20 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:37:21 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:37:22 INFO  StorageContainerManager:644 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-2efc32fe-20b6-4973-83a8-7d56971997f8;layoutVersion=0
scm_1               | 2023-06-22 08:37:22 INFO  StorageContainerManagerStarter:124 - SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at ff488ad866e6/172.20.0.7
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:37:34 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = ff488ad866e6/172.20.0.7
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:37:34 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:37:35 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:37:35 WARN  DBStoreBuilder:277 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:37:36 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@5884a914
scm_1               | 2023-06-22 08:37:36 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1               | 2023-06-22 08:37:36 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1               | 2023-06-22 08:37:36 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-22 08:37:37 INFO  SCMPipelineManager:161 - No pipeline exists in current db
scm_1               | 2023-06-22 08:37:37 INFO  HealthyPipelineSafeModeRule:89 - Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:37:37 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:37:37 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
scm_1               | 2023-06-22 08:37:38 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:37:38 INFO  Server:1219 - Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-22 08:37:38 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:37:38 INFO  Server:1219 - Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-22 08:37:38 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:37:38 INFO  Server:1219 - Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-22 08:37:38 INFO  BaseHttpServer:207 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:37:38 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:37:39 INFO  log:169 - Logging initialized @15028ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:37:39 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-22 08:37:39 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:37:39 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:37:39 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:37:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:37:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:37:39 INFO  StorageContainerManager:784 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:37:39 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:37:39 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:37:39 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:37:40 INFO  SCMClientProtocolServer:156 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:37:40 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-22 08:37:40 INFO  Server:1298 - IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:37:40 INFO  StorageContainerManager:796 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:37:40 INFO  SCMBlockProtocolServer:149 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:37:40 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-22 08:37:40 INFO  Server:1298 - IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:37:40 INFO  StorageContainerManager:802 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:37:40 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:37:40 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-22 08:37:40 INFO  Server:1298 - IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:37:40 INFO  HttpServer2:1237 - Jetty bound to port 9876
scm_1               | 2023-06-22 08:37:40 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1               | 2023-06-22 08:37:40 INFO  Server:990 - IPC Server handler 9 on default port 9861: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.11:39116
scm_1               | 2023-06-22 08:37:40 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:37:40 INFO  session:338 - No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:37:40 INFO  session:140 - node0 Scavenging every 660000ms
scm_1               | 2023-06-22 08:37:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@70e02081{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:37:40 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@51b1a8f6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:37:40 WARN  Server:1670 - IPC Server handler 2 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.5:37540: output error
scm_1               | 2023-06-22 08:37:40 WARN  Server:1670 - IPC Server handler 1 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.9:49304: output error
scm_1               | 2023-06-22 08:37:40 INFO  Server:2928 - IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-06-22 08:37:40 INFO  Server:2928 - IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-06-22 08:37:41 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@5300cac{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_0_0_jar-_-any-14308145475971691767.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/scm}
scm_1               | 2023-06-22 08:37:41 INFO  AbstractConnector:330 - Started ServerConnector@16c8b7bd{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:37:41 INFO  Server:399 - Started @17974ms
scm_1               | 2023-06-22 08:37:41 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1               | 2023-06-22 08:37:41 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1               | 2023-06-22 08:37:41 INFO  BaseHttpServer:327 - HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:37:42 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1               | 2023-06-22 08:38:36 INFO  NetworkTopology:111 - Added a new node: /default-rack/7359d79c-a308-41a2-b591-63e1620e32e7
scm_1               | 2023-06-22 08:38:36 INFO  SCMNodeManager:273 - Registered Data node : 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-22 08:38:36 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:38:36 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:38:36 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=75e5d1a3-5198-45a8-8c57-27e665e9fafb to datanode:7359d79c-a308-41a2-b591-63e1620e32e7
scm_1               | 2023-06-22 08:38:36 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 75e5d1a3-5198-45a8-8c57-27e665e9fafb, Nodes: 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:36.591161Z]
scm_1               | 2023-06-22 08:38:37 INFO  NetworkTopology:111 - Added a new node: /default-rack/b89efc6d-7472-423a-976a-f1525a889aa3
scm_1               | 2023-06-22 08:38:37 INFO  SCMNodeManager:273 - Registered Data node : b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-22 08:38:37 INFO  SCMSafeModeManager:71 - SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:38:37 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:38:37 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=11dccb04-a62e-47fe-9b28-df01e0140de8 to datanode:b89efc6d-7472-423a-976a-f1525a889aa3
scm_1               | 2023-06-22 08:38:37 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 11dccb04-a62e-47fe-9b28-df01e0140de8, Nodes: b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:37.030956Z]
scm_1               | 2023-06-22 08:38:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/387acfa3-4677-4ee6-a8f7-8806d41030cc
scm_1               | 2023-06-22 08:38:38 INFO  SCMNodeManager:273 - Registered Data node : 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-22 08:38:38 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=214dfd40-4b1d-42e9-915f-426d7b30e994 to datanode:387acfa3-4677-4ee6-a8f7-8806d41030cc
scm_1               | 2023-06-22 08:38:38 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 214dfd40-4b1d-42e9-915f-426d7b30e994, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:38.209151Z]
scm_1               | 2023-06-22 08:38:38 INFO  SCMSafeModeManager:71 - SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:38:38 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:38:38 INFO  SCMSafeModeManager:214 - DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:38:38 INFO  SCMSafeModeManager:242 - All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:38:38 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 to datanode:387acfa3-4677-4ee6-a8f7-8806d41030cc
scm_1               | 2023-06-22 08:38:38 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 to datanode:7359d79c-a308-41a2-b591-63e1620e32e7
scm_1               | 2023-06-22 08:38:38 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=04a23e87-0fa9-48c2-9a0a-96d39eefa696 to datanode:b89efc6d-7472-423a-976a-f1525a889aa3
scm_1               | 2023-06-22 08:38:38 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 04a23e87-0fa9-48c2-9a0a-96d39eefa696, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:38:38.221006Z]
scm_1               | 2023-06-22 08:38:39 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 75e5d1a3-5198-45a8-8c57-27e665e9fafb, Nodes: 7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:7359d79c-a308-41a2-b591-63e1620e32e7, CreationTimestamp2023-06-22T08:38:36.591161Z] moved to OPEN state
scm_1               | 2023-06-22 08:38:39 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:38:39 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:38:39 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 11dccb04-a62e-47fe-9b28-df01e0140de8, Nodes: b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:b89efc6d-7472-423a-976a-f1525a889aa3, CreationTimestamp2023-06-22T08:38:37.030956Z] moved to OPEN state
scm_1               | 2023-06-22 08:38:39 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:38:39 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:38:41 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 214dfd40-4b1d-42e9-915f-426d7b30e994, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:387acfa3-4677-4ee6-a8f7-8806d41030cc, CreationTimestamp2023-06-22T08:38:38.209151Z] moved to OPEN state
scm_1               | 2023-06-22 08:38:41 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:38:41 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:38:44 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 04a23e87-0fa9-48c2-9a0a-96d39eefa696, Nodes: 387acfa3-4677-4ee6-a8f7-8806d41030cc{ip: 172.20.0.11, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}7359d79c-a308-41a2-b591-63e1620e32e7{ip: 172.20.0.5, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b89efc6d-7472-423a-976a-f1525a889aa3{ip: 172.20.0.9, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:7359d79c-a308-41a2-b591-63e1620e32e7, CreationTimestamp2023-06-22T08:38:38.221006Z] moved to OPEN state
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:214 - HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:38:44 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-06-22 08:39:25 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-22 08:39:35 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-22 08:40:32 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-22 08:40:42 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
Attaching to xcompat_old_client_1_3_0_1, xcompat_datanode_3, xcompat_old_client_1_1_0_1, xcompat_old_client_1_2_1_1, xcompat_datanode_2, xcompat_recon_1, xcompat_datanode_1, xcompat_new_client_1, xcompat_om_1, xcompat_scm_1, xcompat_old_client_1_0_0_1, xcompat_s3g_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:41:03,801 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 02a8f3cfae97/172.21.0.6
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.1.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_1          | STARTUP_MSG:   java = 11.0.10
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:41:03,859 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:41:05,597 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:41:06,229 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:41:07,114 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:41:07,114 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:41:07,657 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:02a8f3cfae97 ip:172.21.0.6
datanode_1          | 2023-06-22 08:41:08,919 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:41:08,926 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-22 08:41:08,934 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:41:08,966 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:41:09,104 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:41:09,353 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:41:09,369 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:41:09,370 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:41:17,412 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:41:17,894 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:41:18,531 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-22 08:41:18,545 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-22 08:41:18,549 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-22 08:41:18,553 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:41:18,556 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:41:18,563 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:41:18,566 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:41:20,469 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-22 08:41:20,494 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:20,505 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:41:20,647 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:41:20,698 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:41:21,554 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:41:21,693 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:41:21,840 [main] INFO util.log: Logging initialized @24001ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:41:22,526 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-22 08:41:22,539 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:41:22,570 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:41:22,579 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:41:22,579 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:41:22,579 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:41:22,761 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-22 08:41:22,769 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_1          | 2023-06-22 08:41:22,981 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:41:22,988 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:41:22,990 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-22 08:41:23,111 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43bdaa1b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:41:23,119 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ea52184{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:41:24,919 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2db33feb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-14518738374965218428/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:41:24,962 [main] INFO server.AbstractConnector: Started ServerConnector@608b1fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:41:24,967 [main] INFO server.Server: Started @27127ms
datanode_1          | 2023-06-22 08:41:24,974 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-22 08:41:24,974 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-22 08:41:24,981 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:41:25,140 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7a05fde1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-22 08:41:25,832 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.7:9891
datanode_1          | 2023-06-22 08:41:26,291 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:41:28,480 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:28,481 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:29,481 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:29,483 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:30,483 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:32,631 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:41:32,636 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-22 08:41:32,887 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:32,939 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.RaftServer: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start RPC server
datanode_1          | 2023-06-22 08:41:32,943 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: GrpcService started, listening on 9856
datanode_1          | 2023-06-22 08:41:32,945 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: GrpcService started, listening on 9857
datanode_1          | 2023-06-22 08:41:32,948 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: GrpcService started, listening on 9858
datanode_1          | 2023-06-22 08:41:32,972 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4 is started using port 9858 for RATIS
datanode_1          | 2023-06-22 08:41:32,972 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-22 08:41:32,972 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-22 08:41:32,991 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@38aa3e01] INFO util.JvmPauseMonitor: JvmPauseMonitor-c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: Started
datanode_1          | 2023-06-22 08:41:35,491 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 02a8f3cfae97/172.21.0.6 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.6:45778 remote=recon/172.21.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.6:45778 remote=recon/172.21.0.7:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_1          | 2023-06-22 08:41:36,305 [Command processor thread] INFO server.RaftServer: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: addNew group-F8E18F5951BC:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:1] returns group-F8E18F5951BC:java.util.concurrent.CompletableFuture@25df1369[Not completed]
datanode_1          | 2023-06-22 08:41:36,341 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: new RaftServerImpl for group-F8E18F5951BC:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:41:36,345 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:41:36,347 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:41:36,348 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:41:36,348 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:36,348 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:41:36,348 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:41:36,349 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:41:36,361 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:41:36,362 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:41:36,373 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:41:36,374 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6fbd2916-7b53-4be9-a2ad-f8e18f5951bc does not exist. Creating ...
datanode_1          | 2023-06-22 08:41:36,414 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6fbd2916-7b53-4be9-a2ad-f8e18f5951bc/in_use.lock acquired by nodename 7@02a8f3cfae97
datanode_1          | 2023-06-22 08:41:36,464 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6fbd2916-7b53-4be9-a2ad-f8e18f5951bc has been successfully formatted.
datanode_1          | 2023-06-22 08:41:36,472 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-F8E18F5951BC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:41:36,513 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:36,530 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:41:36,583 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:41:36,583 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:41:36,592 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC
datanode_1          | 2023-06-22 08:41:36,630 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:36,665 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:41:36,666 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:41:36,670 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6fbd2916-7b53-4be9-a2ad-f8e18f5951bc
datanode_1          | 2023-06-22 08:41:36,671 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:41:36,671 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:41:36,684 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:36,685 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:41:36,685 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:41:36,686 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:41:36,686 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:41:36,687 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:41:36,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:36,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:36,736 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:36,736 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:36,745 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:36,751 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:41:36,751 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:41:36,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:41:36,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:41:36,755 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:41:36,853 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC
datanode_1          | 2023-06-22 08:41:36,861 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC
datanode_1          | 2023-06-22 08:41:36,905 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:41:36,907 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:41:36,908 [pool-19-thread-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState
datanode_1          | 2023-06-22 08:41:36,961 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F8E18F5951BC,id=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:36,962 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC
datanode_1          | 2023-06-22 08:41:37,015 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6fbd2916-7b53-4be9-a2ad-f8e18f5951bc.
datanode_1          | 2023-06-22 08:41:37,016 [Command processor thread] INFO server.RaftServer: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: addNew group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] returns group-DA504C82122D:java.util.concurrent.CompletableFuture@3521bda9[Not completed]
datanode_1          | 2023-06-22 08:41:37,023 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: new RaftServerImpl for group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:41:37,026 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:41:37,026 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:41:37,026 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:41:37,026 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:37,026 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:41:37,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:41:37,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:41:37,027 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:41:37,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:41:37,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:41:37,028 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d does not exist. Creating ...
datanode_1          | 2023-06-22 08:41:37,038 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/in_use.lock acquired by nodename 7@02a8f3cfae97
datanode_1          | 2023-06-22 08:41:37,048 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d has been successfully formatted.
datanode_1          | 2023-06-22 08:41:37,050 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-DA504C82122D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:41:37,050 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:37,052 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:41:37,064 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:41:37,064 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:41:37,064 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D
datanode_1          | 2023-06-22 08:41:37,071 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:37,072 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:41:37,072 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:41:37,072 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d
datanode_1          | 2023-06-22 08:41:37,072 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:41:37,073 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:37,082 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:41:37,083 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:41:37,084 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D
datanode_1          | 2023-06-22 08:41:37,088 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D
datanode_1          | 2023-06-22 08:41:37,089 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:41:37,089 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:41:37,089 [pool-19-thread-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState
datanode_1          | 2023-06-22 08:41:37,090 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA504C82122D,id=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:37,090 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D
datanode_1          | 2023-06-22 08:41:37,246 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5B6BAE44658C->dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_1          | 2023-06-22 08:41:39,892 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:39,908 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3B598EE5AA47->91793454-5c66-4822-8d37-c138f18a6b3c
datanode_1          | 2023-06-22 08:41:40,501 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d.
datanode_1          | 2023-06-22 08:41:40,502 [Command processor thread] INFO server.RaftServer: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: addNew group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0] returns group-09120C0C2CE6:java.util.concurrent.CompletableFuture@da02974[Not completed]
datanode_1          | 2023-06-22 08:41:40,503 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: new RaftServerImpl for group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:41:40,504 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:41:40,504 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:41:40,504 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:41:40,504 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:40,504 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:41:40,505 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:41:40,505 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:41:40,505 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:41:40,505 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:41:40,506 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:41:40,506 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 does not exist. Creating ...
datanode_1          | 2023-06-22 08:41:40,512 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/in_use.lock acquired by nodename 7@02a8f3cfae97
datanode_1          | 2023-06-22 08:41:40,516 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 has been successfully formatted.
datanode_1          | 2023-06-22 08:41:40,621 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-09120C0C2CE6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:41:40,624 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:41:40,624 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:41:40,624 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:41:40,625 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:41:40,625 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6
datanode_1          | 2023-06-22 08:41:40,625 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:40,626 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:41:40,626 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:41:40,627 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6
datanode_1          | 2023-06-22 08:41:40,627 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:41:40,627 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:41:40,631 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:40,631 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:41:40,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:41:40,639 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:40,641 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:40,641 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:41:40,645 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:41:40,645 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:41:40,646 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:41:40,647 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:41:40,647 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:41:40,647 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:41:40,648 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6
datanode_1          | 2023-06-22 08:41:40,648 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6
datanode_1          | 2023-06-22 08:41:40,651 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:41:40,652 [pool-19-thread-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:41:40,656 [pool-19-thread-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:40,656 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-09120C0C2CE6,id=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:40,656 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6
datanode_1          | 2023-06-22 08:41:40,675 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-55CCC8B9DDB7->91793454-5c66-4822-8d37-c138f18a6b3c
datanode_1          | 2023-06-22 08:41:40,953 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-9077A14790C6->dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_1          | 2023-06-22 08:41:41,000 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6.
datanode_1          | 2023-06-22 08:41:42,037 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5129378900ns, electionTimeout:5110ms
datanode_1          | 2023-06-22 08:41:42,037 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState
datanode_1          | 2023-06-22 08:41:42,038 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:41:42,041 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:41:42,041 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1
datanode_1          | 2023-06-22 08:41:42,047 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:41:42,047 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-22 08:41:42,048 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1
datanode_1          | 2023-06-22 08:41:42,052 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:41:42,052 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F8E18F5951BC with new leaderId: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_1          | 2023-06-22 08:41:42,055 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: change Leader from null to c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4 at term 1 for becomeLeader, leader elected after 5539ms
datanode_1          | 2023-06-22 08:41:42,067 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:41:42,070 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC
datanode_1          | 2023-06-22 08:41:42,075 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:41:04,078 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 7f1a3ed57e5e/172.21.0.13
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.1.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_3          | STARTUP_MSG:   java = 11.0.10
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:41:04,122 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:41:05,962 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:41:06,578 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:41:07,452 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:41:07,452 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:41:07,921 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7f1a3ed57e5e ip:172.21.0.13
datanode_3          | 2023-06-22 08:41:09,204 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:41:09,215 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-22 08:41:09,217 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:41:09,270 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:41:09,350 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:41:09,653 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:41:09,653 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:41:09,653 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:41:17,352 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:41:17,924 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:41:18,662 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-22 08:41:18,671 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-22 08:41:18,671 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-22 08:41:18,677 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:41:18,678 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:18,705 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:41:18,712 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:41:20,900 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-22 08:41:20,945 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:21,000 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:41:21,135 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:41:21,155 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:41:21,997 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:41:22,259 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:41:22,451 [main] INFO util.log: Logging initialized @23714ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:41:23,230 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-22 08:41:23,260 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:41:23,288 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:41:23,290 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:41:23,297 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-22 08:41:23,297 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:41:23,527 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-22 08:41:23,537 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_3          | 2023-06-22 08:41:23,787 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:41:23,788 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:41:23,821 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-22 08:41:23,948 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-22 08:41:23,948 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:41:26,042 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-352314253612114112/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:41:26,136 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:41:26,136 [main] INFO server.Server: Started @27399ms
datanode_3          | 2023-06-22 08:41:26,157 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-22 08:41:26,157 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-22 08:41:26,172 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:41:26,328 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4d6f5f30] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-22 08:41:26,916 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.7:9891
datanode_3          | 2023-06-22 08:41:27,250 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:41:29,455 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:41:42,078 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-22 08:41:42,083 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:41:42,091 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:41:42,092 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:41:42,110 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderStateImpl
datanode_1          | 2023-06-22 08:41:42,145 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:41:42,214 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-LeaderElection1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-22 08:41:42,217 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5128129638ns, electionTimeout:5110ms
datanode_1          | 2023-06-22 08:41:42,219 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState
datanode_1          | 2023-06-22 08:41:42,220 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:41:42,221 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:41:42,221 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2
datanode_1          | 2023-06-22 08:41:42,253 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:41:42,443 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-F8E18F5951BC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6fbd2916-7b53-4be9-a2ad-f8e18f5951bc/current/log_inprogress_0
datanode_1          | 2023-06-22 08:41:42,492 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:41:42,493 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection:   Response 0: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:OK-t1
datanode_1          | 2023-06-22 08:41:42,493 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection:   Response 1: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec#0:FAIL-t1
datanode_1          | 2023-06-22 08:41:42,494 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:41:42,495 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-22 08:41:42,495 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2
datanode_1          | 2023-06-22 08:41:42,495 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-LeaderElection2] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState
datanode_1          | 2023-06-22 08:41:45,737 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5080986442ns, electionTimeout:5076ms
datanode_1          | 2023-06-22 08:41:45,737 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:45,740 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:41:45,740 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:41:45,741 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3
datanode_1          | 2023-06-22 08:41:45,746 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:41:45,793 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:41:45,794 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.LeaderElection:   Response 0: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:FAIL-t1
datanode_1          | 2023-06-22 08:41:45,794 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:41:45,794 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-22 08:41:45,794 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3
datanode_1          | 2023-06-22 08:41:45,795 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection3] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:47,485 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: receive requestVote(ELECTION, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, group-DA504C82122D, 2, (t:0, i:0))
datanode_1          | 2023-06-22 08:41:47,487 [grpc-default-executor-1] INFO impl.VoteContext: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FOLLOWER: accept ELECTION from dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:41:47,487 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_1          | 2023-06-22 08:41:47,488 [grpc-default-executor-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState
datanode_1          | 2023-06-22 08:41:47,488 [grpc-default-executor-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState
datanode_1          | 2023-06-22 08:41:47,488 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:41:47,522 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D replies to ELECTION vote request: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec<-c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4#0:OK-t2. Peer's state: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D:t2, leader=null, voted=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, raftlog=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:41:47,666 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA504C82122D with new leaderId: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_1          | 2023-06-22 08:41:47,666 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: change Leader from null to dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec at term 2 for appendEntries, leader elected after 10615ms
datanode_1          | 2023-06-22 08:41:47,724 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-22 08:41:47,724 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:41:47,732 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-DA504C82122D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/current/log_inprogress_0
datanode_1          | 2023-06-22 08:41:50,815 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5019903037ns, electionTimeout:5017ms
datanode_1          | 2023-06-22 08:41:50,815 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:50,818 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-22 08:41:50,818 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:41:50,818 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4
datanode_1          | 2023-06-22 08:41:50,827 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:41:50,840 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:41:50,844 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.LeaderElection:   Response 0: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:FAIL-t2
datanode_1          | 2023-06-22 08:41:50,844 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.LeaderElection: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:41:50,845 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-22 08:41:50,845 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4
datanode_1          | 2023-06-22 08:41:50,845 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-LeaderElection4] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:56,004 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: receive requestVote(ELECTION, 91793454-5c66-4822-8d37-c138f18a6b3c, group-09120C0C2CE6, 3, (t:0, i:0))
datanode_1          | 2023-06-22 08:41:56,006 [grpc-default-executor-1] INFO impl.VoteContext: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FOLLOWER: accept ELECTION from 91793454-5c66-4822-8d37-c138f18a6b3c: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:41:56,006 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:91793454-5c66-4822-8d37-c138f18a6b3c
datanode_1          | 2023-06-22 08:41:56,006 [grpc-default-executor-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: shutdown c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:56,008 [grpc-default-executor-1] INFO impl.RoleInfo: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: start c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState
datanode_1          | 2023-06-22 08:41:56,010 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:41:56,014 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6 replies to ELECTION vote request: 91793454-5c66-4822-8d37-c138f18a6b3c<-c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4#0:OK-t3. Peer's state: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6:t3, leader=null, voted=91793454-5c66-4822-8d37-c138f18a6b3c, raftlog=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:41:56,151 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-09120C0C2CE6 with new leaderId: 91793454-5c66-4822-8d37-c138f18a6b3c
datanode_1          | 2023-06-22 08:41:56,151 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: change Leader from null to 91793454-5c66-4822-8d37-c138f18a6b3c at term 3 for appendEntries, leader elected after 15526ms
datanode_1          | 2023-06-22 08:41:56,176 [grpc-default-executor-1] INFO server.RaftServer$Division: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-22 08:41:56,176 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:41:56,179 [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4@group-09120C0C2CE6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/current/log_inprogress_0
datanode_3          | 2023-06-22 08:41:29,461 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:41:30,456 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:41:30,462 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:41:32,633 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-22 08:41:32,640 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-22 08:41:32,970 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:33,150 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.RaftServer: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start RPC server
datanode_3          | 2023-06-22 08:41:33,163 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: GrpcService started, listening on 9856
datanode_3          | 2023-06-22 08:41:33,188 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: GrpcService started, listening on 9857
datanode_3          | 2023-06-22 08:41:33,193 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: GrpcService started, listening on 9858
datanode_3          | 2023-06-22 08:41:33,213 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec is started using port 9858 for RATIS
datanode_3          | 2023-06-22 08:41:33,213 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-22 08:41:33,213 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-22 08:41:33,214 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@3d39ea06] INFO util.JvmPauseMonitor: JvmPauseMonitor-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: Started
datanode_3          | 2023-06-22 08:41:35,491 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 7f1a3ed57e5e/172.21.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.13:37202 remote=recon/172.21.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.13:37202 remote=recon/172.21.0.7:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_3          | 2023-06-22 08:41:37,407 [Command processor thread] INFO server.RaftServer: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: addNew group-E69F506E2CDC:[dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] returns group-E69F506E2CDC:java.util.concurrent.CompletableFuture@113f40e4[Not completed]
datanode_3          | 2023-06-22 08:41:37,476 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: new RaftServerImpl for group-E69F506E2CDC:[dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:41:37,493 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:41:37,499 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:41:37,499 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:41:37,500 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:37,500 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:41:37,501 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:41:37,501 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:41:37,516 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: ConfigurationManager, init=-1: [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:41:37,541 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:41:37,562 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:41:37,564 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7ca9a268-5757-4e20-b759-e69f506e2cdc does not exist. Creating ...
datanode_3          | 2023-06-22 08:41:37,584 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7ca9a268-5757-4e20-b759-e69f506e2cdc/in_use.lock acquired by nodename 6@7f1a3ed57e5e
datanode_3          | 2023-06-22 08:41:37,612 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7ca9a268-5757-4e20-b759-e69f506e2cdc has been successfully formatted.
datanode_3          | 2023-06-22 08:41:37,634 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-E69F506E2CDC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:41:37,651 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:37,664 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:41:37,728 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:41:37,728 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:37,741 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC
datanode_3          | 2023-06-22 08:41:37,755 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:37,788 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:41:37,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:41:37,817 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7ca9a268-5757-4e20-b759-e69f506e2cdc
datanode_3          | 2023-06-22 08:41:37,824 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:41:37,841 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:41:37,843 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:37,854 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:41:37,855 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:41:37,859 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:41:37,861 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:41:37,863 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:41:37,913 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:37,914 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:37,930 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:37,930 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:37,956 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:37,969 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:41:37,973 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:41:37,974 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:41:37,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:41:37,983 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:41:38,122 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC
datanode_3          | 2023-06-22 08:41:38,130 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC
datanode_3          | 2023-06-22 08:41:38,168 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: start as a follower, conf=-1: [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:41:38,173 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:41:38,178 [pool-19-thread-1] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState
datanode_3          | 2023-06-22 08:41:38,208 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E69F506E2CDC,id=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:38,209 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC
datanode_3          | 2023-06-22 08:41:38,311 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7ca9a268-5757-4e20-b759-e69f506e2cdc.
datanode_3          | 2023-06-22 08:41:38,319 [Command processor thread] INFO server.RaftServer: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: addNew group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] returns group-DA504C82122D:java.util.concurrent.CompletableFuture@1108c496[Not completed]
datanode_3          | 2023-06-22 08:41:38,327 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: new RaftServerImpl for group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:41:38,332 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:41:38,332 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:41:38,336 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:41:38,337 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:38,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:41:38,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:41:38,338 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:41:38,338 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:41:38,415 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:41:38,416 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:41:38,418 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d does not exist. Creating ...
datanode_3          | 2023-06-22 08:41:38,424 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/in_use.lock acquired by nodename 6@7f1a3ed57e5e
datanode_3          | 2023-06-22 08:41:38,428 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d has been successfully formatted.
datanode_3          | 2023-06-22 08:41:38,437 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-DA504C82122D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:41:38,438 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:38,438 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:41:38,440 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:41:38,440 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:38,440 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:38,441 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:38,441 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:41:38,448 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:41:38,448 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d
datanode_3          | 2023-06-22 08:41:38,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:41:38,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:41:38,451 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:38,452 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:41:38,452 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:41:38,452 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:41:38,452 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:41:38,452 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:41:38,453 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:38,468 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:38,471 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:38,471 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:38,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:38,476 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:41:38,477 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:41:38,477 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:41:38,477 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:41:38,477 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:41:38,479 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:38,494 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:38,503 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:41:38,516 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:41:38,517 [pool-19-thread-1] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState
datanode_3          | 2023-06-22 08:41:38,518 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA504C82122D,id=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:38,518 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:38,741 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-8C4E65C0A177->c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_3          | 2023-06-22 08:41:39,327 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:40,127 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CD6078097A4D->91793454-5c66-4822-8d37-c138f18a6b3c
datanode_3          | 2023-06-22 08:41:40,584 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d.
datanode_3          | 2023-06-22 08:41:40,585 [Command processor thread] INFO server.RaftServer: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: addNew group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0] returns group-09120C0C2CE6:java.util.concurrent.CompletableFuture@33caf163[Not completed]
datanode_3          | 2023-06-22 08:41:40,586 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: new RaftServerImpl for group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:41:40,592 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:41:40,593 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:41:40,594 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:41:40,594 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:41:40,594 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:41:40,595 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 does not exist. Creating ...
datanode_3          | 2023-06-22 08:41:40,596 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/in_use.lock acquired by nodename 6@7f1a3ed57e5e
datanode_3          | 2023-06-22 08:41:40,598 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 has been successfully formatted.
datanode_3          | 2023-06-22 08:41:40,600 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-09120C0C2CE6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:41:40,600 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:41:40,600 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:41:40,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:41:40,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:40,602 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6
datanode_3          | 2023-06-22 08:41:40,602 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:40,609 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:41:40,635 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6
datanode_3          | 2023-06-22 08:41:40,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:41:40,636 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:41:40,637 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:40,637 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:41:40,637 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:41:40,638 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:41:40,640 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:41:40,640 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:41:40,644 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:41:40,647 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:40,648 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:40,649 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:41:40,660 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:41:40,665 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:41:40,665 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:41:40,665 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:41:40,667 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:41:40,667 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:41:40,667 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6
datanode_3          | 2023-06-22 08:41:40,668 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6
datanode_3          | 2023-06-22 08:41:40,670 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:41:40,676 [pool-19-thread-1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:41:40,677 [pool-19-thread-1] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:40,681 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-09120C0C2CE6,id=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:40,682 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6
datanode_3          | 2023-06-22 08:41:40,699 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-35EFA7D4C103->91793454-5c66-4822-8d37-c138f18a6b3c
datanode_3          | 2023-06-22 08:41:40,872 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C7C7AACB3829->c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_3          | 2023-06-22 08:41:40,936 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6.
datanode_3          | 2023-06-22 08:41:42,377 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-DA504C82122D, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:41:42,379 [grpc-default-executor-0] INFO impl.VoteContext: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FOLLOWER: reject ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-22 08:41:42,380 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_3          | 2023-06-22 08:41:42,381 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState
datanode_3          | 2023-06-22 08:41:42,382 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-22 08:41:42,382 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState
datanode_3          | 2023-06-22 08:41:42,428 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec#0:FAIL-t1. Peer's state: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D:t1, leader=null, voted=null, raftlog=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:41:43,252 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5074279851ns, electionTimeout:5042ms
datanode_3          | 2023-06-22 08:41:43,253 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState
datanode_3          | 2023-06-22 08:41:43,253 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:41:43,256 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:41:43,258 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-FollowerState] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1
datanode_3          | 2023-06-22 08:41:43,264 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO impl.LeaderElection: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:41:43,265 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO impl.LeaderElection: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-22 08:41:43,265 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1
datanode_3          | 2023-06-22 08:41:43,266 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:41:43,266 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E69F506E2CDC with new leaderId: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:43,269 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: change Leader from null to dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec at term 1 for becomeLeader, leader elected after 5625ms
datanode_3          | 2023-06-22 08:41:43,284 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:41:43,290 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC
datanode_3          | 2023-06-22 08:41:43,293 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:41:43,294 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-22 08:41:43,304 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:41:43,305 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:41:43,309 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:41:43,334 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderStateImpl
datanode_3          | 2023-06-22 08:41:43,366 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:41:43,444 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-LeaderElection1] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC: set configuration 0: [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-22 08:41:43,660 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-E69F506E2CDC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7ca9a268-5757-4e20-b759-e69f506e2cdc/current/log_inprogress_0
datanode_3          | 2023-06-22 08:41:45,787 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-09120C0C2CE6, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:41:45,787 [grpc-default-executor-0] INFO impl.VoteContext: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FOLLOWER: accept ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-22 08:41:45,787 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_3          | 2023-06-22 08:41:45,787 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:45,787 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:45,787 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-22 08:41:45,792 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6 replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec#0:OK-t1. Peer's state: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6:t1, leader=null, voted=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, raftlog=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:41:47,397 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5014611944ns, electionTimeout:5010ms
datanode_3          | 2023-06-22 08:41:47,398 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState
datanode_3          | 2023-06-22 08:41:47,398 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-22 08:41:47,398 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:41:47,398 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-FollowerState] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2
datanode_3          | 2023-06-22 08:41:47,406 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:41:47,494 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:41:47,495 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection:   Response 0: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec<-91793454-5c66-4822-8d37-c138f18a6b3c#0:OK-t2
datanode_3          | 2023-06-22 08:41:47,495 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.LeaderElection: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-06-22 08:41:47,495 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2
datanode_3          | 2023-06-22 08:41:47,495 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-06-22 08:41:47,496 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA504C82122D with new leaderId: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_3          | 2023-06-22 08:41:47,496 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: change Leader from null to dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec at term 2 for becomeLeader, leader elected after 9057ms
datanode_3          | 2023-06-22 08:41:47,496 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:41:47,496 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:47,497 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:41:47,497 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-22 08:41:47,497 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:41:47,498 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:41:47,498 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:41:47,512 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:41:47,512 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:47,513 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:41:47,530 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:41:47,530 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:41:47,531 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:41:47,532 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D
datanode_3          | 2023-06-22 08:41:47,576 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:41:47,577 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:41:47,580 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:41:47,580 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:41:47,581 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:41:47,581 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:41:47,595 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderStateImpl
datanode_3          | 2023-06-22 08:41:47,598 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:41:47,602 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/current/log_inprogress_0
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:41:04,366 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 7063c1ad42e2/172.21.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.1.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | 2023-06-22 08:41:47,627 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D-LeaderElection2] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-DA504C82122D: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-22 08:41:50,830 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-09120C0C2CE6, 2, (t:0, i:0))
datanode_3          | 2023-06-22 08:41:50,831 [grpc-default-executor-0] INFO impl.VoteContext: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FOLLOWER: accept ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-22 08:41:50,832 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_3          | 2023-06-22 08:41:50,832 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:50,832 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-22 08:41:50,834 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:50,846 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6 replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec#0:OK-t2. Peer's state: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6:t2, leader=null, voted=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, raftlog=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:41:55,989 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: receive requestVote(ELECTION, 91793454-5c66-4822-8d37-c138f18a6b3c, group-09120C0C2CE6, 3, (t:0, i:0))
datanode_3          | 2023-06-22 08:41:55,990 [grpc-default-executor-0] INFO impl.VoteContext: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FOLLOWER: accept ELECTION from 91793454-5c66-4822-8d37-c138f18a6b3c: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-22 08:41:55,990 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:91793454-5c66-4822-8d37-c138f18a6b3c
datanode_3          | 2023-06-22 08:41:55,991 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: shutdown dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:55,991 [grpc-default-executor-0] INFO impl.RoleInfo: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: start dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState
datanode_3          | 2023-06-22 08:41:55,996 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-22 08:41:56,025 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6 replies to ELECTION vote request: 91793454-5c66-4822-8d37-c138f18a6b3c<-dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec#0:OK-t3. Peer's state: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6:t3, leader=null, voted=91793454-5c66-4822-8d37-c138f18a6b3c, raftlog=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:41:56,139 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-09120C0C2CE6 with new leaderId: 91793454-5c66-4822-8d37-c138f18a6b3c
datanode_3          | 2023-06-22 08:41:56,140 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: change Leader from null to 91793454-5c66-4822-8d37-c138f18a6b3c at term 3 for appendEntries, leader elected after 15539ms
datanode_3          | 2023-06-22 08:41:56,169 [grpc-default-executor-0] INFO server.RaftServer$Division: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-06-22 08:41:56,170 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:41:56,173 [dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec@group-09120C0C2CE6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/current/log_inprogress_0
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_2          | STARTUP_MSG:   java = 11.0.10
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:41:04,418 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:41:06,232 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:41:06,808 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:41:07,503 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:41:07,520 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:41:08,191 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7063c1ad42e2 ip:172.21.0.10
datanode_2          | 2023-06-22 08:41:09,480 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:41:09,494 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-22 08:41:09,495 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:41:09,533 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:41:09,751 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:41:09,996 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:41:10,009 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:41:10,009 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:41:18,144 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:41:18,594 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:41:19,329 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-22 08:41:19,341 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-22 08:41:19,348 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-22 08:41:19,358 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:41:19,361 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:19,372 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:41:19,376 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:41:21,178 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-22 08:41:21,198 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:21,198 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:41:21,497 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:41:21,501 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:41:22,394 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:41:22,606 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:41:22,760 [main] INFO util.log: Logging initialized @24266ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:41:23,558 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-22 08:41:23,610 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:41:23,661 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:41:23,684 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:41:23,685 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:41:23,685 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:41:24,022 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-22 08:41:24,045 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_2          | 2023-06-22 08:41:24,355 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:41:24,355 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:41:24,363 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-22 08:41:24,582 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43bdaa1b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:41:24,620 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@ea52184{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:41:26,686 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2db33feb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-12275499182215155900/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-22 08:41:26,797 [main] INFO server.AbstractConnector: Started ServerConnector@608b1fd2{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:41:26,797 [main] INFO server.Server: Started @28303ms
datanode_2          | 2023-06-22 08:41:26,836 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-22 08:41:26,836 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-22 08:41:26,851 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:41:26,974 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@60aa8b9b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-22 08:41:27,503 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.7:9891
datanode_2          | 2023-06-22 08:41:27,763 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:41:30,115 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:41:30,128 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.21.0.7:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:41:30,999 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:41:32,583 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-22 08:41:32,593 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-22 08:41:33,005 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:41:33,087 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:33,225 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.RaftServer: 91793454-5c66-4822-8d37-c138f18a6b3c: start RPC server
datanode_2          | 2023-06-22 08:41:33,251 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: 91793454-5c66-4822-8d37-c138f18a6b3c: GrpcService started, listening on 9856
datanode_2          | 2023-06-22 08:41:33,269 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: 91793454-5c66-4822-8d37-c138f18a6b3c: GrpcService started, listening on 9857
datanode_2          | 2023-06-22 08:41:33,295 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO server.GrpcService: 91793454-5c66-4822-8d37-c138f18a6b3c: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:41:33,309 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 91793454-5c66-4822-8d37-c138f18a6b3c is started using port 9858 for RATIS
datanode_2          | 2023-06-22 08:41:33,309 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 91793454-5c66-4822-8d37-c138f18a6b3c is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-22 08:41:33,309 [EndpointStateMachine task thread for scm/172.21.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 91793454-5c66-4822-8d37-c138f18a6b3c is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-22 08:41:33,313 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@753eba2a] INFO util.JvmPauseMonitor: JvmPauseMonitor-91793454-5c66-4822-8d37-c138f18a6b3c: Started
datanode_2          | 2023-06-22 08:41:35,032 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:41:35,162 [EndpointStateMachine task thread for recon/172.21.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 7063c1ad42e2/172.21.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.10:36436 remote=recon/172.21.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:836)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:780)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.10:36436 remote=recon/172.21.0.7:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1880)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
datanode_2          | 2023-06-22 08:41:38,110 [Command processor thread] INFO server.RaftServer: 91793454-5c66-4822-8d37-c138f18a6b3c: addNew group-8BBAD4DF43BF:[91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1] returns group-8BBAD4DF43BF:java.util.concurrent.CompletableFuture@64e77fe9[Not completed]
datanode_2          | 2023-06-22 08:41:38,176 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c: new RaftServerImpl for group-8BBAD4DF43BF:[91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:41:38,185 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:41:38,187 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:41:38,188 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:41:38,188 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:38,188 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:41:38,188 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:41:38,189 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:41:38,200 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: ConfigurationManager, init=-1: [91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:41:38,201 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:41:38,213 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:41:38,218 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/94d1c07d-4e28-4eb3-acdc-8bbad4df43bf does not exist. Creating ...
datanode_2          | 2023-06-22 08:41:38,234 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/94d1c07d-4e28-4eb3-acdc-8bbad4df43bf/in_use.lock acquired by nodename 7@7063c1ad42e2
datanode_2          | 2023-06-22 08:41:38,253 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/94d1c07d-4e28-4eb3-acdc-8bbad4df43bf has been successfully formatted.
datanode_2          | 2023-06-22 08:41:38,296 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-8BBAD4DF43BF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:41:38,312 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:38,315 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:41:38,355 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:41:38,355 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:38,360 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF
datanode_2          | 2023-06-22 08:41:38,423 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,456 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:41:38,457 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:41:38,491 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/94d1c07d-4e28-4eb3-acdc-8bbad4df43bf
datanode_2          | 2023-06-22 08:41:38,497 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:41:38,506 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:41:38,516 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,517 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:41:38,519 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:41:38,534 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:41:38,535 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:41:38,536 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:41:38,554 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,558 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:38,588 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:38,589 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:38,605 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:38,619 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:41:38,621 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:41:38,627 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:41:38,628 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:41:38,630 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:41:38,739 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF
datanode_2          | 2023-06-22 08:41:38,743 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF
datanode_2          | 2023-06-22 08:41:38,784 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: start as a follower, conf=-1: [91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:41:38,793 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:41:38,795 [pool-19-thread-1] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState
datanode_2          | 2023-06-22 08:41:38,818 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8BBAD4DF43BF,id=91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:38,821 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF
datanode_2          | 2023-06-22 08:41:38,884 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=94d1c07d-4e28-4eb3-acdc-8bbad4df43bf.
datanode_2          | 2023-06-22 08:41:38,891 [Command processor thread] INFO server.RaftServer: 91793454-5c66-4822-8d37-c138f18a6b3c: addNew group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] returns group-DA504C82122D:java.util.concurrent.CompletableFuture@5d79e9ac[Not completed]
datanode_2          | 2023-06-22 08:41:38,905 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c: new RaftServerImpl for group-DA504C82122D:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:41:38,910 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:41:38,911 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:41:38,911 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:41:38,915 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:38,922 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:41:38,922 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:41:38,922 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:41:38,924 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:41:38,925 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:41:38,926 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:41:38,928 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d does not exist. Creating ...
datanode_2          | 2023-06-22 08:41:38,938 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/in_use.lock acquired by nodename 7@7063c1ad42e2
datanode_2          | 2023-06-22 08:41:38,941 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d has been successfully formatted.
datanode_2          | 2023-06-22 08:41:38,953 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-DA504C82122D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:41:38,954 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:38,954 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:41:38,954 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:41:38,955 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:38,955 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D
datanode_2          | 2023-06-22 08:41:38,955 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,956 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:41:38,956 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:41:38,956 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d
datanode_2          | 2023-06-22 08:41:38,956 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:41:38,958 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:41:38,959 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,959 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:41:38,960 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:41:38,964 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:41:38,968 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:41:38,968 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:41:38,970 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:38,971 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:38,972 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:38,976 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:38,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:38,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:41:38,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:41:38,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:41:38,979 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:41:38,980 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:41:38,980 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D
datanode_2          | 2023-06-22 08:41:38,987 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D
datanode_2          | 2023-06-22 08:41:38,995 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:41:38,995 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:41:38,995 [pool-19-thread-1] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState
datanode_2          | 2023-06-22 08:41:38,996 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA504C82122D,id=91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:38,996 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D
datanode_2          | 2023-06-22 08:41:39,084 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1B62A70B3E5B->c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_2          | 2023-06-22 08:41:40,283 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:40,663 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1967C2AC0031->dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_2          | 2023-06-22 08:41:40,712 [grpc-default-executor-0] INFO server.RaftServer: 91793454-5c66-4822-8d37-c138f18a6b3c: addNew group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0] returns group-09120C0C2CE6:java.util.concurrent.CompletableFuture@a942375[Not completed]
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c: new RaftServerImpl for group-09120C0C2CE6:[c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:41:40,721 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:41:40,722 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: ConfigurationManager, init=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:41:40,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:41:40,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:41:40,722 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 does not exist. Creating ...
datanode_2          | 2023-06-22 08:41:40,728 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/in_use.lock acquired by nodename 7@7063c1ad42e2
datanode_2          | 2023-06-22 08:41:40,733 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6 has been successfully formatted.
datanode_2          | 2023-06-22 08:41:40,733 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-09120C0C2CE6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:41:40,733 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:41:40,734 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:41:40,734 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:41:40,734 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:40,744 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:40,745 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:40,746 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:41:40,746 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:41:40,746 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6
datanode_2          | 2023-06-22 08:41:40,747 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:41:40,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:41:40,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:40,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:41:40,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:41:40,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:41:40,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:41:40,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:41:40,754 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:41:40,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:40,759 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:40,760 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:41:40,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:41:40,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:41:40,777 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:41:40,778 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:41:40,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:41:40,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:41:40,779 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:40,780 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:40,781 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: start as a follower, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:41:40,781 [pool-19-thread-1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:41:40,784 [pool-19-thread-1] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:40,784 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-09120C0C2CE6,id=91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:40,785 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:40,866 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d.
datanode_2          | 2023-06-22 08:41:42,384 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-DA504C82122D, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:41:42,386 [grpc-default-executor-0] INFO impl.VoteContext: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FOLLOWER: accept ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:41:42,387 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_2          | 2023-06-22 08:41:42,387 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState
datanode_2          | 2023-06-22 08:41:42,398 [91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:41:42,399 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState
datanode_2          | 2023-06-22 08:41:42,461 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:OK-t1. Peer's state: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D:t1, leader=null, voted=c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, raftlog=91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:41:43,852 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057102918ns, electionTimeout:5030ms
datanode_2          | 2023-06-22 08:41:43,852 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState
datanode_2          | 2023-06-22 08:41:43,853 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:41:43,855 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:41:43,856 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-FollowerState] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1
datanode_2          | 2023-06-22 08:41:43,867 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO impl.LeaderElection: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:41:43,869 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO impl.LeaderElection: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-22 08:41:43,869 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1
datanode_2          | 2023-06-22 08:41:43,870 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:41:43,870 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8BBAD4DF43BF with new leaderId: 91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:43,871 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: change Leader from null to 91793454-5c66-4822-8d37-c138f18a6b3c at term 1 for becomeLeader, leader elected after 5566ms
datanode_2          | 2023-06-22 08:41:43,886 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:41:43,888 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF
datanode_2          | 2023-06-22 08:41:43,894 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:41:43,894 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-22 08:41:43,943 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:41:43,943 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:41:43,944 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:41:43,964 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderStateImpl
datanode_2          | 2023-06-22 08:41:43,991 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:41:44,026 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-LeaderElection1] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF: set configuration 0: [91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-22 08:41:44,137 [91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-8BBAD4DF43BF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/94d1c07d-4e28-4eb3-acdc-8bbad4df43bf/current/log_inprogress_0
datanode_2          | 2023-06-22 08:41:45,752 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-09120C0C2CE6, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:41:45,752 [grpc-default-executor-0] INFO impl.VoteContext: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FOLLOWER: reject ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-22 08:41:45,753 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_2          | 2023-06-22 08:41:45,753 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:45,753 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:45,753 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:41:45,769 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6 replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:FAIL-t1. Peer's state: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6:t1, leader=null, voted=null, raftlog=91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:41:47,476 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: receive requestVote(ELECTION, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, group-DA504C82122D, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:41:47,477 [grpc-default-executor-0] INFO impl.VoteContext: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FOLLOWER: accept ELECTION from dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:41:47,477 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_2          | 2023-06-22 08:41:47,477 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState
datanode_2          | 2023-06-22 08:41:47,478 [91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:41:47,478 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-FollowerState
datanode_2          | 2023-06-22 08:41:47,488 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D replies to ELECTION vote request: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec<-91793454-5c66-4822-8d37-c138f18a6b3c#0:OK-t2. Peer's state: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D:t2, leader=null, voted=dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, raftlog=91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:41:47,649 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA504C82122D with new leaderId: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
datanode_2          | 2023-06-22 08:41:47,656 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: change Leader from null to dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec at term 2 for appendEntries, leader elected after 8695ms
datanode_2          | 2023-06-22 08:41:47,714 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:0, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-22 08:41:47,725 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:41:47,735 [91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-DA504C82122D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/37dff028-c4e3-464b-9153-da504c82122d/current/log_inprogress_0
datanode_2          | 2023-06-22 08:41:50,832 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: receive requestVote(ELECTION, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, group-09120C0C2CE6, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:41:50,832 [grpc-default-executor-0] INFO impl.VoteContext: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FOLLOWER: reject ELECTION from c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-22 08:41:50,832 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
datanode_2          | 2023-06-22 08:41:50,833 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:50,833 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:41:50,833 [grpc-default-executor-0] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:50,836 [grpc-default-executor-0] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6 replies to ELECTION vote request: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4<-91793454-5c66-4822-8d37-c138f18a6b3c#0:FAIL-t2. Peer's state: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6:t2, leader=null, voted=null, raftlog=91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:41:55,928 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO impl.FollowerState: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5094708596ns, electionTimeout:5092ms
datanode_2          | 2023-06-22 08:41:55,928 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState
datanode_2          | 2023-06-22 08:41:55,929 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-06-22 08:41:55,932 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:41:55,932 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-FollowerState] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2
datanode_2          | 2023-06-22 08:41:55,953 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.LeaderElection: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2 ELECTION round 0: submit vote requests at term 3 for -1: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:41:56,028 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.LeaderElection: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:41:56,034 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.LeaderElection:   Response 0: 91793454-5c66-4822-8d37-c138f18a6b3c<-c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4#0:OK-t3
datanode_2          | 2023-06-22 08:41:56,034 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.LeaderElection: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2 ELECTION round 0: result PASSED
datanode_2          | 2023-06-22 08:41:56,034 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: shutdown 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2
datanode_2          | 2023-06-22 08:41:56,034 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_2          | 2023-06-22 08:41:56,034 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-09120C0C2CE6 with new leaderId: 91793454-5c66-4822-8d37-c138f18a6b3c
datanode_2          | 2023-06-22 08:41:56,035 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: change Leader from null to 91793454-5c66-4822-8d37-c138f18a6b3c at term 3 for becomeLeader, leader elected after 15301ms
datanode_2          | 2023-06-22 08:41:56,035 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:41:56,035 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:56,036 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:41:56,036 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-22 08:41:56,036 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:41:56,040 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:41:56,044 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:41:56,054 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:41:56,055 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:56,060 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:41:56,067 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:41:56,069 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:41:56,070 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:41:56,070 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6
datanode_2          | 2023-06-22 08:41:56,075 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:41:56,075 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:41:56,075 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:41:56,076 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:41:56,076 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:41:56,076 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:41:56,079 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO impl.RoleInfo: 91793454-5c66-4822-8d37-c138f18a6b3c: start 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderStateImpl
datanode_2          | 2023-06-22 08:41:56,081 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:41:56,084 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-LeaderElection2] INFO server.RaftServer$Division: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6: set configuration 0: [c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4|rpc:172.21.0.6:9856|admin:172.21.0.6:9857|client:172.21.0.6:9858|dataStream:|priority:0, 91793454-5c66-4822-8d37-c138f18a6b3c|rpc:172.21.0.10:9856|admin:172.21.0.10:9857|client:172.21.0.10:9858|dataStream:|priority:1, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:41:56,084 [91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 91793454-5c66-4822-8d37-c138f18a6b3c@group-09120C0C2CE6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1a63873b-3deb-44d5-ba30-09120c0c2ce6/current/log_inprogress_0
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:41:02,254 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 1f46ef874f11/172.21.0.7
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.1.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.1.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | STARTUP_MSG:   java = 11.0.10
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:41:02,338 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:41:06,599 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-22 08:41:08,445 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-22 08:41:09,892 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:41:17,436 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:41:19,466 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:41:19,581 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:41:19,585 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-22 08:41:25,084 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:41:25,236 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-22 08:41:25,344 [main] INFO util.log: Logging initialized @27451ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:41:25,826 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-22 08:41:25,843 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-22 08:41:25,867 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:41:25,884 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:41:25,884 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:41:25,884 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:41:26,596 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:41:27,800 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:41:27,843 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-22 08:41:27,966 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:41:27,966 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:41:28,488 [main] INFO Configuration.deprecation: No unit for ozone.recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-22 08:41:28,965 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:41:29,325 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:41:29,388 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:41:29,402 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:41:29,611 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:41:29,754 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-22 08:41:29,803 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:41:29,814 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-22 08:41:29,851 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:41:29,869 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:41:29,947 [Listener at 0.0.0.0/9891] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1             | 2023-06-22 08:41:29,985 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-22 08:41:29,985 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-22 08:41:30,194 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:41:30,247 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:41:30,247 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-22 08:41:30,685 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-22 08:41:30,693 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1             | 2023-06-22 08:41:30,791 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:41:30,791 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:41:30,794 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:41:30,826 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7bc6935c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:41:30,828 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47248a48{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-22 08:41:34,547 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@33e8694b{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_1_0_jar-_-any-10758470312441614128/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/recon}
recon_1             | 2023-06-22 08:41:34,582 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6418e39e{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:41:34,583 [Listener at 0.0.0.0/9891] INFO server.Server: Started @36692ms
recon_1             | 2023-06-22 08:41:34,587 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-22 08:41:34,587 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-22 08:41:34,589 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:41:34,589 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:41:34,603 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:41:34,614 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:41:34,614 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:41:34,614 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:41:34,614 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-22 08:41:34,615 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:41:34,902 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 2 pipelines from SCM.
recon_1             | 2023-06-22 08:41:34,902 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:41:34,903 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=7ca9a268-5757-4e20-b759-e69f506e2cdc from SCM.
recon_1             | 2023-06-22 08:41:34,918 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7ca9a268-5757-4e20-b759-e69f506e2cdc, Nodes: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:34.413Z]
recon_1             | 2023-06-22 08:41:34,922 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6fbd2916-7b53-4be9-a2ad-f8e18f5951bc from SCM.
recon_1             | 2023-06-22 08:41:34,924 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6fbd2916-7b53-4be9-a2ad-f8e18f5951bc, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:33.700Z]
recon_1             | 2023-06-22 08:41:34,925 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:41:34,969 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-22 08:41:34,985 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:41:35,037 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-22 08:41:35,038 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-22 08:41:35,134 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 2 pipelines in house.
recon_1             | 2023-06-22 08:41:35,144 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=94d1c07d-4e28-4eb3-acdc-8bbad4df43bf from SCM.
recon_1             | 2023-06-22 08:41:35,145 [PipelineSyncTask] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 94d1c07d-4e28-4eb3-acdc-8bbad4df43bf, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.079Z]
recon_1             | 2023-06-22 08:41:35,155 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=37dff028-c4e3-464b-9153-da504c82122d from SCM.
recon_1             | 2023-06-22 08:41:35,156 [PipelineSyncTask] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 37dff028-c4e3-464b-9153-da504c82122d, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.097Z]
recon_1             | 2023-06-22 08:41:35,164 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 from SCM.
recon_1             | 2023-06-22 08:41:35,172 [PipelineSyncTask] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1a63873b-3deb-44d5-ba30-09120c0c2ce6, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.100Z]
recon_1             | 2023-06-22 08:41:35,173 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 99 milliseconds.
recon_1             | 2023-06-22 08:41:35,175 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-22 08:41:35,181 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-22 08:41:35,363 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 155 milliseconds to process 0 existing database records.
recon_1             | 2023-06-22 08:41:35,382 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 18 milliseconds for processing 0 containers.
recon_1             | 2023-06-22 08:41:35,577 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.10:36436: output error
recon_1             | 2023-06-22 08:41:35,577 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.6:45778: output error
recon_1             | 2023-06-22 08:41:35,579 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-06-22 08:41:35,580 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-06-22 08:41:35,580 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.21.0.13:37202: output error
recon_1             | 2023-06-22 08:41:35,585 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.AsynchronousCloseException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3594)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1657)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1727)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2828)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1799)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:889)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1046)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:41:06,399 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = c7983575c117/172.21.0.3
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:41:04,037 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 9900bf583029/172.21.0.9
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:41:06,616 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:41:07,252 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:41:07,690 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-7e2e2361-88c5-4888-8bb1-0d46f9795aa3;layoutVersion=0
scm_1               | 2023-06-22 08:41:07,812 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at c7983575c117/172.21.0.3
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:41:21,197 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = c7983575c117/172.21.0.3
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:41:21,300 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:41:22,301 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:41:24,040 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:41:24,952 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-22 08:41:24,956 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-22 08:41:25,575 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-22 08:41:26,466 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-22 08:41:26,590 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-22 08:41:26,612 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1               | 2023-06-22 08:41:26,759 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-22 08:41:26,998 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:41:27,009 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:41:29,448 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:41:29,501 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-22 08:41:29,593 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:41:29,594 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-22 08:41:29,629 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:41:29,640 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-22 08:41:29,795 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:41:30,052 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:41:30,107 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:41:30,107 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:41:30,771 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:41:30,771 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:41:30,812 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:41:30,982 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:41:30,988 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:41:31,008 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:41:31,017 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:41:31,348 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:41:31,387 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:41:31,396 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:41:31,397 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:41:31,689 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4a14c44f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-22 08:41:31,713 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:41:31,713 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:41:31,846 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @22004ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:41:32,650 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-22 08:41:32,658 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:41:32,821 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:41:32,831 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:41:32,843 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:41:32,844 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:41:33,121 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-22 08:41:33,133 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm_1               | 2023-06-22 08:41:33,498 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
scm_1               | 2023-06-22 08:41:33,518 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:41:33,545 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:41:33,545 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:41:33,546 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:41:33,548 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-22 08:41:33,632 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:41:33,657 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:33,713 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6fbd2916-7b53-4be9-a2ad-f8e18f5951bc to datanode:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
scm_1               | 2023-06-22 08:41:33,719 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4339baec{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:41:33,728 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bc7e78e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:41:33,777 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6fbd2916-7b53-4be9-a2ad-f8e18f5951bc, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:33.700093Z]
scm_1               | 2023-06-22 08:41:34,403 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
scm_1               | 2023-06-22 08:41:34,405 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:41:34,406 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:41:34,413 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7ca9a268-5757-4e20-b759-e69f506e2cdc to datanode:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
scm_1               | 2023-06-22 08:41:34,417 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7ca9a268-5757-4e20-b759-e69f506e2cdc, Nodes: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:34.413521Z]
scm_1               | 2023-06-22 08:41:34,408 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:41:34,424 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:34,632 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5843db{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_1_0_jar-_-any-8757478965189509590/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/scm}
scm_1               | 2023-06-22 08:41:34,670 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@68a4dcc6{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:41:34,672 [Listener at 0.0.0.0/9860] INFO server.Server: Started @24829ms
scm_1               | 2023-06-22 08:41:34,684 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-22 08:41:34,684 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-22 08:41:34,689 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:41:35,078 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/91793454-5c66-4822-8d37-c138f18a6b3c
scm_1               | 2023-06-22 08:41:35,078 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:41:35,079 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:35,079 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:41:35,079 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:41:35,079 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:41:35,079 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:41:35,080 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=94d1c07d-4e28-4eb3-acdc-8bbad4df43bf to datanode:91793454-5c66-4822-8d37-c138f18a6b3c
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
recon_1             | 2023-06-22 08:41:36,396 [IPC Server handler 1 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
recon_1             | 2023-06-22 08:41:36,419 [IPC Server handler 1 on default port 9891] INFO node.SCMNodeManager: Registered Data node : dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:36,557 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec to Node DB.
recon_1             | 2023-06-22 08:41:36,548 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
recon_1             | 2023-06-22 08:41:36,566 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4 to Node DB.
recon_1             | 2023-06-22 08:41:36,568 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:36,576 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=6fbd2916-7b53-4be9-a2ad-f8e18f5951bc reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:36,577 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6fbd2916-7b53-4be9-a2ad-f8e18f5951bc, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, CreationTimestamp2023-06-22T08:41:33.700Z] moved to OPEN state
recon_1             | 2023-06-22 08:41:37,041 [IPC Server handler 99 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/91793454-5c66-4822-8d37-c138f18a6b3c
recon_1             | 2023-06-22 08:41:37,042 [IPC Server handler 99 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:37,043 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 91793454-5c66-4822-8d37-c138f18a6b3c to Node DB.
recon_1             | 2023-06-22 08:41:37,077 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:37,718 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=7ca9a268-5757-4e20-b759-e69f506e2cdc reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:37,718 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 7ca9a268-5757-4e20-b759-e69f506e2cdc, Nodes: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, CreationTimestamp2023-06-22T08:41:34.413Z] moved to OPEN state
recon_1             | 2023-06-22 08:41:38,324 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=94d1c07d-4e28-4eb3-acdc-8bbad4df43bf reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:38,324 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 94d1c07d-4e28-4eb3-acdc-8bbad4df43bf, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:91793454-5c66-4822-8d37-c138f18a6b3c, CreationTimestamp2023-06-22T08:41:35.079Z] moved to OPEN state
recon_1             | 2023-06-22 08:41:38,471 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:38,946 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:40,604 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:41:35,080 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 94d1c07d-4e28-4eb3-acdc-8bbad4df43bf, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.079986Z]
scm_1               | 2023-06-22 08:41:35,097 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=37dff028-c4e3-464b-9153-da504c82122d to datanode:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
scm_1               | 2023-06-22 08:41:35,097 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=37dff028-c4e3-464b-9153-da504c82122d to datanode:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
scm_1               | 2023-06-22 08:41:35,098 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=37dff028-c4e3-464b-9153-da504c82122d to datanode:91793454-5c66-4822-8d37-c138f18a6b3c
scm_1               | 2023-06-22 08:41:35,098 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 37dff028-c4e3-464b-9153-da504c82122d, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.097475Z]
scm_1               | 2023-06-22 08:41:35,100 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 to datanode:91793454-5c66-4822-8d37-c138f18a6b3c
scm_1               | 2023-06-22 08:41:35,100 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 to datanode:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4
scm_1               | 2023-06-22 08:41:35,100 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 to datanode:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
scm_1               | 2023-06-22 08:41:35,101 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1a63873b-3deb-44d5-ba30-09120c0c2ce6, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:41:35.100066Z]
scm_1               | 2023-06-22 08:41:35,102 [RatisPipelineUtilsThread] INFO pipeline.SCMPipelineManager: Pipeline: PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 contains same datanodes as previous pipelines: PipelineID=37dff028-c4e3-464b-9153-da504c82122d nodeIds: 91793454-5c66-4822-8d37-c138f18a6b3c, c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec
scm_1               | 2023-06-22 08:41:36,541 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:36,567 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6fbd2916-7b53-4be9-a2ad-f8e18f5951bc, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4, CreationTimestamp2023-06-22T08:41:33.700093Z] moved to OPEN state
scm_1               | 2023-06-22 08:41:36,571 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:37,085 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:37,085 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:37,731 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 7ca9a268-5757-4e20-b759-e69f506e2cdc, Nodes: dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, CreationTimestamp2023-06-22T08:41:34.413521Z] moved to OPEN state
scm_1               | 2023-06-22 08:41:37,731 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:37,731 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:38,331 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:38,331 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 94d1c07d-4e28-4eb3-acdc-8bbad4df43bf, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:91793454-5c66-4822-8d37-c138f18a6b3c, CreationTimestamp2023-06-22T08:41:35.079986Z] moved to OPEN state
scm_1               | 2023-06-22 08:41:38,333 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:38,451 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:38,454 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:38,952 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:38,953 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:40,632 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:40,634 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:40,633 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:40,640 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:40,737 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:40,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:42,071 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:42,072 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:43,288 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:43,288 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:43,873 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:43,873 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:47,507 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:41:47,510 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:47,510 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 37dff028-c4e3-464b-9153-da504c82122d, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, CreationTimestamp2023-06-22T08:41:35.097475Z] moved to OPEN state
scm_1               | 2023-06-22 08:41:47,513 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:41:47,513 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:41:47,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:41:47,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-22 08:41:56,053 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1a63873b-3deb-44d5-ba30-09120c0c2ce6, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:91793454-5c66-4822-8d37-c138f18a6b3c, CreationTimestamp2023-06-22T08:41:35.100066Z] moved to OPEN state
scm_1               | 2023-06-22 08:42:27,882 [IPC Server handler 18 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.5
scm_1               | 2023-06-22 08:42:38,877 [IPC Server handler 18 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.5
scm_1               | 2023-06-22 08:43:38,903 [IPC Server handler 7 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.5
scm_1               | 2023-06-22 08:43:49,345 [IPC Server handler 86 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.5
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-22 08:41:04,102 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:41:12,421 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:41:12,724 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-22 08:41:12,725 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:41:12,725 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:41:12,767 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:41:15,721 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:16,723 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:17,724 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:18,733 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:19,733 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:20,735 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:21,736 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:22,737 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:23,738 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:24,739 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.3:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-22 08:41:24,742 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-7e2e2361-88c5-4888-8bb1-0d46f9795aa3;layoutVersion=0
om_1                | 2023-06-22 08:41:32,649 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 9900bf583029/172.21.0.9
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:41:35,598 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 9900bf583029/172.21.0.9
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-22 08:41:35,609 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:41:38,450 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:41:38,603 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-22 08:41:38,613 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:41:38,613 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:41:38,638 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:41:38,689 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:41:41,645 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:41:41,881 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:41:41,882 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:41:42,169 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:41:42,179 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:41:42,179 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-22 08:41:42,216 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:41:42,327 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:41:42,473 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872
om_1                | 2023-06-22 08:41:42,509 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-22 08:41:42,538 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-22 08:41:42,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-22 08:41:42,625 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:41:42,629 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-22 08:41:42,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:41:42,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:41:42,631 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-22 08:41:42,634 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:41:42,639 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-22 08:41:42,640 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:41:43,116 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-22 08:41:43,118 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:41:43,119 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:41:43,141 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:41:43,151 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@3f598450[Not completed]
om_1                | 2023-06-22 08:41:43,151 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-22 08:41:43,218 [pool-17-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-22 08:41:43,220 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-22 08:41:43,226 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-22 08:41:43,226 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-22 08:41:43,226 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:41:43,226 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:41:43,226 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-22 08:41:43,227 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-22 08:41:43,231 [pool-17-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-22 08:41:43,234 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:41:43,241 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:41:43,245 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-22 08:41:43,255 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-22 08:41:43,267 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:41:43,318 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@9900bf583029
om_1                | 2023-06-22 08:41:43,396 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-22 08:41:43,404 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-22 08:41:43,414 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-22 08:41:43,437 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-22 08:41:43,445 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:41:43,465 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-C5BA1605619E
om_1                | 2023-06-22 08:41:43,535 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:41:43,565 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-22 08:41:43,566 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-22 08:41:43,631 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-22 08:41:43,635 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:41:43,640 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-22 08:41:43,641 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:41:43,642 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-22 08:41:43,642 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-22 08:41:43,650 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-22 08:41:43,651 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-22 08:41:43,656 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-22 08:41:43,697 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-22 08:41:43,698 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-22 08:41:43,711 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:41:43,712 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:41:43,719 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-22 08:41:43,720 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-22 08:41:43,720 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-22 08:41:43,721 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-22 08:41:43,724 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-22 08:41:43,724 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-22 08:41:43,785 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-C5BA1605619E
om_1                | 2023-06-22 08:41:43,794 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:41:43,800 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-C5BA1605619E
recon_1             | 2023-06-22 08:41:40,604 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:40,625 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:40,626 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:40,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:40,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:42,058 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:42,058 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:43,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:43,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:43,874 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:43,874 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:47,529 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=37dff028-c4e3-464b-9153-da504c82122d reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:47,530 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 37dff028-c4e3-464b-9153-da504c82122d, Nodes: c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec, CreationTimestamp2023-06-22T08:41:35.097Z] moved to OPEN state
recon_1             | 2023-06-22 08:41:47,531 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:55,301 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:55,303 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:41:55,365 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-22 08:41:55,473 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:55,587 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:56,058 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1a63873b-3deb-44d5-ba30-09120c0c2ce6 reported by 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:41:56,058 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1a63873b-3deb-44d5-ba30-09120c0c2ce6, Nodes: 91793454-5c66-4822-8d37-c138f18a6b3c{ip: 172.21.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}c28aa5ac-7785-4cef-82e6-f53cd6bf6ae4{ip: 172.21.0.6, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}dbf0c3e4-48c4-4c7f-97a7-31fb68afb3ec{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:91793454-5c66-4822-8d37-c138f18a6b3c, CreationTimestamp2023-06-22T08:41:35.100Z] moved to OPEN state
recon_1             | 2023-06-22 08:42:05,881 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:42:05,891 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:42:16,466 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:42:16,479 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-22 08:42:34,615 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:42:34,616 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:42:35,133 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687423354616
recon_1             | 2023-06-22 08:42:35,146 [pool-14-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:42:35,147 [pool-14-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:42:35,188 [pool-14-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687423354616.
recon_1             | 2023-06-22 08:42:35,220 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:42:35,488 [pool-15-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-22 08:42:35,492 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:42:35,538 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1687423355500
recon_1             | 2023-06-22 08:42:35,540 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container key DB at /data/metadata/recon/recon-container-key.db_1687423269008.
recon_1             | 2023-06-22 08:42:35,688 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:42:35,688 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.195 seconds to process 4 keys.
recon_1             | 2023-06-22 08:42:35,724 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-22 08:42:35,751 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
om_1                | 2023-06-22 08:41:43,826 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:41:43,826 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-22 08:41:43,904 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.21.0.9:9862
om_1                | 2023-06-22 08:41:43,905 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-22 08:41:43,906 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-22 08:41:43,908 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-22 08:41:43,911 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:41:43,915 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-22 08:41:43,918 [Listener at om/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-C5BA1605619E
om_1                | 2023-06-22 08:41:43,938 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-22 08:41:44,031 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-22 08:41:44,043 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$354/0x0000000840484440@c1050f2] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-22 08:41:44,118 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:41:44,119 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:41:44,169 [Listener at om/9862] INFO util.log: Logging initialized @10752ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:41:44,294 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-22 08:41:44,298 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:41:44,303 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:41:44,305 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:41:44,305 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:41:44,306 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:41:44,343 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-22 08:41:44,345 [Listener at om/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om_1                | 2023-06-22 08:41:44,411 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-22 08:41:44,411 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:41:44,413 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-22 08:41:44,439 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c25cfe1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:41:44,440 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a083b96{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:41:44,748 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@740a0d5e{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_1_0_jar-_-any-11121347532858324303/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:41:44,759 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7c96c85{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-22 08:41:44,761 [Listener at om/9862] INFO server.Server: Started @11344ms
om_1                | 2023-06-22 08:41:44,775 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-22 08:41:44,776 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-22 08:41:44,778 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:41:44,780 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-22 08:41:44,785 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:41:44,810 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-22 08:41:44,826 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5c313224] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-22 08:41:49,113 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5203173319ns, electionTimeout:5177ms
om_1                | 2023-06-22 08:41:49,115 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:41:49,116 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-22 08:41:49,119 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-22 08:41:49,119 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:41:49,130 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-22 08:41:49,131 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-22 08:41:49,131 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:41:49,132 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-22 08:41:49,133 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5728ms
om_1                | 2023-06-22 08:41:49,147 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-22 08:41:49,151 [om1@group-C5BA1605619E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-C5BA1605619E
om_1                | 2023-06-22 08:41:49,160 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:41:49,160 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:41:49,177 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-22 08:41:49,177 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-22 08:41:49,177 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-22 08:41:49,213 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-22 08:41:49,345 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-22 08:41:49,436 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-22 08:41:49,547 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-22 08:41:52,817 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-22 08:42:34,957 [qtp628402659-39] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-22 08:42:34,990 [qtp628402659-39] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423354962 in 27 milliseconds
om_1                | 2023-06-22 08:42:35,033 [qtp628402659-39] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 41 milliseconds
om_1                | 2023-06-22 08:42:35,034 [qtp628402659-39] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423354962
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-22 08:41:04,611 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-22 08:41:04,636 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-22 08:41:04,795 [main] INFO util.log: Logging initialized @7231ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-22 08:41:05,504 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-22 08:41:05,679 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-22 08:41:05,719 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-22 08:41:05,737 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-22 08:41:05,737 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-22 08:41:05,737 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-22 08:41:06,248 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 406cd259e755/172.21.0.4
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.1.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
s3g_1               | STARTUP_MSG:   java = 11.0.10
s3g_1               | ************************************************************/
s3g_1               | 2023-06-22 08:41:06,337 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-22 08:41:06,581 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-22 08:41:06,662 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-22 08:41:06,678 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1               | 2023-06-22 08:41:06,972 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-22 08:41:06,987 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-22 08:41:07,017 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-22 08:41:07,148 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@19b843ba{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-22 08:41:07,157 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b0d80ed{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 22, 2023 8:41:27 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-22 08:41:27,304 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7fe8c7db{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_1_0_jar-_-any-11083724931885875783/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-22 08:41:27,380 [main] INFO server.AbstractConnector: Started ServerConnector@51c693d{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-22 08:41:27,381 [main] INFO server.Server: Started @29818ms
s3g_1               | 2023-06-22 08:41:27,388 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
Attaching to xcompat_old_client_1_0_0_1, xcompat_datanode_2, xcompat_old_client_1_1_0_1, xcompat_datanode_3, xcompat_datanode_1, xcompat_s3g_1, xcompat_scm_1, xcompat_old_client_1_2_1_1, xcompat_recon_1, xcompat_new_client_1, xcompat_old_client_1_3_0_1, xcompat_om_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:44:13,357 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 2f8b727981ec/172.22.0.13
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.2.1
datanode_1          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_1          | STARTUP_MSG:   java = 11.0.13
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:44:13,431 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:44:15,052 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:44:15,455 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:44:16,304 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:44:16,304 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:44:17,116 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2f8b727981ec ip:172.22.0.13
datanode_1          | 2023-06-22 08:44:18,549 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_1          | 2023-06-22 08:44:19,285 [main] INFO reflections.Reflections: Reflections took 613 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_1          | 2023-06-22 08:44:21,022 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:44:21,111 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-22 08:44:21,121 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:44:21,126 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:44:21,219 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:44:21,435 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:44:21,443 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-22 08:44:21,470 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-22 08:44:21,471 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-22 08:44:21,495 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-22 08:44:21,669 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:44:21,678 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:44:29,943 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:44:30,394 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:44:31,437 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-22 08:44:31,438 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-22 08:44:31,438 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-22 08:44:31,453 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:44:31,460 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:44:31,464 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:44:31,478 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:44:33,579 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-22 08:44:33,584 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:33,597 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:44:33,718 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:44:34,765 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:44:34,903 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:44:35,047 [main] INFO util.log: Logging initialized @27727ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:44:35,629 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-22 08:44:35,682 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:44:35,687 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:44:35,706 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:44:35,708 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:44:35,712 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:44:35,978 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-22 08:44:35,994 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_1          | 2023-06-22 08:44:36,235 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:44:36,264 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:44:36,266 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-22 08:44:36,368 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a1b8a46{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:44:36,369 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@40d52be7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:44:38,831 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@45a1d057{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-11588437908072817816/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:44:38,924 [main] INFO server.AbstractConnector: Started ServerConnector@322e49ee{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:44:38,951 [main] INFO server.Server: Started @31632ms
datanode_1          | 2023-06-22 08:44:38,954 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-22 08:44:38,954 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-22 08:44:38,958 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:44:38,964 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-22 08:44:39,235 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@402ce966] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-22 08:44:39,958 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.4:9891
datanode_1          | 2023-06-22 08:44:40,163 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:44:42,665 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:44:42,668 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:44:43,669 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:44:44,670 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:44:45,671 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:44:47,635 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:44:47,636 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-22 08:44:47,771 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 2f8b727981ec/172.22.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.13:45008 remote=recon/172.22.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.13:45008 remote=recon/172.22.0.4:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-06-22 08:44:47,947 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:44:48,095 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.RaftServer: 1819c6fd-24b7-4837-96e6-3b133f381369: start RPC server
datanode_1          | 2023-06-22 08:44:48,105 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 1819c6fd-24b7-4837-96e6-3b133f381369: GrpcService started, listening on 9856
datanode_1          | 2023-06-22 08:44:48,108 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 1819c6fd-24b7-4837-96e6-3b133f381369: GrpcService started, listening on 9857
datanode_1          | 2023-06-22 08:44:48,109 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 1819c6fd-24b7-4837-96e6-3b133f381369: GrpcService started, listening on 9858
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:44:12,753 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 9370495d315d/172.22.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.2.1
datanode_1          | 2023-06-22 08:44:48,130 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1819c6fd-24b7-4837-96e6-3b133f381369 is started using port 9858 for RATIS
datanode_1          | 2023-06-22 08:44:48,130 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1819c6fd-24b7-4837-96e6-3b133f381369 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-22 08:44:48,130 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1819c6fd-24b7-4837-96e6-3b133f381369 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-22 08:44:48,133 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@7597d638] INFO util.JvmPauseMonitor: JvmPauseMonitor-1819c6fd-24b7-4837-96e6-3b133f381369: Started
datanode_1          | 2023-06-22 08:44:52,353 [Command processor thread] INFO server.RaftServer: 1819c6fd-24b7-4837-96e6-3b133f381369: addNew group-F9798200EC72:[1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] returns group-F9798200EC72:java.util.concurrent.CompletableFuture@65945e75[Not completed]
datanode_1          | 2023-06-22 08:44:52,416 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369: new RaftServerImpl for group-F9798200EC72:[1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:44:52,420 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:44:52,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:44:52,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:44:52,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:52,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:44:52,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:44:52,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:44:52,459 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: ConfigurationManager, init=-1: [1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:44:52,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:44:52,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:44:52,500 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:44:52,504 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/60f33ce4-a920-4392-b1f2-f9798200ec72 does not exist. Creating ...
datanode_1          | 2023-06-22 08:44:52,538 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/60f33ce4-a920-4392-b1f2-f9798200ec72/in_use.lock acquired by nodename 7@2f8b727981ec
datanode_1          | 2023-06-22 08:44:52,581 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/60f33ce4-a920-4392-b1f2-f9798200ec72 has been successfully formatted.
datanode_1          | 2023-06-22 08:44:52,617 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:44:52,618 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-F9798200EC72: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:44:52,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:52,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:44:52,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:44:52,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:44:52,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:52,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:44:52,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:44:52,884 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/60f33ce4-a920-4392-b1f2-f9798200ec72
datanode_1          | 2023-06-22 08:44:52,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:44:52,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:44:52,893 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:52,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:44:52,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:44:52,901 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:44:52,904 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:44:52,905 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:44:52,945 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:52,959 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:44:53,002 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:53,002 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:53,017 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:44:53,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:44:53,021 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:44:53,023 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:44:53,025 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:44:13,302 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = edaf10a71c57/172.22.0.8
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.2.1
datanode_3          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_3          | STARTUP_MSG:   java = 11.0.13
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:44:13,375 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:44:15,102 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:44:15,635 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:44:16,410 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:44:16,410 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:44:17,495 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:edaf10a71c57 ip:172.22.0.8
datanode_3          | 2023-06-22 08:44:18,979 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_3          | 2023-06-22 08:44:19,840 [main] INFO reflections.Reflections: Reflections took 702 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_3          | 2023-06-22 08:44:21,475 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:44:21,570 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-22 08:44:21,603 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:44:21,605 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:44:21,716 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:44:21,865 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:44:21,875 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-22 08:44:21,896 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-22 08:44:21,896 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-22 08:44:21,898 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-22 08:44:22,047 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:44:22,057 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:44:30,079 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:44:30,629 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:44:31,749 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-22 08:44:31,752 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-22 08:44:31,757 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-22 08:44:31,761 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:44:31,771 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:44:31,778 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:44:31,791 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:44:33,377 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-22 08:44:33,389 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:33,391 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:44:33,474 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:44:34,568 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:44:34,730 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:44:34,869 [main] INFO util.log: Logging initialized @27771ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:44:35,591 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-22 08:44:35,632 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:44:35,707 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:44:35,725 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:44:35,738 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-22 08:44:35,738 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:44:36,029 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-22 08:44:36,045 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_3          | 2023-06-22 08:44:36,229 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:44:36,229 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:44:36,231 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-22 08:44:36,279 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e1232cf{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_2          | STARTUP_MSG:   java = 11.0.13
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:44:12,836 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:44:14,441 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:44:14,900 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:44:15,805 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:44:15,805 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:44:16,684 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:9370495d315d ip:172.22.0.10
datanode_2          | 2023-06-22 08:44:17,951 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_2          | 2023-06-22 08:44:18,841 [main] INFO reflections.Reflections: Reflections took 751 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_2          | 2023-06-22 08:44:20,535 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:44:20,548 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-22 08:44:20,587 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:44:20,603 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:44:20,815 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:44:21,003 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:44:21,031 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-22 08:44:21,034 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-22 08:44:21,036 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-22 08:44:21,054 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-22 08:44:21,195 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:44:21,196 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:44:29,246 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:44:29,716 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:44:30,809 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-22 08:44:30,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-22 08:44:30,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-22 08:44:30,814 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:44:30,820 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:44:30,822 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:44:30,825 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:44:32,925 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-22 08:44:32,950 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:32,952 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:44:33,231 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:44:34,403 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:44:34,554 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:44:34,842 [main] INFO util.log: Logging initialized @27756ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:44:35,879 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-22 08:44:35,950 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:44:35,983 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:44:36,046 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:44:36,047 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:44:36,047 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:44:36,334 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-22 08:44:36,363 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_2          | 2023-06-22 08:44:36,618 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:44:36,618 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:44:36,637 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-06-22 08:44:36,744 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f13811b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:44:36,745 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@629a9f26{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:44:38,913 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4315c28c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-3023975286573479341/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:44:53,028 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:44:53,154 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: start as a follower, conf=-1: [1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:44:53,160 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:44:53,161 [pool-22-thread-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState
datanode_1          | 2023-06-22 08:44:53,186 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F9798200EC72,id=1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:44:53,330 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=60f33ce4-a920-4392-b1f2-f9798200ec72
datanode_1          | 2023-06-22 08:44:53,331 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=60f33ce4-a920-4392-b1f2-f9798200ec72.
datanode_1          | 2023-06-22 08:44:53,340 [Command processor thread] INFO server.RaftServer: 1819c6fd-24b7-4837-96e6-3b133f381369: addNew group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] returns group-C79DE405DD4E:java.util.concurrent.CompletableFuture@48810b17[Not completed]
datanode_1          | 2023-06-22 08:44:53,381 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369: new RaftServerImpl for group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:44:53,382 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:44:53,383 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:44:53,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:44:53,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:53,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:44:53,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:44:53,386 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:44:53,389 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:44:53,389 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:44:53,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:44:53,391 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:44:53,393 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e does not exist. Creating ...
datanode_1          | 2023-06-22 08:44:53,401 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/in_use.lock acquired by nodename 7@2f8b727981ec
datanode_1          | 2023-06-22 08:44:53,411 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e has been successfully formatted.
datanode_1          | 2023-06-22 08:44:53,412 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:44:53,413 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C79DE405DD4E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:44:53,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:53,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:44:53,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:44:53,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:44:53,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:53,419 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:44:53,420 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:44:53,421 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_1          | 2023-06-22 08:44:53,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:44:53,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:44:53,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:53,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:44:53,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:44:53,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:44:53,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:44:53,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:44:53,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:53,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:36,301 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3730f716{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:44:38,439 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c931134{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-9995572335671215465/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:44:38,527 [main] INFO server.AbstractConnector: Started ServerConnector@5809fa26{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:44:38,549 [main] INFO server.Server: Started @31451ms
datanode_3          | 2023-06-22 08:44:38,592 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-22 08:44:38,592 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-22 08:44:38,594 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:44:38,625 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-22 08:44:38,999 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1b46c7a1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-22 08:44:39,642 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.4:9891
datanode_3          | 2023-06-22 08:44:39,901 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:44:42,385 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:44:42,395 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:44:43,386 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:44:44,387 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:44:45,388 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:44:47,440 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From edaf10a71c57/172.22.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.8:50530 remote=recon/172.22.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.8:50530 remote=recon/172.22.0.4:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_3          | 2023-06-22 08:44:47,586 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-22 08:44:47,587 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-22 08:44:47,867 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_3          | 2023-06-22 08:44:48,050 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.RaftServer: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start RPC server
datanode_3          | 2023-06-22 08:44:48,065 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: GrpcService started, listening on 9856
datanode_3          | 2023-06-22 08:44:48,084 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: GrpcService started, listening on 9857
datanode_3          | 2023-06-22 08:44:48,086 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:44:39,067 [main] INFO server.AbstractConnector: Started ServerConnector@47b33e07{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:44:39,067 [main] INFO server.Server: Started @31982ms
datanode_2          | 2023-06-22 08:44:39,084 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-22 08:44:39,084 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-22 08:44:39,086 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:44:39,146 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-22 08:44:39,326 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5458dec4] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-22 08:44:39,800 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.4:9891
datanode_2          | 2023-06-22 08:44:40,071 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:44:42,757 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:44:42,772 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.22.0.4:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:44:43,330 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:44:43,758 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:44:44,759 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:44:45,332 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:44:45,765 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.9:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:44:47,333 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:456)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 2023-06-22 08:44:48,104 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis aaa19418-41d5-4f5e-8e8c-0763398c2d2b is started using port 9858 for RATIS
datanode_3          | 2023-06-22 08:44:48,104 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis aaa19418-41d5-4f5e-8e8c-0763398c2d2b is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-22 08:44:48,104 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis aaa19418-41d5-4f5e-8e8c-0763398c2d2b is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-22 08:44:48,120 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@c9aa9ce] INFO util.JvmPauseMonitor: JvmPauseMonitor-aaa19418-41d5-4f5e-8e8c-0763398c2d2b: Started
datanode_3          | 2023-06-22 08:44:52,047 [Command processor thread] INFO server.RaftServer: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: addNew group-FFD69B6E9600:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:1] returns group-FFD69B6E9600:java.util.concurrent.CompletableFuture@82dab75[Not completed]
datanode_3          | 2023-06-22 08:44:52,080 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: new RaftServerImpl for group-FFD69B6E9600:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:44:52,082 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:44:52,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:44:52,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:44:52,089 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:52,089 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:44:52,089 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:44:52,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:44:52,098 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:44:52,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:44:52,102 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:44:52,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:44:52,109 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/102362c7-0eeb-4278-ae60-ffd69b6e9600 does not exist. Creating ...
datanode_3          | 2023-06-22 08:44:52,122 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/102362c7-0eeb-4278-ae60-ffd69b6e9600/in_use.lock acquired by nodename 7@edaf10a71c57
datanode_3          | 2023-06-22 08:44:52,134 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/102362c7-0eeb-4278-ae60-ffd69b6e9600 has been successfully formatted.
datanode_3          | 2023-06-22 08:44:52,147 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:44:52,153 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-FFD69B6E9600: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:44:52,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:52,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:44:52,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:44:52,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:44:52,251 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:44:52,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:44:52,321 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/102362c7-0eeb-4278-ae60-ffd69b6e9600
datanode_3          | 2023-06-22 08:44:52,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:44:52,340 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:44:52,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:44:52,351 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:44:52,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:44:52,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:44:52,361 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:44:52,421 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:52,455 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:52,465 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:52,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:52,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:44:52,505 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:44:52,506 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:44:52,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:44:53,441 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:53,441 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:53,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:44:53,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:44:53,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:44:53,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:44:53,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:44:53,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:44:53,452 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:44:53,460 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:44:53,461 [pool-22-thread-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState
datanode_1          | 2023-06-22 08:44:53,464 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C79DE405DD4E,id=1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:44:53,494 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_1          | 2023-06-22 08:44:56,307 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e.
datanode_1          | 2023-06-22 08:44:56,313 [Command processor thread] INFO server.RaftServer: 1819c6fd-24b7-4837-96e6-3b133f381369: addNew group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] returns group-06EF95967583:java.util.concurrent.CompletableFuture@3e5f430f[Not completed]
datanode_1          | 2023-06-22 08:44:56,479 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369: new RaftServerImpl for group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:44:56,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:44:56,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:44:56,484 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:44:56,485 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:56,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:44:56,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:44:56,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:44:56,490 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:44:56,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:44:56,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:44:56,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:44:56,492 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 does not exist. Creating ...
datanode_1          | 2023-06-22 08:44:56,499 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/in_use.lock acquired by nodename 7@2f8b727981ec
datanode_1          | 2023-06-22 08:44:56,501 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 has been successfully formatted.
datanode_1          | 2023-06-22 08:44:56,505 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:44:56,515 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06EF95967583: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:44:56,515 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:44:56,515 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:44:56,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:44:56,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:44:56,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:56,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:44:56,537 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:44:56,537 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_1          | 2023-06-22 08:44:56,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-22 08:44:56,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:44:56,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:56,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:44:56,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:44:56,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:44:56,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:44:56,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:44:56,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:44:56,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:44:56,556 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:56,564 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:44:56,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:44:56,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:44:56,577 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:44:56,577 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:44:56,577 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:44:56,577 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:44:56,581 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:44:56,581 [pool-22-thread-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:44:56,582 [pool-22-thread-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:44:56,583 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06EF95967583,id=1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:44:56,585 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_1          | 2023-06-22 08:44:56,836 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583.
datanode_1          | 2023-06-22 08:44:58,241 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-C79DE405DD4E, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:44:58,245 [grpc-default-executor-2] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FOLLOWER: accept ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-22 08:44:58,253 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_1          | 2023-06-22 08:44:58,253 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState
datanode_1          | 2023-06-22 08:44:58,253 [1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:44:58,254 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState
datanode_1          | 2023-06-22 08:44:58,283 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5121542018ns, electionTimeout:5086ms
datanode_1          | 2023-06-22 08:44:58,284 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState
datanode_1          | 2023-06-22 08:44:58,289 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:44:58,297 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:44:58,297 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:44:47,645 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-22 08:44:47,648 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-22 08:44:47,822 [EndpointStateMachine task thread for recon/172.22.0.4:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 9370495d315d/172.22.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.10:55848 remote=recon/172.22.0.4:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.10:55848 remote=recon/172.22.0.4:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_2          | 2023-06-22 08:44:47,956 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:44:48,009 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.RaftServer: 5601e644-03c9-4bbe-8b76-b414432ac23a: start RPC server
datanode_2          | 2023-06-22 08:44:48,015 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 5601e644-03c9-4bbe-8b76-b414432ac23a: GrpcService started, listening on 9856
datanode_2          | 2023-06-22 08:44:48,016 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 5601e644-03c9-4bbe-8b76-b414432ac23a: GrpcService started, listening on 9857
datanode_2          | 2023-06-22 08:44:48,022 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO server.GrpcService: 5601e644-03c9-4bbe-8b76-b414432ac23a: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:44:48,191 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@163a97f9] INFO util.JvmPauseMonitor: JvmPauseMonitor-5601e644-03c9-4bbe-8b76-b414432ac23a: Started
datanode_2          | 2023-06-22 08:44:48,195 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5601e644-03c9-4bbe-8b76-b414432ac23a is started using port 9858 for RATIS
datanode_2          | 2023-06-22 08:44:48,195 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5601e644-03c9-4bbe-8b76-b414432ac23a is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-22 08:44:48,195 [EndpointStateMachine task thread for scm/172.22.0.9:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5601e644-03c9-4bbe-8b76-b414432ac23a is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-22 08:44:52,402 [Command processor thread] INFO server.RaftServer: 5601e644-03c9-4bbe-8b76-b414432ac23a: addNew group-E4A9024D98AA:[5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1] returns group-E4A9024D98AA:java.util.concurrent.CompletableFuture@5d6496d9[Not completed]
datanode_2          | 2023-06-22 08:44:52,503 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a: new RaftServerImpl for group-E4A9024D98AA:[5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:44:52,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:44:52,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:44:52,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:44:52,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:44:52,739 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:44:52,740 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:44:52,751 [pool-22-thread-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState
datanode_3          | 2023-06-22 08:44:52,785 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FFD69B6E9600,id=aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_3          | 2023-06-22 08:44:52,883 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=102362c7-0eeb-4278-ae60-ffd69b6e9600
datanode_3          | 2023-06-22 08:44:52,891 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=102362c7-0eeb-4278-ae60-ffd69b6e9600.
datanode_3          | 2023-06-22 08:44:52,892 [Command processor thread] INFO server.RaftServer: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: addNew group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] returns group-C79DE405DD4E:java.util.concurrent.CompletableFuture@49a69e65[Not completed]
datanode_3          | 2023-06-22 08:44:52,924 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: new RaftServerImpl for group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:44:52,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:44:52,932 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:44:52,932 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:44:52,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:52,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:44:52,933 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:44:52,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:44:52,934 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:44:52,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:44:52,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:44:52,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:44:52,935 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e does not exist. Creating ...
datanode_3          | 2023-06-22 08:44:52,938 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/in_use.lock acquired by nodename 7@edaf10a71c57
datanode_3          | 2023-06-22 08:44:52,940 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e has been successfully formatted.
datanode_3          | 2023-06-22 08:44:52,941 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:44:52,945 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C79DE405DD4E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:44:52,946 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:52,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:44:52,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:44:52,952 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:44:52,952 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,953 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:44:52,953 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:44:52,953 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_3          | 2023-06-22 08:44:52,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:44:52,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:44:52,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,984 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:44:52,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:44:52,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:44:52,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:44:52,989 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:44:52,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:52,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:52,991 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:52,992 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:52,992 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:52,993 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:44:52,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:44:52,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:44:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:44:52,997 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:44:53,001 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:44:53,003 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:44:53,003 [pool-22-thread-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState
datanode_3          | 2023-06-22 08:44:53,045 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C79DE405DD4E,id=aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_3          | 2023-06-22 08:44:53,048 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_3          | 2023-06-22 08:44:56,442 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e.
datanode_3          | 2023-06-22 08:44:56,451 [Command processor thread] INFO server.RaftServer: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: addNew group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] returns group-06EF95967583:java.util.concurrent.CompletableFuture@5dbde7d5[Not completed]
datanode_3          | 2023-06-22 08:44:56,453 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: new RaftServerImpl for group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:44:56,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:44:56,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:44:56,457 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:44:56,457 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:56,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:44:56,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:44:56,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:44:56,468 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:44:56,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:44:56,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:44:56,470 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:44:56,471 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 does not exist. Creating ...
datanode_3          | 2023-06-22 08:44:56,477 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/in_use.lock acquired by nodename 7@edaf10a71c57
datanode_3          | 2023-06-22 08:44:56,483 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 has been successfully formatted.
datanode_3          | 2023-06-22 08:44:56,489 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:44:56,496 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06EF95967583: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:44:56,497 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:44:56,501 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:44:56,501 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:44:56,506 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:44:56,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:56,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:44:56,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:44:58,357 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:OK-t1. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E:t1, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:44:58,361 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:44:58,377 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-22 08:44:58,378 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1
datanode_1          | 2023-06-22 08:44:58,380 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:44:58,387 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F9798200EC72 with new leaderId: 1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:44:58,388 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:44:58,393 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: change Leader from null to 1819c6fd-24b7-4837-96e6-3b133f381369 at term 1 for becomeLeader, leader elected after 5741ms
datanode_1          | 2023-06-22 08:44:58,426 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:44:58,451 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:44:58,452 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:44:58,470 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:44:58,480 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:44:58,481 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:44:58,494 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:44:58,514 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:44:58,552 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderStateImpl
datanode_1          | 2023-06-22 08:44:58,642 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:44:58,749 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-LeaderElection1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72: set configuration 0: [1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-22 08:44:58,927 [1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-F9798200EC72-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60f33ce4-a920-4392-b1f2-f9798200ec72/current/log_inprogress_0
datanode_1          | 2023-06-22 08:45:01,627 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: receive requestVote(ELECTION, 5601e644-03c9-4bbe-8b76-b414432ac23a, group-06EF95967583, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:01,628 [grpc-default-executor-2] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FOLLOWER: reject ELECTION from 5601e644-03c9-4bbe-8b76-b414432ac23a: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-22 08:45:01,628 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_1          | 2023-06-22 08:45:01,628 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:01,628 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:01,629 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:45:01,648 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583 replies to ELECTION vote request: 5601e644-03c9-4bbe-8b76-b414432ac23a<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t1. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583:t1, leader=null, voted=null, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:03,333 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: receive requestVote(ELECTION, 5601e644-03c9-4bbe-8b76-b414432ac23a, group-C79DE405DD4E, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:44:52,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:52,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:44:52,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:44:52,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:44:52,535 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: ConfigurationManager, init=-1: [5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:44:52,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:44:52,554 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:44:52,554 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:44:52,556 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f15019ab-2490-4fc7-ab0d-e4a9024d98aa does not exist. Creating ...
datanode_2          | 2023-06-22 08:44:52,606 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f15019ab-2490-4fc7-ab0d-e4a9024d98aa/in_use.lock acquired by nodename 7@9370495d315d
datanode_2          | 2023-06-22 08:44:52,672 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f15019ab-2490-4fc7-ab0d-e4a9024d98aa has been successfully formatted.
datanode_2          | 2023-06-22 08:44:52,727 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-E4A9024D98AA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:44:52,738 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:52,750 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:44:52,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:44:52,812 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:44:52,871 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:52,941 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:44:52,943 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:44:52,986 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f15019ab-2490-4fc7-ab0d-e4a9024d98aa
datanode_2          | 2023-06-22 08:44:53,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:44:53,016 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:44:53,023 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:53,024 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:44:53,026 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:44:53,029 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:44:53,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:44:53,045 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:44:53,106 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:53,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:53,159 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:44:53,159 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:44:53,193 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:53,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:44:53,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:44:53,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:44:53,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:44:53,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:44:53,383 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: start as a follower, conf=-1: [5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:44:53,388 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:44:53,391 [pool-22-thread-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState
datanode_2          | 2023-06-22 08:44:53,448 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4A9024D98AA,id=5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:44:53,526 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=f15019ab-2490-4fc7-ab0d-e4a9024d98aa
datanode_2          | 2023-06-22 08:44:53,527 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f15019ab-2490-4fc7-ab0d-e4a9024d98aa.
datanode_2          | 2023-06-22 08:44:53,528 [Command processor thread] INFO server.RaftServer: 5601e644-03c9-4bbe-8b76-b414432ac23a: addNew group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] returns group-C79DE405DD4E:java.util.concurrent.CompletableFuture@31f1a329[Not completed]
datanode_2          | 2023-06-22 08:44:53,542 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a: new RaftServerImpl for group-C79DE405DD4E:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:44:53,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:44:53,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:44:53,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:44:53,554 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:53,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:44:53,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:44:53,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:44:53,559 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:44:53,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:44:53,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:44:53,561 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:44:53,563 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e does not exist. Creating ...
datanode_2          | 2023-06-22 08:44:53,573 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/in_use.lock acquired by nodename 7@9370495d315d
datanode_2          | 2023-06-22 08:44:53,576 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e has been successfully formatted.
datanode_2          | 2023-06-22 08:44:53,579 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-22 08:44:53,582 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C79DE405DD4E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:44:53,614 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:53,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:44:53,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:44:53,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:44:53,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:53,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:44:53,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:44:53,629 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_3          | 2023-06-22 08:44:56,511 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_3          | 2023-06-22 08:44:56,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-22 08:44:56,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:44:56,511 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:56,512 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:44:56,512 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:44:56,512 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:44:56,512 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:44:56,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:44:56,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:44:56,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:56,527 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:56,531 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:44:56,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:44:56,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:44:56,535 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:44:56,535 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:44:56,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:44:56,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:44:56,537 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:44:56,540 [pool-22-thread-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:44:56,540 [pool-22-thread-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:44:56,543 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06EF95967583,id=aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_3          | 2023-06-22 08:44:56,556 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_3          | 2023-06-22 08:44:56,759 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583.
datanode_3          | 2023-06-22 08:44:57,837 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5085479510ns, electionTimeout:5050ms
datanode_3          | 2023-06-22 08:44:57,837 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState
datanode_3          | 2023-06-22 08:44:57,838 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:44:57,842 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:44:57,842 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1
datanode_3          | 2023-06-22 08:44:57,850 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:44:57,851 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-22 08:44:57,852 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1
datanode_3          | 2023-06-22 08:44:57,852 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:44:57,853 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FFD69B6E9600 with new leaderId: aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_3          | 2023-06-22 08:44:57,853 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: change Leader from null to aaa19418-41d5-4f5e-8e8c-0763398c2d2b at term 1 for becomeLeader, leader elected after 5696ms
datanode_3          | 2023-06-22 08:44:57,854 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:44:57,866 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:44:57,886 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:44:57,887 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:44:57,902 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:44:57,903 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:44:57,904 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:44:57,930 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:44:57,933 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:44:57,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderStateImpl
datanode_3          | 2023-06-22 08:44:57,974 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:44:58,053 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-LeaderElection1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-22 08:44:58,120 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-FFD69B6E9600-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/102362c7-0eeb-4278-ae60-ffd69b6e9600/current/log_inprogress_0
datanode_3          | 2023-06-22 08:44:58,157 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153744459ns, electionTimeout:5112ms
datanode_3          | 2023-06-22 08:44:58,157 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState
datanode_3          | 2023-06-22 08:44:58,158 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:44:58,158 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:44:58,159 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2
datanode_3          | 2023-06-22 08:44:58,174 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:44:58,296 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:44:58,297 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t1
datanode_3          | 2023-06-22 08:44:58,298 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:44:58,298 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-22 08:44:58,299 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2
datanode_3          | 2023-06-22 08:44:58,299 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection2] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState
datanode_3          | 2023-06-22 08:45:01,636 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: receive requestVote(ELECTION, 5601e644-03c9-4bbe-8b76-b414432ac23a, group-06EF95967583, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:45:01,639 [grpc-default-executor-1] INFO impl.VoteContext: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FOLLOWER: accept ELECTION from 5601e644-03c9-4bbe-8b76-b414432ac23a: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-22 08:45:01,639 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_3          | 2023-06-22 08:45:01,639 [grpc-default-executor-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:01,639 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-22 08:45:01,640 [grpc-default-executor-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:01,662 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583 replies to ELECTION vote request: 5601e644-03c9-4bbe-8b76-b414432ac23a<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:OK-t1. Peer's state: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583:t1, leader=null, voted=5601e644-03c9-4bbe-8b76-b414432ac23a, raftlog=aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:03,317 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5018075007ns, electionTimeout:5008ms
datanode_3          | 2023-06-22 08:45:03,317 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState
datanode_3          | 2023-06-22 08:45:03,317 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-22 08:45:03,318 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:45:03,318 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3
datanode_3          | 2023-06-22 08:45:03,325 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:45:03,334 [grpc-default-executor-2] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FOLLOWER: accept ELECTION from 5601e644-03c9-4bbe-8b76-b414432ac23a: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:45:03,334 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_1          | 2023-06-22 08:45:03,334 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState
datanode_1          | 2023-06-22 08:45:03,334 [1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:45:03,339 [grpc-default-executor-2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FollowerState
datanode_1          | 2023-06-22 08:45:03,351 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-C79DE405DD4E, 2, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:03,353 [grpc-default-executor-2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E replies to ELECTION vote request: 5601e644-03c9-4bbe-8b76-b414432ac23a<-1819c6fd-24b7-4837-96e6-3b133f381369#0:OK-t2. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E:t2, leader=null, voted=5601e644-03c9-4bbe-8b76-b414432ac23a, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:45:03,353 [grpc-default-executor-1] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-FOLLOWER: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: already has voted for 5601e644-03c9-4bbe-8b76-b414432ac23a at current term 2
datanode_1          | 2023-06-22 08:45:03,359 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t2. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E:t2, leader=null, voted=5601e644-03c9-4bbe-8b76-b414432ac23a, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_1          | 2023-06-22 08:45:03,454 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C79DE405DD4E with new leaderId: 5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_1          | 2023-06-22 08:45:03,461 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: change Leader from null to 5601e644-03c9-4bbe-8b76-b414432ac23a at term 2 for appendEntries, leader elected after 10040ms
datanode_1          | 2023-06-22 08:45:03,495 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-22 08:45:03,496 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:45:03,498 [1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-C79DE405DD4E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/current/log_inprogress_0
datanode_1          | 2023-06-22 08:45:06,694 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5065320213ns, electionTimeout:5064ms
datanode_1          | 2023-06-22 08:45:06,694 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:06,694 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-22 08:45:06,694 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:45:06,694 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2
datanode_1          | 2023-06-22 08:45:06,704 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:06,755 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 2, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:06,755 [grpc-default-executor-1] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-CANDIDATE: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: already has voted for 1819c6fd-24b7-4837-96e6-3b133f381369 at current term 2
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:44:53,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:44:53,632 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:44:53,636 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:44:53,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:53,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:53,640 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:44:53,640 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:44:53,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:53,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:44:53,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:44:53,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:44:53,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:44:53,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:44:53,669 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_2          | 2023-06-22 08:44:53,672 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:44:53,672 [pool-22-thread-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState
datanode_2          | 2023-06-22 08:44:53,677 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C79DE405DD4E,id=5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:44:53,681 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e
datanode_2          | 2023-06-22 08:44:56,428 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e.
datanode_2          | 2023-06-22 08:44:56,429 [Command processor thread] INFO server.RaftServer: 5601e644-03c9-4bbe-8b76-b414432ac23a: addNew group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] returns group-06EF95967583:java.util.concurrent.CompletableFuture@2ff9e143[Not completed]
datanode_2          | 2023-06-22 08:44:56,430 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a: new RaftServerImpl for group-06EF95967583:[aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:44:56,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:44:56,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:44:56,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:44:56,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:56,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:44:56,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:44:56,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:44:56,437 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: ConfigurationManager, init=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:44:56,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:44:56,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:44:56,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:44:56,438 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 does not exist. Creating ...
datanode_2          | 2023-06-22 08:44:56,440 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/in_use.lock acquired by nodename 7@9370495d315d
datanode_2          | 2023-06-22 08:44:56,446 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583 has been successfully formatted.
datanode_2          | 2023-06-22 08:44:56,447 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06EF95967583: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:44:56,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:44:56,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:44:56,449 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:44:56,449 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-22 08:44:56,473 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:44:56,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:56,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:44:56,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:44:56,478 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_2          | 2023-06-22 08:44:56,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-22 08:44:56,480 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:44:56,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:56,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:44:56,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:44:56,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:44:56,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:44:56,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:44:56,486 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:44:56,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:56,499 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:44:56,501 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:45:03,350 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:45:03,351 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t2
datanode_3          | 2023-06-22 08:45:03,351 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:45:03,351 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-06-22 08:45:03,351 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3
datanode_3          | 2023-06-22 08:45:03,351 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-LeaderElection3] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FollowerState
datanode_3          | 2023-06-22 08:45:03,358 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: receive requestVote(ELECTION, 5601e644-03c9-4bbe-8b76-b414432ac23a, group-C79DE405DD4E, 2, (t:0, i:0))
datanode_3          | 2023-06-22 08:45:03,358 [grpc-default-executor-1] INFO impl.VoteContext: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-FOLLOWER: reject ELECTION from 5601e644-03c9-4bbe-8b76-b414432ac23a: already has voted for aaa19418-41d5-4f5e-8e8c-0763398c2d2b at current term 2
datanode_3          | 2023-06-22 08:45:03,359 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E replies to ELECTION vote request: 5601e644-03c9-4bbe-8b76-b414432ac23a<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:FAIL-t2. Peer's state: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E:t2, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_3          | 2023-06-22 08:45:03,515 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C79DE405DD4E with new leaderId: 5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_3          | 2023-06-22 08:45:03,515 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: change Leader from null to 5601e644-03c9-4bbe-8b76-b414432ac23a at term 2 for appendEntries, leader elected after 10569ms
datanode_3          | 2023-06-22 08:45:03,575 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-06-22 08:45:03,576 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:45:03,579 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-C79DE405DD4E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/current/log_inprogress_0
datanode_3          | 2023-06-22 08:45:06,692 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5040054176ns, electionTimeout:5040ms
datanode_3          | 2023-06-22 08:45:06,692 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:06,693 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-22 08:45:06,693 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:45:06,693 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4
datanode_1          | 2023-06-22 08:45:06,755 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t2. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583:t2, leader=null, voted=1819c6fd-24b7-4837-96e6-3b133f381369, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:06,805 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:45:06,808 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1819c6fd-24b7-4837-96e6-3b133f381369<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:FAIL-t2
datanode_1          | 2023-06-22 08:45:06,808 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection:   Response 1: 1819c6fd-24b7-4837-96e6-3b133f381369<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t2
datanode_1          | 2023-06-22 08:45:06,808 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:45:06,808 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-22 08:45:06,809 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2
datanode_1          | 2023-06-22 08:45:06,809 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection2] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:11,817 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 3, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:11,818 [grpc-default-executor-1] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FOLLOWER: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-22 08:45:11,818 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_1          | 2023-06-22 08:45:11,818 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:11,819 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:11,819 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:45:11,828 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t3. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583:t3, leader=null, voted=null, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:12,615 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:45:16,952 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 4, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:16,954 [grpc-default-executor-1] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FOLLOWER: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-22 08:45:16,954 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_1          | 2023-06-22 08:45:16,958 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:16,958 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:16,958 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:45:16,972 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t4. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583:t4, leader=null, voted=null, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:22,046 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 5, (t:0, i:0))
datanode_1          | 2023-06-22 08:45:22,046 [grpc-default-executor-1] INFO impl.VoteContext: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FOLLOWER: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-22 08:45:22,046 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_1          | 2023-06-22 08:45:22,046 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:22,048 [grpc-default-executor-1] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:22,048 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-22 08:45:22,059 [grpc-default-executor-1] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t5. Peer's state: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583:t5, leader=null, voted=null, raftlog=1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:27,091 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.FollowerState: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5043207361ns, electionTimeout:5029ms
datanode_1          | 2023-06-22 08:45:27,092 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState
datanode_1          | 2023-06-22 08:45:27,092 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
datanode_1          | 2023-06-22 08:45:27,092 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:45:27,092 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3
datanode_1          | 2023-06-22 08:45:27,100 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3 ELECTION round 0: submit vote requests at term 6 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-22 08:45:27,129 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:45:27,129 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.LeaderElection:   Response 0: 1819c6fd-24b7-4837-96e6-3b133f381369<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:OK-t6
datanode_1          | 2023-06-22 08:45:27,129 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.LeaderElection: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-06-22 08:45:27,129 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: shutdown 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3
datanode_1          | 2023-06-22 08:45:27,130 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
datanode_1          | 2023-06-22 08:45:27,130 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06EF95967583 with new leaderId: 1819c6fd-24b7-4837-96e6-3b133f381369
datanode_1          | 2023-06-22 08:45:27,130 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: change Leader from null to 1819c6fd-24b7-4837-96e6-3b133f381369 at term 6 for becomeLeader, leader elected after 30614ms
datanode_1          | 2023-06-22 08:45:27,140 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:45:27,154 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:45:27,158 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:45:27,159 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:45:27,159 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:45:27,131 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-22 08:45:27,159 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:45:27,164 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:45:27,165 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:45:27,175 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:45:27,176 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:45:27,177 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:45:27,184 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:45:27,184 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:45:27,184 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:45:27,199 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:45:27,200 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO impl.RoleInfo: 1819c6fd-24b7-4837-96e6-3b133f381369: start 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderStateImpl
datanode_1          | 2023-06-22 08:45:27,201 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:45:27,209 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/current/log_inprogress_0
datanode_1          | 2023-06-22 08:45:27,216 [1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583-LeaderElection3] INFO server.RaftServer$Division: 1819c6fd-24b7-4837-96e6-3b133f381369@group-06EF95967583: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-22 08:45:33,050 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-22 08:44:56,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:44:56,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:44:56,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:44:56,513 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:44:56,513 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:44:56,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:44:56,519 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: start as a follower, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:44:56,519 [pool-22-thread-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:44:56,519 [pool-22-thread-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:44:56,526 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06EF95967583,id=5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:44:56,534 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583
datanode_2          | 2023-06-22 08:44:56,857 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583.
datanode_2          | 2023-06-22 08:44:58,259 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-C79DE405DD4E, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:44:58,263 [grpc-default-executor-1] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FOLLOWER: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-22 08:44:58,264 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_2          | 2023-06-22 08:44:58,265 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState
datanode_2          | 2023-06-22 08:44:58,265 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:44:58,266 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState
datanode_2          | 2023-06-22 08:44:58,275 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t1. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E:t1, leader=null, voted=null, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_2          | 2023-06-22 08:44:58,459 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068769747ns, electionTimeout:5023ms
datanode_2          | 2023-06-22 08:44:58,463 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState
datanode_2          | 2023-06-22 08:44:58,470 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:44:58,477 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:44:58,484 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1
datanode_2          | 2023-06-22 08:44:58,505 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:44:58,507 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-22 08:44:58,507 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1
datanode_2          | 2023-06-22 08:44:58,513 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:44:58,513 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4A9024D98AA with new leaderId: 5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:44:58,515 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: change Leader from null to 5601e644-03c9-4bbe-8b76-b414432ac23a at term 1 for becomeLeader, leader elected after 5775ms
datanode_2          | 2023-06-22 08:44:58,563 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:44:58,596 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:44:58,598 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:44:58,614 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:44:58,623 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:44:58,624 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:44:58,650 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:44:58,654 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:44:58,680 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderStateImpl
datanode_2          | 2023-06-22 08:44:58,758 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:44:58,868 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-LeaderElection1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA: set configuration 0: [5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-22 08:44:58,995 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-E4A9024D98AA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f15019ab-2490-4fc7-ab0d-e4a9024d98aa/current/log_inprogress_0
datanode_2          | 2023-06-22 08:45:01,586 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5067086584ns, electionTimeout:5054ms
datanode_2          | 2023-06-22 08:45:01,587 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:01,587 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:45:01,588 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:45:01,588 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2
datanode_2          | 2023-06-22 08:45:01,592 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:01,672 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:45:01,673 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection:   Response 0: 5601e644-03c9-4bbe-8b76-b414432ac23a<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t1
datanode_2          | 2023-06-22 08:45:01,673 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-22 08:45:01,673 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-06-22 08:45:01,674 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2
datanode_2          | 2023-06-22 08:45:01,674 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-LeaderElection2] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:03,324 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057848268ns, electionTimeout:5056ms
datanode_2          | 2023-06-22 08:45:03,324 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState
datanode_2          | 2023-06-22 08:45:03,325 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-06-22 08:45:03,325 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:45:03,325 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-FollowerState] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3
datanode_2          | 2023-06-22 08:45:03,328 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_2          | 2023-06-22 08:45:03,346 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-C79DE405DD4E, 2, (t:0, i:0))
datanode_3          | 2023-06-22 08:45:06,711 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:06,759 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:45:06,759 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t2
datanode_3          | 2023-06-22 08:45:06,759 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:45:06,760 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-06-22 08:45:06,760 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4
datanode_3          | 2023-06-22 08:45:06,761 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection4] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:06,779 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: receive requestVote(ELECTION, 1819c6fd-24b7-4837-96e6-3b133f381369, group-06EF95967583, 2, (t:0, i:0))
datanode_3          | 2023-06-22 08:45:06,780 [grpc-default-executor-1] INFO impl.VoteContext: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FOLLOWER: reject ELECTION from 1819c6fd-24b7-4837-96e6-3b133f381369: already has voted for aaa19418-41d5-4f5e-8e8c-0763398c2d2b at current term 2
datanode_3          | 2023-06-22 08:45:06,780 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583 replies to ELECTION vote request: 1819c6fd-24b7-4837-96e6-3b133f381369<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:FAIL-t2. Peer's state: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583:t2, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:11,791 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5029799599ns, electionTimeout:5018ms
datanode_3          | 2023-06-22 08:45:11,791 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:11,792 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_3          | 2023-06-22 08:45:11,792 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:45:11,792 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5
datanode_3          | 2023-06-22 08:45:11,795 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5 ELECTION round 0: submit vote requests at term 3 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:11,831 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:45:11,831 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t3
datanode_3          | 2023-06-22 08:45:11,831 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.LeaderElection:   Response 1: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t3
datanode_3          | 2023-06-22 08:45:11,833 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:45:11,833 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_3          | 2023-06-22 08:45:11,833 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5
datanode_3          | 2023-06-22 08:45:11,834 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection5] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:12,512 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:45:16,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5100815875ns, electionTimeout:5096ms
datanode_3          | 2023-06-22 08:45:16,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:16,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_3          | 2023-06-22 08:45:16,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:45:16,935 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6
datanode_2          | 2023-06-22 08:45:03,346 [grpc-default-executor-1] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-CANDIDATE: reject ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: already has voted for 5601e644-03c9-4bbe-8b76-b414432ac23a at current term 2
datanode_2          | 2023-06-22 08:45:03,346 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t2. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E:t2, leader=null, voted=5601e644-03c9-4bbe-8b76-b414432ac23a, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0], old=null
datanode_2          | 2023-06-22 08:45:03,372 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:45:03,375 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection:   Response 0: 5601e644-03c9-4bbe-8b76-b414432ac23a<-1819c6fd-24b7-4837-96e6-3b133f381369#0:OK-t2
datanode_2          | 2023-06-22 08:45:03,375 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.LeaderElection: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-06-22 08:45:03,375 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3
datanode_2          | 2023-06-22 08:45:03,376 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_2          | 2023-06-22 08:45:03,376 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C79DE405DD4E with new leaderId: 5601e644-03c9-4bbe-8b76-b414432ac23a
datanode_2          | 2023-06-22 08:45:03,376 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: change Leader from null to 5601e644-03c9-4bbe-8b76-b414432ac23a at term 2 for becomeLeader, leader elected after 9761ms
datanode_2          | 2023-06-22 08:45:03,376 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:45:03,377 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:45:03,377 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:45:03,377 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:45:03,382 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:45:03,382 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:45:03,383 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:45:03,383 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:45:03,378 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-22 08:45:03,397 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:45:03,400 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:45:03,401 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:45:03,409 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:45:03,409 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:45:03,409 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:45:03,415 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:45:03,415 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:45:03,416 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:45:03,416 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:45:03,416 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:45:03,416 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:45:03,421 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderStateImpl
datanode_2          | 2023-06-22 08:45:03,422 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:45:03,423 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e307222d-5344-4ad8-a63c-c79de405dd4e/current/log_inprogress_0
datanode_2          | 2023-06-22 08:45:03,431 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E-LeaderElection3] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-C79DE405DD4E: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:1, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-22 08:45:06,738 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:06,738 [grpc-default-executor-1] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: accept ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:45:06,738 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_2          | 2023-06-22 08:45:06,738 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:06,738 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:06,738 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:45:06,762 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t2. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t2, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:06,799 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, 1819c6fd-24b7-4837-96e6-3b133f381369, group-06EF95967583, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:06,799 [grpc-default-executor-1] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: reject ELECTION from 1819c6fd-24b7-4837-96e6-3b133f381369: already has voted for aaa19418-41d5-4f5e-8e8c-0763398c2d2b at current term 2
datanode_2          | 2023-06-22 08:45:06,799 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: 1819c6fd-24b7-4837-96e6-3b133f381369<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:FAIL-t2. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t2, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:11,798 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 3, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:11,799 [grpc-default-executor-1] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: accept ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:45:11,799 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_2          | 2023-06-22 08:45:11,799 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:11,799 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:45:11,800 [grpc-default-executor-1] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:11,812 [grpc-default-executor-1] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t3. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t3, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:12,468 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-22 08:45:16,945 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 4, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:16,946 [grpc-default-executor-0] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: accept ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:45:16,946 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_2          | 2023-06-22 08:45:16,946 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:16,946 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:45:16,950 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:16,954 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t4. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t4, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:22,040 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, group-06EF95967583, 5, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:22,040 [grpc-default-executor-0] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: accept ELECTION from aaa19418-41d5-4f5e-8e8c-0763398c2d2b: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:45:22,040 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
datanode_2          | 2023-06-22 08:45:22,040 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:22,040 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:45:22,041 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:22,047 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t5. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t5, leader=null, voted=aaa19418-41d5-4f5e-8e8c-0763398c2d2b, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:27,123 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: receive requestVote(ELECTION, 1819c6fd-24b7-4837-96e6-3b133f381369, group-06EF95967583, 6, (t:0, i:0))
datanode_2          | 2023-06-22 08:45:27,123 [grpc-default-executor-0] INFO impl.VoteContext: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FOLLOWER: accept ELECTION from 1819c6fd-24b7-4837-96e6-3b133f381369: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:45:27,123 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:1819c6fd-24b7-4837-96e6-3b133f381369
datanode_2          | 2023-06-22 08:45:27,123 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: shutdown 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:27,124 [grpc-default-executor-0] INFO impl.RoleInfo: 5601e644-03c9-4bbe-8b76-b414432ac23a: start 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState
datanode_2          | 2023-06-22 08:45:27,124 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState] INFO impl.FollowerState: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-22 08:45:27,126 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583 replies to ELECTION vote request: 1819c6fd-24b7-4837-96e6-3b133f381369<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t6. Peer's state: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583:t6, leader=null, voted=1819c6fd-24b7-4837-96e6-3b133f381369, raftlog=5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_2          | 2023-06-22 08:45:27,248 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06EF95967583 with new leaderId: 1819c6fd-24b7-4837-96e6-3b133f381369
datanode_2          | 2023-06-22 08:45:27,248 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: change Leader from null to 1819c6fd-24b7-4837-96e6-3b133f381369 at term 6 for appendEntries, leader elected after 30800ms
datanode_2          | 2023-06-22 08:45:27,303 [grpc-default-executor-0] INFO server.RaftServer$Division: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-22 08:45:27,303 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:45:27,308 [5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5601e644-03c9-4bbe-8b76-b414432ac23a@group-06EF95967583-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/current/log_inprogress_0
datanode_2          | 2023-06-22 08:45:32,984 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:44:09,909 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = a30154934a0d/172.22.0.4
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.2.1
datanode_3          | 2023-06-22 08:45:16,941 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6 ELECTION round 0: submit vote requests at term 4 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:16,975 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:45:16,975 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t4
datanode_3          | 2023-06-22 08:45:16,975 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.LeaderElection:   Response 1: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t4
datanode_3          | 2023-06-22 08:45:16,975 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:45:16,975 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_3          | 2023-06-22 08:45:16,976 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6
datanode_3          | 2023-06-22 08:45:16,976 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection6] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:22,031 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5054778090ns, electionTimeout:5054ms
datanode_3          | 2023-06-22 08:45:22,031 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:22,031 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
datanode_3          | 2023-06-22 08:45:22,032 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:45:22,032 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7
datanode_3          | 2023-06-22 08:45:22,035 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7 ELECTION round 0: submit vote requests at term 5 for -1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:22,065 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:45:22,065 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.LeaderElection:   Response 0: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-5601e644-03c9-4bbe-8b76-b414432ac23a#0:OK-t5
datanode_3          | 2023-06-22 08:45:22,066 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.LeaderElection:   Response 1: aaa19418-41d5-4f5e-8e8c-0763398c2d2b<-1819c6fd-24b7-4837-96e6-3b133f381369#0:FAIL-t5
datanode_3          | 2023-06-22 08:45:22,066 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.LeaderElection: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:45:22,073 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
datanode_3          | 2023-06-22 08:45:22,074 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7
datanode_3          | 2023-06-22 08:45:22,080 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-LeaderElection7] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:22,381 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-22 08:45:27,110 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: receive requestVote(ELECTION, 1819c6fd-24b7-4837-96e6-3b133f381369, group-06EF95967583, 6, (t:0, i:0))
datanode_3          | 2023-06-22 08:45:27,110 [grpc-default-executor-1] INFO impl.VoteContext: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FOLLOWER: accept ELECTION from 1819c6fd-24b7-4837-96e6-3b133f381369: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-22 08:45:27,111 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:1819c6fd-24b7-4837-96e6-3b133f381369
datanode_3          | 2023-06-22 08:45:27,111 [grpc-default-executor-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: shutdown aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:27,111 [grpc-default-executor-1] INFO impl.RoleInfo: aaa19418-41d5-4f5e-8e8c-0763398c2d2b: start aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState
datanode_3          | 2023-06-22 08:45:27,111 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState] INFO impl.FollowerState: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
recon_1             | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
recon_1             | STARTUP_MSG:   java = 11.0.13
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:44:09,957 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:44:14,457 [main] INFO reflections.Reflections: Reflections took 326 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1             | 2023-06-22 08:44:17,199 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-22 08:44:18,473 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:44:25,613 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:44:28,130 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:44:28,366 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:44:28,380 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-22 08:44:34,846 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:44:35,042 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-22 08:44:35,120 [main] INFO util.log: Logging initialized @29905ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:44:35,838 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-22 08:44:35,873 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-22 08:44:35,912 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:44:35,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:44:35,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:44:35,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:44:36,857 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:44:38,089 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:44:38,146 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-22 08:44:38,198 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-22 08:44:38,351 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:44:38,351 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:44:40,327 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:44:40,582 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:44:40,630 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:44:40,634 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:44:40,772 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:44:41,066 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1             | 2023-06-22 08:44:41,310 [main] INFO reflections.Reflections: Reflections took 240 ms to scan 3 urls, producing 103 keys and 211 values 
recon_1             | 2023-06-22 08:44:41,502 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-22 08:44:41,633 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-22 08:44:41,676 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:44:41,690 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-22 08:44:41,848 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:44:42,105 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:44:42,173 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-22 08:44:42,286 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-22 08:44:42,287 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-22 08:44:42,424 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:44:42,445 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:44:42,445 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-22 08:44:42,990 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-22 08:44:42,991 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
recon_1             | 2023-06-22 08:44:43,067 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:44:43,073 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:44:43,075 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-22 08:44:43,123 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6da4feeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:44:43,124 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@498a612d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-22 08:44:46,690 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@40239b34{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_2_1_jar-_-any-14201102010226451965/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/recon}
recon_1             | 2023-06-22 08:44:46,712 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1e98b788{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:44:46,713 [Listener at 0.0.0.0/9891] INFO server.Server: Started @41497ms
recon_1             | 2023-06-22 08:44:46,722 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-22 08:44:46,722 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-22 08:44:46,734 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:44:46,734 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:44:46,752 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:44:46,762 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:44:46,763 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:44:46,763 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:44:46,763 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-22 08:44:46,766 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:44:48,084 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-22 08:44:48,095 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:44:48,096 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:44:48,102 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-22 08:44:48,145 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:44:48,380 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-22 08:44:48,392 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-22 08:44:48,444 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:44:48,457 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 43 milliseconds.
recon_1             | 2023-06-22 08:44:48,457 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-22 08:44:48,457 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-22 08:44:48,812 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 337 milliseconds to process 0 existing database records.
recon_1             | 2023-06-22 08:44:48,894 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 82 milliseconds for processing 0 containers.
recon_1             | 2023-06-22 08:44:49,151 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.13:45008: output error
recon_1             | 2023-06-22 08:44:49,152 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.8:50530: output error
recon_1             | 2023-06-22 08:44:49,160 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-22 08:44:49,155 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-22 08:44:49,166 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.10:55848: output error
recon_1             | 2023-06-22 08:44:49,167 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-22 08:44:49,365 [IPC Server handler 93 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1819c6fd-24b7-4837-96e6-3b133f381369
recon_1             | 2023-06-22 08:44:49,374 [IPC Server handler 93 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:49,381 [IPC Server handler 3 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5601e644-03c9-4bbe-8b76-b414432ac23a
recon_1             | 2023-06-22 08:44:49,381 [IPC Server handler 3 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:49,459 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5601e644-03c9-4bbe-8b76-b414432ac23a to Node DB.
recon_1             | 2023-06-22 08:44:49,467 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1819c6fd-24b7-4837-96e6-3b133f381369 to Node DB.
recon_1             | 2023-06-22 08:44:51,017 [IPC Server handler 4 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/aaa19418-41d5-4f5e-8e8c-0763398c2d2b
recon_1             | 2023-06-22 08:44:51,017 [IPC Server handler 4 on default port 9891] INFO node.SCMNodeManager: Registered Data node : aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:51,019 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node aaa19418-41d5-4f5e-8e8c-0763398c2d2b to Node DB.
recon_1             | 2023-06-22 08:44:51,335 [IPC Server handler 98 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-22 08:44:51,342 [IPC Server handler 97 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-22 08:44:52,224 [IPC Server handler 53 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-22 08:44:52,226 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=102362c7-0eeb-4278-ae60-ffd69b6e9600. Trying to get from SCM.
recon_1             | 2023-06-22 08:44:52,265 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 102362c7-0eeb-4278-ae60-ffd69b6e9600, Nodes: aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:aaa19418-41d5-4f5e-8e8c-0763398c2d2b, CreationTimestamp2023-06-22T08:44:49.242Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:44:52,358 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 102362c7-0eeb-4278-ae60-ffd69b6e9600, Nodes: aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:aaa19418-41d5-4f5e-8e8c-0763398c2d2b, CreationTimestamp2023-06-22T08:44:49.242Z[UTC]].
recon_1             | 2023-06-22 08:44:52,537 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 241454.199us
recon_1             | 2023-06-22 08:44:52,649 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-22 08:44:52,703 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=60f33ce4-a920-4392-b1f2-f9798200ec72. Trying to get from SCM.
recon_1             | 2023-06-22 08:44:52,710 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 60f33ce4-a920-4392-b1f2-f9798200ec72, Nodes: 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.345Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:44:52,711 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 60f33ce4-a920-4392-b1f2-f9798200ec72, Nodes: 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.345Z[UTC]].
recon_1             | 2023-06-22 08:44:52,720 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 9780.081us
recon_1             | 2023-06-22 08:44:52,720 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=60f33ce4-a920-4392-b1f2-f9798200ec72 reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:52,721 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 60f33ce4-a920-4392-b1f2-f9798200ec72, Nodes: 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1819c6fd-24b7-4837-96e6-3b133f381369, CreationTimestamp2023-06-22T08:44:49.345Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:44:52,722 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 945.966us
recon_1             | 2023-06-22 08:44:52,740 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-22 08:44:52,755 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=f15019ab-2490-4fc7-ab0d-e4a9024d98aa. Trying to get from SCM.
recon_1             | 2023-06-22 08:44:52,760 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: f15019ab-2490-4fc7-ab0d-e4a9024d98aa, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.394Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:44:52,761 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f15019ab-2490-4fc7-ab0d-e4a9024d98aa, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.394Z[UTC]].
recon_1             | 2023-06-22 08:44:52,768 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 7606.829us
recon_1             | 2023-06-22 08:44:52,768 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=f15019ab-2490-4fc7-ab0d-e4a9024d98aa reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_3          | 2023-06-22 08:45:27,121 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583 replies to ELECTION vote request: 1819c6fd-24b7-4837-96e6-3b133f381369<-aaa19418-41d5-4f5e-8e8c-0763398c2d2b#0:OK-t6. Peer's state: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583:t6, leader=null, voted=1819c6fd-24b7-4837-96e6-3b133f381369, raftlog=aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLog:OPENED:c-1, conf=-1: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-22 08:45:27,243 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06EF95967583 with new leaderId: 1819c6fd-24b7-4837-96e6-3b133f381369
datanode_3          | 2023-06-22 08:45:27,244 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: change Leader from null to 1819c6fd-24b7-4837-96e6-3b133f381369 at term 6 for appendEntries, leader elected after 30745ms
datanode_3          | 2023-06-22 08:45:27,269 [grpc-default-executor-1] INFO server.RaftServer$Division: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583: set configuration 0: [aaa19418-41d5-4f5e-8e8c-0763398c2d2b|rpc:172.22.0.8:9856|admin:172.22.0.8:9857|client:172.22.0.8:9858|dataStream:|priority:0, 5601e644-03c9-4bbe-8b76-b414432ac23a|rpc:172.22.0.10:9856|admin:172.22.0.10:9857|client:172.22.0.10:9858|dataStream:|priority:0, 1819c6fd-24b7-4837-96e6-3b133f381369|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-22 08:45:27,270 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:45:27,272 [aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: aaa19418-41d5-4f5e-8e8c-0763398c2d2b@group-06EF95967583-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/82d9f2da-ff5d-4907-aacb-06ef95967583/current/log_inprogress_0
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-22 08:44:13,919 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-22 08:44:13,926 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-22 08:44:14,107 [main] INFO util.log: Logging initialized @6996ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-22 08:44:15,027 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-22 08:44:15,210 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-22 08:44:15,280 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-22 08:44:15,283 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-22 08:44:15,288 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-22 08:44:15,288 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-22 08:44:15,539 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 02f374ed7d15/172.22.0.7
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.2.1
recon_1             | 2023-06-22 08:44:52,769 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f15019ab-2490-4fc7-ab0d-e4a9024d98aa, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5601e644-03c9-4bbe-8b76-b414432ac23a, CreationTimestamp2023-06-22T08:44:49.394Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:44:52,770 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 566.639us
recon_1             | 2023-06-22 08:44:52,969 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-22 08:44:52,970 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e. Trying to get from SCM.
recon_1             | 2023-06-22 08:44:52,974 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e307222d-5344-4ad8-a63c-c79de405dd4e, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.416Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:44:52,975 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e307222d-5344-4ad8-a63c-c79de405dd4e, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.416Z[UTC]].
recon_1             | 2023-06-22 08:44:52,975 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 939.666us
recon_1             | 2023-06-22 08:44:52,975 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:53,421 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:53,600 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,450 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583. Trying to get from SCM.
recon_1             | 2023-06-22 08:44:56,465 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 82d9f2da-ff5d-4907-aacb-06ef95967583, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.419Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:44:56,466 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 82d9f2da-ff5d-4907-aacb-06ef95967583, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.419Z[UTC]].
recon_1             | 2023-06-22 08:44:56,467 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 1147.88us
recon_1             | 2023-06-22 08:44:56,467 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,467 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,495 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,496 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,525 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:56,526 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:57,867 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:57,868 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:58,391 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:58,391 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:58,520 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:44:58,520 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:45:03,383 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:45:03,384 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e reported by 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:45:03,384 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e307222d-5344-4ad8-a63c-c79de405dd4e, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5601e644-03c9-4bbe-8b76-b414432ac23a, CreationTimestamp2023-06-22T08:44:49.416Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:45:03,387 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 2518.373us
recon_1             | 2023-06-22 08:45:12,511 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-22 08:45:12,591 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 19215.671us
recon_1             | 2023-06-22 08:45:12,594 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-22 08:45:22,387 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:45:22,391 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 437.63us
recon_1             | 2023-06-22 08:45:22,391 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:45:27,149 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 reported by 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:45:27,151 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 82d9f2da-ff5d-4907-aacb-06ef95967583, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1819c6fd-24b7-4837-96e6-3b133f381369, CreationTimestamp2023-06-22T08:44:49.419Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:45:27,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 624.54us
recon_1             | 2023-06-22 08:45:32,981 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-22 08:45:33,002 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@5790e055, cost 411.228us
recon_1             | 2023-06-22 08:45:33,003 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-22 08:45:46,764 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:45:46,765 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:45:47,351 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687423546765
recon_1             | 2023-06-22 08:45:47,372 [pool-16-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:45:47,374 [pool-16-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:45:47,475 [pool-16-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687423546765.
recon_1             | 2023-06-22 08:45:47,501 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:45:47,513 [pool-17-thread-1] INFO tasks.NSSummaryTask: Completed a reprocess run of NSSummaryTask
recon_1             | 2023-06-22 08:45:47,884 [pool-17-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-22 08:45:47,885 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:45:48,150 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:45:48,150 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.264 seconds to process 4 keys.
recon_1             | 2023-06-22 08:45:48,184 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-22 08:45:48,218 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
s3g_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
s3g_1               | STARTUP_MSG:   java = 11.0.13
s3g_1               | ************************************************************/
s3g_1               | 2023-06-22 08:44:15,565 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-22 08:44:15,839 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-22 08:44:15,918 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-22 08:44:15,944 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
s3g_1               | 2023-06-22 08:44:16,214 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-22 08:44:16,215 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-22 08:44:16,232 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1               | 2023-06-22 08:44:16,361 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@407a7f2a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-22 08:44:16,369 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5f0fd5a0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 22, 2023 8:44:37 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-22 08:44:37,426 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@544e3679{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_1_jar-_-any-4739958534630198705/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/s3gateway}
s3g_1               | 2023-06-22 08:44:37,630 [main] INFO server.AbstractConnector: Started ServerConnector@2a225dd7{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-22 08:44:37,630 [main] INFO server.Server: Started @30520ms
s3g_1               | 2023-06-22 08:44:37,644 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:44:13,116 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4fb2ebdff2b0/172.22.0.2
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-22 08:44:13,144 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:44:21,008 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:44:21,553 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.2:9862
om_1                | 2023-06-22 08:44:21,554 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:44:21,554 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:44:21,622 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:44:26,484 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:28,486 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:30,488 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:32,489 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:34,491 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:36,493 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:38,494 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:40,496 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:42,501 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:44:44,502 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4fb2ebdff2b0/172.22.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-c688576b-090c-4a2a-adfb-ff1b92c63bd5;layoutVersion=0
om_1                | 2023-06-22 08:44:47,700 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 4fb2ebdff2b0/172.22.0.2
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:44:50,245 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4fb2ebdff2b0/172.22.0.2
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.2.1
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:44:20,118 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 5977fa990487/172.22.0.9
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:44:20,186 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:44:20,936 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:44:21,409 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:44:21,409 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:44:22,488 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-c688576b-090c-4a2a-adfb-ff1b92c63bd5; layoutVersion=2; scmId=cd752bcb-2ffa-4dd3-b12a-c3faf54bb0ac
scm_1               | 2023-06-22 08:44:22,776 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 5977fa990487/172.22.0.9
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:44:37,723 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 5977fa990487/172.22.0.9
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-22 08:44:50,251 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:44:52,200 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:44:52,632 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.2:9862
om_1                | 2023-06-22 08:44:52,633 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:44:52,633 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:44:52,697 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:44:37,826 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:44:38,479 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:44:38,490 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:44:39,182 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:44:39,453 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm_1               | 2023-06-22 08:44:40,731 [main] INFO reflections.Reflections: Reflections took 347 ms to scan 3 urls, producing 103 keys and 211 values 
scm_1               | 2023-06-22 08:44:42,389 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:44:42,709 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:44:43,113 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
scm_1               | 2023-06-22 08:44:43,115 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-22 08:44:43,244 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-22 08:44:43,273 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-22 08:44:43,274 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-22 08:44:43,277 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-22 08:44:43,282 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-22 08:44:43,343 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-22 08:44:43,369 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-22 08:44:43,381 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-22 08:44:43,430 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-22 08:44:43,437 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-22 08:44:43,439 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:44:43,486 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-22 08:44:43,511 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-22 08:44:43,544 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-22 08:44:43,546 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-22 08:44:43,579 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 0 containers.
scm_1               | 2023-06-22 08:44:43,590 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-22 08:44:43,597 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:44:43,600 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:44:45,258 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:44:45,359 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-22 08:44:45,415 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:44:45,418 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om_1                | 2023-06-22 08:44:52,867 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om_1                | 2023-06-22 08:44:54,313 [main] INFO reflections.Reflections: Reflections took 1222 ms to scan 1 urls, producing 95 keys and 258 values [using 2 cores]
om_1                | 2023-06-22 08:44:54,371 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:44:57,464 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:44:57,798 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:44:57,805 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:44:58,282 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:44:58,423 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:44:58,423 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-22 08:44:58,488 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:44:58,513 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:44:58,619 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-22 08:44:58,656 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-22 08:44:58,751 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-22 08:44:59,021 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-22 08:44:59,028 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:44:59,029 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-22 08:44:59,030 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:44:59,030 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:44:59,031 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-22 08:44:59,034 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:44:59,040 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-22 08:44:59,041 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:44:59,446 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-22 08:44:59,452 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:44:59,453 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:44:59,480 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:44:59,497 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@628b819d[Not completed]
om_1                | 2023-06-22 08:44:59,499 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-22 08:44:59,564 [pool-23-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-22 08:44:59,571 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-22 08:44:59,571 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-22 08:44:59,577 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-22 08:44:59,577 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:44:59,578 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:44:59,578 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-22 08:44:59,579 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-22 08:44:59,589 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:44:59,596 [pool-23-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-22 08:44:59,597 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:44:59,610 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-22 08:44:59,616 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-22 08:44:59,627 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-22 08:44:59,646 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:44:59,733 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@4fb2ebdff2b0
om_1                | 2023-06-22 08:44:59,833 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-22 08:44:59,851 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-22 08:44:59,866 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-22 08:44:59,904 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-22 08:44:59,905 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:44:59,916 [Listener at om/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om_1                | 2023-06-22 08:44:59,979 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:45:00,009 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-22 08:45:00,011 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:44:45,442 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:44:45,445 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-22 08:44:45,630 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-22 08:44:45,640 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          0.1
scm_1               | Max Datanodes to Involve per Iteration(ratio)      0.2
scm_1               | Max Size to Move per Iteration                     32212254720B
scm_1               | 
scm_1               | 2023-06-22 08:44:45,640 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:44:45,640 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-22 08:44:45,646 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:44:45,944 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:44:46,006 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:44:46,006 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:44:46,793 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:44:46,796 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:44:46,974 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:44:46,975 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:44:46,976 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:44:46,977 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:44:46,982 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:44:47,006 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:44:47,006 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:44:47,007 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:44:47,007 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:44:47,052 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@342e690b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-22 08:44:47,096 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:44:47,096 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:44:47,191 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @22004ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:44:47,761 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-22 08:44:47,790 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:44:47,875 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:44:47,879 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:44:47,879 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:44:47,880 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:44:48,253 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-22 08:44:48,257 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
scm_1               | 2023-06-22 08:44:48,442 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:44:48,442 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:44:48,459 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-22 08:44:48,570 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6a15b73{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:44:48,574 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@41b64020{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:44:49,125 [IPC Server handler 57 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/aaa19418-41d5-4f5e-8e8c-0763398c2d2b
scm_1               | 2023-06-22 08:44:49,136 [IPC Server handler 57 on default port 9861] INFO node.SCMNodeManager: Registered Data node : aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:44:49,188 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:44:49,170 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:44:49,192 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:44:49,210 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:44:49,251 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=102362c7-0eeb-4278-ae60-ffd69b6e9600 to datanode:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
om_1                | 2023-06-22 08:45:00,038 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-22 08:45:00,039 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:45:00,044 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-22 08:45:00,049 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:45:00,050 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-22 08:45:00,051 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-22 08:45:00,052 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-22 08:45:00,056 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-22 08:45:00,056 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-22 08:45:00,104 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-22 08:45:00,105 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-22 08:45:00,134 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:45:00,134 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:45:00,145 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-22 08:45:00,165 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-22 08:45:00,165 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-22 08:45:00,167 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-22 08:45:00,175 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-22 08:45:00,176 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-22 08:45:00,252 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:45:00,307 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:45:00,309 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-22 08:45:00,363 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.22.0.2:9862
om_1                | 2023-06-22 08:45:00,365 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-22 08:45:00,374 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-22 08:45:00,376 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-22 08:45:00,377 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:45:00,379 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-22 08:45:00,390 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-22 08:45:00,435 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-22 08:45:00,440 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-22 08:45:00,442 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$370/0x00000008404df040@110b7837] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-22 08:45:00,470 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:45:00,471 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:45:00,493 [Listener at om/9862] INFO util.log: Logging initialized @12118ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:45:00,643 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-22 08:45:00,654 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:45:00,659 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:45:00,669 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:45:00,669 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:45:00,669 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:45:00,725 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-22 08:45:00,733 [Listener at om/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
om_1                | 2023-06-22 08:45:00,782 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-22 08:45:00,782 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:45:00,784 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-22 08:45:00,801 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c951ada{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:45:00,802 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11f23038{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:45:01,139 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@ad3f70a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_1_jar-_-any-7762239280742365312/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:45:01,151 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@2eb0cefe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-22 08:45:01,152 [Listener at om/9862] INFO server.Server: Started @12777ms
om_1                | 2023-06-22 08:45:01,154 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-22 08:45:01,154 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-22 08:45:01,156 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:45:01,160 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-22 08:45:01,203 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:45:01,210 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-22 08:45:01,229 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@45bbc52f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-22 08:45:05,503 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5125801870ns, electionTimeout:5121ms
om_1                | 2023-06-22 08:45:05,504 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:45:05,505 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-22 08:45:05,508 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-22 08:45:05,508 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:45:05,523 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-22 08:45:05,524 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-22 08:45:05,525 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:45:05,525 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-22 08:45:05,525 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5674ms
om_1                | 2023-06-22 08:45:05,536 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-22 08:45:05,542 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:45:05,544 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:45:05,549 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-22 08:45:05,555 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-22 08:45:05,555 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-22 08:45:05,569 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:45:05,574 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-22 08:45:05,580 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-22 08:45:05,629 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-22 08:44:49,281 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 102362c7-0eeb-4278-ae60-ffd69b6e9600, Nodes: aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.242Z[UTC]].
scm_1               | 2023-06-22 08:44:49,329 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1819c6fd-24b7-4837-96e6-3b133f381369
scm_1               | 2023-06-22 08:44:49,330 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:44:49,336 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:44:49,345 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=60f33ce4-a920-4392-b1f2-f9798200ec72 to datanode:1819c6fd-24b7-4837-96e6-3b133f381369
scm_1               | 2023-06-22 08:44:49,360 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 60f33ce4-a920-4392-b1f2-f9798200ec72, Nodes: 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.345Z[UTC]].
scm_1               | 2023-06-22 08:44:49,372 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:44:49,388 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5601e644-03c9-4bbe-8b76-b414432ac23a
scm_1               | 2023-06-22 08:44:49,393 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:44:49,393 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:44:49,394 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f15019ab-2490-4fc7-ab0d-e4a9024d98aa to datanode:5601e644-03c9-4bbe-8b76-b414432ac23a
scm_1               | 2023-06-22 08:44:49,395 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f15019ab-2490-4fc7-ab0d-e4a9024d98aa, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.394Z[UTC]].
scm_1               | 2023-06-22 08:44:49,402 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:44:49,402 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:44:49,402 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:44:49,402 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:44:49,409 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-22 08:44:49,410 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:44:49,416 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e to datanode:5601e644-03c9-4bbe-8b76-b414432ac23a
scm_1               | 2023-06-22 08:44:49,417 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e to datanode:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
scm_1               | 2023-06-22 08:44:49,417 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e to datanode:1819c6fd-24b7-4837-96e6-3b133f381369
scm_1               | 2023-06-22 08:44:49,418 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e307222d-5344-4ad8-a63c-c79de405dd4e, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.416Z[UTC]].
scm_1               | 2023-06-22 08:44:49,419 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 to datanode:5601e644-03c9-4bbe-8b76-b414432ac23a
scm_1               | 2023-06-22 08:44:49,419 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 to datanode:aaa19418-41d5-4f5e-8e8c-0763398c2d2b
scm_1               | 2023-06-22 08:44:49,419 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 to datanode:1819c6fd-24b7-4837-96e6-3b133f381369
om_1                | 2023-06-22 08:45:05,704 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-22 08:45:05,965 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-22 08:45:06,158 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | ]
om_1                | 2023-06-22 08:45:10,357 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-22 08:45:47,109 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-22 08:45:47,144 [qtp2081751971-43] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423547110 in 33 milliseconds
om_1                | 2023-06-22 08:45:47,206 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 60 milliseconds
om_1                | 2023-06-22 08:45:47,206 [qtp2081751971-43] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423547110
scm_1               | 2023-06-22 08:44:49,420 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 82d9f2da-ff5d-4907-aacb-06ef95967583, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:44:49.419Z[UTC]].
scm_1               | 2023-06-22 08:44:49,420 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=82d9f2da-ff5d-4907-aacb-06ef95967583 contains same datanodes as previous pipelines: PipelineID=e307222d-5344-4ad8-a63c-c79de405dd4e nodeIds: 5601e644-03c9-4bbe-8b76-b414432ac23a, aaa19418-41d5-4f5e-8e8c-0763398c2d2b, 1819c6fd-24b7-4837-96e6-3b133f381369
scm_1               | 2023-06-22 08:44:49,653 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c628f6a{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_1_jar-_-any-1289181555700194933/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/scm}
scm_1               | 2023-06-22 08:44:49,668 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@78307a56{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:44:49,668 [Listener at 0.0.0.0/9860] INFO server.Server: Started @24481ms
scm_1               | 2023-06-22 08:44:49,671 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-22 08:44:49,671 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-22 08:44:49,673 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:44:52,220 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 102362c7-0eeb-4278-ae60-ffd69b6e9600, Nodes: aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:aaa19418-41d5-4f5e-8e8c-0763398c2d2b, CreationTimestamp2023-06-22T08:44:49.242Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:44:52,259 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:52,716 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 60f33ce4-a920-4392-b1f2-f9798200ec72, Nodes: 1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1819c6fd-24b7-4837-96e6-3b133f381369, CreationTimestamp2023-06-22T08:44:49.345Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:44:52,723 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:52,819 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f15019ab-2490-4fc7-ab0d-e4a9024d98aa, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5601e644-03c9-4bbe-8b76-b414432ac23a, CreationTimestamp2023-06-22T08:44:49.394Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:44:52,825 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:52,989 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:53,442 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:53,617 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:56,482 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:56,509 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:56,522 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:57,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:58,397 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:44:58,526 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:45:03,390 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e307222d-5344-4ad8-a63c-c79de405dd4e, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5601e644-03c9-4bbe-8b76-b414432ac23a, CreationTimestamp2023-06-22T08:44:49.416Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:45:03,394 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-22 08:45:03,395 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-22 08:45:03,395 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-22 08:45:10,476 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-22 08:45:10,489 [IPC Server handler 0 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-22 08:45:10,489 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-22 08:45:27,164 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 82d9f2da-ff5d-4907-aacb-06ef95967583, Nodes: 5601e644-03c9-4bbe-8b76-b414432ac23a{ip: 172.22.0.10, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}aaa19418-41d5-4f5e-8e8c-0763398c2d2b{ip: 172.22.0.8, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1819c6fd-24b7-4837-96e6-3b133f381369{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1819c6fd-24b7-4837-96e6-3b133f381369, CreationTimestamp2023-06-22T08:44:49.419Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:45:43,978 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.5
scm_1               | 2023-06-22 08:45:55,044 [IPC Server handler 6 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.5
scm_1               | 2023-06-22 08:46:58,747 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.5
scm_1               | 2023-06-22 08:47:08,638 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.5
Attaching to xcompat_datanode_3, xcompat_old_client_1_0_0_1, xcompat_datanode_1, xcompat_datanode_2, xcompat_recon_1, xcompat_old_client_1_2_1_1, xcompat_new_client_1, xcompat_s3g_1, xcompat_old_client_1_3_0_1, xcompat_old_client_1_1_0_1, xcompat_om_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:47:37,915 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 9aca8b47df40/172.23.0.10
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.3.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_1          | STARTUP_MSG:   java = 11.0.14.1
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:47:37,958 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:47:38,226 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:47:38,748 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:47:39,408 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:47:39,409 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:47:40,093 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:9aca8b47df40 ip:172.23.0.10
datanode_1          | 2023-06-22 08:47:41,749 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1          | 2023-06-22 08:47:42,771 [main] INFO reflections.Reflections: Reflections took 814 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_1          | 2023-06-22 08:47:43,399 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-22 08:47:44,333 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:47:44,466 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-22 08:47:44,471 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:47:44,477 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:47:44,563 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:47:44,714 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:47:44,762 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-22 08:47:44,763 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-22 08:47:44,763 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-22 08:47:44,782 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-22 08:47:44,986 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:47:44,992 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:47:52,319 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-22 08:47:52,824 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:47:53,290 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:47:53,906 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:47:53,908 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-22 08:47:53,972 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:47:53,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-22 08:47:53,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-22 08:47:53,973 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-22 08:47:53,981 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:47:53,981 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:47:53,997 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:47:53,998 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:47:54,108 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:47:54,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-22 08:47:54,132 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-22 08:47:55,729 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-22 08:47:55,764 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-22 08:47:55,766 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-22 08:47:55,772 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:47:55,772 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:47:55,797 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:47:55,958 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-22 08:47:56,670 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:47:56,752 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:47:56,907 [main] INFO util.log: Logging initialized @26793ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:47:57,653 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-22 08:47:57,710 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:47:57,831 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:47:57,870 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:47:57,904 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:47:57,904 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:47:58,710 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-22 08:47:58,719 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1          | 2023-06-22 08:47:58,996 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:47:59,013 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:47:59,022 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-22 08:47:59,123 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@353f472a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:47:59,135 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5c931134{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:48:00,633 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39832280{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-14379182666487221514/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:48:00,729 [main] INFO server.AbstractConnector: Started ServerConnector@7c46c9c3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:48:00,736 [main] INFO server.Server: Started @30622ms
datanode_1          | 2023-06-22 08:48:00,769 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-22 08:48:00,770 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-22 08:48:00,776 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:48:00,793 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-22 08:48:01,027 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10c9218d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-22 08:48:01,640 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.8:9891
datanode_1          | 2023-06-22 08:48:02,026 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:48:04,266 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:04,270 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:05,272 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:05,272 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:06,273 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:06,273 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:07,276 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:08,277 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:09,278 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:10,279 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:48:11,304 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 9aca8b47df40/172.23.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:43374 remote=recon/172.23.0.8:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:43374 remote=recon/172.23.0.8:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-22 08:48:15,291 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 9aca8b47df40/172.23.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:35440 remote=scm/172.23.0.3:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.10:35440 remote=scm/172.23.0.3:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-22 08:48:16,209 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-02132d45-5798-4fab-b480-bf009352286d/container.db to cache
datanode_1          | 2023-06-22 08:48:16,209 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-02132d45-5798-4fab-b480-bf009352286d/container.db for volume DS-02132d45-5798-4fab-b480-bf009352286d
datanode_1          | 2023-06-22 08:48:16,218 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:48:16,222 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-22 08:48:16,497 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1b7c5580-f1a4-419a-996c-bb798c761974
datanode_1          | 2023-06-22 08:48:16,581 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.RaftServer: 1b7c5580-f1a4-419a-996c-bb798c761974: start RPC server
datanode_1          | 2023-06-22 08:48:16,594 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: 1b7c5580-f1a4-419a-996c-bb798c761974: GrpcService started, listening on 9858
datanode_1          | 2023-06-22 08:48:16,600 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: 1b7c5580-f1a4-419a-996c-bb798c761974: GrpcService started, listening on 9856
datanode_1          | 2023-06-22 08:48:16,608 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: 1b7c5580-f1a4-419a-996c-bb798c761974: GrpcService started, listening on 9857
datanode_1          | 2023-06-22 08:48:16,637 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1b7c5580-f1a4-419a-996c-bb798c761974 is started using port 9858 for RATIS
datanode_1          | 2023-06-22 08:48:16,637 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1b7c5580-f1a4-419a-996c-bb798c761974 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-22 08:48:16,637 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1b7c5580-f1a4-419a-996c-bb798c761974 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-22 08:48:16,649 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1b7c5580-f1a4-419a-996c-bb798c761974: Started
datanode_1          | 2023-06-22 08:48:20,189 [Command processor thread] INFO server.RaftServer: 1b7c5580-f1a4-419a-996c-bb798c761974: addNew group-893682C86859:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-893682C86859:java.util.concurrent.CompletableFuture@293ca49e[Not completed]
datanode_1          | 2023-06-22 08:48:20,233 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974: new RaftServerImpl for group-893682C86859:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:48:20,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:48:20,269 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:48:20,269 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:48:20,278 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:48:20,278 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:48:20,292 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:20,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:48:20,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:48:20,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:48:20,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:48:20,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:48:20,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:48:20,516 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:48:20,516 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/cc62a49f-5ae5-4985-9dbe-893682c86859 does not exist. Creating ...
datanode_1          | 2023-06-22 08:48:20,550 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/cc62a49f-5ae5-4985-9dbe-893682c86859/in_use.lock acquired by nodename 6@9aca8b47df40
datanode_1          | 2023-06-22 08:48:20,577 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/cc62a49f-5ae5-4985-9dbe-893682c86859 has been successfully formatted.
datanode_1          | 2023-06-22 08:48:20,617 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-893682C86859: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:48:20,655 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:48:20,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:48:20,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:48:20,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:48:20,722 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:48:20,731 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:20,752 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:48:20,753 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:48:20,799 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/cc62a49f-5ae5-4985-9dbe-893682c86859
datanode_1          | 2023-06-22 08:48:20,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:48:20,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:48:20,814 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:20,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:48:20,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:48:20,822 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:48:20,830 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:48:20,830 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:48:20,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:20,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:20,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:20,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:20,955 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:20,955 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:20,971 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:20,971 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:48:20,980 [pool-22-thread-1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState
datanode_1          | 2023-06-22 08:48:20,996 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-893682C86859,id=1b7c5580-f1a4-419a-996c-bb798c761974
datanode_1          | 2023-06-22 08:48:20,997 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:20,997 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:21,006 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:21,006 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:48:21,014 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:48:21,015 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:48:21,130 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=cc62a49f-5ae5-4985-9dbe-893682c86859
datanode_1          | 2023-06-22 08:48:21,131 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=cc62a49f-5ae5-4985-9dbe-893682c86859.
datanode_1          | 2023-06-22 08:48:21,131 [Command processor thread] INFO server.RaftServer: 1b7c5580-f1a4-419a-996c-bb798c761974: addNew group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-C5CBD08DF48F:java.util.concurrent.CompletableFuture@55079e26[Not completed]
datanode_1          | 2023-06-22 08:48:21,153 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974: new RaftServerImpl for group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:48:21,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:48:21,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:48:21,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:48:21,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:21,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:48:21,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:48:21,194 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:48:21,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:48:21,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:48:21,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:48:21,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:21,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:48:21,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:48:21,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:48:21,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:48:21,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:48:21,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:48:21,228 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:48:21,228 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f does not exist. Creating ...
datanode_1          | 2023-06-22 08:48:21,231 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/in_use.lock acquired by nodename 6@9aca8b47df40
datanode_1          | 2023-06-22 08:48:21,234 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f has been successfully formatted.
datanode_1          | 2023-06-22 08:48:21,235 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C5CBD08DF48F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:48:21,260 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:48:21,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:48:21,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:48:21,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:48:21,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:47:37,992 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 9ac8bf9954d7/172.23.0.12
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.3.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_2          | STARTUP_MSG:   java = 11.0.14.1
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:47:38,013 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:47:38,281 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:47:38,734 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:47:39,644 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:47:39,644 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:47:40,326 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:9ac8bf9954d7 ip:172.23.0.12
datanode_2          | 2023-06-22 08:47:41,886 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2          | 2023-06-22 08:47:42,865 [main] INFO reflections.Reflections: Reflections took 818 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_2          | 2023-06-22 08:47:43,556 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-22 08:47:44,291 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:47:44,394 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-22 08:47:44,406 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:47:44,417 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:47:44,530 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:47:44,684 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:47:44,690 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-22 08:47:44,691 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-22 08:47:44,696 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-22 08:47:44,696 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-22 08:47:44,817 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:47:44,824 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:47:52,195 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-22 08:47:52,792 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:47:52,934 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:47:53,549 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:47:53,556 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-22 08:47:53,570 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:47:53,575 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-22 08:47:53,576 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-22 08:47:53,576 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-22 08:47:53,577 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:47:53,591 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:47:53,596 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:47:53,601 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:47:53,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-22 08:47:53,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-22 08:47:53,708 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-22 08:47:55,867 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-22 08:47:55,879 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-22 08:47:55,915 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-22 08:47:55,926 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:47:55,956 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:47:55,962 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:47:56,142 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-22 08:47:56,953 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:47:57,060 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:47:57,350 [main] INFO util.log: Logging initialized @27270ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:47:58,439 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-22 08:47:58,490 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:47:58,530 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:47:58,574 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:47:58,577 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:48:21,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:21,262 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:48:21,262 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:48:21,271 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_1          | 2023-06-22 08:48:21,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:48:21,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:48:21,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:21,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:48:21,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:48:21,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:48:21,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:48:21,273 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:48:21,275 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:21,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:21,294 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:21,294 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:21,299 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:21,303 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:21,303 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:21,304 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:48:21,309 [pool-22-thread-1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:21,322 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5CBD08DF48F,id=1b7c5580-f1a4-419a-996c-bb798c761974
datanode_1          | 2023-06-22 08:48:21,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:21,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:48:21,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:48:21,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:48:21,335 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_1          | 2023-06-22 08:48:21,372 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:21,373 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:24,677 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f.
datanode_1          | 2023-06-22 08:48:24,684 [Command processor thread] INFO server.RaftServer: 1b7c5580-f1a4-419a-996c-bb798c761974: addNew group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-6955AAFEDB90:java.util.concurrent.CompletableFuture@69400159[Not completed]
datanode_1          | 2023-06-22 08:48:24,687 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974: new RaftServerImpl for group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:48:24,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:48:24,700 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:48:24,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:48:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:48:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:48:24,703 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:48:24,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:48:24,707 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:48:24,712 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:48:24,713 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:48:24,715 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:48:24,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:48:24,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:48:24,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:47:58,577 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:47:58,871 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-22 08:47:58,904 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2          | 2023-06-22 08:47:59,174 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:47:59,204 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:47:59,209 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-22 08:47:59,293 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5b251fb9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:47:59,297 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@96897c8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:48:01,058 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@448c92fc{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-14245469987402711076/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-22 08:48:01,147 [main] INFO server.AbstractConnector: Started ServerConnector@382dc417{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:48:01,148 [main] INFO server.Server: Started @31067ms
datanode_2          | 2023-06-22 08:48:01,156 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-22 08:48:01,166 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-22 08:48:01,168 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:48:01,200 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-22 08:48:01,312 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@17c794cd] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-22 08:48:01,866 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.8:9891
datanode_2          | 2023-06-22 08:48:02,184 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:48:04,584 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:04,585 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:05,394 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-22 08:48:05,586 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:05,586 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:06,587 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:06,587 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:07,588 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:08,590 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:09,590 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:48:11,614 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 9ac8bf9954d7/172.23.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.12:55384 remote=recon/172.23.0.8:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.12:55384 remote=recon/172.23.0.8:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-22 08:48:14,598 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 9ac8bf9954d7/172.23.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.12:47664 remote=scm/172.23.0.3:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.12:47664 remote=scm/172.23.0.3:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-22 08:48:16,335 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-9758ee3b-2e22-42ed-841b-a3243c2fe9bd/container.db to cache
datanode_2          | 2023-06-22 08:48:16,335 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-9758ee3b-2e22-42ed-841b-a3243c2fe9bd/container.db for volume DS-9758ee3b-2e22-42ed-841b-a3243c2fe9bd
datanode_2          | 2023-06-22 08:48:16,361 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:48:24,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:48:24,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:48:24,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:48:24,748 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 does not exist. Creating ...
datanode_1          | 2023-06-22 08:48:24,752 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/in_use.lock acquired by nodename 6@9aca8b47df40
datanode_1          | 2023-06-22 08:48:24,756 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 has been successfully formatted.
datanode_1          | 2023-06-22 08:48:24,765 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6955AAFEDB90: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:48:24,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:48:24,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:48:24,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:48:24,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:48:24,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:48:24,787 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:24,788 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:48:24,795 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:48:24,796 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_1          | 2023-06-22 08:48:24,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:48:24,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:48:24,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:24,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:48:24,804 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:48:24,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:48:24,805 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:48:24,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:48:24,810 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:48:24,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:24,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:48:24,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:24,824 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:24,827 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:24,831 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:24,832 [pool-22-thread-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:48:24,835 [pool-22-thread-1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState
datanode_1          | 2023-06-22 08:48:24,871 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:24,872 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:24,872 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6955AAFEDB90,id=1b7c5580-f1a4-419a-996c-bb798c761974
datanode_1          | 2023-06-22 08:48:24,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:48:24,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:48:16,394 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-22 08:48:16,675 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:16,771 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.RaftServer: a617724e-4b77-4a5a-a2b9-215179594314: start RPC server
datanode_2          | 2023-06-22 08:48:16,786 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: a617724e-4b77-4a5a-a2b9-215179594314: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:48:16,787 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: a617724e-4b77-4a5a-a2b9-215179594314: GrpcService started, listening on 9856
datanode_2          | 2023-06-22 08:48:16,787 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: a617724e-4b77-4a5a-a2b9-215179594314: GrpcService started, listening on 9857
datanode_2          | 2023-06-22 08:48:16,808 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a617724e-4b77-4a5a-a2b9-215179594314 is started using port 9858 for RATIS
datanode_2          | 2023-06-22 08:48:16,809 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a617724e-4b77-4a5a-a2b9-215179594314 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-22 08:48:16,809 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a617724e-4b77-4a5a-a2b9-215179594314 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-22 08:48:16,813 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a617724e-4b77-4a5a-a2b9-215179594314: Started
datanode_2          | 2023-06-22 08:48:20,561 [Command processor thread] INFO server.RaftServer: a617724e-4b77-4a5a-a2b9-215179594314: addNew group-8CC44DF914E3:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-8CC44DF914E3:java.util.concurrent.CompletableFuture@5201e43c[Not completed]
datanode_2          | 2023-06-22 08:48:20,633 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314: new RaftServerImpl for group-8CC44DF914E3:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:48:20,635 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:48:20,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:48:20,663 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:48:20,663 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:20,663 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:48:20,663 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:48:20,698 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: ConfigurationManager, init=-1: peers:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:48:20,698 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:48:20,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:48:20,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:48:20,803 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:20,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:48:20,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:48:21,003 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:48:21,004 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:48:21,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:48:21,022 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:48:21,023 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:48:21,023 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/462257b1-74ee-4efa-84c7-8cc44df914e3 does not exist. Creating ...
datanode_2          | 2023-06-22 08:48:21,035 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/462257b1-74ee-4efa-84c7-8cc44df914e3/in_use.lock acquired by nodename 7@9ac8bf9954d7
datanode_2          | 2023-06-22 08:48:21,059 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/462257b1-74ee-4efa-84c7-8cc44df914e3 has been successfully formatted.
datanode_2          | 2023-06-22 08:48:21,137 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8CC44DF914E3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:48:21,139 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:48:21,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:48:21,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:48:21,224 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:48:21,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:48:21,248 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,279 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:48:21,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:48:21,319 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/462257b1-74ee-4efa-84c7-8cc44df914e3
datanode_2          | 2023-06-22 08:48:21,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:48:21,319 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:21,340 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:48:21,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:48:21,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:48:21,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:48:21,364 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:48:21,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:21,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:21,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:21,483 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:48:21,486 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:48:21,488 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: start as a follower, conf=-1: peers:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:21,491 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:48:21,503 [pool-22-thread-1] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState
datanode_2          | 2023-06-22 08:48:21,509 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:21,511 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:21,524 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8CC44DF914E3,id=a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:21,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:21,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:48:21,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:48:21,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:48:21,684 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=462257b1-74ee-4efa-84c7-8cc44df914e3
datanode_2          | 2023-06-22 08:48:21,685 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=462257b1-74ee-4efa-84c7-8cc44df914e3.
datanode_2          | 2023-06-22 08:48:21,685 [Command processor thread] INFO server.RaftServer: a617724e-4b77-4a5a-a2b9-215179594314: addNew group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-C5CBD08DF48F:java.util.concurrent.CompletableFuture@187dff58[Not completed]
datanode_2          | 2023-06-22 08:48:21,708 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314: new RaftServerImpl for group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:48:21,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:48:21,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:48:21,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:48:24,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:48:24,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:48:24,879 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_1          | 2023-06-22 08:48:25,100 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90.
datanode_1          | 2023-06-22 08:48:26,048 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068613693ns, electionTimeout:5043ms
datanode_1          | 2023-06-22 08:48:26,049 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState
datanode_1          | 2023-06-22 08:48:26,049 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:48:26,059 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:48:26,060 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1
datanode_1          | 2023-06-22 08:48:26,066 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:26,067 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-22 08:48:26,068 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1
datanode_1          | 2023-06-22 08:48:26,068 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:48:26,069 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-893682C86859 with new leaderId: 1b7c5580-f1a4-419a-996c-bb798c761974
datanode_1          | 2023-06-22 08:48:26,074 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: change Leader from null to 1b7c5580-f1a4-419a-996c-bb798c761974 at term 1 for becomeLeader, leader elected after 5780ms
datanode_1          | 2023-06-22 08:48:26,083 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:48:26,115 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:48:26,124 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:48:26,128 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:48:26,138 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:48:26,139 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:48:26,151 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:48:26,156 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:48:26,160 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderStateImpl
datanode_1          | 2023-06-22 08:48:26,207 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:48:26,265 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-LeaderElection1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:26,368 [1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-893682C86859-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/cc62a49f-5ae5-4985-9dbe-893682c86859/current/log_inprogress_0
datanode_1          | 2023-06-22 08:48:26,519 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5209863166ns, electionTimeout:5146ms
datanode_1          | 2023-06-22 08:48:26,520 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:26,520 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:48:26,521 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:48:26,521 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:47:38,187 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 3e2ef3bffcdc/172.23.0.11
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.3.0
datanode_1          | 2023-06-22 08:48:26,528 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:26,537 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:26,538 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for a617724e-4b77-4a5a-a2b9-215179594314
datanode_1          | 2023-06-22 08:48:26,543 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:26,547 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_1          | 2023-06-22 08:48:26,666 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:48:26,667 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t1
datanode_1          | 2023-06-22 08:48:26,669 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:48:26,673 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-22 08:48:26,673 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2
datanode_1          | 2023-06-22 08:48:26,675 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection2] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:26,689 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:26,689 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:29,793 [grpc-default-executor-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: receive requestVote(ELECTION, a617724e-4b77-4a5a-a2b9-215179594314, group-6955AAFEDB90, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:48:29,796 [grpc-default-executor-1] INFO impl.VoteContext: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FOLLOWER: accept ELECTION from a617724e-4b77-4a5a-a2b9-215179594314: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:48:29,796 [grpc-default-executor-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a617724e-4b77-4a5a-a2b9-215179594314
datanode_1          | 2023-06-22 08:48:29,796 [grpc-default-executor-1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState
datanode_1          | 2023-06-22 08:48:29,796 [grpc-default-executor-1] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState
datanode_1          | 2023-06-22 08:48:29,796 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState was interrupted
datanode_1          | 2023-06-22 08:48:29,805 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:29,806 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:29,814 [grpc-default-executor-1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90 replies to ELECTION vote request: a617724e-4b77-4a5a-a2b9-215179594314<-1b7c5580-f1a4-419a-996c-bb798c761974#0:OK-t1. Peer's state: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90:t1, leader=null, voted=a617724e-4b77-4a5a-a2b9-215179594314, raftlog=Memoized:1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:30,059 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6955AAFEDB90 with new leaderId: a617724e-4b77-4a5a-a2b9-215179594314
datanode_1          | 2023-06-22 08:48:30,059 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: change Leader from null to a617724e-4b77-4a5a-a2b9-215179594314 at term 1 for appendEntries, leader elected after 5346ms
datanode_1          | 2023-06-22 08:48:30,082 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:30,082 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:48:30,088 [1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-6955AAFEDB90-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/current/log_inprogress_0
datanode_1          | 2023-06-22 08:48:31,842 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5166543690ns, electionTimeout:5152ms
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_3          | STARTUP_MSG:   java = 11.0.14.1
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:47:38,212 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:47:38,524 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:47:39,042 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:47:39,940 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:47:39,940 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:47:40,583 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3e2ef3bffcdc ip:172.23.0.11
datanode_3          | 2023-06-22 08:47:41,957 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3          | 2023-06-22 08:47:42,918 [main] INFO reflections.Reflections: Reflections took 791 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_3          | 2023-06-22 08:47:43,671 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-22 08:47:44,532 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:47:44,613 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-22 08:47:44,623 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:47:44,633 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:47:44,745 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:47:44,833 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:47:44,835 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-22 08:47:44,836 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-22 08:47:44,837 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-22 08:47:44,838 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-22 08:47:44,982 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:47:44,996 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:47:52,419 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-22 08:47:52,937 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:47:53,386 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:47:53,903 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:47:53,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-22 08:47:53,922 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:47:53,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-22 08:47:53,927 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-22 08:47:53,932 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-22 08:47:53,941 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:47:53,942 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:47:53,944 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:47:53,947 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:47:54,002 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:47:54,021 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-22 08:47:54,030 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-22 08:47:55,889 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-22 08:47:55,928 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-22 08:47:55,963 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-22 08:47:55,964 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:47:55,984 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:47:56,028 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:47:56,235 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-22 08:47:57,246 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:47:57,361 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:47:57,528 [main] INFO util.log: Logging initialized @27499ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:47:58,485 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-22 08:47:58,527 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:47:58,569 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:47:58,592 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:47:58,592 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:48:21,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:48:21,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:21,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:48:21,718 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:48:21,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:48:21,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:48:21,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:48:21,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:48:21,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:48:21,720 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f does not exist. Creating ...
datanode_2          | 2023-06-22 08:48:21,736 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/in_use.lock acquired by nodename 7@9ac8bf9954d7
datanode_2          | 2023-06-22 08:48:21,738 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f has been successfully formatted.
datanode_2          | 2023-06-22 08:48:21,740 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C5CBD08DF48F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:48:21,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:48:21,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:48:21,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:48:21,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:48:21,746 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:48:21,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:48:21,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:48:21,747 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_2          | 2023-06-22 08:48:21,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:48:21,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:48:21,749 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:21,752 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:21,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:21,770 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:21,770 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:48:21,770 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:48:21,780 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:21,780 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:48:21,780 [pool-22-thread-1] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:21,801 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5CBD08DF48F,id=a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:21,801 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:21,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:48:21,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:48:21,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:48:21,803 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:21,852 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:21,842 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_2          | 2023-06-22 08:48:24,527 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f.
datanode_2          | 2023-06-22 08:48:24,527 [Command processor thread] INFO server.RaftServer: a617724e-4b77-4a5a-a2b9-215179594314: addNew group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-6955AAFEDB90:java.util.concurrent.CompletableFuture@2796affa[Not completed]
datanode_1          | 2023-06-22 08:48:31,842 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:31,842 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-22 08:48:31,843 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:48:31,843 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3
datanode_1          | 2023-06-22 08:48:31,850 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:31,856 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:31,856 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection:   Response 0: 1b7c5580-f1a4-419a-996c-bb798c761974<-a617724e-4b77-4a5a-a2b9-215179594314#0:OK-t2
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection:   Response 1: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t2
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3
datanode_1          | 2023-06-22 08:48:31,928 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection3] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:31,944 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:31,968 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:31,969 [grpc-default-executor-0] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: receive requestVote(ELECTION, b154d2ec-a7d7-4c34-b9eb-19131e12eba0, group-C5CBD08DF48F, 2, (t:0, i:0))
datanode_1          | 2023-06-22 08:48:31,970 [grpc-default-executor-0] INFO impl.VoteContext: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FOLLOWER: reject ELECTION from b154d2ec-a7d7-4c34-b9eb-19131e12eba0: already has voted for 1b7c5580-f1a4-419a-996c-bb798c761974 at current term 2
datanode_1          | 2023-06-22 08:48:31,973 [grpc-default-executor-0] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F replies to ELECTION vote request: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-1b7c5580-f1a4-419a-996c-bb798c761974#0:FAIL-t2. Peer's state: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F:t2, leader=null, voted=1b7c5580-f1a4-419a-996c-bb798c761974, raftlog=Memoized:1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:37,001 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072390932ns, electionTimeout:5032ms
datanode_1          | 2023-06-22 08:48:37,001 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:37,001 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-06-22 08:48:37,001 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:48:37,001 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4
datanode_1          | 2023-06-22 08:48:37,004 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:37,005 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:37,005 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:24,530 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314: new RaftServerImpl for group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:48:24,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:48:24,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:48:24,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:48:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:48:24,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:48:24,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:48:24,537 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:48:24,537 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:48:24,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:48:24,539 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:48:24,539 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:48:24,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:48:24,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:48:24,541 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 does not exist. Creating ...
datanode_2          | 2023-06-22 08:48:24,545 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/in_use.lock acquired by nodename 7@9ac8bf9954d7
datanode_2          | 2023-06-22 08:48:24,561 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 has been successfully formatted.
datanode_2          | 2023-06-22 08:48:24,563 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6955AAFEDB90: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:48:24,563 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:48:24,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:48:24,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:48:24,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:48:24,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:48:24,572 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:24,573 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:48:24,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:48:24,596 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:48:24,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:48:24,598 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:48:24,598 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:48:24,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:48:24,605 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:24,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:48:24,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:24,630 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:48:24,660 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:48:37,035 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:48:37,035 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.LeaderElection:   Response 0: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t3
datanode_1          | 2023-06-22 08:48:37,035 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-22 08:48:37,035 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_1          | 2023-06-22 08:48:37,036 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4
datanode_1          | 2023-06-22 08:48:37,036 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection4] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:37,036 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:37,051 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:42,076 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5040669846ns, electionTimeout:5024ms
datanode_1          | 2023-06-22 08:48:42,077 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:42,077 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_1          | 2023-06-22 08:48:42,077 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-22 08:48:42,077 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection5
datanode_1          | 2023-06-22 08:48:42,083 [grpc-default-executor-0] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: receive requestVote(ELECTION, b154d2ec-a7d7-4c34-b9eb-19131e12eba0, group-C5CBD08DF48F, 4, (t:0, i:0))
datanode_1          | 2023-06-22 08:48:42,084 [grpc-default-executor-0] INFO impl.VoteContext: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-CANDIDATE: accept ELECTION from b154d2ec-a7d7-4c34-b9eb-19131e12eba0: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:48:42,084 [grpc-default-executor-0] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: changes role from CANDIDATE to FOLLOWER at term 4 for candidate:b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_1          | 2023-06-22 08:48:42,084 [grpc-default-executor-0] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: shutdown 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection5
datanode_1          | 2023-06-22 08:48:42,084 [grpc-default-executor-0] INFO impl.RoleInfo: 1b7c5580-f1a4-419a-996c-bb798c761974: start 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState
datanode_1          | 2023-06-22 08:48:42,085 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection5] INFO impl.LeaderElection: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-LeaderElection5: skip running since this is already CLOSING
datanode_1          | 2023-06-22 08:48:42,092 [grpc-default-executor-0] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F replies to ELECTION vote request: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-1b7c5580-f1a4-419a-996c-bb798c761974#0:OK-t4. Peer's state: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F:t4, leader=null, voted=b154d2ec-a7d7-4c34-b9eb-19131e12eba0, raftlog=Memoized:1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:42,095 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:48:42,095 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:48:42,188 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C5CBD08DF48F with new leaderId: b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_1          | 2023-06-22 08:48:42,193 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: change Leader from null to b154d2ec-a7d7-4c34-b9eb-19131e12eba0 at term 4 for appendEntries, leader elected after 20988ms
datanode_1          | 2023-06-22 08:48:42,194 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO server.RaftServer$Division: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:48:42,195 [1b7c5580-f1a4-419a-996c-bb798c761974-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:48:42,197 [1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1b7c5580-f1a4-419a-996c-bb798c761974@group-C5CBD08DF48F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/current/log_inprogress_0
datanode_3          | 2023-06-22 08:47:58,593 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:47:59,057 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-22 08:47:59,069 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3          | 2023-06-22 08:47:59,381 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:47:59,388 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:47:59,405 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-22 08:47:59,502 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79afa369{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-22 08:47:59,521 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79ba0a6f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:48:01,551 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5a4dda2{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-10747473669579744374/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:48:01,620 [main] INFO server.AbstractConnector: Started ServerConnector@4f363abd{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:48:01,621 [main] INFO server.Server: Started @31591ms
datanode_3          | 2023-06-22 08:48:01,651 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-22 08:48:01,654 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-22 08:48:01,656 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:48:01,674 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-22 08:48:01,782 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@69dc9123] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-22 08:48:02,260 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.8:9891
datanode_3          | 2023-06-22 08:48:02,472 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:48:04,990 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:04,991 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:05,991 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:05,992 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.8:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:06,992 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:07,993 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:08,994 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:09,995 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.3:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:48:11,027 [EndpointStateMachine task thread for recon/172.23.0.8:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 3e2ef3bffcdc/172.23.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:51332 remote=recon/172.23.0.8:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:51332 remote=recon/172.23.0.8:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-06-22 08:48:15,010 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 3e2ef3bffcdc/172.23.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:47030 remote=scm/172.23.0.3:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.11:47030 remote=scm/172.23.0.3:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-06-22 08:48:16,207 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-624a669c-37ff-45ca-b97b-a4f3e27a38c8/container.db to cache
datanode_3          | 2023-06-22 08:48:16,207 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc/DS-624a669c-37ff-45ca-b97b-a4f3e27a38c8/container.db for volume DS-624a669c-37ff-45ca-b97b-a4f3e27a38c8
datanode_3          | 2023-06-22 08:48:16,216 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-22 08:48:16,217 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-22 08:48:16,327 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:16,459 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.RaftServer: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start RPC server
datanode_3          | 2023-06-22 08:48:16,521 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: GrpcService started, listening on 9858
datanode_3          | 2023-06-22 08:48:16,529 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: GrpcService started, listening on 9856
datanode_3          | 2023-06-22 08:48:16,538 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO server.GrpcService: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: GrpcService started, listening on 9857
datanode_3          | 2023-06-22 08:48:16,552 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b154d2ec-a7d7-4c34-b9eb-19131e12eba0: Started
datanode_3          | 2023-06-22 08:48:16,565 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b154d2ec-a7d7-4c34-b9eb-19131e12eba0 is started using port 9858 for RATIS
datanode_3          | 2023-06-22 08:48:16,565 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b154d2ec-a7d7-4c34-b9eb-19131e12eba0 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-22 08:48:16,565 [EndpointStateMachine task thread for scm/172.23.0.3:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b154d2ec-a7d7-4c34-b9eb-19131e12eba0 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-22 08:48:21,028 [Command processor thread] INFO server.RaftServer: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: addNew group-C793397D0186:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-C793397D0186:java.util.concurrent.CompletableFuture@50a7157d[Not completed]
datanode_3          | 2023-06-22 08:48:21,125 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: new RaftServerImpl for group-C793397D0186:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:48:21,139 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:48:21,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:48:21,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:48:21,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:21,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:48:21,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:48:21,167 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: ConfigurationManager, init=-1: peers:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:48:21,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:48:21,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:48:21,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:48:21,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:21,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:48:21,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:48:21,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:48:21,461 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:48:21,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:48:21,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:48:21,466 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:48:21,466 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/457ee19a-07d7-4722-90e0-c793397d0186 does not exist. Creating ...
datanode_3          | 2023-06-22 08:48:21,492 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/457ee19a-07d7-4722-90e0-c793397d0186/in_use.lock acquired by nodename 7@3e2ef3bffcdc
datanode_3          | 2023-06-22 08:48:21,523 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/457ee19a-07d7-4722-90e0-c793397d0186 has been successfully formatted.
datanode_3          | 2023-06-22 08:48:21,553 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C793397D0186: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:48:21,561 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:48:21,631 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:48:21,649 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:48:21,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:48:21,662 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:48:21,665 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:21,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:48:21,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:48:21,759 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/457ee19a-07d7-4722-90e0-c793397d0186
datanode_3          | 2023-06-22 08:48:21,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:48:21,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:21,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:21,787 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:48:21,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:48:21,790 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:48:21,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:48:21,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:48:21,867 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:21,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:21,887 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:21,887 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:21,914 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:21,920 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:21,927 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: start as a follower, conf=-1: peers:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:21,929 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:48:24,675 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:24,676 [pool-22-thread-1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:48:24,676 [pool-22-thread-1] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState
datanode_2          | 2023-06-22 08:48:24,690 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6955AAFEDB90,id=a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:24,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:48:24,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:48:24,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:48:24,691 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:48:24,692 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:24,692 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_2          | 2023-06-22 08:48:24,715 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:24,965 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90.
datanode_2          | 2023-06-22 08:48:26,554 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058974595ns, electionTimeout:5041ms
datanode_2          | 2023-06-22 08:48:26,554 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState
datanode_2          | 2023-06-22 08:48:26,555 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:48:26,558 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:48:26,558 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-FollowerState] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1
datanode_2          | 2023-06-22 08:48:26,566 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO impl.LeaderElection: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:26,567 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO impl.LeaderElection: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-22 08:48:26,567 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1
datanode_2          | 2023-06-22 08:48:26,568 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:48:26,568 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8CC44DF914E3 with new leaderId: a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:26,570 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: change Leader from null to a617724e-4b77-4a5a-a2b9-215179594314 at term 1 for becomeLeader, leader elected after 5780ms
datanode_2          | 2023-06-22 08:48:26,585 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:48:26,606 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:48:26,617 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:26,617 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:48:26,624 [grpc-default-executor-0] INFO impl.VoteContext: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FOLLOWER: accept ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:48:26,627 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1b7c5580-f1a4-419a-996c-bb798c761974
datanode_2          | 2023-06-22 08:48:26,629 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:26,630 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState was interrupted
datanode_2          | 2023-06-22 08:48:26,645 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:26,658 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:48:26,667 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:26,668 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:26,668 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:48:26,669 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:48:26,717 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:26,729 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:48:26,731 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-a617724e-4b77-4a5a-a2b9-215179594314#0:OK-t1. Peer's state: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F:t1, leader=null, voted=1b7c5580-f1a4-419a-996c-bb798c761974, raftlog=Memoized:a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:26,740 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderStateImpl
datanode_2          | 2023-06-22 08:48:26,814 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:48:26,850 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-LeaderElection1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3: set configuration 0: peers:[a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:26,949 [a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-8CC44DF914E3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/462257b1-74ee-4efa-84c7-8cc44df914e3/current/log_inprogress_0
datanode_2          | 2023-06-22 08:48:29,752 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5075774834ns, electionTimeout:5037ms
datanode_2          | 2023-06-22 08:48:29,752 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState
datanode_2          | 2023-06-22 08:48:29,753 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:48:29,753 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-22 08:48:29,753 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-FollowerState] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2
datanode_2          | 2023-06-22 08:48:29,760 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.LeaderElection: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:29,774 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 1b7c5580-f1a4-419a-996c-bb798c761974
datanode_2          | 2023-06-22 08:48:29,774 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:29,774 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:29,778 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_2          | 2023-06-22 08:48:29,843 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.LeaderElection: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:48:29,843 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.LeaderElection:   Response 0: a617724e-4b77-4a5a-a2b9-215179594314<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:OK-t1
datanode_2          | 2023-06-22 08:48:29,843 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.LeaderElection: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2 ELECTION round 0: result PASSED
datanode_2          | 2023-06-22 08:48:29,843 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2
datanode_2          | 2023-06-22 08:48:29,843 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:48:29,844 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6955AAFEDB90 with new leaderId: a617724e-4b77-4a5a-a2b9-215179594314
datanode_2          | 2023-06-22 08:48:29,871 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: change Leader from null to a617724e-4b77-4a5a-a2b9-215179594314 at term 1 for becomeLeader, leader elected after 5307ms
datanode_2          | 2023-06-22 08:48:29,871 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:48:21,931 [pool-22-thread-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState
datanode_3          | 2023-06-22 08:48:21,959 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C793397D0186,id=b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:21,965 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:21,965 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:21,971 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:21,971 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:48:21,974 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:48:21,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:48:22,080 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=457ee19a-07d7-4722-90e0-c793397d0186
datanode_3          | 2023-06-22 08:48:22,096 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=457ee19a-07d7-4722-90e0-c793397d0186.
datanode_3          | 2023-06-22 08:48:22,097 [Command processor thread] INFO server.RaftServer: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: addNew group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-C5CBD08DF48F:java.util.concurrent.CompletableFuture@71481569[Not completed]
datanode_3          | 2023-06-22 08:48:22,120 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: new RaftServerImpl for group-C5CBD08DF48F:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:48:22,123 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:48:22,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:48:22,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:48:22,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:22,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:48:22,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:48:22,128 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:48:22,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:48:22,129 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:48:22,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:48:22,140 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:22,141 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:48:22,141 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:48:22,143 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:48:22,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:48:22,150 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:48:22,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:48:22,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:48:22,151 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f does not exist. Creating ...
datanode_3          | 2023-06-22 08:48:22,161 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/in_use.lock acquired by nodename 7@3e2ef3bffcdc
datanode_3          | 2023-06-22 08:48:22,166 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f has been successfully formatted.
datanode_3          | 2023-06-22 08:48:22,168 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-C5CBD08DF48F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:48:22,170 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:48:22,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:48:22,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:48:22,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:48:22,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:48:22,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:22,185 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:48:22,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:48:22,188 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_3          | 2023-06-22 08:48:22,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:48:22,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:22,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:22,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:48:22,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:48:22,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:48:22,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:48:22,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:48:22,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:22,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:22,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:22,215 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:22,216 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:22,216 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:22,220 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:22,226 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:48:22,227 [pool-22-thread-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:22,236 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5CBD08DF48F,id=b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:22,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:22,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:48:22,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:48:22,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:48:22,246 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:22,262 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f
datanode_3          | 2023-06-22 08:48:22,271 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:24,672 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f.
datanode_3          | 2023-06-22 08:48:24,684 [Command processor thread] INFO server.RaftServer: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: addNew group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] returns group-6955AAFEDB90:java.util.concurrent.CompletableFuture@6768f91f[Not completed]
datanode_3          | 2023-06-22 08:48:24,689 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: new RaftServerImpl for group-6955AAFEDB90:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:48:24,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:48:24,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:48:24,700 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:48:24,700 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:24,700 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:48:24,700 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:48:24,701 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: ConfigurationManager, init=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:48:24,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:48:24,703 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:48:29,872 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:29,872 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:48:29,872 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:48:29,872 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:48:29,872 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:48:29,873 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:48:29,873 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:48:29,903 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:48:29,903 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:48:29,908 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:48:29,914 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:48:29,914 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:48:29,919 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:48:29,919 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-22 08:48:29,919 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-22 08:48:29,921 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-22 08:48:29,921 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:48:29,926 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-22 08:48:29,926 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-22 08:48:29,926 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:48:29,927 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:48:29,927 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-22 08:48:29,927 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-22 08:48:29,928 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderStateImpl
datanode_2          | 2023-06-22 08:48:29,932 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:48:29,935 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/current/log_inprogress_0
datanode_2          | 2023-06-22 08:48:29,953 [a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90-LeaderElection2] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-6955AAFEDB90: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:31,727 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:31,727 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:31,878 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:48:31,891 [grpc-default-executor-0] INFO impl.VoteContext: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FOLLOWER: accept ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:48:31,891 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:1b7c5580-f1a4-419a-996c-bb798c761974
datanode_2          | 2023-06-22 08:48:31,891 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:31,891 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState was interrupted
datanode_2          | 2023-06-22 08:48:31,891 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:31,894 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:31,894 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:31,901 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-a617724e-4b77-4a5a-a2b9-215179594314#0:OK-t2. Peer's state: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F:t2, leader=null, voted=1b7c5580-f1a4-419a-996c-bb798c761974, raftlog=Memoized:a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:31,957 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: receive requestVote(ELECTION, b154d2ec-a7d7-4c34-b9eb-19131e12eba0, group-C5CBD08DF48F, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:48:31,957 [grpc-default-executor-0] INFO impl.VoteContext: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FOLLOWER: reject ELECTION from b154d2ec-a7d7-4c34-b9eb-19131e12eba0: already has voted for 1b7c5580-f1a4-419a-996c-bb798c761974 at current term 2
datanode_2          | 2023-06-22 08:48:31,957 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F replies to ELECTION vote request: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-a617724e-4b77-4a5a-a2b9-215179594314#0:FAIL-t2. Peer's state: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F:t2, leader=null, voted=1b7c5580-f1a4-419a-996c-bb798c761974, raftlog=Memoized:a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:36,937 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:36,937 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:37,030 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 3, (t:0, i:0))
datanode_2          | 2023-06-22 08:48:37,030 [grpc-default-executor-0] INFO impl.VoteContext: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FOLLOWER: accept ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:48:37,032 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:1b7c5580-f1a4-419a-996c-bb798c761974
datanode_2          | 2023-06-22 08:48:37,032 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:37,033 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:37,033 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState was interrupted
datanode_2          | 2023-06-22 08:48:37,042 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:37,044 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-a617724e-4b77-4a5a-a2b9-215179594314#0:OK-t3. Peer's state: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F:t3, leader=null, voted=1b7c5580-f1a4-419a-996c-bb798c761974, raftlog=Memoized:a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:37,054 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:42,088 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: receive requestVote(ELECTION, b154d2ec-a7d7-4c34-b9eb-19131e12eba0, group-C5CBD08DF48F, 4, (t:0, i:0))
datanode_2          | 2023-06-22 08:48:42,090 [grpc-default-executor-0] INFO impl.VoteContext: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FOLLOWER: accept ELECTION from b154d2ec-a7d7-4c34-b9eb-19131e12eba0: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:48:42,090 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_2          | 2023-06-22 08:48:42,090 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: shutdown a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:42,090 [grpc-default-executor-0] INFO impl.RoleInfo: a617724e-4b77-4a5a-a2b9-215179594314: start a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState
datanode_2          | 2023-06-22 08:48:42,091 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState was interrupted
datanode_2          | 2023-06-22 08:48:42,102 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:48:42,102 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:48:42,105 [grpc-default-executor-0] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F replies to ELECTION vote request: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-a617724e-4b77-4a5a-a2b9-215179594314#0:OK-t4. Peer's state: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F:t4, leader=null, voted=b154d2ec-a7d7-4c34-b9eb-19131e12eba0, raftlog=Memoized:a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:42,186 [a617724e-4b77-4a5a-a2b9-215179594314-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C5CBD08DF48F with new leaderId: b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_2          | 2023-06-22 08:48:42,195 [a617724e-4b77-4a5a-a2b9-215179594314-server-thread1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: change Leader from null to b154d2ec-a7d7-4c34-b9eb-19131e12eba0 at term 4 for appendEntries, leader elected after 20468ms
datanode_2          | 2023-06-22 08:48:42,209 [a617724e-4b77-4a5a-a2b9-215179594314-server-thread1] INFO server.RaftServer$Division: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:48:42,209 [a617724e-4b77-4a5a-a2b9-215179594314-server-thread1] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:48:42,213 [a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a617724e-4b77-4a5a-a2b9-215179594314@group-C5CBD08DF48F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/current/log_inprogress_0
datanode_3          | 2023-06-22 08:48:24,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:48:24,705 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:48:24,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:48:24,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:48:24,707 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:48:24,709 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:48:24,712 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:48:24,712 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:48:24,713 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:48:24,713 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 does not exist. Creating ...
datanode_3          | 2023-06-22 08:48:24,734 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/in_use.lock acquired by nodename 7@3e2ef3bffcdc
datanode_3          | 2023-06-22 08:48:24,741 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90 has been successfully formatted.
datanode_3          | 2023-06-22 08:48:24,755 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6955AAFEDB90: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:48:24,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:48:24,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:48:24,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:48:24,757 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:48:24,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:48:24,764 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:24,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:48:24,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:48:24,765 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_3          | 2023-06-22 08:48:24,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:48:24,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:24,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:24,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:48:24,772 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:48:24,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:48:24,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:48:24,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:48:24,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:48:24,781 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:24,793 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:48:24,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:24,802 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:24,802 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:48:24,810 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: start as a follower, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:24,811 [pool-22-thread-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:48:24,811 [pool-22-thread-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState
datanode_3          | 2023-06-22 08:48:24,812 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6955AAFEDB90,id=b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:24,813 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:24,829 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:48:24,829 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:48:24,829 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:48:24,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:48:24,829 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:24,858 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90
datanode_3          | 2023-06-22 08:48:25,138 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90.
datanode_3          | 2023-06-22 08:48:26,629 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:48:26,631 [grpc-default-executor-1] INFO impl.VoteContext: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FOLLOWER: reject ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-22 08:48:26,632 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1b7c5580-f1a4-419a-996c-bb798c761974
datanode_3          | 2023-06-22 08:48:26,633 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:26,633 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState was interrupted
datanode_3          | 2023-06-22 08:48:26,633 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:26,644 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:26,644 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:26,656 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t1. Peer's state: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F:t1, leader=null, voted=null, raftlog=Memoized:b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:27,067 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5136933337ns, electionTimeout:5101ms
datanode_3          | 2023-06-22 08:48:27,069 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState
datanode_3          | 2023-06-22 08:48:27,069 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:48:27,073 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:48:27,073 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1
datanode_3          | 2023-06-22 08:48:27,080 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:27,081 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-22 08:48:27,081 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1
datanode_3          | 2023-06-22 08:48:27,082 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:48:27,082 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C793397D0186 with new leaderId: b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:27,082 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: change Leader from null to b154d2ec-a7d7-4c34-b9eb-19131e12eba0 at term 1 for becomeLeader, leader elected after 5855ms
datanode_3          | 2023-06-22 08:48:27,096 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:48:27,106 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:27,109 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:48:27,113 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:48:27,119 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:48:27,120 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:48:27,128 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:27,129 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:48:27,131 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderStateImpl
datanode_3          | 2023-06-22 08:48:27,155 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:48:27,204 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-LeaderElection1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186: set configuration 0: peers:[b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:27,368 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C793397D0186-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/457ee19a-07d7-4722-90e0-c793397d0186/current/log_inprogress_0
datanode_3          | 2023-06-22 08:48:29,821 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: receive requestVote(ELECTION, a617724e-4b77-4a5a-a2b9-215179594314, group-6955AAFEDB90, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:48:29,822 [grpc-default-executor-1] INFO impl.VoteContext: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FOLLOWER: accept ELECTION from a617724e-4b77-4a5a-a2b9-215179594314: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-22 08:48:29,822 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:a617724e-4b77-4a5a-a2b9-215179594314
datanode_3          | 2023-06-22 08:48:29,822 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState
datanode_3          | 2023-06-22 08:48:29,822 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState was interrupted
datanode_3          | 2023-06-22 08:48:29,823 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState
datanode_3          | 2023-06-22 08:48:29,825 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:29,826 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:29,829 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90 replies to ELECTION vote request: a617724e-4b77-4a5a-a2b9-215179594314<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:OK-t1. Peer's state: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90:t1, leader=null, voted=a617724e-4b77-4a5a-a2b9-215179594314, raftlog=Memoized:b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:30,057 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6955AAFEDB90 with new leaderId: a617724e-4b77-4a5a-a2b9-215179594314
datanode_3          | 2023-06-22 08:48:30,058 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0-server-thread2] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: change Leader from null to a617724e-4b77-4a5a-a2b9-215179594314 at term 1 for appendEntries, leader elected after 5352ms
datanode_3          | 2023-06-22 08:48:30,068 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0-server-thread2] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:1|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:30,073 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0-server-thread2] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:48:30,077 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-6955AAFEDB90-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b4ac565-95b2-4c6c-b245-6955aafedb90/current/log_inprogress_0
datanode_3          | 2023-06-22 08:48:31,798 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5164118090ns, electionTimeout:5153ms
datanode_3          | 2023-06-22 08:48:31,798 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:31,799 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-22 08:48:31,799 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:48:31,799 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2
datanode_3          | 2023-06-22 08:48:31,829 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:31,847 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:31,847 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:31,847 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 1b7c5580-f1a4-419a-996c-bb798c761974
datanode_3          | 2023-06-22 08:48:31,851 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for a617724e-4b77-4a5a-a2b9-215179594314
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:47:36,696 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = a0e5902270c2/172.23.0.2
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.3.0
datanode_3          | 2023-06-22 08:48:31,917 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 2, (t:0, i:0))
datanode_3          | 2023-06-22 08:48:31,918 [grpc-default-executor-1] INFO impl.VoteContext: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-CANDIDATE: reject ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: already has voted for b154d2ec-a7d7-4c34-b9eb-19131e12eba0 at current term 2
datanode_3          | 2023-06-22 08:48:31,918 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t2. Peer's state: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F:t2, leader=null, voted=b154d2ec-a7d7-4c34-b9eb-19131e12eba0, raftlog=Memoized:b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:31,980 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:48:31,980 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection:   Response 0: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-1b7c5580-f1a4-419a-996c-bb798c761974#0:FAIL-t2
datanode_3          | 2023-06-22 08:48:31,980 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection:   Response 1: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-a617724e-4b77-4a5a-a2b9-215179594314#0:FAIL-t2
datanode_3          | 2023-06-22 08:48:31,985 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-22 08:48:31,985 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-06-22 08:48:31,986 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2
datanode_3          | 2023-06-22 08:48:31,986 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection2] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:31,988 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:31,989 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:37,009 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: receive requestVote(ELECTION, 1b7c5580-f1a4-419a-996c-bb798c761974, group-C5CBD08DF48F, 3, (t:0, i:0))
datanode_3          | 2023-06-22 08:48:37,010 [grpc-default-executor-1] INFO impl.VoteContext: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FOLLOWER: reject ELECTION from 1b7c5580-f1a4-419a-996c-bb798c761974: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-22 08:48:37,010 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:1b7c5580-f1a4-419a-996c-bb798c761974
datanode_3          | 2023-06-22 08:48:37,010 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:37,010 [grpc-default-executor-1] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:37,011 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:37,011 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState was interrupted
datanode_3          | 2023-06-22 08:48:37,011 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:37,013 [grpc-default-executor-1] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F replies to ELECTION vote request: 1b7c5580-f1a4-419a-996c-bb798c761974<-b154d2ec-a7d7-4c34-b9eb-19131e12eba0#0:FAIL-t3. Peer's state: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F:t3, leader=null, voted=null, raftlog=Memoized:b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:42,063 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.FollowerState: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5052972131ns, electionTimeout:5051ms
datanode_3          | 2023-06-22 08:48:42,063 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState
datanode_3          | 2023-06-22 08:48:42,064 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_3          | 2023-06-22 08:48:42,064 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-22 08:48:42,064 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-FollowerState] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-22 08:47:36,731 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:47:43,459 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:47:46,053 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:47:46,416 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.2:9862
om_1                | 2023-06-22 08:47:46,417 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:47:46,417 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:47:46,572 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:47:47,551 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863]
om_1                | 2023-06-22 08:47:50,912 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:47:52,914 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:47:54,916 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:47:56,917 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:47:58,919 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:00,921 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:02,923 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:04,925 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:06,927 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:08,929 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a0e5902270c2/172.23.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:11,556 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3584b68e-f715-43e2-a929-0d9b7ab53a61 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:13,562 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3584b68e-f715-43e2-a929-0d9b7ab53a61 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-22 08:48:15,569 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3584b68e-f715-43e2-a929-0d9b7ab53a61 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc;layoutVersion=3
om_1                | 2023-06-22 08:48:17,633 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:47:33,695 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 47653feb929c/172.23.0.3
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-22 08:47:37,803 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-22 08:47:37,821 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-22 08:47:38,010 [main] INFO util.log: Logging initialized @7821ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-22 08:47:39,056 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-22 08:47:39,302 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-22 08:47:39,350 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-22 08:47:39,372 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-22 08:47:39,376 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-22 08:47:39,377 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-22 08:47:39,711 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 02ca2ecef529/172.23.0.6
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1               | STARTUP_MSG:   java = 11.0.14.1
s3g_1               | ************************************************************/
s3g_1               | 2023-06-22 08:47:39,732 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-22 08:47:39,887 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-22 08:47:40,421 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-22 08:47:41,410 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-22 08:47:41,416 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-22 08:47:41,661 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-22 08:47:41,667 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1               | 2023-06-22 08:47:41,884 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-22 08:47:41,900 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-22 08:47:41,902 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-22 08:47:42,218 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-22 08:47:42,238 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 22, 2023 8:48:03 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-22 08:48:03,362 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-8542396621216180576/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-22 08:48:03,374 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-22 08:48:03,375 [main] INFO server.Server: Started @33186ms
s3g_1               | 2023-06-22 08:48:03,378 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-22 08:48:03,379 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-22 08:48:03,381 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at a0e5902270c2/172.23.0.2
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:48:19,742 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = a0e5902270c2/172.23.0.2
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:47:33,809 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:47:34,441 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:47:34,869 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:47:35,032 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:47:37,089 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-22 08:47:38,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:47:38,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:47:38,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:47:38,684 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:47:38,688 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-22 08:47:38,697 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-22 08:47:38,698 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-22 08:47:38,736 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:47:38,741 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-22 08:47:38,816 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:47:39,007 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-22 08:47:39,079 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-22 08:47:39,143 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-22 08:47:42,328 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-22 08:47:42,394 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-22 08:47:42,405 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-22 08:47:42,432 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:47:42,440 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:47:42,520 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:47:42,723 [main] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: addNew group-1EE81D72D3BC:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|priority:0|startupRole:FOLLOWER] returns group-1EE81D72D3BC:java.util.concurrent.CompletableFuture@6e16b8b5[Not completed]
scm_1               | 2023-06-22 08:47:43,058 [pool-2-thread-1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61: new RaftServerImpl for group-1EE81D72D3BC:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-22 08:47:43,100 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-22 08:47:43,121 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-22 08:47:43,121 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-22 08:47:43,121 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:47:43,124 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:47:43,124 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-22 08:47:43,214 [pool-2-thread-1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: ConfigurationManager, init=-1: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-22 08:47:43,232 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:47:43,439 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-22 08:47:43,464 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:47:43,705 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-22 08:47:43,787 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-22 08:47:43,799 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-22 08:47:44,246 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-22 08:47:46,306 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-22 08:47:46,344 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-22 08:47:46,376 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-22 08:47:46,383 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-22 08:47:46,384 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-22 08:47:46,397 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc does not exist. Creating ...
scm_1               | 2023-06-22 08:47:46,521 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/in_use.lock acquired by nodename 13@47653feb929c
scm_1               | 2023-06-22 08:47:46,734 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc has been successfully formatted.
scm_1               | 2023-06-22 08:47:46,810 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-22 08:47:46,905 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-22 08:47:46,920 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:47:46,929 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-22 08:47:46,961 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-22 08:47:46,970 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:47:47,082 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-22 08:47:47,100 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:47:47,447 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc
scm_1               | 2023-06-22 08:47:47,465 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:47:47,466 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:47:47,468 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:47:47,512 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-22 08:47:47,516 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-22 08:47:47,528 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-22 08:47:47,529 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:47:47,542 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:47:47,654 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:47:47,678 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:47:47,696 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:47:47,697 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:47:47,752 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:47:47,763 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:47:47,791 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: start as a follower, conf=-1: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:47:47,793 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-22 08:47:47,809 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState
scm_1               | 2023-06-22 08:47:47,874 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-22 08:47:47,889 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-22 08:47:47,918 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1EE81D72D3BC,id=3584b68e-f715-43e2-a929-0d9b7ab53a61
scm_1               | 2023-06-22 08:47:47,923 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-22 08:47:47,972 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-22 08:47:47,988 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:47:47,990 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-22 08:47:48,124 [main] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start RPC server
scm_1               | 2023-06-22 08:47:48,753 [main] INFO server.GrpcService: 3584b68e-f715-43e2-a929-0d9b7ab53a61: GrpcService started, listening on 9894
scm_1               | 2023-06-22 08:47:48,789 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3584b68e-f715-43e2-a929-0d9b7ab53a61: Started
scm_1               | 2023-06-22 08:47:53,052 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.FollowerState: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5244014193ns, electionTimeout:5155ms
scm_1               | 2023-06-22 08:47:53,060 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-22 08:48:19,762 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:48:23,714 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:48:25,176 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:48:25,299 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.2:9862
om_1                | 2023-06-22 08:48:25,299 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:48:25,299 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:48:25,336 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:48:25,387 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1                | 2023-06-22 08:48:25,971 [main] INFO reflections.Reflections: Reflections took 506 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1                | 2023-06-22 08:48:25,996 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:48:26,675 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863]
om_1                | 2023-06-22 08:48:26,794 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9863]
om_1                | 2023-06-22 08:48:28,050 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:48:28,405 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:48:28,407 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-22 08:48:28,852 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-22 08:48:28,934 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:48:28,990 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:48:28,991 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-22 08:48:29,013 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:48:29,023 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:48:29,050 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-22 08:48:29,059 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-22 08:48:29,149 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-22 08:48:29,212 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:48:29,213 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:48:29,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:48:29,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:48:29,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-22 08:48:29,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:48:29,215 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-22 08:48:29,217 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:48:29,217 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-22 08:48:29,218 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:48:29,227 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-22 08:48:29,230 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:47:37,373 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = b5f36d7f7473/172.23.0.8
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.3.0
om_1                | 2023-06-22 08:48:29,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-22 08:48:29,407 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-22 08:48:29,410 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-22 08:48:29,411 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-22 08:48:29,411 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:48:29,411 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:48:29,415 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:48:29,432 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@364fd4ae[Not completed]
om_1                | 2023-06-22 08:48:29,433 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-22 08:48:29,470 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-22 08:48:29,471 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-22 08:48:29,473 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-22 08:48:29,473 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-22 08:48:29,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-22 08:48:29,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:48:29,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:48:29,474 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-22 08:48:29,492 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-22 08:48:29,493 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:48:29,507 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-22 08:48:29,508 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-22 08:48:29,556 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-22 08:48:29,565 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-22 08:48:29,565 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-22 08:48:29,724 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-22 08:48:29,730 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-22 08:48:29,738 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-22 08:48:29,739 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-22 08:48:29,748 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-22 08:48:30,389 [main] INFO reflections.Reflections: Reflections took 891 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om_1                | 2023-06-22 08:48:30,496 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:48:30,508 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:48:30,645 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:48:30,689 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:48:30,689 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-22 08:48:30,755 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.23.0.2:9862
om_1                | 2023-06-22 08:48:30,756 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-22 08:48:30,758 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-22 08:48:30,764 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@a0e5902270c2
om_1                | 2023-06-22 08:48:30,779 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-22 08:48:30,783 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-22 08:48:30,799 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-22 08:48:30,800 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:48:30,805 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-22 08:48:30,806 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-22 08:48:30,809 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:48:30,815 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-22 08:48:30,816 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-22 08:48:30,822 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-22 08:48:30,823 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:48:30,824 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-22 08:48:30,825 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:48:30,825 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-22 08:48:30,825 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:48:42,067 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for -1: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:42,081 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:48:42,081 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:48:42,098 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:48:42,098 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection:   Response 0: b154d2ec-a7d7-4c34-b9eb-19131e12eba0<-1b7c5580-f1a4-419a-996c-bb798c761974#0:OK-t4
datanode_3          | 2023-06-22 08:48:42,099 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.LeaderElection: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-06-22 08:48:42,099 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: shutdown b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3
datanode_3          | 2023-06-22 08:48:42,099 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
datanode_3          | 2023-06-22 08:48:42,099 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C5CBD08DF48F with new leaderId: b154d2ec-a7d7-4c34-b9eb-19131e12eba0
datanode_3          | 2023-06-22 08:48:42,100 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: change Leader from null to b154d2ec-a7d7-4c34-b9eb-19131e12eba0 at term 4 for becomeLeader, leader elected after 19958ms
datanode_3          | 2023-06-22 08:48:42,100 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:48:42,101 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:42,102 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:48:42,103 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:48:42,103 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:48:42,103 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:48:42,104 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:48:42,104 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:48:42,124 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:48:42,124 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:48:42,124 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:48:42,127 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:48:42,128 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:48:42,128 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:48:42,128 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:48:42,129 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:48:42,132 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:48:42,132 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:48:42,132 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:48:42,133 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:48:42,133 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:48:42,138 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:48:42,138 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:48:42,139 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:48:42,140 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO impl.RoleInfo: b154d2ec-a7d7-4c34-b9eb-19131e12eba0: start b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderStateImpl
datanode_3          | 2023-06-22 08:48:42,140 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:48:42,143 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-LeaderElection3] INFO server.RaftServer$Division: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F: set configuration 0: peers:[1b7c5580-f1a4-419a-996c-bb798c761974|rpc:172.23.0.10:9856|admin:172.23.0.10:9857|client:172.23.0.10:9858|dataStream:|priority:0|startupRole:FOLLOWER, a617724e-4b77-4a5a-a2b9-215179594314|rpc:172.23.0.12:9856|admin:172.23.0.12:9857|client:172.23.0.12:9858|dataStream:|priority:0|startupRole:FOLLOWER, b154d2ec-a7d7-4c34-b9eb-19131e12eba0|rpc:172.23.0.11:9856|admin:172.23.0.11:9857|client:172.23.0.11:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:48:42,143 [b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b154d2ec-a7d7-4c34-b9eb-19131e12eba0@group-C5CBD08DF48F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d3c68544-ae0d-40bb-b43c-c5cbd08df48f/current/log_inprogress_0
scm_1               | 2023-06-22 08:47:53,080 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-22 08:47:53,092 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-22 08:47:53,096 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1
scm_1               | 2023-06-22 08:47:53,173 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.LeaderElection: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:47:53,181 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.LeaderElection: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-22 08:47:53,183 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1
scm_1               | 2023-06-22 08:47:53,188 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-22 08:47:53,193 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: change Leader from null to 3584b68e-f715-43e2-a929-0d9b7ab53a61 at term 1 for becomeLeader, leader elected after 9488ms
scm_1               | 2023-06-22 08:47:53,289 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-22 08:47:53,356 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:47:53,372 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:47:53,474 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-22 08:47:53,530 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-22 08:47:53,531 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-22 08:47:53,614 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:47:53,646 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-22 08:47:53,688 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderStateImpl
scm_1               | 2023-06-22 08:47:54,074 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-22 08:47:54,537 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: set configuration 0: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:47:55,343 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/log_inprogress_0
scm_1               | 2023-06-22 08:47:56,821 [main] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: close
scm_1               | 2023-06-22 08:47:56,822 [main] INFO server.GrpcService: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-22 08:47:56,829 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: shutdown
scm_1               | 2023-06-22 08:47:56,888 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-1EE81D72D3BC,id=3584b68e-f715-43e2-a929-0d9b7ab53a61
scm_1               | 2023-06-22 08:47:56,888 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderStateImpl
scm_1               | 2023-06-22 08:47:56,973 [main] INFO server.GrpcService: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-22 08:47:57,015 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO impl.PendingRequests: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-22 08:47:57,084 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO impl.StateMachineUpdater: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-22 08:47:57,100 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO impl.StateMachineUpdater: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-22 08:47:57,111 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO impl.StateMachineUpdater: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-22 08:47:57,145 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: closes. applyIndex: 0
scm_1               | 2023-06-22 08:47:57,151 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1               | 2023-06-22 08:47:57,166 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker close()
scm_1               | 2023-06-22 08:47:57,185 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3584b68e-f715-43e2-a929-0d9b7ab53a61: Stopped
scm_1               | 2023-06-22 08:47:57,192 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:47:57,211 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-630f4fe5-7863-4ef0-9602-1ee81d72d3bc; layoutVersion=4; scmId=3584b68e-f715-43e2-a929-0d9b7ab53a61
om_1                | 2023-06-22 08:48:30,827 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-22 08:48:30,828 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-22 08:48:30,829 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-22 08:48:30,845 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-22 08:48:30,848 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-22 08:48:30,849 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-22 08:48:30,849 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-22 08:48:30,859 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:48:30,859 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:48:30,862 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:48:30,862 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-22 08:48:30,864 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:48:30,865 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-22 08:48:30,865 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-22 08:48:30,872 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-22 08:48:30,877 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-22 08:48:30,878 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-22 08:48:30,879 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-22 08:48:30,880 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-22 08:48:30,884 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-22 08:48:30,930 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-22 08:48:30,935 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-22 08:48:30,936 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-22 08:48:30,966 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:48:30,966 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:48:30,988 [Listener at om/9862] INFO util.log: Logging initialized @13089ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:48:31,107 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-22 08:48:31,122 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:48:31,130 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:48:31,135 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:48:31,135 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:48:31,135 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:48:31,222 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-22 08:48:31,228 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1                | 2023-06-22 08:48:31,293 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-22 08:48:31,294 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:48:31,296 [Listener at om/9862] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-22 08:48:31,360 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@440ef8d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:48:31,361 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b170235{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:48:31,890 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6a7a1a0d{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-16407301273414956571/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:48:31,937 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7d66a126{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-22 08:48:31,938 [Listener at om/9862] INFO server.Server: Started @14038ms
om_1                | 2023-06-22 08:48:31,941 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-22 08:48:31,942 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-22 08:48:31,948 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:48:31,955 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-22 08:48:32,001 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:48:32,021 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-22 08:48:32,039 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@919086] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-22 08:48:35,944 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5080570311ns, electionTimeout:5078ms
om_1                | 2023-06-22 08:48:35,946 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:48:35,947 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-22 08:48:35,950 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-22 08:48:35,950 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:48:35,960 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:48:35,961 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-22 08:48:35,962 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:48:35,963 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-22 08:48:35,963 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 6436ms
om_1                | 2023-06-22 08:48:35,969 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-22 08:48:35,974 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:48:35,975 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:48:35,980 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-22 08:48:35,981 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-22 08:48:35,992 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-22 08:48:36,003 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:48:36,005 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-22 08:48:36,008 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-22 08:48:36,055 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-22 08:48:36,155 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:48:36,198 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-22 08:48:36,293 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-22 08:48:39,294 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-22 08:48:39,366 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-22 08:49:00,718 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-22 08:49:00,718 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-22 08:49:00,718 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-22 08:49:10,791 [qtp1436610577-47] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-22 08:49:10,866 [qtp1436610577-47] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423750818 in 47 milliseconds
om_1                | 2023-06-22 08:49:10,965 [qtp1436610577-47] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 97 milliseconds
om_1                | 2023-06-22 08:49:10,965 [qtp1436610577-47] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423750818
om_1                | 2023-06-22 08:49:21,058 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | 2023-06-22 08:47:57,258 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 47653feb929c/172.23.0.3
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:48:04,071 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 47653feb929c/172.23.0.3
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.3.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1             | STARTUP_MSG:   java = 11.0.14.1
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:47:37,419 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:47:41,127 [main] INFO reflections.Reflections: Reflections took 389 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1             | 2023-06-22 08:47:44,679 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-22 08:47:46,458 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:47:53,182 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:47:54,831 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:47:54,845 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.013 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:47:54,860 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:47:55,021 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:47:55,024 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-22 08:47:57,506 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 2023-06-22 08:48:00,338 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:48:00,452 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:48:04,087 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:48:04,163 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:48:04,206 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:48:04,224 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:48:05,185 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:48:05,448 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:48:05,830 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-22 08:48:05,837 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-22 08:48:05,941 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-22 08:48:05,973 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:3584b68e-f715-43e2-a929-0d9b7ab53a61
scm_1               | 2023-06-22 08:48:06,076 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-22 08:48:06,148 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:48:06,151 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:48:06,153 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:48:06,154 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:48:06,155 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-22 08:48:06,155 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-22 08:48:06,156 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-22 08:48:06,158 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:48:06,161 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-22 08:48:06,163 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:48:06,174 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-22 08:48:06,178 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-22 08:48:06,179 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-22 08:48:06,701 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-22 08:48:06,703 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-22 08:48:06,705 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-22 08:48:06,705 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:48:06,706 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:48:06,716 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:48:06,737 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: found a subdirectory /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc
scm_1               | 2023-06-22 08:48:06,748 [main] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: addNew group-1EE81D72D3BC:[] returns group-1EE81D72D3BC:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm_1               | 2023-06-22 08:48:06,785 [pool-16-thread-1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61: new RaftServerImpl for group-1EE81D72D3BC:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-22 08:48:06,788 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-22 08:48:06,792 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-22 08:48:06,793 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-22 08:48:06,793 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:48:06,794 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:48:06,794 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-22 08:48:06,808 [pool-16-thread-1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-22 08:48:06,809 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:48:06,813 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-22 08:48:06,813 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:48:06,838 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-22 08:48:06,847 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-22 08:48:06,847 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1             | 2023-06-22 08:48:00,541 [main] INFO util.log: Logging initialized @30342ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:48:01,288 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-22 08:48:01,327 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-22 08:48:01,367 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:48:01,394 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:48:01,409 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:48:01,409 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:48:02,430 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:48:03,429 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:48:03,534 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-22 08:48:03,561 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-22 08:48:03,602 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:48:03,602 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:48:04,415 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:48:04,663 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:48:04,788 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:48:04,794 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:48:04,926 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:48:05,196 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1             | 2023-06-22 08:48:05,323 [main] INFO reflections.Reflections: Reflections took 116 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1             | 2023-06-22 08:48:05,432 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-22 08:48:05,472 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-22 08:48:05,492 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:48:05,503 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-22 08:48:05,554 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-22 08:48:05,590 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:48:05,636 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:48:05,678 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-22 08:48:05,847 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-22 08:48:05,853 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-22 08:48:06,041 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:48:06,098 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:48:06,098 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-22 08:48:06,598 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-22 08:48:06,603 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1             | 2023-06-22 08:48:06,695 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:48:06,696 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:48:06,702 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:48:06,727 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:48:06,737 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-22 08:48:10,056 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@643a73fa{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-1599436276121008430/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1             | 2023-06-22 08:48:10,076 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:48:10,076 [Listener at 0.0.0.0/9891] INFO server.Server: Started @39877ms
recon_1             | 2023-06-22 08:48:10,092 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-22 08:48:10,092 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-22 08:48:10,095 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:48:10,096 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:48:10,106 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:48:10,120 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:48:10,121 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:48:10,121 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:48:10,122 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-22 08:48:10,123 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:48:13,687 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3584b68e-f715-43e2-a929-0d9b7ab53a61 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-22 08:48:15,919 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-22 08:48:15,919 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:48:15,920 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-22 08:48:15,920 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:48:15,978 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:48:15,988 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-22 08:48:16,173 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-22 08:48:16,173 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-22 08:48:16,372 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-22 08:48:16,373 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-22 08:48:16,519 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:48:16,523 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 134 milliseconds.
recon_1             | 2023-06-22 08:48:16,527 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.11:51346
recon_1             | 2023-06-22 08:48:16,527 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.10:43382
recon_1             | 2023-06-22 08:48:16,987 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.10:43374: output error
recon_1             | 2023-06-22 08:48:16,988 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.12:55392: output error
recon_1             | 2023-06-22 08:48:16,987 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.12:55384: output error
recon_1             | 2023-06-22 08:48:16,989 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.11:51332: output error
recon_1             | 2023-06-22 08:48:16,990 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-22 08:48:16,990 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-22 08:48:16,990 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-22 08:48:16,991 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-22 08:48:17,077 [IPC Server handler 52 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1b7c5580-f1a4-419a-996c-bb798c761974
recon_1             | 2023-06-22 08:48:17,081 [IPC Server handler 52 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:17,178 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1b7c5580-f1a4-419a-996c-bb798c761974 to Node DB.
recon_1             | 2023-06-22 08:48:17,427 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a617724e-4b77-4a5a-a2b9-215179594314
recon_1             | 2023-06-22 08:48:17,428 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:17,430 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a617724e-4b77-4a5a-a2b9-215179594314 to Node DB.
recon_1             | 2023-06-22 08:48:17,832 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b154d2ec-a7d7-4c34-b9eb-19131e12eba0
recon_1             | 2023-06-22 08:48:17,833 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:17,834 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node b154d2ec-a7d7-4c34-b9eb-19131e12eba0 to Node DB.
recon_1             | 2023-06-22 08:48:19,050 [IPC Server handler 47 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-22 08:48:19,426 [IPC Server handler 7 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-22 08:48:19,825 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-22 08:48:20,659 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-22 08:48:20,662 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=cc62a49f-5ae5-4985-9dbe-893682c86859. Trying to get from SCM.
recon_1             | 2023-06-22 08:48:20,698 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: cc62a49f-5ae5-4985-9dbe-893682c86859, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.188Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:48:20,752 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cc62a49f-5ae5-4985-9dbe-893682c86859, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.188Z[UTC]].
recon_1             | 2023-06-22 08:48:20,775 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=cc62a49f-5ae5-4985-9dbe-893682c86859 reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:20,776 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cc62a49f-5ae5-4985-9dbe-893682c86859, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1b7c5580-f1a4-419a-996c-bb798c761974, CreationTimestamp2023-06-22T08:48:17.188Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:48:21,110 [IPC Server handler 99 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-22 08:48:21,113 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=462257b1-74ee-4efa-84c7-8cc44df914e3. Trying to get from SCM.
recon_1             | 2023-06-22 08:48:21,127 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 462257b1-74ee-4efa-84c7-8cc44df914e3, Nodes: a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.435Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:48:21,128 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 462257b1-74ee-4efa-84c7-8cc44df914e3, Nodes: a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.435Z[UTC]].
recon_1             | 2023-06-22 08:48:21,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=462257b1-74ee-4efa-84c7-8cc44df914e3 reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:21,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 462257b1-74ee-4efa-84c7-8cc44df914e3, Nodes: a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a617724e-4b77-4a5a-a2b9-215179594314, CreationTimestamp2023-06-22T08:48:17.435Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:48:21,272 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f. Trying to get from SCM.
recon_1             | 2023-06-22 08:48:21,278 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d3c68544-ae0d-40bb-b43c-c5cbd08df48f, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.863Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:48:21,289 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d3c68544-ae0d-40bb-b43c-c5cbd08df48f, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.863Z[UTC]].
recon_1             | 2023-06-22 08:48:21,290 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:21,573 [IPC Server handler 6 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-22 08:48:21,575 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=457ee19a-07d7-4722-90e0-c793397d0186. Trying to get from SCM.
recon_1             | 2023-06-22 08:48:21,577 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 457ee19a-07d7-4722-90e0-c793397d0186, Nodes: b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.841Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:48:21,578 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 457ee19a-07d7-4722-90e0-c793397d0186, Nodes: b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.841Z[UTC]].
recon_1             | 2023-06-22 08:48:21,578 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=457ee19a-07d7-4722-90e0-c793397d0186 reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:21,578 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 457ee19a-07d7-4722-90e0-c793397d0186, Nodes: b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b154d2ec-a7d7-4c34-b9eb-19131e12eba0, CreationTimestamp2023-06-22T08:48:17.841Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:48:21,781 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:22,195 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,575 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,575 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90. Trying to get from SCM.
recon_1             | 2023-06-22 08:48:24,590 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 6b4ac565-95b2-4c6c-b245-6955aafedb90, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.873Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-22 08:48:24,593 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b4ac565-95b2-4c6c-b245-6955aafedb90, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.873Z[UTC]].
recon_1             | 2023-06-22 08:48:24,595 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,746 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:24,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:26,076 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:26,078 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:26,577 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:26,577 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:27,086 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:27,086 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:29,855 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:29,858 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 reported by a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:29,858 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6b4ac565-95b2-4c6c-b245-6955aafedb90, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a617724e-4b77-4a5a-a2b9-215179594314, CreationTimestamp2023-06-22T08:48:17.873Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:48:41,558 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-22 08:48:41,627 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-22 08:48:42,112 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f reported by b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:48:42,113 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d3c68544-ae0d-40bb-b43c-c5cbd08df48f, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b154d2ec-a7d7-4c34-b9eb-19131e12eba0, CreationTimestamp2023-06-22T08:48:17.863Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:48:51,556 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-22 08:48:51,561 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-22 08:48:51,566 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:48:51,572 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-22 08:49:10,124 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:49:10,124 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:49:11,126 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687423750125
recon_1             | 2023-06-22 08:49:11,143 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:49:11,145 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-22 08:49:11,285 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687423750125.
recon_1             | 2023-06-22 08:49:11,329 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:49:11,353 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-22 08:49:11,382 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-22 08:49:11,768 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-22 08:49:11,769 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:49:11,769 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:49:11,770 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:49:11,833 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:49:11,833 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.064 seconds to process 4 keys.
recon_1             | 2023-06-22 08:49:11,877 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-22 08:49:11,924 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | 2023-06-22 08:48:07,112 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-22 08:48:07,118 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-22 08:48:07,119 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-22 08:48:07,119 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-22 08:48:07,120 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-22 08:48:07,124 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-22 08:48:07,124 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-22 08:48:07,124 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-22 08:48:07,148 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1               | 2023-06-22 08:48:07,480 [main] INFO reflections.Reflections: Reflections took 265 ms to scan 3 urls, producing 112 keys and 252 values 
scm_1               | 2023-06-22 08:48:07,603 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-22 08:48:07,604 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-22 08:48:07,611 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-22 08:48:07,612 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-22 08:48:07,675 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-22 08:48:07,697 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-22 08:48:07,704 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-22 08:48:07,713 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-22 08:48:07,761 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-22 08:48:07,761 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-22 08:48:07,777 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-22 08:48:07,777 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:48:07,782 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-22 08:48:07,783 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-22 08:48:07,797 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-22 08:48:07,798 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-22 08:48:07,877 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-22 08:48:07,910 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-22 08:48:07,974 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-22 08:48:08,015 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-22 08:48:08,026 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-22 08:48:08,037 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-22 08:48:08,043 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:08,046 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:48:09,297 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:48:09,348 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:48:09,463 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-22 08:48:09,619 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:48:09,766 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:48:09,768 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-22 08:48:09,861 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:48:09,873 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:48:09,874 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-22 08:48:09,994 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-22 08:48:09,996 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        true
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-22 08:48:09,998 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:48:09,998 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-22 08:48:10,021 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:48:10,026 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-22 08:48:10,048 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/in_use.lock acquired by nodename 7@47653feb929c
scm_1               | 2023-06-22 08:48:10,058 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=3584b68e-f715-43e2-a929-0d9b7ab53a61} from /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/raft-meta
scm_1               | 2023-06-22 08:48:10,127 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: set configuration 0: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:48:10,137 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-22 08:48:10,157 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-22 08:48:10,161 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:48:10,163 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-22 08:48:10,172 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-22 08:48:10,178 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:48:10,193 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-22 08:48:10,199 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:48:10,213 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc
scm_1               | 2023-06-22 08:48:10,216 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:48:10,217 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:48:10,218 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:48:10,220 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-22 08:48:10,222 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-22 08:48:10,223 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-22 08:48:10,223 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:48:10,232 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:48:10,251 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:48:10,253 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:48:10,253 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:48:10,255 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:48:10,326 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: set configuration 0: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:48:10,333 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/log_inprogress_0
scm_1               | 2023-06-22 08:48:10,336 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1               | 2023-06-22 08:48:10,341 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:48:10,428 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: start as a follower, conf=0: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:48:10,429 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-22 08:48:10,430 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState
scm_1               | 2023-06-22 08:48:10,439 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-22 08:48:10,439 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1EE81D72D3BC,id=3584b68e-f715-43e2-a929-0d9b7ab53a61
scm_1               | 2023-06-22 08:48:10,440 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-22 08:48:10,448 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-22 08:48:10,449 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-22 08:48:10,449 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:48:10,450 [3584b68e-f715-43e2-a929-0d9b7ab53a61-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-22 08:48:10,458 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start RPC server
scm_1               | 2023-06-22 08:48:10,555 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 3584b68e-f715-43e2-a929-0d9b7ab53a61: GrpcService started, listening on 9894
scm_1               | 2023-06-22 08:48:10,558 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3584b68e-f715-43e2-a929-0d9b7ab53a61: Started
scm_1               | 2023-06-22 08:48:10,568 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-22 08:48:10,569 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-22 08:48:10,641 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:48:10,655 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:48:10,655 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:48:11,036 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:48:11,043 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:48:11,045 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:48:11,072 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:48:11,073 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:48:11,081 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:48:11,143 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:48:11,177 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@40c8067] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-22 08:48:11,230 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:48:11,230 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:48:11,282 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @11672ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:48:11,679 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-22 08:48:11,688 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:48:11,694 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:48:11,696 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:48:11,696 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:48:11,696 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:48:11,728 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-22 08:48:11,730 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1               | 2023-06-22 08:48:11,756 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:48:11,756 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:48:11,758 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-22 08:48:11,769 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b832551{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:48:11,770 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@24a04257{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:48:12,049 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3d4ecc67{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-6977350570530246634/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm_1               | 2023-06-22 08:48:12,096 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@51424203{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:48:12,096 [Listener at 0.0.0.0/9860] INFO server.Server: Started @12487ms
scm_1               | 2023-06-22 08:48:12,100 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-22 08:48:12,100 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-22 08:48:12,102 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:48:15,557 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.FollowerState: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5126524920ns, electionTimeout:5115ms
scm_1               | 2023-06-22 08:48:15,558 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState
scm_1               | 2023-06-22 08:48:15,559 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-22 08:48:15,562 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-22 08:48:15,562 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-FollowerState] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1
scm_1               | 2023-06-22 08:48:15,585 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.LeaderElection: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:48:15,586 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.LeaderElection: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-22 08:48:15,586 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: shutdown 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1
scm_1               | 2023-06-22 08:48:15,587 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-22 08:48:15,587 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-22 08:48:15,587 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-22 08:48:15,589 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: change Leader from null to 3584b68e-f715-43e2-a929-0d9b7ab53a61 at term 2 for becomeLeader, leader elected after 8749ms
scm_1               | 2023-06-22 08:48:15,597 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-22 08:48:15,602 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:48:15,602 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:48:15,607 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-22 08:48:15,607 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-22 08:48:15,608 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-22 08:48:15,612 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:48:15,614 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-22 08:48:15,616 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO impl.RoleInfo: 3584b68e-f715-43e2-a929-0d9b7ab53a61: start 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderStateImpl
scm_1               | 2023-06-22 08:48:15,623 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-22 08:48:15,628 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/log_inprogress_0 to /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/log_0-0
scm_1               | 2023-06-22 08:48:15,639 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-LeaderElection1] INFO server.RaftServer$Division: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC: set configuration 1: peers:[3584b68e-f715-43e2-a929-0d9b7ab53a61|rpc:47653feb929c:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:48:15,648 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/630f4fe5-7863-4ef0-9602-1ee81d72d3bc/current/log_inprogress_1
scm_1               | 2023-06-22 08:48:15,655 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-22 08:48:15,656 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-22 08:48:15,662 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:15,663 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-22 08:48:15,664 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:48:15,664 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:48:15,668 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:48:15,676 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:48:15,976 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.11:47030: output error
scm_1               | 2023-06-22 08:48:16,001 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-22 08:48:16,012 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.12:47664: output error
scm_1               | 2023-06-22 08:48:16,014 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-22 08:48:16,012 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.10:35440: output error
scm_1               | 2023-06-22 08:48:16,025 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-22 08:48:17,111 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1b7c5580-f1a4-419a-996c-bb798c761974
scm_1               | 2023-06-22 08:48:17,115 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:48:17,138 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:48:17,174 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:48:17,178 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:48:17,188 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:48:17,190 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=cc62a49f-5ae5-4985-9dbe-893682c86859 to datanode:1b7c5580-f1a4-419a-996c-bb798c761974
scm_1               | 2023-06-22 08:48:17,343 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: cc62a49f-5ae5-4985-9dbe-893682c86859, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.188Z[UTC]].
scm_1               | 2023-06-22 08:48:17,345 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:17,429 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a617724e-4b77-4a5a-a2b9-215179594314
scm_1               | 2023-06-22 08:48:17,431 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:48:17,432 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:48:17,432 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:48:17,435 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=462257b1-74ee-4efa-84c7-8cc44df914e3 to datanode:a617724e-4b77-4a5a-a2b9-215179594314
scm_1               | 2023-06-22 08:48:17,441 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 462257b1-74ee-4efa-84c7-8cc44df914e3, Nodes: a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.435Z[UTC]].
scm_1               | 2023-06-22 08:48:17,444 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:17,838 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b154d2ec-a7d7-4c34-b9eb-19131e12eba0
scm_1               | 2023-06-22 08:48:17,839 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:48:17,840 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:48:17,841 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=457ee19a-07d7-4722-90e0-c793397d0186 to datanode:b154d2ec-a7d7-4c34-b9eb-19131e12eba0
scm_1               | 2023-06-22 08:48:17,842 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:48:17,848 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:48:17,848 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:48:17,851 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 457ee19a-07d7-4722-90e0-c793397d0186, Nodes: b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.841Z[UTC]].
scm_1               | 2023-06-22 08:48:17,848 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:48:17,854 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-22 08:48:17,855 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:48:17,855 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:17,863 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f to datanode:1b7c5580-f1a4-419a-996c-bb798c761974
scm_1               | 2023-06-22 08:48:17,865 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f to datanode:b154d2ec-a7d7-4c34-b9eb-19131e12eba0
scm_1               | 2023-06-22 08:48:17,865 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f to datanode:a617724e-4b77-4a5a-a2b9-215179594314
scm_1               | 2023-06-22 08:48:17,870 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d3c68544-ae0d-40bb-b43c-c5cbd08df48f, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.863Z[UTC]].
scm_1               | 2023-06-22 08:48:17,870 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:17,873 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 to datanode:1b7c5580-f1a4-419a-996c-bb798c761974
scm_1               | 2023-06-22 08:48:17,875 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 to datanode:a617724e-4b77-4a5a-a2b9-215179594314
scm_1               | 2023-06-22 08:48:17,875 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 to datanode:b154d2ec-a7d7-4c34-b9eb-19131e12eba0
scm_1               | 2023-06-22 08:48:17,878 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b4ac565-95b2-4c6c-b245-6955aafedb90, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:48:17.873Z[UTC]].
scm_1               | 2023-06-22 08:48:17,879 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:17,888 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=6b4ac565-95b2-4c6c-b245-6955aafedb90 contains same datanodes as previous pipelines: PipelineID=d3c68544-ae0d-40bb-b43c-c5cbd08df48f nodeIds: 1b7c5580-f1a4-419a-996c-bb798c761974, a617724e-4b77-4a5a-a2b9-215179594314, b154d2ec-a7d7-4c34-b9eb-19131e12eba0
scm_1               | 2023-06-22 08:48:20,714 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: cc62a49f-5ae5-4985-9dbe-893682c86859, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1b7c5580-f1a4-419a-996c-bb798c761974, CreationTimestamp2023-06-22T08:48:17.188Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:48:20,729 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:20,758 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:21,205 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 462257b1-74ee-4efa-84c7-8cc44df914e3, Nodes: a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a617724e-4b77-4a5a-a2b9-215179594314, CreationTimestamp2023-06-22T08:48:17.435Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:48:21,222 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:21,250 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:21,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:21,625 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 457ee19a-07d7-4722-90e0-c793397d0186, Nodes: b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b154d2ec-a7d7-4c34-b9eb-19131e12eba0, CreationTimestamp2023-06-22T08:48:17.841Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:48:21,638 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:21,644 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:21,791 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:22,215 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:24,584 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:24,768 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:24,797 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:26,077 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:26,602 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:27,094 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:29,856 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6b4ac565-95b2-4c6c-b245-6955aafedb90, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a617724e-4b77-4a5a-a2b9-215179594314, CreationTimestamp2023-06-22T08:48:17.873Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:48:29,857 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:29,865 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-22 08:48:29,886 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:48:29,894 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:48:29,899 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-22 08:48:29,901 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-22 08:48:39,455 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-22 08:48:39,481 [3584b68e-f715-43e2-a929-0d9b7ab53a61@group-1EE81D72D3BC-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-22 08:48:39,493 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-22 08:48:42,109 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d3c68544-ae0d-40bb-b43c-c5cbd08df48f, Nodes: 1b7c5580-f1a4-419a-996c-bb798c761974{ip: 172.23.0.10, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b154d2ec-a7d7-4c34-b9eb-19131e12eba0{ip: 172.23.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a617724e-4b77-4a5a-a2b9-215179594314{ip: 172.23.0.12, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b154d2ec-a7d7-4c34-b9eb-19131e12eba0, CreationTimestamp2023-06-22T08:48:17.863Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:49:14,030 [IPC Server handler 4 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.7
scm_1               | 2023-06-22 08:49:24,565 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.7
scm_1               | 2023-06-22 08:50:31,795 [IPC Server handler 22 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.7
scm_1               | 2023-06-22 08:50:42,190 [IPC Server handler 28 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.7
Attaching to xcompat_old_client_1_1_0_1, xcompat_datanode_2, xcompat_old_client_1_0_0_1, xcompat_s3g_1, xcompat_datanode_5, xcompat_datanode_1, xcompat_datanode_4, xcompat_datanode_3, xcompat_new_client_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_old_client_1_2_1_1, xcompat_recon_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-22 08:51:23,342 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = b325136153fd/172.24.0.13
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-22 08:51:23,486 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-22 08:51:23,817 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-22 08:51:24,749 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-22 08:51:26,185 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-22 08:51:26,194 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-22 08:51:27,503 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:b325136153fd ip:172.24.0.13
datanode_1          | 2023-06-22 08:51:28,827 [main] INFO reflections.Reflections: Reflections took 951 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_1          | 2023-06-22 08:51:33,117 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-22 08:51:33,600 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-22 08:51:35,309 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-22 08:51:35,451 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-22 08:51:35,475 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-22 08:51:35,522 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-22 08:51:35,786 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:51:35,954 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:51:35,988 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-22 08:51:35,990 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-22 08:51:35,990 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-22 08:51:35,991 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-22 08:51:36,346 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-22 08:51:36,347 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-22 08:51:48,256 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-22 08:51:49,001 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-22 08:51:49,490 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-22 08:51:50,886 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:51:50,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-22 08:51:50,960 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-22 08:51:50,971 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-22 08:51:50,996 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-22 08:51:51,000 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-22 08:51:51,002 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-22 08:51:51,025 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:51:51,048 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-22 08:51:51,077 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:51:51,233 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:51:51,293 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-22 08:51:51,296 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-22 08:51:54,364 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-22 08:51:54,369 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-22 08:51:54,385 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-22 08:51:54,394 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:51:54,395 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:51:54,420 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:51:54,897 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-22 08:51:55,495 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-22 08:51:56,771 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:51:56,954 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-22 08:51:57,314 [main] INFO util.log: Logging initialized @47237ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-22 08:51:58,389 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-22 08:51:58,446 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-22 08:51:58,512 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-22 08:51:58,534 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-22 08:51:58,552 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-22 08:51:58,556 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-22 08:51:58,959 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-22 08:51:58,998 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-22 08:51:59,016 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-22 08:51:59,293 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-22 08:51:59,294 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-22 08:51:59,309 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-22 08:51:59,446 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@49482761{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-22 08:51:59,453 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2305aad0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-22 08:52:00,419 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5ff8d76f{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-13329695285456474159/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-22 08:52:00,519 [main] INFO server.AbstractConnector: Started ServerConnector@75f446df{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-22 08:52:00,540 [main] INFO server.Server: Started @50463ms
datanode_1          | 2023-06-22 08:52:00,560 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-22 08:52:00,560 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-22 08:52:00,564 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-22 08:52:00,956 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-22 08:52:01,199 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-22 08:52:03,063 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_1          | 2023-06-22 08:52:03,167 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-22 08:52:03,167 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-22 08:52:03,168 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-22 08:52:03,246 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-22 08:52:03,272 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-22 08:52:04,204 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_1          | 2023-06-22 08:52:04,768 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-22 08:52:06,937 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:06,937 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:07,939 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:07,940 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:08,940 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:08,941 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:09,941 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:10,942 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:11,943 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:12,944 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:13,944 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:14,011 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From b325136153fd/172.24.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:33458 remote=recon/172.24.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:33458 remote=recon/172.24.0.6:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_1          | 2023-06-22 08:52:14,945 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:15,946 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:16,947 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:52:21,963 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-22 08:51:23,482 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = ada99c0504f5/172.24.0.12
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-22 08:51:23,574 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-22 08:51:23,953 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-22 08:51:24,912 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-22 08:51:26,054 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-22 08:51:26,055 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-22 08:51:27,205 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ada99c0504f5 ip:172.24.0.12
datanode_3          | 2023-06-22 08:51:28,802 [main] INFO reflections.Reflections: Reflections took 1171 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_3          | 2023-06-22 08:51:32,604 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-22 08:51:33,182 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-22 08:51:35,157 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-22 08:51:35,306 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-22 08:51:35,353 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-22 08:51:35,369 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-22 08:51:35,782 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:51:36,059 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:51:36,064 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-22 08:51:36,093 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-22 08:51:36,096 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-22 08:51:36,104 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-22 08:51:36,472 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-22 08:51:36,480 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-22 08:51:49,273 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | java.net.SocketTimeoutException: Call From b325136153fd/172.24.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:60700 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:60700 remote=scm/172.24.0.2:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_1          | 2023-06-22 08:52:23,751 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-20c9464b-69db-439e-8e1f-f883b17217ff/container.db to cache
datanode_1          | 2023-06-22 08:52:23,752 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-20c9464b-69db-439e-8e1f-f883b17217ff/container.db for volume DS-20c9464b-69db-439e-8e1f-f883b17217ff
datanode_1          | 2023-06-22 08:52:23,790 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-22 08:52:23,814 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-22 08:52:24,283 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-22 08:52:24,285 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:52:24,455 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 1c635413-c797-403f-bedc-bdd0587a593c: start RPC server
datanode_1          | 2023-06-22 08:52:24,498 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 1c635413-c797-403f-bedc-bdd0587a593c: GrpcService started, listening on 9858
datanode_1          | 2023-06-22 08:52:24,523 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 1c635413-c797-403f-bedc-bdd0587a593c: GrpcService started, listening on 9856
datanode_1          | 2023-06-22 08:52:24,532 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 1c635413-c797-403f-bedc-bdd0587a593c: GrpcService started, listening on 9857
datanode_1          | 2023-06-22 08:52:24,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1c635413-c797-403f-bedc-bdd0587a593c is started using port 9858 for RATIS
datanode_1          | 2023-06-22 08:52:24,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1c635413-c797-403f-bedc-bdd0587a593c is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-22 08:52:24,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1c635413-c797-403f-bedc-bdd0587a593c is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-22 08:52:24,557 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1c635413-c797-403f-bedc-bdd0587a593c: Started
datanode_1          | 2023-06-22 08:52:24,698 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:52:58,744 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1c635413-c797-403f-bedc-bdd0587a593c: addNew group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-D0B46C7C7CF5:java.util.concurrent.CompletableFuture@70097473[Not completed]
datanode_3          | 2023-06-22 08:51:49,973 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-22 08:51:50,572 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-22 08:51:51,491 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:51:51,581 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-22 08:51:51,588 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-22 08:51:51,613 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-22 08:51:51,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-22 08:51:51,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-22 08:51:51,632 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-22 08:51:51,695 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:51:51,699 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-22 08:51:51,709 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:51:52,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:51:52,295 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-22 08:51:52,335 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-22 08:51:55,190 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-22 08:51:55,256 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-22 08:51:55,308 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-22 08:51:55,312 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:51:55,312 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:51:55,356 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:51:56,137 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-22 08:51:56,818 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-22 08:51:58,057 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:51:58,168 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-22 08:51:58,405 [main] INFO util.log: Logging initialized @48730ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-22 08:51:59,334 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-22 08:51:59,396 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-22 08:51:59,491 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-22 08:51:59,509 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-22 08:51:59,511 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-22 08:51:59,516 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-22 08:51:59,820 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-22 08:51:59,854 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-22 08:51:59,866 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-22 08:52:00,157 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-22 08:52:00,161 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-22 08:52:00,185 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-22 08:52:00,263 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47fbf95e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-22 08:52:00,289 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@58189132{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-22 08:52:01,395 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3df54fd5{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2183986431595897342/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-22 08:52:01,490 [main] INFO server.AbstractConnector: Started ServerConnector@543c887{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-22 08:52:01,492 [main] INFO server.Server: Started @51817ms
datanode_3          | 2023-06-22 08:52:01,520 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-22 08:52:01,520 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-22 08:52:01,522 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-22 08:52:01,805 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-22 08:52:02,053 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-22 08:52:03,383 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_3          | 2023-06-22 08:52:03,437 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-22 08:52:03,437 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-22 08:52:03,438 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-22 08:52:03,468 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-22 08:52:03,468 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-22 08:52:04,286 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_3          | 2023-06-22 08:52:04,673 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-22 08:52:07,215 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4          | 2023-06-22 08:51:22,709 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4          | /************************************************************
datanode_4          | STARTUP_MSG: Starting HddsDatanodeService
datanode_4          | STARTUP_MSG:   host = 2e3fa1f00372/172.24.0.8
datanode_4          | STARTUP_MSG:   args = []
datanode_4          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_4          | STARTUP_MSG:   java = 11.0.19
datanode_4          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | ************************************************************/
datanode_4          | 2023-06-22 08:51:22,802 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-06-22 08:51:23,190 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 2023-06-22 08:51:24,118 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-06-22 08:51:25,425 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-06-22 08:51:25,426 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4          | 2023-06-22 08:51:26,494 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2e3fa1f00372 ip:172.24.0.8
datanode_4          | 2023-06-22 08:51:28,097 [main] INFO reflections.Reflections: Reflections took 1097 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_4          | 2023-06-22 08:51:32,075 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-06-22 08:51:32,572 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4          | 2023-06-22 08:51:34,365 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4          | 2023-06-22 08:51:34,617 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4          | 2023-06-22 08:51:34,645 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4          | 2023-06-22 08:51:34,657 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4          | 2023-06-22 08:51:34,976 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4          | 2023-06-22 08:51:35,136 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-22 08:51:35,141 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-06-22 08:51:35,144 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-06-22 08:51:35,161 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4          | 2023-06-22 08:51:35,171 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_4          | 2023-06-22 08:51:35,516 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4          | 2023-06-22 08:51:35,530 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4          | 2023-06-22 08:51:48,161 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-06-22 08:51:49,131 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-22 08:51:49,507 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-06-22 08:51:50,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-22 08:51:50,674 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4          | 2023-06-22 08:51:50,675 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-22 08:51:50,687 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4          | 2023-06-22 08:51:50,688 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 2023-06-22 08:51:50,688 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4          | 2023-06-22 08:51:50,688 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 2023-06-22 08:51:50,699 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:51:50,700 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4          | 2023-06-22 08:51:50,714 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-22 08:51:50,883 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-06-22 08:51:50,954 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4          | 2023-06-22 08:51:50,975 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4          | 2023-06-22 08:51:54,091 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_4          | 2023-06-22 08:51:54,172 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4          | 2023-06-22 08:51:54,180 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4          | 2023-06-22 08:51:54,190 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-22 08:51:54,190 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-22 08:51:54,258 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-22 08:51:54,602 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4          | 2023-06-22 08:51:55,233 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_4          | 2023-06-22 08:51:56,616 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4          | 2023-06-22 08:51:56,823 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4          | 2023-06-22 08:51:57,054 [main] INFO util.log: Logging initialized @47585ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4          | 2023-06-22 08:51:57,948 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4          | 2023-06-22 08:51:58,001 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 2023-06-22 08:51:58,080 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-06-22 08:51:58,100 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4          | 2023-06-22 08:51:58,104 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 2023-06-22 08:51:58,105 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-06-22 08:51:58,481 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_4          | 2023-06-22 08:51:58,530 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4          | 2023-06-22 08:51:58,539 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-22 08:52:07,233 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:08,217 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:08,234 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:09,217 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:10,218 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:11,219 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:12,220 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:13,221 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:13,268 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From ada99c0504f5/172.24.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:60804 remote=recon/172.24.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:60804 remote=recon/172.24.0.6:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_3          | 2023-06-22 08:52:14,222 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:15,222 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:16,223 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:17,225 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-22 08:52:22,240 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | 2023-06-22 08:52:59,002 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c: new RaftServerImpl for group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:52:59,029 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:52:59,046 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:52:59,048 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:52:59,050 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:52:59,071 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:52:59,076 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:52:59,179 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:52:59,185 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:52:59,218 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:52:59,220 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:52:59,309 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:52:59,348 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:52:59,399 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:52:59,400 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:52:59,679 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:52:59,898 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:52:59,926 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:52:59,937 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:52:59,937 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:52:59,937 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:52:59,939 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:52:59,940 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 does not exist. Creating ...
datanode_1          | 2023-06-22 08:52:59,959 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/in_use.lock acquired by nodename 7@b325136153fd
datanode_1          | 2023-06-22 08:52:59,988 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 has been successfully formatted.
datanode_1          | 2023-06-22 08:53:00,069 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO ratis.ContainerStateMachine: group-D0B46C7C7CF5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:53:00,076 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:53:00,153 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:53:00,157 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:00,166 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:53:00,170 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:53:00,185 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:00,255 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:53:00,256 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:53:00,262 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:00,306 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_1          | 2023-06-22 08:53:00,311 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:53:00,311 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:00,320 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-06-22 08:51:22,565 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | /************************************************************
datanode_5          | STARTUP_MSG: Starting HddsDatanodeService
datanode_5          | STARTUP_MSG:   host = 92add0f1b6b7/172.24.0.11
datanode_5          | STARTUP_MSG:   args = []
datanode_5          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_5          | STARTUP_MSG:   java = 11.0.19
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-22 08:51:22,817 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 7c8c150a4dbd/172.24.0.15
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | 2023-06-22 08:53:00,322 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:53:00,326 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:53:00,328 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:53:00,330 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:53:00,331 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:53:00,398 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:00,401 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:00,463 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:00,467 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:00,480 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:00,523 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:00,524 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:00,533 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:00,539 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:53:00,546 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState
datanode_1          | 2023-06-22 08:53:00,577 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:00,578 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:00,606 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D0B46C7C7CF5,id=1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:53:00,627 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:00,631 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:53:00,638 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:53:00,644 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:53:00,894 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_1          | 2023-06-22 08:53:05,753 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO impl.FollowerState: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5207381866ns, electionTimeout:5151ms
datanode_1          | 2023-06-22 08:53:05,754 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState
datanode_1          | 2023-06-22 08:53:05,754 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:53:05,762 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-22 08:53:05,762 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1
datanode_1          | 2023-06-22 08:53:05,787 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:05,838 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:05,839 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:05,855 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_1          | 2023-06-22 08:53:05,864 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_1          | 2023-06-22 08:53:06,345 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:53:06,348 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 1c635413-c797-403f-bedc-bdd0587a593c<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:FAIL-t0
datanode_1          | 2023-06-22 08:53:06,350 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: result REJECTED
datanode_1          | 2023-06-22 08:53:06,352 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_1          | 2023-06-22 08:53:06,357 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1
datanode_1          | 2023-06-22 08:53:06,358 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-LeaderElection1] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState
datanode_1          | 2023-06-22 08:53:06,877 [grpc-default-executor-2] WARN server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Failed requestVote 73ad851d-99db-49b7-ac3d-75be53f012c8->1c635413-c797-403f-bedc-bdd0587a593c#0
datanode_1          | org.apache.ratis.protocol.exceptions.GroupMismatchException: 1c635413-c797-403f-bedc-bdd0587a593c: group-61BE3005BF20 not found.
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
datanode_1          | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
datanode_1          | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
datanode_1          | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1          | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | 2023-06-22 08:53:06,917 [grpc-default-executor-0] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, cd5a4933-0666-4f07-ab50-d6c55125d602, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_1          | 2023-06-22 08:53:06,920 [grpc-default-executor-1] INFO server.RaftServer: 1c635413-c797-403f-bedc-bdd0587a593c: addNew group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] returns group-61BE3005BF20:java.util.concurrent.CompletableFuture@5e4170c7[Not completed]
datanode_1          | 2023-06-22 08:53:06,929 [grpc-default-executor-0] INFO impl.VoteContext: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FOLLOWER: accept PRE_VOTE from cd5a4933-0666-4f07-ab50-d6c55125d602: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-22 08:53:06,933 [grpc-default-executor-0] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: cd5a4933-0666-4f07-ab50-d6c55125d602<-1c635413-c797-403f-bedc-bdd0587a593c#0:OK-t0. Peer's state: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5:t0, leader=null, voted=, raftlog=Memoized:1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:06,984 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c: new RaftServerImpl for group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:53:07,049 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:53:07,088 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:53:07,088 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:53:07,088 [grpc-default-executor-1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, 5cccec7c-2f8e-4410-a445-f064595c8b02, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_1          | 2023-06-22 08:53:07,090 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:53:07,129 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:53:07,130 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_5          | ************************************************************/
datanode_5          | 2023-06-22 08:51:22,670 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-06-22 08:51:23,053 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5          | 2023-06-22 08:51:24,002 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5          | 2023-06-22 08:51:25,014 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5          | 2023-06-22 08:51:25,016 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5          | 2023-06-22 08:51:25,824 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:92add0f1b6b7 ip:172.24.0.11
datanode_5          | 2023-06-22 08:51:27,384 [main] INFO reflections.Reflections: Reflections took 1009 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_5          | 2023-06-22 08:51:31,353 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_5          | 2023-06-22 08:51:31,880 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5          | 2023-06-22 08:51:33,806 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5          | 2023-06-22 08:51:34,049 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-06-22 08:51:34,075 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5          | 2023-06-22 08:51:34,089 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5          | 2023-06-22 08:51:34,469 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5          | 2023-06-22 08:51:34,661 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-22 08:51:34,703 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_5          | 2023-06-22 08:51:34,712 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5          | 2023-06-22 08:51:34,717 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_5          | 2023-06-22 08:51:34,720 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-06-22 08:51:35,069 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5          | 2023-06-22 08:51:35,088 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5          | 2023-06-22 08:51:48,159 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-06-22 08:51:58,869 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-06-22 08:51:58,896 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-06-22 08:51:58,898 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_4          | 2023-06-22 08:51:59,029 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@956cf3d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4          | 2023-06-22 08:51:59,071 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e085259{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-06-22 08:51:59,947 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2cdd156b{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-11289865695135232359/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4          | 2023-06-22 08:52:00,048 [main] INFO server.AbstractConnector: Started ServerConnector@51d34f02{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4          | 2023-06-22 08:52:00,051 [main] INFO server.Server: Started @50581ms
datanode_4          | 2023-06-22 08:52:00,097 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-06-22 08:52:00,097 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-06-22 08:52:00,124 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4          | 2023-06-22 08:52:00,458 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-06-22 08:52:00,954 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_4          | 2023-06-22 08:52:02,707 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_4          | 2023-06-22 08:52:02,892 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 2023-06-22 08:52:02,892 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_4          | 2023-06-22 08:52:02,893 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_4          | 2023-06-22 08:52:02,947 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_4          | 2023-06-22 08:52:02,987 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4          | 2023-06-22 08:52:03,863 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_4          | 2023-06-22 08:52:04,354 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4          | 2023-06-22 08:52:06,404 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | java.net.SocketTimeoutException: Call From ada99c0504f5/172.24.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:41386 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:41386 remote=scm/172.24.0.2:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_3          | 2023-06-22 08:52:23,879 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-76b113c5-bb70-4150-a262-3815539f773f/container.db to cache
datanode_3          | 2023-06-22 08:52:23,881 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-76b113c5-bb70-4150-a262-3815539f773f/container.db for volume DS-76b113c5-bb70-4150-a262-3815539f773f
datanode_3          | 2023-06-22 08:52:23,922 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-22 08:52:23,943 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-22 08:52:24,309 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-22 08:52:24,312 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:52:24,507 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 73ad851d-99db-49b7-ac3d-75be53f012c8: start RPC server
datanode_3          | 2023-06-22 08:52:24,545 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 73ad851d-99db-49b7-ac3d-75be53f012c8: GrpcService started, listening on 9858
datanode_3          | 2023-06-22 08:52:24,555 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 73ad851d-99db-49b7-ac3d-75be53f012c8: GrpcService started, listening on 9856
datanode_3          | 2023-06-22 08:52:24,587 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 73ad851d-99db-49b7-ac3d-75be53f012c8: GrpcService started, listening on 9857
datanode_3          | 2023-06-22 08:52:24,590 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 73ad851d-99db-49b7-ac3d-75be53f012c8 is started using port 9858 for RATIS
datanode_3          | 2023-06-22 08:52:24,590 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 73ad851d-99db-49b7-ac3d-75be53f012c8 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-22 08:52:24,595 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 73ad851d-99db-49b7-ac3d-75be53f012c8 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-22 08:52:24,597 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-73ad851d-99db-49b7-ac3d-75be53f012c8: Started
datanode_3          | 2023-06-22 08:52:24,741 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:52:59,119 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 73ad851d-99db-49b7-ac3d-75be53f012c8: addNew group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] returns group-61BE3005BF20:java.util.concurrent.CompletableFuture@72512ff7[Not completed]
datanode_3          | 2023-06-22 08:52:59,298 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8: new RaftServerImpl for group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-22 08:52:06,404 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:07,406 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:07,407 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:08,407 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:08,408 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:09,410 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:10,412 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:11,413 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:12,414 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:13,415 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:13,460 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 2e3fa1f00372/172.24.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:56970 remote=recon/172.24.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:56970 remote=recon/172.24.0.6:9891]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_4          | 2023-06-22 08:52:14,417 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:15,418 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:16,419 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-22 08:52:21,435 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5          | 2023-06-22 08:51:49,379 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-22 08:51:49,917 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_5          | 2023-06-22 08:51:51,291 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-22 08:51:51,360 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_5          | 2023-06-22 08:51:51,376 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-22 08:51:51,377 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_5          | 2023-06-22 08:51:51,377 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_5          | 2023-06-22 08:51:51,378 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_5          | 2023-06-22 08:51:51,379 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5          | 2023-06-22 08:51:51,380 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:51:51,392 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5          | 2023-06-22 08:51:51,395 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:51:51,568 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-22 08:51:51,643 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5          | 2023-06-22 08:51:51,659 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-06-22 08:51:55,285 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-06-22 08:51:55,324 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5          | 2023-06-22 08:51:55,336 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-06-22 08:51:55,343 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:51:55,352 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-22 08:51:55,448 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-22 08:51:55,904 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_5          | 2023-06-22 08:51:56,613 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_5          | 2023-06-22 08:51:57,841 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5          | 2023-06-22 08:51:58,004 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_5          | 2023-06-22 08:51:58,178 [main] INFO util.log: Logging initialized @48369ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5          | 2023-06-22 08:51:58,980 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5          | 2023-06-22 08:51:59,031 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5          | 2023-06-22 08:51:59,098 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5          | 2023-06-22 08:51:59,119 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5          | 2023-06-22 08:51:59,136 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5          | 2023-06-22 08:51:59,142 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5          | 2023-06-22 08:51:59,451 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_5          | 2023-06-22 08:51:59,532 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5          | 2023-06-22 08:51:59,535 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_5          | 2023-06-22 08:51:59,759 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5          | 2023-06-22 08:51:59,759 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5          | 2023-06-22 08:51:59,768 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_5          | 2023-06-22 08:51:59,869 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6ce8bf64{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_5          | 2023-06-22 08:51:59,885 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61c4fc34{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5          | 2023-06-22 08:52:00,826 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@40da1644{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16510963044804542988/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5          | 2023-06-22 08:52:00,891 [main] INFO server.AbstractConnector: Started ServerConnector@3cfab56d{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_5          | 2023-06-22 08:52:00,891 [main] INFO server.Server: Started @51082ms
datanode_5          | 2023-06-22 08:52:00,916 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5          | 2023-06-22 08:52:00,916 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5          | 2023-06-22 08:52:00,922 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5          | 2023-06-22 08:52:01,272 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_5          | 2023-06-22 08:52:01,503 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_5          | 2023-06-22 08:52:03,091 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_5          | 2023-06-22 08:52:03,221 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_5          | 2023-06-22 08:52:03,228 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_5          | 2023-06-22 08:52:03,235 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | 2023-06-22 08:52:03,275 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_5          | 2023-06-22 08:52:03,307 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5          | 2023-06-22 08:52:03,876 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_5          | 2023-06-22 08:52:04,250 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5          | 2023-06-22 08:52:06,951 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-22 08:51:22,933 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-22 08:51:23,334 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-22 08:51:24,230 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-22 08:51:25,686 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-22 08:51:25,686 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-22 08:51:26,868 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7c8c150a4dbd ip:172.24.0.15
datanode_2          | 2023-06-22 08:51:28,293 [main] INFO reflections.Reflections: Reflections took 901 ms to scan 2 urls, producing 106 keys and 230 values 
datanode_2          | 2023-06-22 08:51:32,437 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-22 08:51:33,000 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-22 08:51:34,844 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-22 08:51:35,029 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-22 08:51:35,071 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-22 08:51:35,085 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-22 08:51:35,450 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:51:35,629 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:51:35,671 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-22 08:51:35,700 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-22 08:51:35,701 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-22 08:51:35,701 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-22 08:51:36,101 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-22 08:51:36,109 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-22 08:51:48,686 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-22 08:52:59,314 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:52:59,317 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:52:59,317 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:52:59,319 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:52:59,320 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:52:59,321 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:52:59,402 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:52:59,409 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:52:59,631 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:52:59,632 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:52:59,778 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:52:59,790 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:52:59,819 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:52:59,828 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:52:59,967 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:53:00,121 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:00,154 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:00,163 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:53:00,164 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:53:00,169 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:53:00,174 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:53:00,177 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 does not exist. Creating ...
datanode_3          | 2023-06-22 08:53:00,222 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/in_use.lock acquired by nodename 7@ada99c0504f5
datanode_3          | 2023-06-22 08:53:00,257 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 has been successfully formatted.
datanode_3          | 2023-06-22 08:53:00,291 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO ratis.ContainerStateMachine: group-61BE3005BF20: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:53:00,301 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:53:00,411 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:53:00,414 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:00,445 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:53:00,446 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:53:00,487 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:00,510 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:53:00,511 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:53:00,512 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:00,537 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20
datanode_3          | 2023-06-22 08:53:00,538 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:53:00,541 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:00,544 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:00,548 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:53:00,549 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:53:00,553 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:53:00,556 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:53:00,558 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:53:00,601 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:00,603 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:00,697 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:00,700 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:00,701 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:00,748 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:00,748 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:00,760 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:00,764 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:53:00,774 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState
datanode_3          | 2023-06-22 08:53:00,777 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:53:00,783 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:53:00,790 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61BE3005BF20,id=73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:00,804 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:00,805 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:53:00,809 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:53:00,811 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:53:00,890 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20
datanode_3          | 2023-06-22 08:53:05,897 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO impl.FollowerState: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5123738996ns, electionTimeout:5104ms
datanode_3          | 2023-06-22 08:53:05,900 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState
datanode_3          | 2023-06-22 08:53:05,920 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:53:05,939 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-22 08:53:05,939 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1
datanode_3          | 2023-06-22 08:53:06,009 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:06,104 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:53:06,106 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:53:06,316 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_3          | 2023-06-22 08:53:06,316 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1c635413-c797-403f-bedc-bdd0587a593c
datanode_3          | 2023-06-22 08:53:07,183 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 1c635413-c797-403f-bedc-bdd0587a593c: group-61BE3005BF20 not found.
datanode_4          | java.net.SocketTimeoutException: Call From 2e3fa1f00372/172.24.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:53846 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.8:53846 remote=scm/172.24.0.2:9861]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_4          | 2023-06-22 08:52:23,891 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-277cc300-0c6f-425a-83e1-3a343c6b3cd7/container.db to cache
datanode_4          | 2023-06-22 08:52:23,893 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-277cc300-0c6f-425a-83e1-3a343c6b3cd7/container.db for volume DS-277cc300-0c6f-425a-83e1-3a343c6b3cd7
datanode_4          | 2023-06-22 08:52:23,916 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4          | 2023-06-22 08:52:23,937 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4          | 2023-06-22 08:52:24,375 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4          | 2023-06-22 08:52:24,376 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 31613e4d-b990-42be-9377-00c7a7ba3288
datanode_4          | 2023-06-22 08:52:24,562 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 31613e4d-b990-42be-9377-00c7a7ba3288: start RPC server
datanode_4          | 2023-06-22 08:52:24,589 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 31613e4d-b990-42be-9377-00c7a7ba3288: GrpcService started, listening on 9858
datanode_4          | 2023-06-22 08:52:24,602 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 31613e4d-b990-42be-9377-00c7a7ba3288: GrpcService started, listening on 9856
datanode_4          | 2023-06-22 08:52:24,618 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 31613e4d-b990-42be-9377-00c7a7ba3288: GrpcService started, listening on 9857
datanode_4          | 2023-06-22 08:52:24,688 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 31613e4d-b990-42be-9377-00c7a7ba3288 is started using port 9858 for RATIS
datanode_4          | 2023-06-22 08:52:24,688 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 31613e4d-b990-42be-9377-00c7a7ba3288 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-06-22 08:52:24,688 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 31613e4d-b990-42be-9377-00c7a7ba3288 is started using port 9856 for RATIS_SERVER
datanode_4          | 2023-06-22 08:52:24,689 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-31613e4d-b990-42be-9377-00c7a7ba3288: Started
datanode_4          | 2023-06-22 08:52:24,786 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:52:58,383 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 31613e4d-b990-42be-9377-00c7a7ba3288: addNew group-8D2E22216578:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-8D2E22216578:java.util.concurrent.CompletableFuture@519f041d[Not completed]
datanode_4          | 2023-06-22 08:52:58,434 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288: new RaftServerImpl for group-8D2E22216578:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-22 08:52:58,437 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-22 08:52:58,439 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:53:07,185 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
datanode_3          | 2023-06-22 08:53:07,188 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection:   Response 0: 73ad851d-99db-49b7-ac3d-75be53f012c8<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t0
datanode_3          | 2023-06-22 08:53:07,194 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 1c635413-c797-403f-bedc-bdd0587a593c: group-61BE3005BF20 not found.
datanode_3          | 2023-06-22 08:53:07,210 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-06-22 08:53:07,228 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:07,274 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:53:07,275 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:53:07,483 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20 is not in [RUNNING]: current state is STARTING
datanode_3          | 2023-06-22 08:53:07,483 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1: ELECTION PASSED received 1 response(s) and 1 exception(s):
datanode_3          | 2023-06-22 08:53:07,483 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection:   Response 0: 73ad851d-99db-49b7-ac3d-75be53f012c8<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t1
datanode_3          | 2023-06-22 08:53:07,484 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20 is not in [RUNNING]: current state is STARTING
datanode_3          | 2023-06-22 08:53:07,484 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1 ELECTION round 0: result PASSED
datanode_3          | 2023-06-22 08:53:07,485 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1
datanode_3          | 2023-06-22 08:53:07,487 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:53:07,487 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:07,487 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 1 for becomeLeader, leader elected after 7710ms
datanode_3          | 2023-06-22 08:53:07,598 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:53:07,641 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:07,649 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:53:07,685 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:53:07,693 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:53:07,724 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:53:07,837 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:07,893 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:53:08,054 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20.
datanode_3          | 2023-06-22 08:53:08,057 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 73ad851d-99db-49b7-ac3d-75be53f012c8: addNew group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-D771BA859F6C:java.util.concurrent.CompletableFuture@1d8ea20b[Not completed]
datanode_3          | 2023-06-22 08:53:08,069 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8: new RaftServerImpl for group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:53:08,071 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:53:08,072 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:53:08,092 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:53:08,093 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:53:08,093 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:53:08,093 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:53:08,093 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: ConfigurationManager, init=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:53:08,094 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:53:08,094 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:53:08,099 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:53:08,099 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:53:08,100 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:53:08,100 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:53:08,101 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:53:08,102 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:53:08,105 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:08,122 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:08,123 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:53:08,130 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:53:08,131 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:53:08,131 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:53:08,135 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c does not exist. Creating ...
datanode_3          | 2023-06-22 08:53:08,137 [grpc-default-executor-0] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: receive requestVote(PRE_VOTE, 31613e4d-b990-42be-9377-00c7a7ba3288, group-D771BA859F6C, 0, (t:0, i:0))
datanode_3          | 2023-06-22 08:53:08,144 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 73ad851d-99db-49b7-ac3d-75be53f012c8: Failed requestVote 31613e4d-b990-42be-9377-00c7a7ba3288->73ad851d-99db-49b7-ac3d-75be53f012c8#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C is not in [RUNNING]: current state is STARTING
datanode_3          | 2023-06-22 08:53:08,146 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:53:08,152 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:08,154 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/in_use.lock acquired by nodename 7@ada99c0504f5
datanode_3          | 2023-06-22 08:53:08,162 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:53:08,171 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c has been successfully formatted.
datanode_3          | 2023-06-22 08:53:08,197 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO ratis.ContainerStateMachine: group-D771BA859F6C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:53:08,197 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:53:08,211 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:53:08,211 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:08,218 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:53:08,220 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:53:08,220 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:53:08,240 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:08,245 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:51:49,349 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-22 08:51:49,891 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-22 08:51:50,772 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:51:50,778 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-22 08:51:50,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-22 08:51:50,782 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-22 08:51:50,817 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-22 08:51:50,817 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-22 08:51:50,825 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-22 08:51:50,833 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:51:50,843 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-22 08:51:50,849 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:51:51,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-22 08:51:51,309 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-22 08:51:51,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-22 08:51:53,440 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-22 08:51:53,469 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-22 08:51:53,476 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-22 08:51:53,478 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:51:53,480 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:51:53,520 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:51:54,089 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-22 08:51:54,587 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-22 08:51:55,581 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:51:55,681 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-22 08:51:55,923 [main] INFO util.log: Logging initialized @46239ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-22 08:51:56,914 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-22 08:51:56,976 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-22 08:51:57,066 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-22 08:51:57,070 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-22 08:51:57,070 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-22 08:51:57,089 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-22 08:51:57,471 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-22 08:51:57,495 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-22 08:51:57,509 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-22 08:51:57,758 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-22 08:51:57,758 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-22 08:51:57,785 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-22 08:51:57,920 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@744199bb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-22 08:51:57,928 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7dc963be{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-22 08:51:58,728 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5746609e{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-673574732024043754/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-22 08:51:58,805 [main] INFO server.AbstractConnector: Started ServerConnector@27e656e6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-22 08:51:58,806 [main] INFO server.Server: Started @49121ms
datanode_2          | 2023-06-22 08:51:58,826 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-22 08:51:58,826 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-22 08:51:58,836 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-22 08:51:59,107 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-22 08:51:59,431 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-22 08:52:01,087 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
datanode_2          | 2023-06-22 08:52:01,264 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-22 08:52:01,277 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-22 08:52:01,285 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-22 08:52:01,322 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-22 08:52:01,322 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-22 08:52:02,190 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_2          | 2023-06-22 08:52:02,608 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-22 08:52:05,211 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:06,970 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:07,952 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:07,971 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:08,953 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:09,020 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:10,022 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:11,023 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:12,024 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:13,025 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:14,032 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:14,055 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 92add0f1b6b7/172.24.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:51842 remote=recon/172.24.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:51842 remote=recon/172.24.0.6:9891]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_5          | 2023-06-22 08:52:15,033 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:16,034 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:17,035 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-22 08:52:22,047 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | 2023-06-22 08:52:05,222 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:06,212 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:06,223 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:07,213 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:07,224 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:08,214 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.6:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:08,225 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:09,226 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:10,227 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:11,232 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:12,235 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:13,237 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:13,273 [EndpointStateMachine task thread for recon/172.24.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 7c8c150a4dbd/172.24.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:43212 remote=recon/172.24.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:43212 remote=recon/172.24.0.6:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_2          | 2023-06-22 08:52:14,238 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:15,239 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-22 08:53:07,131 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:53:07,132 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:53:07,135 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:53:07,136 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:53:07,136 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:53:07,137 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:53:07,139 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:53:07,140 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:53:07,145 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:53:07,152 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:53:07,154 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:53:07,154 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:53:07,155 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:53:07,155 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:53:07,156 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:53:07,157 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 does not exist. Creating ...
datanode_1          | 2023-06-22 08:53:07,162 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/in_use.lock acquired by nodename 7@b325136153fd
datanode_1          | 2023-06-22 08:53:07,178 [grpc-default-executor-4] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: receive requestVote(ELECTION, 5cccec7c-2f8e-4410-a445-f064595c8b02, group-D0B46C7C7CF5, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:53:07,130 [grpc-default-executor-1] INFO impl.VoteContext: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FOLLOWER: accept PRE_VOTE from 5cccec7c-2f8e-4410-a445-f064595c8b02: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:53:07,205 [grpc-default-executor-1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: 5cccec7c-2f8e-4410-a445-f064595c8b02<-1c635413-c797-403f-bedc-bdd0587a593c#0:OK-t0. Peer's state: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5:t0, leader=null, voted=, raftlog=Memoized:1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:07,181 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 has been successfully formatted.
datanode_1          | 2023-06-22 08:53:07,215 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO ratis.ContainerStateMachine: group-61BE3005BF20: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:53:07,218 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:53:07,222 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:53:07,218 [grpc-default-executor-4] INFO impl.VoteContext: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FOLLOWER: accept ELECTION from 5cccec7c-2f8e-4410-a445-f064595c8b02: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-22 08:53:07,281 [grpc-default-executor-4] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_1          | 2023-06-22 08:53:07,281 [grpc-default-executor-4] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState
datanode_1          | 2023-06-22 08:53:07,282 [grpc-default-executor-4] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState
datanode_1          | 2023-06-22 08:53:07,247 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,283 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:53:07,284 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:53:07,285 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:07,292 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:53:07,293 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:53:07,294 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,294 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20
datanode_1          | 2023-06-22 08:53:07,294 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:53:07,295 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:07,295 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:07,297 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:53:07,297 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:53:07,298 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:53:07,298 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:53:07,298 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:53:07,305 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:07,307 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,285 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState] INFO impl.FollowerState: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-FollowerState was interrupted
datanode_1          | 2023-06-22 08:53:07,429 [grpc-default-executor-4] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5 replies to ELECTION vote request: 5cccec7c-2f8e-4410-a445-f064595c8b02<-1c635413-c797-403f-bedc-bdd0587a593c#0:OK-t1. Peer's state: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5:t1, leader=null, voted=5cccec7c-2f8e-4410-a445-f064595c8b02, raftlog=Memoized:1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:07,438 [grpc-default-executor-4] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: receive requestVote(ELECTION, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-61BE3005BF20, 1, (t:0, i:0))
datanode_1          | 2023-06-22 08:53:07,445 [grpc-default-executor-4] WARN server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Failed requestVote 73ad851d-99db-49b7-ac3d-75be53f012c8->1c635413-c797-403f-bedc-bdd0587a593c#0: org.apache.ratis.protocol.exceptions.ServerNotReadyException: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20 is not in [RUNNING]: current state is STARTING
datanode_1          | 2023-06-22 08:53:07,504 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:07,514 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:07,515 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:07,515 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:07,524 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:07,539 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:07,540 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:53:07,542 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState
datanode_1          | 2023-06-22 08:53:07,552 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61BE3005BF20,id=1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:53:07,555 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:07,556 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:53:07,556 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:53:07,557 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:53:07,561 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:07,590 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:07,751 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5.
datanode_4          | 2023-06-22 08:52:58,439 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-22 08:52:58,440 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-22 08:52:58,440 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-22 08:52:58,440 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-22 08:52:58,470 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: ConfigurationManager, init=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-22 08:52:58,472 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-22 08:52:58,487 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-22 08:52:58,489 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-22 08:52:58,545 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-22 08:52:58,567 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-22 08:52:58,580 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-22 08:52:58,580 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-22 08:52:58,686 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-22 08:52:58,846 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-22 08:52:58,911 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-22 08:52:58,918 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-22 08:52:58,921 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-22 08:52:58,926 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-22 08:52:58,952 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-22 08:52:58,958 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9284cc47-3eb5-4374-917c-8d2e22216578 does not exist. Creating ...
datanode_4          | 2023-06-22 08:52:59,025 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9284cc47-3eb5-4374-917c-8d2e22216578/in_use.lock acquired by nodename 7@2e3fa1f00372
datanode_4          | 2023-06-22 08:52:59,078 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9284cc47-3eb5-4374-917c-8d2e22216578 has been successfully formatted.
datanode_4          | 2023-06-22 08:52:59,209 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO ratis.ContainerStateMachine: group-8D2E22216578: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-22 08:52:59,232 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-22 08:52:59,338 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-22 08:52:59,338 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:52:59,356 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-22 08:52:59,373 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-22 08:52:59,375 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-22 08:52:59,444 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-22 08:52:59,450 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-22 08:52:59,450 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:52:59,507 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9284cc47-3eb5-4374-917c-8d2e22216578
datanode_4          | 2023-06-22 08:52:59,515 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-22 08:52:59,517 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-22 08:52:59,537 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-22 08:52:59,560 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-22 08:52:59,560 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-22 08:52:59,561 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-22 08:52:59,561 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-22 08:52:59,562 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-22 08:52:59,690 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:52:16,240 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:17,241 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.2:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-22 08:52:22,257 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 7c8c150a4dbd/172.24.0.15 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:45004 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:45004 remote=scm/172.24.0.2:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_2          | 2023-06-22 08:52:23,851 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-b37b0063-6055-43d9-baf0-294afd45ec3e/container.db to cache
datanode_2          | 2023-06-22 08:52:23,853 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-b37b0063-6055-43d9-baf0-294afd45ec3e/container.db for volume DS-b37b0063-6055-43d9-baf0-294afd45ec3e
datanode_2          | 2023-06-22 08:52:23,878 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-22 08:52:23,901 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-22 08:52:24,314 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-22 08:52:24,315 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_2          | 2023-06-22 08:52:24,480 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: cd5a4933-0666-4f07-ab50-d6c55125d602: start RPC server
datanode_2          | 2023-06-22 08:52:24,500 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: cd5a4933-0666-4f07-ab50-d6c55125d602: GrpcService started, listening on 9858
datanode_2          | 2023-06-22 08:52:24,513 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: cd5a4933-0666-4f07-ab50-d6c55125d602: GrpcService started, listening on 9856
datanode_2          | 2023-06-22 08:52:24,523 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: cd5a4933-0666-4f07-ab50-d6c55125d602: GrpcService started, listening on 9857
datanode_2          | 2023-06-22 08:52:24,556 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cd5a4933-0666-4f07-ab50-d6c55125d602 is started using port 9858 for RATIS
datanode_2          | 2023-06-22 08:52:24,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cd5a4933-0666-4f07-ab50-d6c55125d602 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-22 08:52:24,557 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis cd5a4933-0666-4f07-ab50-d6c55125d602 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-22 08:52:24,560 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-cd5a4933-0666-4f07-ab50-d6c55125d602: Started
datanode_2          | 2023-06-22 08:52:24,682 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:52:59,060 [PipelineCommandHandlerThread-0] INFO server.RaftServer: cd5a4933-0666-4f07-ab50-d6c55125d602: addNew group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-D0B46C7C7CF5:java.util.concurrent.CompletableFuture@6f695300[Not completed]
datanode_2          | 2023-06-22 08:52:59,243 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602: new RaftServerImpl for group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:52:59,253 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:52:59,262 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:52:59,262 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:52:59,265 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:52:59,268 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:52:59,269 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:52:59,323 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:52:59,328 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:52:59,376 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:52:59,381 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:52:59,483 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:52:59,511 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:52:59,596 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:52:59,606 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:52:59,725 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:52:59,921 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:52:59,958 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:52:59,961 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:52:59,967 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:52:59,969 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:52:59,975 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:52:59,979 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 does not exist. Creating ...
datanode_2          | 2023-06-22 08:53:00,016 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/in_use.lock acquired by nodename 7@7c8c150a4dbd
datanode_2          | 2023-06-22 08:53:00,058 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 has been successfully formatted.
datanode_2          | 2023-06-22 08:53:00,177 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO ratis.ContainerStateMachine: group-D0B46C7C7CF5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:53:00,196 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:53:00,314 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:53:00,316 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:00,328 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:53:00,331 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:53:00,357 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:00,413 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:53:00,414 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:53:00,415 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,753 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1c635413-c797-403f-bedc-bdd0587a593c: addNew group-61D23AAFFFF1:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-61D23AAFFFF1:java.util.concurrent.CompletableFuture@50e5dcb[Not completed]
datanode_1          | 2023-06-22 08:53:07,766 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c: new RaftServerImpl for group-61D23AAFFFF1:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-22 08:53:07,771 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-22 08:53:07,772 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-22 08:53:07,772 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-22 08:53:07,773 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:53:07,776 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-22 08:53:07,776 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:53:07,777 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: ConfigurationManager, init=-1: peers:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-22 08:53:07,779 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-22 08:53:07,783 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-22 08:53:07,784 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-22 08:53:07,784 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:53:07,785 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-22 08:53:07,785 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-22 08:53:07,785 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-22 08:53:07,787 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-22 08:53:07,796 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:53:07,797 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:53:07,797 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-22 08:53:07,798 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-22 08:53:07,798 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-22 08:53:07,802 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-22 08:53:07,803 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5141abe2-7210-4ceb-8a28-61d23aaffff1 does not exist. Creating ...
datanode_1          | 2023-06-22 08:53:07,806 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5141abe2-7210-4ceb-8a28-61d23aaffff1/in_use.lock acquired by nodename 7@b325136153fd
datanode_1          | 2023-06-22 08:53:07,828 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5141abe2-7210-4ceb-8a28-61d23aaffff1 has been successfully formatted.
datanode_1          | 2023-06-22 08:53:07,853 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO ratis.ContainerStateMachine: group-61D23AAFFFF1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-22 08:53:07,854 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-22 08:53:07,866 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-22 08:53:07,868 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,869 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-22 08:53:07,869 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-22 08:53:07,871 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:07,872 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-22 08:53:07,875 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-22 08:53:07,876 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,876 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5141abe2-7210-4ceb-8a28-61d23aaffff1
datanode_1          | 2023-06-22 08:53:07,876 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-22 08:53:07,876 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:07,876 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:08,245 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:53:08,258 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:08,226 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:53:08,272 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:08,273 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:53:08,276 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c
datanode_3          | 2023-06-22 08:53:08,281 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:53:08,282 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:08,285 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:53:08,285 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:08,285 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:08,285 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:53:08,286 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:53:08,289 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:53:08,293 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:53:08,303 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-22 08:53:08,303 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:53:08,304 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:08,324 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:08,446 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:53:08,446 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:08,460 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:53:08,461 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:53:08,461 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:53:08,467 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:08,468 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:08,468 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:08,469 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:08,470 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:08,474 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:08,474 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:53:08,505 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:53:08,506 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:08,506 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:53:08,476 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: start as a follower, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:08,510 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:53:08,511 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState
datanode_5          | java.net.SocketTimeoutException: Call From 92add0f1b6b7/172.24.0.11 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:38914 remote=scm/172.24.0.2:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.11:38914 remote=scm/172.24.0.2:9861]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
datanode_5          | 2023-06-22 08:52:23,830 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-57ee0d8b-c814-4250-b490-c6bbd8c38d25/container.db to cache
datanode_5          | 2023-06-22 08:52:23,838 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-7056bc73-01b7-404b-90a0-81c09d0d73e5/DS-57ee0d8b-c814-4250-b490-c6bbd8c38d25/container.db for volume DS-57ee0d8b-c814-4250-b490-c6bbd8c38d25
datanode_5          | 2023-06-22 08:52:23,851 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5          | 2023-06-22 08:52:23,885 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_5          | 2023-06-22 08:52:24,286 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5          | 2023-06-22 08:52:24,287 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:52:24,464 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.RaftServer: 5cccec7c-2f8e-4410-a445-f064595c8b02: start RPC server
datanode_5          | 2023-06-22 08:52:24,471 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 5cccec7c-2f8e-4410-a445-f064595c8b02: GrpcService started, listening on 9858
datanode_5          | 2023-06-22 08:52:24,496 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 5cccec7c-2f8e-4410-a445-f064595c8b02: GrpcService started, listening on 9856
datanode_5          | 2023-06-22 08:52:24,501 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO server.GrpcService: 5cccec7c-2f8e-4410-a445-f064595c8b02: GrpcService started, listening on 9857
datanode_5          | 2023-06-22 08:52:24,528 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5cccec7c-2f8e-4410-a445-f064595c8b02 is started using port 9858 for RATIS
datanode_5          | 2023-06-22 08:52:24,528 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5cccec7c-2f8e-4410-a445-f064595c8b02 is started using port 9857 for RATIS_ADMIN
datanode_5          | 2023-06-22 08:52:24,534 [EndpointStateMachine task thread for scm/172.24.0.2:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5cccec7c-2f8e-4410-a445-f064595c8b02 is started using port 9856 for RATIS_SERVER
datanode_5          | 2023-06-22 08:52:24,535 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5cccec7c-2f8e-4410-a445-f064595c8b02: Started
datanode_5          | 2023-06-22 08:52:24,727 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:52:58,909 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 5cccec7c-2f8e-4410-a445-f064595c8b02: addNew group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] returns group-D0B46C7C7CF5:java.util.concurrent.CompletableFuture@6ca697b1[Not completed]
datanode_3          | 2023-06-22 08:53:08,525 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderStateImpl
datanode_3          | 2023-06-22 08:53:08,562 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D771BA859F6C,id=73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:08,586 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:08,586 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:53:08,587 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:53:08,587 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:53:08,602 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c
datanode_3          | 2023-06-22 08:53:08,616 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:53:08,626 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:53:08,768 [grpc-default-executor-0] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: receive requestVote(ELECTION, 31613e4d-b990-42be-9377-00c7a7ba3288, group-D771BA859F6C, 1, (t:0, i:0))
datanode_3          | 2023-06-22 08:53:08,780 [grpc-default-executor-0] INFO impl.VoteContext: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FOLLOWER: reject ELECTION from 31613e4d-b990-42be-9377-00c7a7ba3288: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-22 08:53:08,784 [grpc-default-executor-0] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:31613e4d-b990-42be-9377-00c7a7ba3288
datanode_3          | 2023-06-22 08:53:08,788 [grpc-default-executor-0] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState
datanode_3          | 2023-06-22 08:53:08,789 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState was interrupted
datanode_3          | 2023-06-22 08:53:08,802 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:53:08,815 [grpc-default-executor-0] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState
datanode_3          | 2023-06-22 08:53:08,867 [grpc-default-executor-0] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C replies to ELECTION vote request: 31613e4d-b990-42be-9377-00c7a7ba3288<-73ad851d-99db-49b7-ac3d-75be53f012c8#0:FAIL-t1. Peer's state: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C:t1, leader=null, voted=null, raftlog=Memoized:73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:09,126 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderElection1] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:09,721 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c.
datanode_3          | 2023-06-22 08:53:09,758 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 73ad851d-99db-49b7-ac3d-75be53f012c8: addNew group-53D02BDE8D21:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-53D02BDE8D21:java.util.concurrent.CompletableFuture@252e81e2[Not completed]
datanode_3          | 2023-06-22 08:53:09,760 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8: new RaftServerImpl for group-53D02BDE8D21:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-22 08:53:09,763 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-22 08:53:09,763 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-22 08:53:09,763 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-22 08:53:09,763 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:53:09,764 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-22 08:53:09,764 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-22 08:53:09,764 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: ConfigurationManager, init=-1: peers:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-22 08:53:09,765 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-22 08:53:09,765 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-22 08:53:09,768 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-22 08:53:09,768 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-22 08:53:09,768 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:53:09,768 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-22 08:53:09,769 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-22 08:53:09,769 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-22 08:53:09,773 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:09,774 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:09,774 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-22 08:53:09,774 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-22 08:53:09,775 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-22 08:53:09,775 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-22 08:53:09,777 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0769956d-ace7-4d84-9f3b-53d02bde8d21 does not exist. Creating ...
datanode_3          | 2023-06-22 08:53:09,784 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0769956d-ace7-4d84-9f3b-53d02bde8d21/in_use.lock acquired by nodename 7@ada99c0504f5
datanode_3          | 2023-06-22 08:53:09,793 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0769956d-ace7-4d84-9f3b-53d02bde8d21 has been successfully formatted.
datanode_3          | 2023-06-22 08:53:09,829 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO ratis.ContainerStateMachine: group-53D02BDE8D21: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-22 08:53:09,829 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-22 08:53:09,830 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-22 08:53:09,832 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:09,832 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-22 08:53:09,832 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-22 08:53:09,833 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:09,852 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-22 08:53:09,852 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-22 08:53:09,852 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:09,853 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0769956d-ace7-4d84-9f3b-53d02bde8d21
datanode_3          | 2023-06-22 08:53:09,854 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-22 08:53:09,855 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:09,855 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:09,856 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-22 08:53:09,857 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-22 08:53:09,858 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-22 08:53:09,859 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:00,475 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_2          | 2023-06-22 08:53:00,480 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:53:00,482 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:53:00,489 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:00,495 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:53:00,499 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:53:00,505 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:53:00,509 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:00,513 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:53:00,588 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:00,591 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:00,657 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:00,658 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:00,662 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:00,709 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:00,714 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:00,726 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:00,726 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:53:00,737 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState
datanode_2          | 2023-06-22 08:53:00,757 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D0B46C7C7CF5,id=cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_2          | 2023-06-22 08:53:00,760 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:53:00,768 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:53:00,776 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:00,780 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:53:00,783 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:53:00,788 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:53:00,902 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_2          | 2023-06-22 08:53:04,311 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-cd5a4933-0666-4f07-ab50-d6c55125d602: Detected pause in JVM or host machine approximately 0.134s without any GCs.
datanode_2          | 2023-06-22 08:53:05,837 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO impl.FollowerState: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5105610597ns, electionTimeout:5063ms
datanode_2          | 2023-06-22 08:53:05,838 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState
datanode_2          | 2023-06-22 08:53:05,845 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:53:05,878 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-22 08:53:05,879 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1
datanode_3          | 2023-06-22 08:53:09,860 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-22 08:53:09,863 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-22 08:53:09,863 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:09,922 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:09,923 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-22 08:53:09,924 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:09,925 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:09,925 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-22 08:53:09,926 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0
datanode_2          | 2023-06-22 08:53:05,982 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:06,170 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:53:06,171 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:53:06,198 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_2          | 2023-06-22 08:53:06,198 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1c635413-c797-403f-bedc-bdd0587a593c
datanode_2          | 2023-06-22 08:53:06,255 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, 1c635413-c797-403f-bedc-bdd0587a593c, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:06,267 [grpc-default-executor-0] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-CANDIDATE: accept PRE_VOTE from 1c635413-c797-403f-bedc-bdd0587a593c: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:53:06,349 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, 5cccec7c-2f8e-4410-a445-f064595c8b02, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:06,446 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: 1c635413-c797-403f-bedc-bdd0587a593c<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t0. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5:t0, leader=null, voted=, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:06,456 [grpc-default-executor-1] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-CANDIDATE: accept PRE_VOTE from 5cccec7c-2f8e-4410-a445-f064595c8b02: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:53:06,464 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: 5cccec7c-2f8e-4410-a445-f064595c8b02<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t0. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5:t0, leader=null, voted=, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:06,647 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: receive requestVote(ELECTION, 5cccec7c-2f8e-4410-a445-f064595c8b02, group-D0B46C7C7CF5, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:06,654 [grpc-default-executor-0] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-CANDIDATE: accept ELECTION from 5cccec7c-2f8e-4410-a445-f064595c8b02: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:53:06,657 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_2          | 2023-06-22 08:53:06,662 [grpc-default-executor-0] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1
datanode_2          | 2023-06-22 08:53:06,663 [grpc-default-executor-0] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-FollowerState
datanode_2          | 2023-06-22 08:53:06,697 [grpc-default-executor-1] INFO server.RaftServer: cd5a4933-0666-4f07-ab50-d6c55125d602: addNew group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-D771BA859F6C:java.util.concurrent.CompletableFuture@e19d3d[Not completed]
datanode_2          | 2023-06-22 08:53:06,704 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602: new RaftServerImpl for group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:53:06,705 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:53:06,705 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:53:06,705 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:53:06,705 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:53:06,706 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:53:06,706 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-22 08:52:59,691 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:52:59,795 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-22 08:52:59,804 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-22 08:52:59,804 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-22 08:52:59,880 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-22 08:52:59,880 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-22 08:52:59,893 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: start as a follower, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:52:59,893 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-22 08:52:59,900 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState
datanode_4          | 2023-06-22 08:52:59,906 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-22 08:52:59,906 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-22 08:52:59,927 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8D2E22216578,id=31613e4d-b990-42be-9377-00c7a7ba3288
datanode_4          | 2023-06-22 08:52:59,929 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-22 08:52:59,930 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-22 08:52:59,930 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-22 08:52:59,931 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-22 08:52:59,982 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9284cc47-3eb5-4374-917c-8d2e22216578
datanode_4          | 2023-06-22 08:52:59,987 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9284cc47-3eb5-4374-917c-8d2e22216578.
datanode_4          | 2023-06-22 08:52:59,993 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 31613e4d-b990-42be-9377-00c7a7ba3288: addNew group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-D771BA859F6C:java.util.concurrent.CompletableFuture@930ace4[Not completed]
datanode_4          | 2023-06-22 08:53:00,036 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288: new RaftServerImpl for group-D771BA859F6C:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-22 08:53:00,037 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-22 08:53:00,037 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-22 08:53:00,037 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-22 08:53:00,038 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-22 08:53:00,039 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-22 08:53:00,039 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-22 08:53:00,040 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: ConfigurationManager, init=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-22 08:53:00,041 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-22 08:53:00,060 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-22 08:53:00,060 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-22 08:53:00,061 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-22 08:53:00,062 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-22 08:53:09,963 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: start as a follower, conf=-1: peers:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:09,963 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-22 08:53:09,964 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState
datanode_3          | 2023-06-22 08:53:09,966 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-53D02BDE8D21,id=73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:09,966 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-22 08:53:09,966 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-22 08:53:09,966 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-22 08:53:09,966 [73ad851d-99db-49b7-ac3d-75be53f012c8-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-22 08:53:09,976 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-22 08:53:09,977 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-22 08:53:09,977 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0769956d-ace7-4d84-9f3b-53d02bde8d21
datanode_3          | 2023-06-22 08:53:09,977 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0769956d-ace7-4d84-9f3b-53d02bde8d21.
datanode_3          | 2023-06-22 08:53:13,878 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5063402695ns, electionTimeout:5017ms
datanode_3          | 2023-06-22 08:53:13,879 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState
datanode_3          | 2023-06-22 08:53:13,879 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-22 08:53:13,881 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-22 08:53:13,882 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2
datanode_3          | 2023-06-22 08:53:13,894 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 1 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:13,902 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 31613e4d-b990-42be-9377-00c7a7ba3288
datanode_3          | 2023-06-22 08:53:13,909 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_3          | 2023-06-22 08:53:13,942 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:53:13,942 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 73ad851d-99db-49b7-ac3d-75be53f012c8<-31613e4d-b990-42be-9377-00c7a7ba3288#0:OK-t1
datanode_3          | 2023-06-22 08:53:13,943 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-06-22 08:53:13,945 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:13,988 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-22 08:53:13,988 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 73ad851d-99db-49b7-ac3d-75be53f012c8<-31613e4d-b990-42be-9377-00c7a7ba3288#0:OK-t2
datanode_3          | 2023-06-22 08:53:13,988 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2 ELECTION round 0: result PASSED
datanode_3          | 2023-06-22 08:53:13,989 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2
datanode_3          | 2023-06-22 08:53:13,989 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-06-22 08:53:13,990 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D771BA859F6C with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:13,990 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 2 for becomeLeader, leader elected after 5890ms
datanode_3          | 2023-06-22 08:53:14,000 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:53:14,004 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:14,004 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:53:14,007 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:53:14,008 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:53:14,009 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:53:14,010 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:14,010 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:53:14,021 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:53:14,021 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:14,021 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:53:14,022 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:53:14,022 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:53:14,022 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:14,023 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:53:14,023 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:53:14,023 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:14,023 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:53:14,024 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-22 08:53:14,027 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-22 08:53:14,027 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-22 08:53:14,027 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-22 08:53:14,028 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-22 08:53:14,029 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-22 08:53:14,029 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-22 08:53:14,029 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-22 08:53:14,029 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-22 08:53:14,030 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-22 08:53:14,031 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderStateImpl
datanode_3          | 2023-06-22 08:53:14,034 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:53:14,047 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/current/log_inprogress_0
datanode_3          | 2023-06-22 08:53:14,093 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C-LeaderElection2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C: set configuration 0: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,030 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO impl.FollowerState: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5066139359ns, electionTimeout:5053ms
datanode_3          | 2023-06-22 08:53:15,030 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState
datanode_1          | 2023-06-22 08:53:07,880 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-22 08:53:07,880 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-22 08:53:07,880 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-22 08:53:07,881 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-22 08:53:07,881 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-22 08:53:07,883 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:07,889 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-22 08:53:07,956 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:07,956 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-22 08:53:07,957 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:07,957 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:07,960 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-22 08:53:07,961 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: start as a follower, conf=-1: peers:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:07,971 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-22 08:53:07,971 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState
datanode_1          | 2023-06-22 08:53:07,985 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61D23AAFFFF1,id=1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:53:07,985 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-22 08:53:07,986 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-22 08:53:07,986 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-22 08:53:07,986 [1c635413-c797-403f-bedc-bdd0587a593c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-22 08:53:07,989 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:07,989 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:07,990 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=5141abe2-7210-4ceb-8a28-61d23aaffff1
datanode_1          | 2023-06-22 08:53:07,995 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=5141abe2-7210-4ceb-8a28-61d23aaffff1.
datanode_1          | 2023-06-22 08:53:08,631 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D0B46C7C7CF5 with new leaderId: 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_1          | 2023-06-22 08:53:08,646 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: change Leader from null to 5cccec7c-2f8e-4410-a445-f064595c8b02 at term 1 for appendEntries, leader elected after 9326ms
datanode_1          | 2023-06-22 08:53:08,847 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:09,050 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:53:09,433 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_1          | 2023-06-22 08:53:09,455 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 1 for appendEntries, leader elected after 2297ms
datanode_1          | 2023-06-22 08:53:09,503 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:09,505 [1c635413-c797-403f-bedc-bdd0587a593c-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:53:15,031 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-22 08:53:15,032 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-22 08:53:15,033 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-FollowerState] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3
datanode_3          | 2023-06-22 08:53:15,051 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,051 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-22 08:53:15,058 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,058 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.LeaderElection: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-22 08:53:15,058 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3
datanode_3          | 2023-06-22 08:53:15,058 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-22 08:53:15,059 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-53D02BDE8D21 with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_3          | 2023-06-22 08:53:15,060 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 1 for becomeLeader, leader elected after 5290ms
datanode_3          | 2023-06-22 08:53:15,072 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-22 08:53:15,076 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:15,079 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-22 08:53:15,080 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-22 08:53:15,080 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-22 08:53:15,084 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-22 08:53:15,084 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-22 08:53:15,085 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-22 08:53:15,094 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderStateImpl
datanode_3          | 2023-06-22 08:53:15,103 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-22 08:53:15,105 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0769956d-ace7-4d84-9f3b-53d02bde8d21/current/log_inprogress_0
datanode_3          | 2023-06-22 08:53:15,115 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21-LeaderElection3] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-53D02BDE8D21: set configuration 0: peers:[73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,158 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderStateImpl] INFO impl.TransferLeadership: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: start transferring leadership to 1c635413-c797-403f-bedc-bdd0587a593c
datanode_3          | 2023-06-22 08:53:15,160 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderStateImpl] INFO impl.TransferLeadership: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: sendStartLeaderElection to follower 1c635413-c797-403f-bedc-bdd0587a593c, lastEntry=(t:1, i:0)
datanode_3          | 2023-06-22 08:53:15,167 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderStateImpl] INFO impl.TransferLeadership: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: SUCCESS sent StartLeaderElection to transferee 1c635413-c797-403f-bedc-bdd0587a593c immediately as it already has up-to-date log
datanode_3          | 2023-06-22 08:53:15,201 [73ad851d-99db-49b7-ac3d-75be53f012c8-server-thread1] INFO impl.TransferLeadership: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: Received startLeaderElection reply from 1c635413-c797-403f-bedc-bdd0587a593c: success? true
datanode_3          | 2023-06-22 08:53:15,328 [grpc-default-executor-2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: receive requestVote(ELECTION, 1c635413-c797-403f-bedc-bdd0587a593c, group-61BE3005BF20, 2, (t:1, i:0))
datanode_3          | 2023-06-22 08:53:15,328 [grpc-default-executor-2] INFO impl.VoteContext: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LEADER: accept ELECTION from 1c635413-c797-403f-bedc-bdd0587a593c: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-22 08:53:15,334 [grpc-default-executor-2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: change Leader from 73ad851d-99db-49b7-ac3d-75be53f012c8 to null at term 2 for updateCurrentTerm
datanode_3          | 2023-06-22 08:53:15,334 [grpc-default-executor-2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: changes role from    LEADER to FOLLOWER at term 2 for candidate:1c635413-c797-403f-bedc-bdd0587a593c
datanode_3          | 2023-06-22 08:53:15,334 [grpc-default-executor-2] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: shutdown 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-LeaderStateImpl
datanode_3          | 2023-06-22 08:53:15,335 [grpc-default-executor-2] INFO impl.PendingRequests: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-PendingRequests: sendNotLeaderResponses
datanode_3          | 2023-06-22 08:53:15,335 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->1c635413-c797-403f-bedc-bdd0587a593c-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->1c635413-c797-403f-bedc-bdd0587a593c-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3          | 2023-06-22 08:53:15,339 [grpc-default-executor-2] INFO impl.RoleInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8: start 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-FollowerState
datanode_3          | 2023-06-22 08:53:15,335 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->5cccec7c-2f8e-4410-a445-f064595c8b02-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->5cccec7c-2f8e-4410-a445-f064595c8b02-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3          | 2023-06-22 08:53:15,388 [grpc-default-executor-2] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20 replies to ELECTION vote request: 1c635413-c797-403f-bedc-bdd0587a593c<-73ad851d-99db-49b7-ac3d-75be53f012c8#0:OK-t2. Peer's state: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20:t2, leader=null, voted=1c635413-c797-403f-bedc-bdd0587a593c, raftlog=Memoized:73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLog:OPENED:c0, conf=0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,792 [73ad851d-99db-49b7-ac3d-75be53f012c8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 1c635413-c797-403f-bedc-bdd0587a593c
datanode_3          | 2023-06-22 08:53:15,792 [73ad851d-99db-49b7-ac3d-75be53f012c8-server-thread1] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: change Leader from null to 1c635413-c797-403f-bedc-bdd0587a593c at term 2 for appendEntries, leader elected after 448ms
datanode_3          | 2023-06-22 08:53:15,855 [grpc-default-executor-3] INFO server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->5cccec7c-2f8e-4410-a445-f064595c8b02-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3          | 2023-06-22 08:53:15,884 [73ad851d-99db-49b7-ac3d-75be53f012c8-server-thread1] INFO server.RaftServer$Division: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20: set configuration 1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-22 08:53:15,886 [grpc-default-executor-0] INFO server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->5cccec7c-2f8e-4410-a445-f064595c8b02-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3          | 2023-06-22 08:53:15,887 [grpc-default-executor-3] INFO leader.FollowerInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->5cccec7c-2f8e-4410-a445-f064595c8b02: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_3          | 2023-06-22 08:53:15,895 [73ad851d-99db-49b7-ac3d-75be53f012c8-server-thread1] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_3          | 2023-06-22 08:53:15,904 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0 to /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_0-0
datanode_3          | 2023-06-22 08:53:15,912 [73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_1
datanode_3          | 2023-06-22 08:53:16,327 [grpc-default-executor-1] INFO server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->1c635413-c797-403f-bedc-bdd0587a593c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3          | 2023-06-22 08:53:16,330 [grpc-default-executor-1] INFO leader.FollowerInfo: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->1c635413-c797-403f-bedc-bdd0587a593c: decreaseNextIndex nextIndex: updateUnconditionally 1 -> 0
datanode_3          | 2023-06-22 08:53:16,343 [grpc-default-executor-1] INFO server.GrpcLogAppender: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-61BE3005BF20->1c635413-c797-403f-bedc-bdd0587a593c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3          | 2023-06-22 08:53:24,742 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:54:24,743 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:55:24,743 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:56:24,744 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:57:24,744 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:58:24,745 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 08:59:24,745 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-22 09:00:24,746 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:53:09,850 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0
datanode_1          | 2023-06-22 08:53:09,863 [1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-D0B46C7C7CF5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/current/log_inprogress_0
datanode_1          | 2023-06-22 08:53:12,661 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:12,662 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:13,115 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO impl.FollowerState: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5143393067ns, electionTimeout:5125ms
datanode_1          | 2023-06-22 08:53:13,115 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState
datanode_1          | 2023-06-22 08:53:13,115 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-22 08:53:13,116 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-22 08:53:13,117 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-FollowerState] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2
datanode_1          | 2023-06-22 08:53:13,139 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:13,140 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-22 08:53:13,161 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:13,164 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-22 08:53:13,164 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2
datanode_1          | 2023-06-22 08:53:13,165 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-22 08:53:13,165 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61D23AAFFFF1 with new leaderId: 1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:53:13,169 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: change Leader from null to 1c635413-c797-403f-bedc-bdd0587a593c at term 1 for becomeLeader, leader elected after 5380ms
datanode_1          | 2023-06-22 08:53:13,207 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:53:13,283 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:13,286 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:53:13,326 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:53:13,327 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:53:13,330 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:53:13,361 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:13,371 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:53:13,405 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderStateImpl
datanode_1          | 2023-06-22 08:53:13,408 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-22 08:53:13,411 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5141abe2-7210-4ceb-8a28-61d23aaffff1/current/log_inprogress_0
datanode_1          | 2023-06-22 08:53:13,426 [1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1-LeaderElection2] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61D23AAFFFF1: set configuration 0: peers:[1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:15,176 [grpc-default-executor-1] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState
datanode_1          | 2023-06-22 08:53:15,178 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState] INFO impl.FollowerState: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-FollowerState was interrupted
datanode_2          | 2023-06-22 08:53:06,706 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: ConfigurationManager, init=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:53:06,709 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:53:06,710 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:53:06,710 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:53:06,710 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:53:06,712 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:06,712 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:53:06,712 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:53:06,712 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:53:06,713 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:53:06,716 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:53:06,717 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:53:06,718 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:53:06,719 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:53:06,739 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=1) received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:53:06,741 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection:   Response 0: cd5a4933-0666-4f07-ab50-d6c55125d602<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:FAIL-t1
datanode_2          | 2023-06-22 08:53:06,742 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=1)
datanode_2          | 2023-06-22 08:53:06,728 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5 replies to ELECTION vote request: 5cccec7c-2f8e-4410-a445-f064595c8b02<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t1. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5:t1, leader=null, voted=5cccec7c-2f8e-4410-a445-f064595c8b02, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:06,752 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:53:06,752 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c does not exist. Creating ...
datanode_2          | 2023-06-22 08:53:06,757 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/in_use.lock acquired by nodename 7@7c8c150a4dbd
datanode_2          | 2023-06-22 08:53:06,766 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c has been successfully formatted.
datanode_2          | 2023-06-22 08:53:06,767 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO ratis.ContainerStateMachine: group-D771BA859F6C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:53:06,769 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:53:06,770 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:53:06,773 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:06,773 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:53:06,774 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:53:06,777 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:06,790 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:53:06,790 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:53:06,791 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:06,792 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c
datanode_2          | 2023-06-22 08:53:06,793 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:53:06,794 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:53:06,800 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-22 08:53:15,178 [grpc-default-executor-1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-22 08:53:15,178 [grpc-default-executor-1] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: change Leader from 73ad851d-99db-49b7-ac3d-75be53f012c8 to null at term 1 for ELECTION
datanode_1          | 2023-06-22 08:53:15,183 [grpc-default-executor-1] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3
datanode_1          | 2023-06-22 08:53:15,209 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:15,211 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-22 08:53:15,211 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-22 08:53:15,232 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.LeaderElection:   Response 0: 1c635413-c797-403f-bedc-bdd0587a593c<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t2
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.LeaderElection: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3 ELECTION round 0: result PASSED
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: shutdown 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 1c635413-c797-403f-bedc-bdd0587a593c
datanode_1          | 2023-06-22 08:53:15,235 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: change Leader from null to 1c635413-c797-403f-bedc-bdd0587a593c at term 2 for becomeLeader, leader elected after 56ms
datanode_1          | 2023-06-22 08:53:15,236 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-22 08:53:15,236 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_1          | 2023-06-22 08:53:15,245 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:15,257 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-22 08:53:15,258 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-22 08:53:15,258 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-22 08:53:15,258 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-22 08:53:15,258 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-22 08:53:15,258 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-22 08:53:15,323 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:53:15,342 [grpc-default-executor-4] INFO server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Completed APPEND_ENTRIES, lastRequest: null
datanode_1          | 2023-06-22 08:53:15,372 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:53:00,065 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-22 08:53:00,066 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-22 08:53:00,070 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-22 08:53:00,075 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-22 08:53:00,080 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-22 08:53:00,080 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-22 08:53:00,080 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-22 08:53:00,081 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-22 08:53:00,081 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-22 08:53:00,082 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c does not exist. Creating ...
datanode_4          | 2023-06-22 08:53:00,089 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/in_use.lock acquired by nodename 7@2e3fa1f00372
datanode_4          | 2023-06-22 08:53:00,092 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c has been successfully formatted.
datanode_4          | 2023-06-22 08:53:00,112 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO ratis.ContainerStateMachine: group-D771BA859F6C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-22 08:53:00,117 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-22 08:53:00,117 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-22 08:53:00,129 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:53:00,130 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-22 08:53:00,131 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-22 08:53:00,132 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-22 08:53:00,133 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-22 08:53:00,135 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-22 08:53:00,140 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:53:00,140 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c
datanode_4          | 2023-06-22 08:53:00,140 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-22 08:53:00,141 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-22 08:53:00,141 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-22 08:53:00,141 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-22 08:53:00,143 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-22 08:53:00,143 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-22 08:53:00,145 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-22 08:53:00,151 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-22 08:53:00,152 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-06-22 08:53:00,156 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-22 08:53:02,018 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-31613e4d-b990-42be-9377-00c7a7ba3288: Detected pause in JVM or host machine approximately 1.739s with 1.838s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=42ms
datanode_4          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1796ms
datanode_4          | 2023-06-22 08:53:02,037 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-22 08:53:02,045 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-22 08:53:02,048 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-22 08:53:02,048 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-22 08:53:02,049 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:52:59,235 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02: new RaftServerImpl for group-D0B46C7C7CF5:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-22 08:52:59,249 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-22 08:52:59,251 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-22 08:52:59,255 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-22 08:52:59,257 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:52:59,262 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-22 08:52:59,263 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-22 08:52:59,307 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-22 08:52:59,310 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-22 08:52:59,353 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-22 08:52:59,360 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-22 08:52:59,504 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:52:59,551 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-22 08:52:59,593 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-22 08:52:59,604 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-22 08:52:59,817 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-22 08:53:00,015 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:53:00,035 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-22 08:53:00,041 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-22 08:53:00,045 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-22 08:53:00,048 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-22 08:53:00,049 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-22 08:53:00,051 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 does not exist. Creating ...
datanode_5          | 2023-06-22 08:53:00,089 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/in_use.lock acquired by nodename 7@92add0f1b6b7
datanode_5          | 2023-06-22 08:53:00,135 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5 has been successfully formatted.
datanode_5          | 2023-06-22 08:53:00,212 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO ratis.ContainerStateMachine: group-D0B46C7C7CF5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-22 08:53:00,225 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-22 08:53:00,324 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-22 08:53:00,325 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:00,346 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-22 08:53:00,353 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-22 08:53:00,391 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:00,431 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-22 08:53:00,440 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-22 08:53:00,440 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:00,491 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_5          | 2023-06-22 08:53:00,495 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-22 08:53:00,497 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:00,504 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:00,510 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-22 08:53:00,510 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-22 08:53:00,520 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-22 08:53:00,520 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-22 08:53:00,522 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-22 08:53:00,598 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:00,614 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:00,703 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-22 08:53:00,705 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-22 08:53:00,709 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:00,750 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:00,755 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:00,766 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:00,771 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-22 08:53:00,778 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState
datanode_5          | 2023-06-22 08:53:00,786 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-22 08:53:00,789 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-22 08:53:00,801 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D0B46C7C7CF5,id=5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:53:00,805 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:00,808 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-22 08:53:00,809 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-22 08:53:00,814 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-22 08:53:00,940 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5
datanode_5          | 2023-06-22 08:53:02,278 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-5cccec7c-2f8e-4410-a445-f064595c8b02: Detected pause in JVM or host machine approximately 0.121s with 0.046s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=46ms
datanode_5          | 2023-06-22 08:53:05,309 [grpc-default-executor-0] INFO server.RaftServer: 5cccec7c-2f8e-4410-a445-f064595c8b02: addNew group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] returns group-61BE3005BF20:java.util.concurrent.CompletableFuture@544179c7[Not completed]
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02: new RaftServerImpl for group-61BE3005BF20:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-22 08:53:05,318 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-22 08:53:15,375 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:53:15,432 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-22 08:53:15,434 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:53:15,445 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:53:15,446 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-22 08:53:15,446 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-22 08:53:15,447 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:53:15,447 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:53:15,467 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-22 08:53:15,480 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:06,802 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:53:06,802 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:53:06,802 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:53:06,803 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:06,803 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:53:06,806 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:06,820 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:06,932 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:06,934 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:06,934 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:06,935 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:06,941 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:06,943 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: start as a follower, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:06,943 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:53:06,943 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState
datanode_2          | 2023-06-22 08:53:06,997 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D771BA859F6C,id=cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_2          | 2023-06-22 08:53:06,997 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:06,998 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:53:06,998 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:53:06,998 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:53:07,006 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:53:07,035 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:53:07,315 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5.
datanode_2          | 2023-06-22 08:53:07,317 [PipelineCommandHandlerThread-0] INFO server.RaftServer: cd5a4933-0666-4f07-ab50-d6c55125d602: addNew group-857F748A2E10:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-857F748A2E10:java.util.concurrent.CompletableFuture@4cd02d07[Not completed]
datanode_2          | 2023-06-22 08:53:07,321 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602: new RaftServerImpl for group-857F748A2E10:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-22 08:53:07,322 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-22 08:53:07,323 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-22 08:53:07,324 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-22 08:53:07,324 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-22 08:53:07,324 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-22 08:53:07,324 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-22 08:53:07,325 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: ConfigurationManager, init=-1: peers:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-22 08:53:07,325 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-22 08:53:07,325 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-22 08:53:07,325 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-22 08:53:07,327 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-22 08:53:15,481 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-22 08:53:15,481 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-22 08:53:15,481 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-22 08:53:15,481 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-22 08:53:15,486 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-22 08:53:15,495 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-22 08:53:15,495 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-22 08:53:15,497 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-22 08:53:15,498 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO impl.RoleInfo: 1c635413-c797-403f-bedc-bdd0587a593c: start 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderStateImpl
datanode_1          | 2023-06-22 08:53:15,521 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_1          | 2023-06-22 08:53:15,527 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0 to /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_0-0
datanode_1          | 2023-06-22 08:53:15,543 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_1
datanode_1          | 2023-06-22 08:53:15,575 [1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20-LeaderElection3] INFO server.RaftServer$Division: 1c635413-c797-403f-bedc-bdd0587a593c@group-61BE3005BF20: set configuration 1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-22 08:53:16,309 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Completed APPEND_ENTRIES, lastRequest: 73ad851d-99db-49b7-ac3d-75be53f012c8->1c635413-c797-403f-bedc-bdd0587a593c#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "5cccec7c-2f8e-4410-a445-f064595c8b02"
datanode_1          | address: "172.24.0.11:9856"
datanode_1          | dataStreamAddress: "172.24.0.11:9858"
datanode_1          | clientAddress: "172.24.0.11:9858"
datanode_1          | adminAddress: "172.24.0.11:9857"
datanode_1          | startupRole: FOLLOWER
datanode_1          | ,id: "1c635413-c797-403f-bedc-bdd0587a593c"
datanode_1          | address: "172.24.0.13:9856"
datanode_1          | priority: 1
datanode_1          | dataStreamAddress: "172.24.0.13:9858"
datanode_1          | clientAddress: "172.24.0.13:9858"
datanode_1          | adminAddress: "172.24.0.13:9857"
datanode_1          | startupRole: FOLLOWER
datanode_1          | ,id: "73ad851d-99db-49b7-ac3d-75be53f012c8"
datanode_1          | address: "172.24.0.12:9856"
datanode_1          | dataStreamAddress: "172.24.0.12:9858"
datanode_1          | clientAddress: "172.24.0.12:9858"
datanode_1          | adminAddress: "172.24.0.12:9857"
datanode_1          | startupRole: FOLLOWER
datanode_1          | , old:)
datanode_1          | 2023-06-22 08:53:16,317 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Completed APPEND_ENTRIES, lastReply: null
datanode_1          | 2023-06-22 08:53:16,329 [grpc-default-executor-4] INFO server.GrpcServerProtocolService: 1c635413-c797-403f-bedc-bdd0587a593c: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_1          |   requestorId: "73ad851d-99db-49b7-ac3d-75be53f012c8"
datanode_1          |   replyId: "1c635413-c797-403f-bedc-bdd0587a593c"
datanode_1          |   raftGroupId {
datanode_1          |     id: "`\343,\214Z\314F\272\275Aa\2760\005\277 "
datanode_1          |   }
datanode_1          |   callId: 6
datanode_1          |   success: true
datanode_1          | }
datanode_1          | term: 1
datanode_1          | nextIndex: 1
datanode_1          | matchIndex: 18446744073709551615
datanode_1          | isHearbeat: true
datanode_1          | 
datanode_1          | 2023-06-22 08:53:24,703 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:54:24,704 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:55:24,706 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:56:24,707 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:57:24,707 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:58:24,708 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 08:59:24,709 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-22 09:00:24,709 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:53:05,321 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-22 08:53:05,321 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-22 08:53:05,321 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-22 08:53:05,321 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-22 08:53:05,321 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:53:05,322 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-22 08:53:05,322 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-22 08:53:05,325 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-22 08:53:05,325 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-22 08:53:05,329 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:53:05,329 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-22 08:53:05,329 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-22 08:53:05,331 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-22 08:53:05,331 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-22 08:53:05,331 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-22 08:53:05,331 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 does not exist. Creating ...
datanode_5          | 2023-06-22 08:53:05,341 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/in_use.lock acquired by nodename 7@92add0f1b6b7
datanode_5          | 2023-06-22 08:53:05,349 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20 has been successfully formatted.
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO ratis.ContainerStateMachine: group-61BE3005BF20: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-22 08:53:05,373 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-22 08:53:05,388 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-22 08:53:05,393 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:05,398 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:05,499 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-22 08:53:05,504 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-22 08:53:02,049 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: start as a follower, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:02,052 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-22 08:53:02,052 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState
datanode_4          | 2023-06-22 08:53:02,055 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D771BA859F6C,id=31613e4d-b990-42be-9377-00c7a7ba3288
datanode_4          | 2023-06-22 08:53:02,055 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-22 08:53:02,055 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-22 08:53:02,056 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-22 08:53:02,056 [31613e4d-b990-42be-9377-00c7a7ba3288-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-22 08:53:02,057 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-22 08:53:02,057 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c
datanode_4          | 2023-06-22 08:53:02,113 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-22 08:53:05,045 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO impl.FollowerState: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5144962289ns, electionTimeout:5136ms
datanode_4          | 2023-06-22 08:53:05,048 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: shutdown 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState
datanode_4          | 2023-06-22 08:53:05,048 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-06-22 08:53:05,060 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-06-22 08:53:05,062 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-FollowerState] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1
datanode_4          | 2023-06-22 08:53:05,095 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:05,108 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_4          | 2023-06-22 08:53:05,123 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:05,124 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_4          | 2023-06-22 08:53:05,124 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: shutdown 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1
datanode_4          | 2023-06-22 08:53:05,125 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4          | 2023-06-22 08:53:05,126 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8D2E22216578 with new leaderId: 31613e4d-b990-42be-9377-00c7a7ba3288
datanode_4          | 2023-06-22 08:53:05,126 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: change Leader from null to 31613e4d-b990-42be-9377-00c7a7ba3288 at term 1 for becomeLeader, leader elected after 6583ms
datanode_4          | 2023-06-22 08:53:05,232 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-06-22 08:53:05,795 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-31613e4d-b990-42be-9377-00c7a7ba3288: Detected pause in JVM or host machine approximately 0.254s with 0.527s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=527ms
datanode_4          | 2023-06-22 08:53:05,965 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-22 08:53:05,983 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-06-22 08:53:06,120 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4          | 2023-06-22 08:53:06,123 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-06-22 08:53:06,135 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-06-22 08:53:06,314 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-22 08:53:06,371 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4          | 2023-06-22 08:53:06,411 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderStateImpl
datanode_4          | 2023-06-22 08:53:06,634 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-22 08:53:06,858 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-LeaderElection1] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578: set configuration 0: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:07,123 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5070671579ns, electionTimeout:5008ms
datanode_4          | 2023-06-22 08:53:07,126 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: shutdown 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState
datanode_4          | 2023-06-22 08:53:07,127 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-06-22 08:53:07,131 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-06-22 08:53:07,131 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2
datanode_4          | 2023-06-22 08:53:07,157 [31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-8D2E22216578-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9284cc47-3eb5-4374-917c-8d2e22216578/current/log_inprogress_0
datanode_4          | 2023-06-22 08:53:07,217 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:07,372 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-22 08:53:07,376 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-22 08:53:07,406 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_4          | 2023-06-22 08:53:07,406 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_4          | 2023-06-22 08:53:08,637 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C is not in [RUNNING]: current state is STARTING
datanode_4          | 2023-06-22 08:53:08,640 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
datanode_4          | 2023-06-22 08:53:08,643 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 31613e4d-b990-42be-9377-00c7a7ba3288<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t0
datanode_4          | 2023-06-22 08:53:08,644 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 73ad851d-99db-49b7-ac3d-75be53f012c8@group-D771BA859F6C is not in [RUNNING]: current state is STARTING
datanode_4          | 2023-06-22 08:53:08,650 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_4          | 2023-06-22 08:53:08,662 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:08,691 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-22 08:53:08,691 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-22 08:53:08,846 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c.
datanode_4          | 2023-06-22 08:53:08,927 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_4          | 2023-06-22 08:53:08,928 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Response 0: 31613e4d-b990-42be-9377-00c7a7ba3288<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t1
datanode_4          | 2023-06-22 08:53:08,928 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection:   Response 1: 31613e4d-b990-42be-9377-00c7a7ba3288<-73ad851d-99db-49b7-ac3d-75be53f012c8#0:FAIL-t1
datanode_4          | 2023-06-22 08:53:08,928 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.LeaderElection: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2 ELECTION round 0: result REJECTED
datanode_5          | 2023-06-22 08:53:05,504 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:05,507 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:05,507 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:05,512 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:05,512 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-22 08:53:05,513 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState
datanode_5          | 2023-06-22 08:53:05,515 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-61BE3005BF20,id=5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:53:05,518 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:05,518 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-22 08:53:05,523 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-22 08:53:05,523 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-22 08:53:05,530 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-22 08:53:05,539 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-22 08:53:05,975 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO impl.FollowerState: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5196279293ns, electionTimeout:5183ms
datanode_5          | 2023-06-22 08:53:05,976 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState
datanode_5          | 2023-06-22 08:53:05,976 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-22 08:53:06,008 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, 1c635413-c797-403f-bedc-bdd0587a593c, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_5          | 2023-06-22 08:53:06,010 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-22 08:53:06,018 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-FollowerState] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1
datanode_5          | 2023-06-22 08:53:06,035 [grpc-default-executor-0] INFO impl.VoteContext: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-CANDIDATE: reject PRE_VOTE from 1c635413-c797-403f-bedc-bdd0587a593c: our priority 1 > candidate's priority 0
datanode_5          | 2023-06-22 08:53:06,088 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: 1c635413-c797-403f-bedc-bdd0587a593c<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:FAIL-t0. Peer's state: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5:t0, leader=null, voted=, raftlog=Memoized:5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:06,097 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:06,201 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_5          | 2023-06-22 08:53:06,216 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-22 08:53:06,233 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-22 08:53:06,218 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1c635413-c797-403f-bedc-bdd0587a593c
datanode_5          | 2023-06-22 08:53:06,499 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-22 08:53:07,327 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:07,327 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-22 08:53:07,328 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-22 08:53:07,328 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-22 08:53:07,337 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/63562320-eb1c-4218-9f26-857f748a2e10 does not exist. Creating ...
datanode_2          | 2023-06-22 08:53:07,341 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/63562320-eb1c-4218-9f26-857f748a2e10/in_use.lock acquired by nodename 7@7c8c150a4dbd
datanode_2          | 2023-06-22 08:53:07,344 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/63562320-eb1c-4218-9f26-857f748a2e10 has been successfully formatted.
datanode_2          | 2023-06-22 08:53:07,367 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO ratis.ContainerStateMachine: group-857F748A2E10: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-22 08:53:07,369 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-22 08:53:07,370 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-22 08:53:07,371 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:07,371 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-22 08:53:07,371 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-22 08:53:07,372 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:07,373 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-22 08:53:07,376 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-22 08:53:07,378 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:07,379 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: new cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/63562320-eb1c-4218-9f26-857f748a2e10
datanode_2          | 2023-06-22 08:53:07,380 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-22 08:53:07,381 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:53:07,381 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:07,381 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-22 08:53:07,382 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-22 08:53:07,382 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-22 08:53:07,386 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-22 08:53:07,386 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-22 08:53:07,391 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-22 08:53:07,394 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-22 08:53:07,623 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:07,623 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-22 08:53:07,623 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:07,623 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:07,625 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-22 08:53:07,633 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: start as a follower, conf=-1: peers:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:08,929 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_4          | 2023-06-22 08:53:08,929 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: shutdown 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2
datanode_4          | 2023-06-22 08:53:08,930 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-LeaderElection2] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState
datanode_4          | 2023-06-22 08:53:13,924 [grpc-default-executor-0] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: receive requestVote(PRE_VOTE, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-D771BA859F6C, 1, (t:0, i:0))
datanode_4          | 2023-06-22 08:53:13,926 [grpc-default-executor-0] INFO impl.VoteContext: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FOLLOWER: accept PRE_VOTE from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-22 08:53:13,928 [grpc-default-executor-0] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C replies to PRE_VOTE vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-31613e4d-b990-42be-9377-00c7a7ba3288#0:OK-t1. Peer's state: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C:t1, leader=null, voted=31613e4d-b990-42be-9377-00c7a7ba3288, raftlog=Memoized:31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:13,960 [grpc-default-executor-0] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: receive requestVote(ELECTION, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-D771BA859F6C, 2, (t:0, i:0))
datanode_4          | 2023-06-22 08:53:13,960 [grpc-default-executor-0] INFO impl.VoteContext: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FOLLOWER: accept ELECTION from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-22 08:53:13,960 [grpc-default-executor-0] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_4          | 2023-06-22 08:53:13,960 [grpc-default-executor-0] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: shutdown 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState
datanode_4          | 2023-06-22 08:53:13,961 [grpc-default-executor-0] INFO impl.RoleInfo: 31613e4d-b990-42be-9377-00c7a7ba3288: start 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState
datanode_4          | 2023-06-22 08:53:13,961 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-FollowerState was interrupted
datanode_4          | 2023-06-22 08:53:13,963 [grpc-default-executor-0] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C replies to ELECTION vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-31613e4d-b990-42be-9377-00c7a7ba3288#0:OK-t2. Peer's state: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C:t2, leader=null, voted=73ad851d-99db-49b7-ac3d-75be53f012c8, raftlog=Memoized:31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:14,135 [31613e4d-b990-42be-9377-00c7a7ba3288-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D771BA859F6C with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_4          | 2023-06-22 08:53:14,135 [31613e4d-b990-42be-9377-00c7a7ba3288-server-thread1] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 2 for appendEntries, leader elected after 14073ms
datanode_4          | 2023-06-22 08:53:14,138 [31613e4d-b990-42be-9377-00c7a7ba3288-server-thread1] INFO server.RaftServer$Division: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C: set configuration 0: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-22 08:53:14,141 [31613e4d-b990-42be-9377-00c7a7ba3288-server-thread1] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-22 08:53:14,146 [31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 31613e4d-b990-42be-9377-00c7a7ba3288@group-D771BA859F6C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/current/log_inprogress_0
datanode_4          | 2023-06-22 08:53:24,787 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:54:24,788 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:55:24,789 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:56:24,789 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:57:24,790 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:58:24,791 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 08:59:24,791 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-22 09:00:24,792 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:53:07,634 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-22 08:53:07,635 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState
datanode_2          | 2023-06-22 08:53:07,742 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: receive requestVote(PRE_VOTE, 31613e4d-b990-42be-9377-00c7a7ba3288, group-D771BA859F6C, 0, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:07,743 [grpc-default-executor-0] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FOLLOWER: accept PRE_VOTE from 31613e4d-b990-42be-9377-00c7a7ba3288: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:53:07,744 [grpc-default-executor-0] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C replies to PRE_VOTE vote request: 31613e4d-b990-42be-9377-00c7a7ba3288<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t0. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C:t0, leader=null, voted=, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:07,743 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-857F748A2E10,id=cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_2          | 2023-06-22 08:53:07,750 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-22 08:53:07,752 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-22 08:53:07,756 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-22 08:53:07,757 [cd5a4933-0666-4f07-ab50-d6c55125d602-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-22 08:53:07,758 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-22 08:53:07,773 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=63562320-eb1c-4218-9f26-857f748a2e10
datanode_2          | 2023-06-22 08:53:07,775 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=63562320-eb1c-4218-9f26-857f748a2e10.
datanode_2          | 2023-06-22 08:53:07,776 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-22 08:53:08,722 [grpc-default-executor-2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: receive requestVote(ELECTION, 31613e4d-b990-42be-9377-00c7a7ba3288, group-D771BA859F6C, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:08,727 [grpc-default-executor-2] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FOLLOWER: accept ELECTION from 31613e4d-b990-42be-9377-00c7a7ba3288: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-22 08:53:08,731 [grpc-default-executor-2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:31613e4d-b990-42be-9377-00c7a7ba3288
datanode_2          | 2023-06-22 08:53:08,732 [grpc-default-executor-2] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState
datanode_2          | 2023-06-22 08:53:08,760 [grpc-default-executor-2] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState
datanode_2          | 2023-06-22 08:53:08,732 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState was interrupted
datanode_2          | 2023-06-22 08:53:08,775 [grpc-default-executor-2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C replies to ELECTION vote request: 31613e4d-b990-42be-9377-00c7a7ba3288<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t1. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C:t1, leader=null, voted=31613e4d-b990-42be-9377-00c7a7ba3288, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:08,799 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D0B46C7C7CF5 with new leaderId: 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_2          | 2023-06-22 08:53:08,804 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: change Leader from null to 5cccec7c-2f8e-4410-a445-f064595c8b02 at term 1 for appendEntries, leader elected after 9315ms
datanode_2          | 2023-06-22 08:53:09,018 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread3] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:09,067 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread3] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:53:09,638 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D0B46C7C7CF5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/current/log_inprogress_0
datanode_2          | 2023-06-22 08:53:12,917 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO impl.FollowerState: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5283126734ns, electionTimeout:5140ms
datanode_5          | 2023-06-22 08:53:06,504 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 5cccec7c-2f8e-4410-a445-f064595c8b02<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t0
datanode_5          | 2023-06-22 08:53:06,505 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_5          | 2023-06-22 08:53:06,522 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:06,539 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-22 08:53:06,539 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-22 08:53:06,620 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: receive requestVote(PRE_VOTE, cd5a4933-0666-4f07-ab50-d6c55125d602, group-D0B46C7C7CF5, 0, (t:0, i:0))
datanode_5          | 2023-06-22 08:53:06,636 [grpc-default-executor-0] INFO impl.VoteContext: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-CANDIDATE: reject PRE_VOTE from cd5a4933-0666-4f07-ab50-d6c55125d602: our priority 1 > candidate's priority 0
datanode_5          | 2023-06-22 08:53:06,637 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5 replies to PRE_VOTE vote request: cd5a4933-0666-4f07-ab50-d6c55125d602<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:FAIL-t1. Peer's state: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5:t1, leader=null, voted=5cccec7c-2f8e-4410-a445-f064595c8b02, raftlog=Memoized:5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:06,799 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: receive requestVote(PRE_VOTE, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-61BE3005BF20, 0, (t:0, i:0))
datanode_5          | 2023-06-22 08:53:06,803 [grpc-default-executor-0] INFO impl.VoteContext: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FOLLOWER: accept PRE_VOTE from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 0
datanode_5          | 2023-06-22 08:53:06,810 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20 replies to PRE_VOTE vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t0. Peer's state: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20:t0, leader=null, voted=, raftlog=Memoized:5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:06,924 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-06-22 08:53:06,936 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection:   Response 0: 5cccec7c-2f8e-4410-a445-f064595c8b02<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t1
datanode_5          | 2023-06-22 08:53:06,936 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1 ELECTION round 0: result PASSED
datanode_5          | 2023-06-22 08:53:06,936 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1
datanode_5          | 2023-06-22 08:53:06,937 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-22 08:53:06,937 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D0B46C7C7CF5 with new leaderId: 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:53:06,937 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: change Leader from null to 5cccec7c-2f8e-4410-a445-f064595c8b02 at term 1 for becomeLeader, leader elected after 7442ms
datanode_5          | 2023-06-22 08:53:06,951 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-06-22 08:53:07,030 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:07,033 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-22 08:53:07,067 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-22 08:53:07,068 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-06-22 08:53:07,070 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-22 08:53:07,149 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:07,207 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-06-22 08:53:07,287 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: receive requestVote(ELECTION, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-61BE3005BF20, 1, (t:0, i:0))
datanode_5          | 2023-06-22 08:53:07,288 [grpc-default-executor-0] INFO impl.VoteContext: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FOLLOWER: accept ELECTION from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 0
datanode_5          | 2023-06-22 08:53:07,291 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_5          | 2023-06-22 08:53:07,292 [grpc-default-executor-0] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState
datanode_5          | 2023-06-22 08:53:07,294 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState] INFO impl.FollowerState: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState was interrupted
datanode_5          | 2023-06-22 08:53:07,321 [grpc-default-executor-0] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState
datanode_5          | 2023-06-22 08:53:07,424 [grpc-default-executor-0] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20 replies to ELECTION vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t1. Peer's state: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20:t1, leader=null, voted=73ad851d-99db-49b7-ac3d-75be53f012c8, raftlog=Memoized:5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:07,534 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-22 08:53:07,534 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:07,535 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-22 08:53:07,562 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-22 08:53:07,588 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5.
datanode_5          | 2023-06-22 08:53:07,602 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 5cccec7c-2f8e-4410-a445-f064595c8b02: addNew group-674198EF2F07:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] returns group-674198EF2F07:java.util.concurrent.CompletableFuture@31915fe6[Not completed]
datanode_5          | 2023-06-22 08:53:07,611 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02: new RaftServerImpl for group-674198EF2F07:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-22 08:53:07,611 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-22 08:53:07,612 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: ConfigurationManager, init=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-22 08:53:07,616 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-22 08:53:07,617 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-22 08:53:07,621 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-22 08:53:07,622 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-22 08:53:12,918 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState
datanode_2          | 2023-06-22 08:53:12,918 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-22 08:53:12,919 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-22 08:53:12,920 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-FollowerState] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2
datanode_2          | 2023-06-22 08:53:12,921 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:12,922 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-22 08:53:12,925 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:12,925 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.LeaderElection: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-22 08:53:12,925 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2
datanode_2          | 2023-06-22 08:53:12,926 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-22 08:53:12,926 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-857F748A2E10 with new leaderId: cd5a4933-0666-4f07-ab50-d6c55125d602
datanode_2          | 2023-06-22 08:53:12,927 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: change Leader from null to cd5a4933-0666-4f07-ab50-d6c55125d602 at term 1 for becomeLeader, leader elected after 5599ms
datanode_2          | 2023-06-22 08:53:12,937 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-22 08:53:12,948 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:53:12,949 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-22 08:53:12,959 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-22 08:53:12,962 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-22 08:53:12,962 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-22 08:53:12,979 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-22 08:53:12,981 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-22 08:53:13,004 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderStateImpl
datanode_2          | 2023-06-22 08:53:13,017 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:53:13,021 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/63562320-eb1c-4218-9f26-857f748a2e10/current/log_inprogress_0
datanode_2          | 2023-06-22 08:53:13,023 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10-LeaderElection2] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-857F748A2E10: set configuration 0: peers:[cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:14,004 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: receive requestVote(PRE_VOTE, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-D771BA859F6C, 1, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:14,004 [grpc-default-executor-1] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FOLLOWER: accept PRE_VOTE from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:53:14,005 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C replies to PRE_VOTE vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t1. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C:t1, leader=null, voted=31613e4d-b990-42be-9377-00c7a7ba3288, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:14,034 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: receive requestVote(ELECTION, 73ad851d-99db-49b7-ac3d-75be53f012c8, group-D771BA859F6C, 2, (t:0, i:0))
datanode_2          | 2023-06-22 08:53:14,035 [grpc-default-executor-1] INFO impl.VoteContext: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FOLLOWER: accept ELECTION from 73ad851d-99db-49b7-ac3d-75be53f012c8: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-22 08:53:14,035 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_2          | 2023-06-22 08:53:14,035 [grpc-default-executor-1] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: shutdown cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState
datanode_2          | 2023-06-22 08:53:14,035 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState] INFO impl.FollowerState: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState was interrupted
datanode_2          | 2023-06-22 08:53:14,035 [grpc-default-executor-1] INFO impl.RoleInfo: cd5a4933-0666-4f07-ab50-d6c55125d602: start cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-FollowerState
datanode_2          | 2023-06-22 08:53:14,039 [grpc-default-executor-1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C replies to ELECTION vote request: 73ad851d-99db-49b7-ac3d-75be53f012c8<-cd5a4933-0666-4f07-ab50-d6c55125d602#0:OK-t2. Peer's state: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C:t2, leader=null, voted=73ad851d-99db-49b7-ac3d-75be53f012c8, raftlog=Memoized:cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:14,107 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D771BA859F6C with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_2          | 2023-06-22 08:53:14,107 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 2 for appendEntries, leader elected after 7396ms
datanode_2          | 2023-06-22 08:53:14,109 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO server.RaftServer$Division: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C: set configuration 0: peers:[31613e4d-b990-42be-9377-00c7a7ba3288|rpc:172.24.0.8:9856|admin:172.24.0.8:9857|client:172.24.0.8:9858|dataStream:172.24.0.8:9858|priority:0|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-22 08:53:14,110 [cd5a4933-0666-4f07-ab50-d6c55125d602-server-thread1] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-22 08:53:14,112 [cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cd5a4933-0666-4f07-ab50-d6c55125d602@group-D771BA859F6C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0d8ca18e-9239-4aef-9aff-d771ba859f6c/current/log_inprogress_0
datanode_2          | 2023-06-22 08:53:24,683 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:54:24,684 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:55:24,687 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:56:24,688 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:57:24,689 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:58:24,690 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 08:59:24,691 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-22 09:00:24,692 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-22 08:53:07,627 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3e62672c-dd20-4d49-a904-674198ef2f07 does not exist. Creating ...
datanode_5          | 2023-06-22 08:53:07,567 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-22 08:53:07,628 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:53:07,631 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-22 08:53:07,632 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-22 08:53:07,632 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-22 08:53:07,632 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-22 08:53:07,640 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3e62672c-dd20-4d49-a904-674198ef2f07/in_use.lock acquired by nodename 7@92add0f1b6b7
datanode_5          | 2023-06-22 08:53:07,664 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3e62672c-dd20-4d49-a904-674198ef2f07 has been successfully formatted.
datanode_5          | 2023-06-22 08:53:07,668 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO ratis.ContainerStateMachine: group-674198EF2F07: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-22 08:53:07,668 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-22 08:53:07,668 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-22 08:53:07,668 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-22 08:53:07,671 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-22 08:53:07,683 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-22 08:53:07,683 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-22 08:53:07,687 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3e62672c-dd20-4d49-a904-674198ef2f07
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-22 08:53:07,688 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-22 08:53:07,693 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-22 08:53:07,693 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-22 08:53:07,693 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-22 08:53:07,693 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-22 08:53:07,696 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderStateImpl
datanode_5          | 2023-06-22 08:53:07,717 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:51:21,564 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f5c3266129bd/172.24.0.4
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | 2023-06-22 08:53:07,726 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-22 08:53:08,018 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-22 08:53:08,067 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-22 08:53:08,067 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-22 08:53:08,067 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:08,068 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:08,068 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-22 08:53:08,081 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: start as a follower, conf=-1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:08,081 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-22 08:53:08,081 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState
datanode_5          | 2023-06-22 08:53:08,082 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-674198EF2F07,id=5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:53:08,082 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-22 08:53:08,087 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-22 08:53:08,087 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-22 08:53:08,087 [5cccec7c-2f8e-4410-a445-f064595c8b02-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-22 08:53:08,087 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-22 08:53:08,091 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-22 08:53:08,091 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3e62672c-dd20-4d49-a904-674198ef2f07
datanode_5          | 2023-06-22 08:53:08,091 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3e62672c-dd20-4d49-a904-674198ef2f07.
datanode_5          | 2023-06-22 08:53:08,270 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-LeaderElection1] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER, cd5a4933-0666-4f07-ab50-d6c55125d602|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:09,117 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-D0B46C7C7CF5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/62317552-41b7-4512-b6c8-d0b46c7c7cf5/current/log_inprogress_0
datanode_5          | 2023-06-22 08:53:09,478 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 73ad851d-99db-49b7-ac3d-75be53f012c8
datanode_5          | 2023-06-22 08:53:09,479 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread1] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: change Leader from null to 73ad851d-99db-49b7-ac3d-75be53f012c8 at term 1 for appendEntries, leader elected after 4156ms
datanode_5          | 2023-06-22 08:53:09,555 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:09,578 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-22 08:53:09,582 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-22 08:51:21,658 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:51:31,215 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:51:33,843 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:51:34,242 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.4:9862
om_1                | 2023-06-22 08:51:34,242 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:51:34,249 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:51:34,991 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:51:37,317 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-22 08:51:41,758 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-22 08:51:43,760 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-22 08:51:45,762 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-22 08:51:47,764 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-22 08:51:49,766 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-22 08:51:51,769 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
datanode_5          | 2023-06-22 08:53:13,140 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO impl.FollowerState: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058854718ns, electionTimeout:5049ms
datanode_5          | 2023-06-22 08:53:13,141 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState
datanode_5          | 2023-06-22 08:53:13,141 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-22 08:53:13,141 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-22 08:53:13,141 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-FollowerState] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2
datanode_5          | 2023-06-22 08:53:13,145 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:13,146 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_5          | 2023-06-22 08:53:13,192 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:13,201 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.LeaderElection: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-06-22 08:53:13,201 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2
datanode_5          | 2023-06-22 08:53:13,205 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-22 08:53:13,211 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-674198EF2F07 with new leaderId: 5cccec7c-2f8e-4410-a445-f064595c8b02
datanode_5          | 2023-06-22 08:53:13,211 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: change Leader from null to 5cccec7c-2f8e-4410-a445-f064595c8b02 at term 1 for becomeLeader, leader elected after 5594ms
datanode_5          | 2023-06-22 08:53:13,213 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-06-22 08:53:13,217 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:13,221 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-22 08:53:13,221 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-22 08:53:13,223 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-06-22 08:53:13,226 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-22 08:53:13,226 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-22 08:53:13,226 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-06-22 08:53:13,228 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderStateImpl
datanode_5          | 2023-06-22 08:53:13,229 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-22 08:53:13,243 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3e62672c-dd20-4d49-a904-674198ef2f07/current/log_inprogress_0
datanode_5          | 2023-06-22 08:53:13,282 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07-LeaderElection2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-674198EF2F07: set configuration 0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:15,217 [grpc-default-executor-3] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: receive requestVote(ELECTION, 1c635413-c797-403f-bedc-bdd0587a593c, group-61BE3005BF20, 2, (t:1, i:0))
datanode_5          | 2023-06-22 08:53:15,217 [grpc-default-executor-3] INFO impl.VoteContext: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FOLLOWER: accept ELECTION from 1c635413-c797-403f-bedc-bdd0587a593c: our priority 0 <= candidate's priority 1
datanode_5          | 2023-06-22 08:53:15,218 [grpc-default-executor-3] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: change Leader from 73ad851d-99db-49b7-ac3d-75be53f012c8 to null at term 2 for updateCurrentTerm
datanode_5          | 2023-06-22 08:53:15,218 [grpc-default-executor-3] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:1c635413-c797-403f-bedc-bdd0587a593c
datanode_5          | 2023-06-22 08:53:15,218 [grpc-default-executor-3] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: shutdown 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState
datanode_5          | 2023-06-22 08:53:15,218 [grpc-default-executor-3] INFO impl.RoleInfo: 5cccec7c-2f8e-4410-a445-f064595c8b02: start 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState
datanode_5          | 2023-06-22 08:53:15,218 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState] INFO impl.FollowerState: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-FollowerState was interrupted
om_1                | 2023-06-22 08:51:53,773 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-22 08:51:55,775 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-22 08:51:57,778 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-22 08:51:59,781 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-22 08:52:01,783 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-22 08:52:03,785 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-22 08:52:05,786 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-22 08:52:07,789 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-06-22 08:52:09,792 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-06-22 08:52:11,797 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | 2023-06-22 08:52:13,801 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om_1                | 2023-06-22 08:52:15,804 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f5c3266129bd/172.24.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 18.
om_1                | 2023-06-22 08:52:18,604 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3c18caf3-4ce6-429a-9998-d643fc26f105 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 19.
om_1                | 2023-06-22 08:52:20,610 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3c18caf3-4ce6-429a-9998-d643fc26f105 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 20.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-7056bc73-01b7-404b-90a0-81c09d0d73e5;layoutVersion=6
om_1                | 2023-06-22 08:52:23,229 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at f5c3266129bd/172.24.0.4
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-22 08:52:30,167 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f5c3266129bd/172.24.0.4
om_1                | STARTUP_MSG:   args = []
datanode_5          | 2023-06-22 08:53:15,221 [grpc-default-executor-3] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20 replies to ELECTION vote request: 1c635413-c797-403f-bedc-bdd0587a593c<-5cccec7c-2f8e-4410-a445-f064595c8b02#0:OK-t2. Peer's state: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20:t2, leader=null, voted=1c635413-c797-403f-bedc-bdd0587a593c, raftlog=Memoized:5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLog:OPENED:c0, conf=0: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:15,381 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 5cccec7c-2f8e-4410-a445-f064595c8b02: Completed APPEND_ENTRIES, lastRequest: null
datanode_5          | 2023-06-22 08:53:15,753 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-61BE3005BF20 with new leaderId: 1c635413-c797-403f-bedc-bdd0587a593c
datanode_5          | 2023-06-22 08:53:15,778 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: change Leader from null to 1c635413-c797-403f-bedc-bdd0587a593c at term 2 for appendEntries, leader elected after 535ms
datanode_5          | 2023-06-22 08:53:15,779 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO server.RaftServer$Division: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20: set configuration 1: peers:[5cccec7c-2f8e-4410-a445-f064595c8b02|rpc:172.24.0.11:9856|admin:172.24.0.11:9857|client:172.24.0.11:9858|dataStream:172.24.0.11:9858|priority:0|startupRole:FOLLOWER, 1c635413-c797-403f-bedc-bdd0587a593c|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 73ad851d-99db-49b7-ac3d-75be53f012c8|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-22 08:53:15,780 [5cccec7c-2f8e-4410-a445-f064595c8b02-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode_5          | 2023-06-22 08:53:15,782 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_0 to /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_0-0
datanode_5          | 2023-06-22 08:53:15,800 [5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5cccec7c-2f8e-4410-a445-f064595c8b02@group-61BE3005BF20-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/60e32c8c-5acc-46ba-bd41-61be3005bf20/current/log_inprogress_1
datanode_5          | 2023-06-22 08:53:15,837 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 5cccec7c-2f8e-4410-a445-f064595c8b02: Completed APPEND_ENTRIES, lastRequest: 73ad851d-99db-49b7-ac3d-75be53f012c8->5cccec7c-2f8e-4410-a445-f064595c8b02#1-t1,previous=(t:0, i:0),leaderCommit=-1,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY(current:id: "5cccec7c-2f8e-4410-a445-f064595c8b02"
datanode_5          | address: "172.24.0.11:9856"
datanode_5          | dataStreamAddress: "172.24.0.11:9858"
datanode_5          | clientAddress: "172.24.0.11:9858"
datanode_5          | adminAddress: "172.24.0.11:9857"
datanode_5          | startupRole: FOLLOWER
datanode_5          | ,id: "1c635413-c797-403f-bedc-bdd0587a593c"
datanode_5          | address: "172.24.0.13:9856"
datanode_5          | priority: 1
datanode_5          | dataStreamAddress: "172.24.0.13:9858"
datanode_5          | clientAddress: "172.24.0.13:9858"
datanode_5          | adminAddress: "172.24.0.13:9857"
datanode_5          | startupRole: FOLLOWER
datanode_5          | ,id: "73ad851d-99db-49b7-ac3d-75be53f012c8"
datanode_5          | address: "172.24.0.12:9856"
datanode_5          | dataStreamAddress: "172.24.0.12:9858"
datanode_5          | clientAddress: "172.24.0.12:9858"
datanode_5          | adminAddress: "172.24.0.12:9857"
datanode_5          | startupRole: FOLLOWER
datanode_5          | , old:)
datanode_5          | 2023-06-22 08:53:15,849 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: 5cccec7c-2f8e-4410-a445-f064595c8b02: Completed APPEND_ENTRIES, lastReply: null
datanode_5          | 2023-06-22 08:53:15,857 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 5cccec7c-2f8e-4410-a445-f064595c8b02: Completed APPEND_ENTRIES, lastReply: serverReply {
datanode_5          |   requestorId: "73ad851d-99db-49b7-ac3d-75be53f012c8"
datanode_5          |   replyId: "5cccec7c-2f8e-4410-a445-f064595c8b02"
datanode_5          |   raftGroupId {
datanode_5          |     id: "`\343,\214Z\314F\272\275Aa\2760\005\277 "
datanode_5          |   }
datanode_5          |   callId: 5
datanode_5          |   success: true
datanode_5          | }
datanode_5          | term: 1
datanode_5          | nextIndex: 1
datanode_5          | matchIndex: 18446744073709551615
datanode_5          | isHearbeat: true
datanode_5          | 
datanode_5          | 2023-06-22 08:53:24,736 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:54:24,742 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:55:24,744 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:56:24,745 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:57:24,745 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:58:24,746 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 08:59:24,746 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-22 09:00:24,747 [BlockDeletingService#4] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-22 08:52:30,191 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-22 08:52:31,916 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-22 08:52:32,657 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-22 08:52:32,860 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.4:9862
om_1                | 2023-06-22 08:52:32,861 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-22 08:52:32,862 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-22 08:52:32,960 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:52:33,126 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-22 08:52:33,859 [main] INFO reflections.Reflections: Reflections took 638 ms to scan 1 urls, producing 137 keys and 395 values [using 2 cores]
om_1                | 2023-06-22 08:52:33,969 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-22 08:52:34,183 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:52:35,146 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-22 08:52:35,301 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9863]
om_1                | 2023-06-22 08:52:36,877 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-22 08:52:36,939 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:52:37,405 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-22 08:52:38,421 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-22 08:52:38,529 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-22 08:52:38,555 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-22 08:52:38,642 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-22 08:52:38,646 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-22 08:52:39,105 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-22 08:52:39,394 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:52:39,402 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-22 08:52:39,465 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-22 08:52:39,489 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-22 08:52:39,572 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-22 08:52:39,614 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-22 08:52:39,766 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-22 08:52:39,793 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:52:39,798 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:52:39,799 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-22 08:52:39,800 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-22 08:52:39,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-22 08:52:39,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-22 08:52:39,804 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-22 08:52:39,807 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:52:39,809 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-22 08:52:39,810 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:52:39,839 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-22 08:52:39,846 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-22 08:52:39,848 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-22 08:52:40,548 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-22 08:52:40,556 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-22 08:52:40,559 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-22 08:52:40,560 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:52:40,561 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:52:40,568 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:52:40,610 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@151ce559[Not completed]
om_1                | 2023-06-22 08:52:40,612 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-22 08:52:40,644 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-22 08:52:40,671 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-22 08:52:40,685 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-22 08:52:40,687 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-22 08:52:40,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-22 08:52:40,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-22 08:52:40,688 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-22 08:52:40,691 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-22 08:52:40,723 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-22 08:52:40,725 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-22 08:52:40,762 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-22 08:52:40,765 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-22 08:52:40,829 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-22 08:52:40,854 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-22 08:52:40,870 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-22 08:52:40,874 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-22 08:52:40,949 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-22 08:52:41,150 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-22 08:52:41,156 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-22 08:52:41,157 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-22 08:52:41,159 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-22 08:52:41,163 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-22 08:52:41,163 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-22 08:52:42,000 [main] INFO reflections.Reflections: Reflections took 1008 ms to scan 8 urls, producing 24 keys and 632 values [using 2 cores]
om_1                | 2023-06-22 08:52:42,273 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-22 08:52:42,287 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-22 08:52:42,309 [Listener at om/9862] INFO hdds.HddsUtils: Restoring thread name: main
om_1                | 2023-06-22 08:52:43,080 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-22 08:52:43,102 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-22 08:52:43,103 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-22 08:52:43,177 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.4:9862
om_1                | 2023-06-22 08:52:43,178 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-22 08:52:43,181 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-22 08:52:43,187 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@f5c3266129bd
om_1                | 2023-06-22 08:52:43,197 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:51:21,816 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = dd4382ec45ae/172.24.0.6
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-22 08:51:21,915 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-22 08:51:28,024 [main] INFO reflections.Reflections: Reflections took 724 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1             | 2023-06-22 08:51:33,965 [main] INFO reflections.Reflections: Reflections took 970 ms to scan 3 urls, producing 131 keys and 286 values 
recon_1             | 2023-06-22 08:51:34,667 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-22 08:51:38,200 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:51:46,928 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-22 08:51:49,449 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:51:49,478 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.025 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:51:50,012 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-22 08:51:50,242 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-22 08:51:50,267 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-22 08:51:54,940 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-22 08:51:55,068 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-22 08:51:55,394 [main] INFO util.log: Logging initialized @47891ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-22 08:51:56,183 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-22 08:51:56,262 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-22 08:51:56,349 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-22 08:51:56,356 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-22 08:51:56,378 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-22 08:51:56,379 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-22 08:51:56,795 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-22 08:51:56,860 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-22 08:51:58,270 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-22 08:51:58,375 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-22 08:51:58,402 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-22 08:51:58,866 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:51:58,908 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-22 08:52:04,534 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:52:06,090 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:52:06,505 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:52:06,507 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:52:06,808 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:52:07,075 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-22 08:52:07,151 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-22 08:52:07,226 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-22 08:52:07,232 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-22 08:52:07,249 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-22 08:52:07,966 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-22 08:52:08,073 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-22 08:52:08,174 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-22 08:52:08,277 [Listener at 0.0.0.0/9891] INFO hdds.HddsUtils: Restoring thread name: main
recon_1             | 2023-06-22 08:52:08,301 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-22 08:52:08,594 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-06-22 08:52:08,602 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-22 08:52:08,602 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-22 08:52:08,823 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-22 08:52:08,856 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-22 08:52:08,857 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-22 08:52:09,745 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-22 08:52:09,750 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-06-22 08:52:09,849 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-22 08:52:09,850 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:52:09,855 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:52:09,902 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@59bbf82e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-22 08:52:09,903 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6bd28e4a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-22 08:52:14,619 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3693c5b0{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-43075674876785233/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-22 08:52:14,641 [main] INFO server.AbstractConnector: Started ServerConnector@299dd381{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-22 08:52:14,641 [main] INFO server.Server: Started @67138ms
recon_1             | 2023-06-22 08:52:14,647 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-22 08:52:14,647 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-22 08:52:14,651 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-22 08:52:14,651 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-22 08:52:14,673 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-22 08:52:14,684 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-22 08:52:14,684 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-22 08:52:14,684 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:52:14,685 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-22 08:52:14,686 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:52:18,721 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3c18caf3-4ce6-429a-9998-d643fc26f105 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-22 08:52:20,730 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:3c18caf3-4ce6-429a-9998-d643fc26f105 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.2:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-22 08:52:25,823 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-22 08:52:25,823 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-22 08:52:25,824 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-22 08:52:25,827 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-22 08:52:25,843 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-22 08:52:25,851 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-22 08:52:26,988 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.15:37444: output error
recon_1             | 2023-06-22 08:52:26,993 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.13:33472: output error
recon_1             | 2023-06-22 08:52:26,993 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.8:56986: output error
recon_1             | 2023-06-22 08:52:26,999 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
om_1                | 2023-06-22 08:52:43,201 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-22 08:52:43,210 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-22 08:52:43,210 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:52:43,212 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-22 08:52:43,214 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-22 08:52:43,221 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:52:43,227 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-22 08:52:43,228 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-22 08:52:43,228 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:52:43,235 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-22 08:52:43,235 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:52:43,236 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-22 08:52:43,238 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-22 08:52:43,239 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-22 08:52:43,240 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-22 08:52:43,241 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-22 08:52:43,244 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-22 08:52:43,245 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-22 08:52:43,254 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-22 08:52:43,254 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-22 08:52:43,267 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-22 08:52:43,268 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-22 08:52:43,269 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-22 08:52:43,278 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:52:43,279 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-22 08:52:43,281 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:52:43,281 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-22 08:52:43,283 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:52:43,286 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-22 08:52:43,290 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-22 08:52:43,291 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-22 08:52:43,292 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-22 08:52:43,293 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-22 08:52:43,295 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-22 08:52:43,296 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-22 08:52:43,304 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-22 08:52:43,422 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-22 08:52:43,429 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-22 08:52:43,433 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-22 08:52:43,492 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-22 08:52:43,492 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-22 08:52:43,524 [main] INFO util.log: Logging initialized @18824ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-22 08:52:43,673 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-22 08:52:43,680 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-22 08:52:43,690 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-22 08:52:43,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-22 08:52:43,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-22 08:52:43,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-22 08:52:43,758 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-06-22 08:52:43,763 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-22 08:52:43,765 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-06-22 08:52:43,859 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-22 08:52:43,860 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-22 08:52:43,864 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-22 08:52:43,909 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@206c8094{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-22 08:52:43,914 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2393b885{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-22 08:52:44,265 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@50769bc1{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-7898110980941261754/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-06-22 08:52:44,295 [main] INFO server.AbstractConnector: Started ServerConnector@abd6027{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-22 08:52:44,295 [main] INFO server.Server: Started @19595ms
om_1                | 2023-06-22 08:52:44,314 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-22 08:52:44,314 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-22 08:52:44,317 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-22 08:52:44,319 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-22 08:52:44,319 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-22 08:52:44,404 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-22 08:52:44,685 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-06-22 08:52:48,436 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153318461ns, electionTimeout:5139ms
om_1                | 2023-06-22 08:52:48,438 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-22 08:52:48,438 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-22 08:52:48,442 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-06-22 08:52:48,442 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:52:48,451 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:52:48,451 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-22 08:52:48,455 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:52:48,455 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-22 08:52:48,455 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-22 08:52:48,456 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-22 08:52:48,459 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 7626ms
om_1                | 2023-06-22 08:52:48,473 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-22 08:52:48,484 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:52:48,486 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-22 08:52:48,496 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-22 08:52:48,496 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-22 08:52:48,499 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-22 08:52:48,515 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-22 08:52:48,520 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-22 08:52:48,525 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-22 08:52:48,605 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-22 08:52:48,707 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-22 08:52:48,811 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-22 08:52:49,077 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-22 08:53:17,019 [qtp1361655888-55] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-06-22 08:53:17,056 [qtp1361655888-55] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1687423997024 in 31 milliseconds
om_1                | 2023-06-22 08:53:17,099 [qtp1361655888-55] INFO db.RDBCheckpointUtils: Waited for 41 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1687423997024 availability.
om_1                | 2023-06-22 08:53:17,781 [qtp1361655888-55] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 677 milliseconds
om_1                | 2023-06-22 08:53:17,781 [qtp1361655888-55] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
om_1                | 2023-06-22 08:53:17,781 [qtp1361655888-55] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1687423997024
om_1                | 2023-06-22 08:53:27,856 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:voli8zd9 for user:hadoop
om_1                | 2023-06-22 08:53:33,766 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: default of layout LEGACY in volume: voli8zd9
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,993 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.12:60804: output error
recon_1             | 2023-06-22 08:52:27,004 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,993 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.8:43170: output error
recon_1             | 2023-06-22 08:52:27,005 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,991 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.15:43212: output error
recon_1             | 2023-06-22 08:52:27,007 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,991 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.11:51854: output error
recon_1             | 2023-06-22 08:52:27,008 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,991 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.13:33458: output error
recon_1             | 2023-06-22 08:52:27,009 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:27,012 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.12:60818: output error
recon_1             | 2023-06-22 08:52:27,012 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:27,000 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.13:41902: output error
recon_1             | 2023-06-22 08:52:26,999 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.15:43222: output error
recon_1             | 2023-06-22 08:52:27,016 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 2023-06-22 08:53:38,899 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ratis of layout LEGACY in volume: voli8zd9
om_1                | 2023-06-22 08:53:44,279 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ecbucket of layout LEGACY in volume: voli8zd9
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,999 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.8:56970: output error
recon_1             | 2023-06-22 08:52:27,016 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:51:22,371 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 52c57281751e/172.24.0.2
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,998 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.11:58000: output error
recon_1             | 2023-06-22 08:52:27,017 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:26,998 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:27,017 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-22 08:51:22,684 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-22 08:51:24,430 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-22 08:51:27,678 [main] INFO reflections.Reflections: Reflections took 2407 ms to scan 3 urls, producing 131 keys and 286 values 
scm_1               | 2023-06-22 08:51:29,646 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-22 08:51:29,964 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-22 08:51:33,241 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-22 08:51:35,381 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:51:35,420 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:51:35,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:51:35,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-22 08:51:35,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | 2023-06-22 08:51:35,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-22 08:51:35,482 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-22 08:51:35,530 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
s3g_1               | 2023-06-22 08:51:22,583 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
scm_1               | 2023-06-22 08:51:35,577 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
s3g_1               | 2023-06-22 08:51:22,619 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:51:35,589 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
s3g_1               | 2023-06-22 08:51:23,052 [main] INFO util.log: Logging initialized @15302ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:51:35,767 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
s3g_1               | 2023-06-22 08:51:24,304 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-22 08:51:35,900 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1               | 2023-06-22 08:51:24,514 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
scm_1               | 2023-06-22 08:51:35,946 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1               | 2023-06-22 08:51:24,604 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:51:40,955 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
s3g_1               | 2023-06-22 08:51:24,613 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
scm_1               | 2023-06-22 08:51:41,095 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
s3g_1               | 2023-06-22 08:51:24,613 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:51:41,096 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-22 08:51:41,100 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
s3g_1               | 2023-06-22 08:51:24,626 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:51:41,152 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 2023-06-22 08:52:26,996 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.11:51842: output error
s3g_1               | 2023-06-22 08:51:25,005 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir12786661079141482698
scm_1               | 2023-06-22 08:51:41,228 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1             | 2023-06-22 08:52:27,018 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
s3g_1               | 2023-06-22 08:51:26,404 [main] INFO s3.Gateway: STARTUP_MSG: 
scm_1               | 2023-06-22 08:51:41,601 [main] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: addNew group-81C09D0D73E5:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER] returns group-81C09D0D73E5:java.util.concurrent.CompletableFuture@10f7c76[Not completed]
recon_1             | java.nio.channels.ClosedChannelException
s3g_1               | /************************************************************
scm_1               | 2023-06-22 08:51:42,061 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105: new RaftServerImpl for group-81C09D0D73E5:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
s3g_1               | STARTUP_MSG: Starting Gateway
scm_1               | 2023-06-22 08:51:42,101 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
s3g_1               | STARTUP_MSG:   host = 3ffa33d85901/172.24.0.9
scm_1               | 2023-06-22 08:51:42,118 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
s3g_1               | STARTUP_MSG:   args = []
scm_1               | 2023-06-22 08:51:42,120 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | 2023-06-22 08:51:42,120 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 2023-06-22 08:51:42,120 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 2023-06-22 08:51:42,121 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:50Z
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
s3g_1               | STARTUP_MSG:   java = 11.0.19
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 2023-06-22 08:51:42,194 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: ConfigurationManager, init=-1: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir12786661079141482698, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | 2023-06-22 08:51:42,221 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
s3g_1               | ************************************************************/
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 2023-06-22 08:51:42,332 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
s3g_1               | 2023-06-22 08:51:26,525 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-22 08:51:42,355 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:51:42,671 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-22 08:51:42,790 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-22 08:51:42,971 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-22 08:51:43,007 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
s3g_1               | 2023-06-22 08:51:26,744 [main] INFO s3.Gateway: Starting Ozone S3 gateway
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 2023-06-22 08:51:43,489 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
s3g_1               | 2023-06-22 08:51:27,718 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 2023-06-22 08:51:43,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1               | 2023-06-22 08:51:28,970 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-22 08:51:45,646 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
s3g_1               | 2023-06-22 08:51:28,970 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-22 08:51:45,671 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
s3g_1               | 2023-06-22 08:51:29,264 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-22 08:51:45,672 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
s3g_1               | 2023-06-22 08:51:29,298 [main] INFO http.HttpServer2: Jetty bound to port 9878
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:51:45,695 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
s3g_1               | 2023-06-22 08:51:29,300 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-06-22 08:51:29,634 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:51:45,721 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1               | 2023-06-22 08:51:29,636 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-22 08:52:26,996 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.24.0.12:49106: output error
scm_1               | 2023-06-22 08:51:45,721 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
s3g_1               | 2023-06-22 08:51:29,648 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-22 08:52:26,996 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
scm_1               | 2023-06-22 08:51:45,722 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5 does not exist. Creating ...
s3g_1               | 2023-06-22 08:51:29,791 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@213e3629{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-06-22 08:51:45,763 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/in_use.lock acquired by nodename 13@52c57281751e
s3g_1               | 2023-06-22 08:51:29,817 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5528a42c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-06-22 08:51:45,868 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5 has been successfully formatted.
s3g_1               | 2023-06-22 08:51:32,257 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.101s with 0.128s GC time.
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-06-22 08:51:46,023 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=128ms
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 2023-06-22 08:51:46,126 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
s3g_1               | 2023-06-22 08:51:48,562 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.155s with 0.166s GC time.
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-06-22 08:51:46,144 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=166ms
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 2023-06-22 08:51:46,149 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-22 08:51:46,176 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
s3g_1               | 2023-06-22 08:52:00,114 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2dc3ece8{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir12786661079141482698/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-14041514026437898237/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 2023-06-22 08:51:46,345 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
s3g_1               | 2023-06-22 08:52:00,218 [main] INFO server.AbstractConnector: Started ServerConnector@22ee2d0{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 2023-06-22 08:51:46,457 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
s3g_1               | 2023-06-22 08:52:00,224 [main] INFO server.Server: Started @52468ms
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 2023-06-22 08:51:46,467 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
s3g_1               | 2023-06-22 08:52:00,235 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 2023-06-22 08:51:46,477 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1               | 2023-06-22 08:52:00,235 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-22 08:51:46,575 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5
s3g_1               | 2023-06-22 08:52:00,253 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-22 08:51:46,656 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 2023-06-22 08:51:46,672 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 2023-06-22 08:51:46,687 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-22 08:51:46,704 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-22 08:51:46,711 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1             | 2023-06-22 08:52:27,019 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-06-22 08:51:46,760 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-06-22 08:51:46,769 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:51:46,787 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:51:46,987 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:51:47,002 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:51:47,277 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:51:47,277 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:51:47,278 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:51:47,431 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 2023-06-22 08:51:47,458 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-06-22 08:51:47,503 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: start as a follower, conf=-1: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 2023-06-22 08:51:47,536 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 2023-06-22 08:51:47,553 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 2023-06-22 08:51:47,611 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 2023-06-22 08:51:47,622 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-22 08:51:47,789 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-81C09D0D73E5,id=3c18caf3-4ce6-429a-9998-d643fc26f105
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-22 08:51:47,818 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 2023-06-22 08:51:47,834 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 2023-06-22 08:51:47,840 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:51:47,844 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-22 08:51:47,968 [main] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: start RPC server
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-22 08:51:48,839 [main] INFO server.GrpcService: 3c18caf3-4ce6-429a-9998-d643fc26f105: GrpcService started, listening on 9894
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-22 08:51:48,995 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3c18caf3-4ce6-429a-9998-d643fc26f105: Started
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:51:52,676 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.FollowerState: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5123825622ns, electionTimeout:5047ms
recon_1             | 2023-06-22 08:52:27,321 [IPC Server handler 20 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/31613e4d-b990-42be-9377-00c7a7ba3288
scm_1               | 2023-06-22 08:51:52,677 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState
recon_1             | 2023-06-22 08:52:27,350 [IPC Server handler 20 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 31613e4d-b990-42be-9377-00c7a7ba3288{ip: 172.24.0.8, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:51:52,696 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1             | 2023-06-22 08:52:27,594 [IPC Server handler 21 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1c635413-c797-403f-bedc-bdd0587a593c
scm_1               | 2023-06-22 08:51:52,729 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1             | 2023-06-22 08:52:27,609 [IPC Server handler 21 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1c635413-c797-403f-bedc-bdd0587a593c{ip: 172.24.0.13, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:52:27,730 [IPC Server handler 22 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5cccec7c-2f8e-4410-a445-f064595c8b02
recon_1             | 2023-06-22 08:52:27,730 [IPC Server handler 22 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5cccec7c-2f8e-4410-a445-f064595c8b02{ip: 172.24.0.11, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:51:52,737 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1
recon_1             | 2023-06-22 08:52:27,789 [IPC Server handler 23 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cd5a4933-0666-4f07-ab50-d6c55125d602
recon_1             | 2023-06-22 08:52:27,789 [IPC Server handler 23 on default port 9891] INFO node.SCMNodeManager: Registered Data node : cd5a4933-0666-4f07-ab50-d6c55125d602{ip: 172.24.0.15, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:51:52,928 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-22 08:52:27,869 [IPC Server handler 24 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/73ad851d-99db-49b7-ac3d-75be53f012c8
recon_1             | 2023-06-22 08:52:27,869 [IPC Server handler 24 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 73ad851d-99db-49b7-ac3d-75be53f012c8{ip: 172.24.0.12, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-22 08:52:28,213 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 31613e4d-b990-42be-9377-00c7a7ba3288 to Node DB.
recon_1             | 2023-06-22 08:52:28,221 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1c635413-c797-403f-bedc-bdd0587a593c to Node DB.
recon_1             | 2023-06-22 08:52:28,223 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5cccec7c-2f8e-4410-a445-f064595c8b02 to Node DB.
recon_1             | 2023-06-22 08:52:28,228 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node cd5a4933-0666-4f07-ab50-d6c55125d602 to Node DB.
scm_1               | 2023-06-22 08:51:52,932 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
recon_1             | 2023-06-22 08:52:28,231 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 73ad851d-99db-49b7-ac3d-75be53f012c8 to Node DB.
scm_1               | 2023-06-22 08:51:52,956 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-22 08:52:59,350 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9284cc47-3eb5-4374-917c-8d2e22216578. Trying to get from SCM.
scm_1               | 2023-06-22 08:51:52,961 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1             | 2023-06-22 08:52:59,554 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9284cc47-3eb5-4374-917c-8d2e22216578, Nodes: 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:31613e4d-b990-42be-9377-00c7a7ba3288, CreationTimestamp2023-06-22T08:52:26.349Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-22 08:51:52,961 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1
recon_1             | 2023-06-22 08:52:59,740 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9284cc47-3eb5-4374-917c-8d2e22216578, Nodes: 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:31613e4d-b990-42be-9377-00c7a7ba3288, CreationTimestamp2023-06-22T08:52:26.349Z[UTC]].
scm_1               | 2023-06-22 08:51:52,962 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1             | 2023-06-22 08:53:00,110 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c. Trying to get from SCM.
scm_1               | 2023-06-22 08:51:52,964 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: change Leader from null to 3c18caf3-4ce6-429a-9998-d643fc26f105 at term 1 for becomeLeader, leader elected after 10293ms
recon_1             | 2023-06-22 08:53:00,128 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 0d8ca18e-9239-4aef-9aff-d771ba859f6c, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.239Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-22 08:51:53,140 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1             | 2023-06-22 08:53:00,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0d8ca18e-9239-4aef-9aff-d771ba859f6c, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.239Z[UTC]].
recon_1             | 2023-06-22 08:53:00,138 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)
scm_1               | 2023-06-22 08:51:53,294 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 2023-06-22 08:53:00,231 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5. Trying to get from SCM.
scm_1               | 2023-06-22 08:51:53,306 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1             | 2023-06-22 08:53:00,234 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 62317552-41b7-4512-b6c8-d0b46c7c7cf5, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:27.649Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-22 08:51:53,372 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 2023-06-22 08:53:00,249 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 62317552-41b7-4512-b6c8-d0b46c7c7cf5, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:27.649Z[UTC]].
scm_1               | 2023-06-22 08:51:53,476 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 2023-06-22 08:53:00,249 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)
scm_1               | 2023-06-22 08:51:53,477 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 2023-06-22 08:53:00,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-22 08:51:53,634 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 2023-06-22 08:53:00,366 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
scm_1               | 2023-06-22 08:51:53,687 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 2023-06-22 08:53:00,470 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20. Trying to get from SCM.
scm_1               | 2023-06-22 08:51:53,717 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderStateImpl
recon_1             | 2023-06-22 08:53:00,474 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 60e32c8c-5acc-46ba-bd41-61be3005bf20, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.021Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-22 08:51:54,586 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 2023-06-22 08:53:00,479 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 60e32c8c-5acc-46ba-bd41-61be3005bf20, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.021Z[UTC]].
scm_1               | 2023-06-22 08:51:55,308 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: set configuration 0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-22 08:53:00,481 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
scm_1               | 2023-06-22 08:51:56,233 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/log_inprogress_0
recon_1             | 2023-06-22 08:53:05,181 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)
scm_1               | 2023-06-22 08:51:56,934 [main] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: close
recon_1             | 2023-06-22 08:53:05,369 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
scm_1               | 2023-06-22 08:51:56,944 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: shutdown
recon_1             | 2023-06-22 08:53:05,369 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
recon_1             | 2023-06-22 08:53:06,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-22 08:51:56,945 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-81C09D0D73E5,id=3c18caf3-4ce6-429a-9998-d643fc26f105
scm_1               | 2023-06-22 08:51:56,948 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderStateImpl
scm_1               | 2023-06-22 08:51:56,952 [main] INFO server.GrpcService: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-22 08:51:56,978 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO impl.PendingRequests: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-22 08:51:57,120 [main] INFO server.GrpcService: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-22 08:51:57,123 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO impl.StateMachineUpdater: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-22 08:51:57,145 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-22 08:51:57,157 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-22 08:51:57,213 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: closes. applyIndex: 0
scm_1               | 2023-06-22 08:51:57,399 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker close()
recon_1             | 2023-06-22 08:53:06,789 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-22 08:51:57,466 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3c18caf3-4ce6-429a-9998-d643fc26f105: Stopped
recon_1             | 2023-06-22 08:53:06,954 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
scm_1               | 2023-06-22 08:51:57,466 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:53:06,954 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 62317552-41b7-4512-b6c8-d0b46c7c7cf5, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.649Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:53:06,972 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
scm_1               | 2023-06-22 08:51:57,489 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-7056bc73-01b7-404b-90a0-81c09d0d73e5; layoutVersion=7; scmId=3c18caf3-4ce6-429a-9998-d643fc26f105
recon_1             | 2023-06-22 08:53:07,271 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 reported by 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)
scm_1               | 2023-06-22 08:51:57,684 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
recon_1             | 2023-06-22 08:53:07,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=63562320-eb1c-4218-9f26-857f748a2e10. Trying to get from SCM.
scm_1               | /************************************************************
recon_1             | 2023-06-22 08:53:07,383 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 63562320-eb1c-4218-9f26-857f748a2e10, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cd5a4933-0666-4f07-ab50-d6c55125d602, CreationTimestamp2023-06-22T08:52:28.299Z[UTC]] to Recon pipeline metadata.
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 52c57281751e/172.24.0.2
recon_1             | 2023-06-22 08:53:07,385 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 63562320-eb1c-4218-9f26-857f748a2e10, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cd5a4933-0666-4f07-ab50-d6c55125d602, CreationTimestamp2023-06-22T08:52:28.299Z[UTC]].
scm_1               | ************************************************************/
recon_1             | 2023-06-22 08:53:07,385 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=63562320-eb1c-4218-9f26-857f748a2e10 reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | 2023-06-22 08:53:07,385 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63562320-eb1c-4218-9f26-857f748a2e10, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cd5a4933-0666-4f07-ab50-d6c55125d602, CreationTimestamp2023-06-22T08:52:28.299Z[UTC]] moved to OPEN state
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-22 08:53:07,386 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-22 08:52:09,486 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1             | 2023-06-22 08:53:07,506 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
scm_1               | /************************************************************
recon_1             | 2023-06-22 08:53:07,506 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 60e32c8c-5acc-46ba-bd41-61be3005bf20, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.021Z[UTC]] moved to OPEN state
scm_1               | STARTUP_MSG: Starting StorageContainerManager
recon_1             | 2023-06-22 08:53:07,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3e62672c-dd20-4d49-a904-674198ef2f07. Trying to get from SCM.
scm_1               | STARTUP_MSG:   host = 52c57281751e/172.24.0.2
recon_1             | 2023-06-22 08:53:07,730 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3e62672c-dd20-4d49-a904-674198ef2f07, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.873Z[UTC]] to Recon pipeline metadata.
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 2023-06-22 08:53:07,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e62672c-dd20-4d49-a904-674198ef2f07, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.873Z[UTC]].
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
recon_1             | 2023-06-22 08:53:07,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=3e62672c-dd20-4d49-a904-674198ef2f07 reported by 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/a01251c8113aee036fd7f229612a33c33cb5e558 ; compiled by 'runner' on 2023-06-22T07:49Z
recon_1             | 2023-06-22 08:53:07,732 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e62672c-dd20-4d49-a904-674198ef2f07, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.873Z[UTC]] moved to OPEN state
scm_1               | STARTUP_MSG:   java = 11.0.19
recon_1             | 2023-06-22 08:53:07,851 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=5141abe2-7210-4ceb-8a28-61d23aaffff1. Trying to get from SCM.
recon_1             | 2023-06-22 08:53:07,857 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 5141abe2-7210-4ceb-8a28-61d23aaffff1, Nodes: 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.099Z[UTC]] to Recon pipeline metadata.
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | 2023-06-22 08:53:07,866 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5141abe2-7210-4ceb-8a28-61d23aaffff1, Nodes: 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.099Z[UTC]].
scm_1               | ************************************************************/
recon_1             | 2023-06-22 08:53:07,882 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=5141abe2-7210-4ceb-8a28-61d23aaffff1 reported by 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)
scm_1               | 2023-06-22 08:52:09,514 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-22 08:53:07,890 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5141abe2-7210-4ceb-8a28-61d23aaffff1, Nodes: 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1c635413-c797-403f-bedc-bdd0587a593c, CreationTimestamp2023-06-22T08:52:28.099Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:52:09,722 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:53:08,210 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
scm_1               | 2023-06-22 08:52:10,075 [main] INFO reflections.Reflections: Reflections took 251 ms to scan 3 urls, producing 131 keys and 286 values 
recon_1             | 2023-06-22 08:53:09,817 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
scm_1               | 2023-06-22 08:52:10,274 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1             | 2023-06-22 08:53:09,818 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=0769956d-ace7-4d84-9f3b-53d02bde8d21. Trying to get from SCM.
scm_1               | 2023-06-22 08:52:10,287 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1             | 2023-06-22 08:53:09,829 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 0769956d-ace7-4d84-9f3b-53d02bde8d21, Nodes: 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.273Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-22 08:52:11,472 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:53:09,832 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0769956d-ace7-4d84-9f3b-53d02bde8d21, Nodes: 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.273Z[UTC]].
scm_1               | 2023-06-22 08:52:11,880 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-22 08:53:09,832 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=0769956d-ace7-4d84-9f3b-53d02bde8d21 reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
scm_1               | 2023-06-22 08:52:12,430 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-22 08:53:09,832 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0769956d-ace7-4d84-9f3b-53d02bde8d21, Nodes: 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.273Z[UTC]] moved to OPEN state
recon_1             | 2023-06-22 08:53:12,943 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-22 08:52:12,432 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-22 08:53:14,020 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c reported by 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)
recon_1             | 2023-06-22 08:53:14,020 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0d8ca18e-9239-4aef-9aff-d771ba859f6c, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.239Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:52:12,598 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1             | 2023-06-22 08:53:14,686 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-22 08:53:14,687 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-22 08:53:17,879 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1687423994687
recon_1             | 2023-06-22 08:53:17,913 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-22 08:53:18,909 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1687423994687.
recon_1             | 2023-06-22 08:53:19,130 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-22 08:53:19,790 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-06-22 08:53:19,807 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-22 08:53:19,808 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-22 08:53:19,808 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:53:19,809 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-22 08:53:19,810 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-22 08:53:19,869 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-22 08:53:19,870 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.061 seconds to process 0 keys.
scm_1               | 2023-06-22 08:52:12,906 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:3c18caf3-4ce6-429a-9998-d643fc26f105
scm_1               | 2023-06-22 08:52:13,112 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1             | 2023-06-22 08:53:19,912 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm_1               | 2023-06-22 08:52:13,133 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:52:13,135 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
recon_1             | 2023-06-22 08:53:19,915 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm_1               | 2023-06-22 08:52:13,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-22 08:52:13,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1             | 2023-06-22 08:53:53,397 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
scm_1               | 2023-06-22 08:52:13,138 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-22 08:52:13,139 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1             | 2023-06-22 08:53:53,615 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm_1               | 2023-06-22 08:52:13,139 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-22 08:52:13,141 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-06-22 08:53:56,139 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
scm_1               | 2023-06-22 08:52:13,144 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-22 08:52:13,145 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:52:13,164 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-22 08:52:13,172 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1             | 2023-06-22 08:53:56,144 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm_1               | 2023-06-22 08:52:13,173 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-22 08:52:13,555 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1             | 2023-06-22 08:53:56,163 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
scm_1               | 2023-06-22 08:52:13,558 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-22 08:52:13,559 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1             | 2023-06-22 08:53:56,165 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm_1               | 2023-06-22 08:52:13,560 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-22 08:52:13,560 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 2023-06-22 08:53:56,227 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
scm_1               | 2023-06-22 08:52:13,565 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:52:13,570 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: found a subdirectory /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5
recon_1             | 2023-06-22 08:53:56,228 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
scm_1               | 2023-06-22 08:52:13,584 [main] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: addNew group-81C09D0D73E5:[] returns group-81C09D0D73E5:java.util.concurrent.CompletableFuture@1d3d76b4[Not completed]
scm_1               | 2023-06-22 08:52:13,640 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105: new RaftServerImpl for group-81C09D0D73E5:[] with SCMStateMachine:uninitialized
recon_1             | 2023-06-22 08:53:56,242 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
scm_1               | 2023-06-22 08:52:13,643 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-22 08:52:13,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 2023-06-22 08:53:56,368 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
scm_1               | 2023-06-22 08:52:13,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-22 08:52:13,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1             | 2023-06-22 08:53:56,431 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 279 milliseconds.
scm_1               | 2023-06-22 08:52:13,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-22 08:52:13,644 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-22 08:52:13,667 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 2023-06-22 08:53:56,454 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 274 milliseconds to process 0 existing database records.
scm_1               | 2023-06-22 08:52:13,667 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-22 08:52:13,679 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 2023-06-22 08:53:56,491 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 36 milliseconds for processing 1 containers.
scm_1               | 2023-06-22 08:52:13,680 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-22 08:52:13,723 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
recon_1             | 2023-06-22 08:54:03,748 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
scm_1               | 2023-06-22 08:52:13,727 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-22 08:52:13,736 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-22 08:52:13,736 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-22 08:52:13,780 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-22 08:52:14,006 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-22 08:52:14,035 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 2023-06-22 08:54:03,758 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
scm_1               | 2023-06-22 08:52:14,036 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-22 08:52:14,049 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-22 08:52:14,050 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 2023-06-22 08:54:13,244 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_2.xcompat_default.
scm_1               | 2023-06-22 08:52:14,050 [3c18caf3-4ce6-429a-9998-d643fc26f105-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-22 08:52:14,063 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1             | 2023-06-22 08:54:13,280 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 not found. Cannot add container #3
scm_1               | 2023-06-22 08:52:14,063 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-22 08:52:14,064 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
recon_1             | 2023-06-22 08:54:13,286 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 2023-06-22 08:52:14,171 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-06-22 08:52:14,281 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
recon_1             | 2023-06-22 08:54:13,363 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
scm_1               | 2023-06-22 08:52:14,285 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1             | 2023-06-22 08:54:13,373 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 not found. Cannot add container #3
scm_1               | 2023-06-22 08:52:14,293 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-22 08:52:14,296 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-22 08:54:13,373 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 2023-06-22 08:52:14,442 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-22 08:52:14,476 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-06-22 08:52:14,483 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 2023-06-22 08:54:13,406 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_4.xcompat_default.
scm_1               | 2023-06-22 08:52:14,512 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-22 08:52:14,560 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-22 08:52:14,565 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 2023-06-22 08:54:13,415 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 not found. Cannot add container #3
scm_1               | 2023-06-22 08:52:14,581 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-22 08:52:14,585 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1             | 2023-06-22 08:54:13,415 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 2023-06-22 08:52:14,601 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-22 08:52:14,607 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
recon_1             | 2023-06-22 08:54:13,465 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_5.xcompat_default.
scm_1               | 2023-06-22 08:52:14,626 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-22 08:52:14,626 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
recon_1             | 2023-06-22 08:54:13,472 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 not found. Cannot add container #3
scm_1               | 2023-06-22 08:52:14,733 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-22 08:52:14,774 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-22 08:52:14,912 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1             | 2023-06-22 08:54:13,476 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 2023-06-22 08:52:14,936 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-22 08:52:14,940 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
recon_1             | 2023-06-22 08:54:13,515 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_1.xcompat_default.
scm_1               | 2023-06-22 08:52:14,962 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-22 08:52:14,974 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
recon_1             | 2023-06-22 08:54:13,521 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 not found. Cannot add container #3
scm_1               | 2023-06-22 08:52:14,989 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:52:15,131 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-22 08:52:16,178 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-22 08:54:13,522 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 3 not found!
scm_1               | 2023-06-22 08:52:16,214 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:52:16,259 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
recon_1             | 2023-06-22 08:54:56,341 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
scm_1               | 2023-06-22 08:52:16,292 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:52:16,320 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-22 08:52:16,330 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:52:16,331 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
recon_1             | 2023-06-22 08:54:56,396 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-06-22 08:52:16,337 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:52:16,489 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-22 08:54:56,398 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 55
scm_1               | 2023-06-22 08:52:16,527 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-22 08:52:16,539 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
recon_1             | 2023-06-22 08:55:18,794 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #4 got from xcompat_datanode_5.xcompat_default.
scm_1               | 2023-06-22 08:52:16,544 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm_1               | 2023-06-22 08:52:16,733 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
recon_1             | 2023-06-22 08:55:18,819 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #4 to Recon.
scm_1               | 2023-06-22 08:52:16,734 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
recon_1             | 2023-06-22 08:55:56,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
recon_1             | 2023-06-22 08:55:56,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 23
recon_1             | 2023-06-22 08:56:56,423 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:56:56,424 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | Key                                                Value
recon_1             | 2023-06-22 08:57:56,425 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-22 08:57:56,425 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | Threshold                                          10
recon_1             | 2023-06-22 08:58:56,425 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
recon_1             | 2023-06-22 08:58:56,427 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 1
recon_1             | 2023-06-22 08:58:56,461 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 8 pipelines in house.
scm_1               | Max Size to Move per Iteration                     500GB
recon_1             | 2023-06-22 08:58:56,472 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=377e4664-757a-4590-b6ac-f03f1d47a3e1 from SCM.
scm_1               | Max Size Entering Target per Iteration             26GB
recon_1             | 2023-06-22 08:58:56,481 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 377e4664-757a-4590-b6ac-f03f1d47a3e1, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: EC{rs-3-2-1024k}, State:OPEN, leaderId:, CreationTimestamp2023-06-22T08:54:10.716Z[UTC]].
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
recon_1             | 2023-06-22 08:58:56,500 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 4 milliseconds to process 0 existing database records.
scm_1               | 2023-06-22 08:52:16,735 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-22 08:52:16,741 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
recon_1             | 2023-06-22 08:58:56,512 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 13 milliseconds for processing 3 containers.
scm_1               | 2023-06-22 08:52:16,745 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-22 08:52:16,753 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/in_use.lock acquired by nodename 7@52c57281751e
recon_1             | 2023-06-22 08:58:56,514 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 61 milliseconds.
scm_1               | 2023-06-22 08:52:16,776 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=3c18caf3-4ce6-429a-9998-d643fc26f105} from /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/raft-meta
scm_1               | 2023-06-22 08:52:16,888 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: set configuration 0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-22 08:59:56,428 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-06-22 08:52:16,897 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-22 08:52:16,908 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1             | 2023-06-22 08:59:56,428 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-06-22 08:52:16,909 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:52:16,911 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1             | 2023-06-22 09:00:56,429 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm_1               | 2023-06-22 08:52:16,916 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1             | 2023-06-22 09:00:56,429 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm_1               | 2023-06-22 08:52:16,919 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:52:16,933 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-22 08:52:16,933 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-22 08:52:16,934 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:52:16,942 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5
scm_1               | 2023-06-22 08:52:16,943 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:52:16,945 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:52:16,952 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-22 08:52:16,956 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-22 08:52:16,957 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-22 08:52:16,958 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-22 08:52:16,959 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-22 08:52:16,959 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-22 08:52:16,971 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-22 08:52:16,972 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-22 08:52:17,190 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-22 08:52:17,191 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-22 08:52:17,192 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-22 08:52:17,229 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: set configuration 0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:52:17,231 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/log_inprogress_0
scm_1               | 2023-06-22 08:52:17,237 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-22 08:52:17,300 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: start as a follower, conf=0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:52:17,300 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-22 08:52:17,302 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState
scm_1               | 2023-06-22 08:52:17,305 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-81C09D0D73E5,id=3c18caf3-4ce6-429a-9998-d643fc26f105
scm_1               | 2023-06-22 08:52:17,308 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-22 08:52:17,308 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-22 08:52:17,309 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-22 08:52:17,310 [3c18caf3-4ce6-429a-9998-d643fc26f105-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-22 08:52:17,312 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-22 08:52:17,312 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-22 08:52:17,314 [main] INFO server.RaftServer: 3c18caf3-4ce6-429a-9998-d643fc26f105: start RPC server
scm_1               | 2023-06-22 08:52:17,367 [main] INFO server.GrpcService: 3c18caf3-4ce6-429a-9998-d643fc26f105: GrpcService started, listening on 9894
scm_1               | 2023-06-22 08:52:17,379 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3c18caf3-4ce6-429a-9998-d643fc26f105: Started
scm_1               | 2023-06-22 08:52:17,389 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-22 08:52:17,389 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-22 08:52:17,392 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-22 08:52:17,392 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-22 08:52:17,392 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-22 08:52:17,643 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-22 08:52:17,677 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-22 08:52:17,678 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-22 08:52:17,963 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-22 08:52:17,963 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:52:17,965 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-22 08:52:18,068 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:52:18,069 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-22 08:52:18,069 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:52:18,073 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-22 08:52:18,376 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-22 08:52:18,377 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-22 08:52:18,499 [main] INFO util.log: Logging initialized @17179ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-22 08:52:18,791 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-22 08:52:18,802 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-22 08:52:18,811 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-22 08:52:18,813 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-22 08:52:18,813 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-22 08:52:18,813 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-22 08:52:18,872 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-22 08:52:18,874 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-22 08:52:18,877 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-22 08:52:18,974 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-22 08:52:18,975 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-22 08:52:18,987 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-22 08:52:19,017 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@22b00077{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-22 08:52:19,018 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc9bde9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-22 08:52:19,189 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7b2adbde{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7795034604459592836/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-22 08:52:19,203 [main] INFO server.AbstractConnector: Started ServerConnector@63cf6497{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-22 08:52:19,203 [main] INFO server.Server: Started @17884ms
scm_1               | 2023-06-22 08:52:19,208 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-22 08:52:19,208 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-22 08:52:19,210 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-22 08:52:22,350 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.FollowerState: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5047708549ns, electionTimeout:5035ms
scm_1               | 2023-06-22 08:52:22,351 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState
scm_1               | 2023-06-22 08:52:22,352 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-22 08:52:22,356 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-22 08:52:22,356 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-FollowerState] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1
scm_1               | 2023-06-22 08:52:22,371 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:52:22,372 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-22 08:52:22,394 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:52:22,394 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.LeaderElection: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-22 08:52:22,395 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: shutdown 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1
scm_1               | 2023-06-22 08:52:22,395 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-22 08:52:22,396 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-22 08:52:22,396 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-22 08:52:22,404 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: change Leader from null to 3c18caf3-4ce6-429a-9998-d643fc26f105 at term 2 for becomeLeader, leader elected after 8673ms
scm_1               | 2023-06-22 08:52:22,421 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-22 08:52:22,426 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:52:22,427 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-22 08:52:22,432 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-22 08:52:22,433 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-22 08:52:22,434 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-22 08:52:22,440 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-22 08:52:22,442 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-22 08:52:22,443 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO impl.RoleInfo: 3c18caf3-4ce6-429a-9998-d643fc26f105: start 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderStateImpl
scm_1               | 2023-06-22 08:52:22,449 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-22 08:52:22,455 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/log_inprogress_0 to /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/log_0-0
scm_1               | 2023-06-22 08:52:22,458 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-LeaderElection1] INFO server.RaftServer$Division: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5: set configuration 1: peers:[3c18caf3-4ce6-429a-9998-d643fc26f105|rpc:52c57281751e:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-22 08:52:22,482 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/7056bc73-01b7-404b-90a0-81c09d0d73e5/current/log_inprogress_1
scm_1               | 2023-06-22 08:52:22,491 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-22 08:52:22,495 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-22 08:52:22,497 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:22,499 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-22 08:52:22,500 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-22 08:52:22,500 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-22 08:52:22,507 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-22 08:52:22,513 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-22 08:52:22,891 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.24.0.15:45004: output error
scm_1               | 2023-06-22 08:52:22,893 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.24.0.12:41386: output error
scm_1               | 2023-06-22 08:52:22,893 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.24.0.13:60700: output error
scm_1               | 2023-06-22 08:52:22,893 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.24.0.8:53846: output error
scm_1               | 2023-06-22 08:52:22,917 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:52:22,922 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:52:22,922 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:52:22,922 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:52:22,941 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.24.0.11:38914: output error
scm_1               | 2023-06-22 08:52:22,943 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm_1               | 2023-06-22 08:52:25,834 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1c635413-c797-403f-bedc-bdd0587a593c
scm_1               | 2023-06-22 08:52:25,933 [IPC Server handler 91 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/31613e4d-b990-42be-9377-00c7a7ba3288
scm_1               | 2023-06-22 08:52:25,946 [IPC Server handler 91 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 31613e4d-b990-42be-9377-00c7a7ba3288{ip: 172.24.0.8, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:52:26,009 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1c635413-c797-403f-bedc-bdd0587a593c{ip: 172.24.0.13, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:52:26,056 [IPC Server handler 12 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5cccec7c-2f8e-4410-a445-f064595c8b02
scm_1               | 2023-06-22 08:52:26,057 [IPC Server handler 12 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5cccec7c-2f8e-4410-a445-f064595c8b02{ip: 172.24.0.11, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:52:26,083 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:52:26,084 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:52:26,083 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,155 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,155 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,215 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-22 08:52:26,182 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:52:26,269 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-22 08:52:26,269 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:52:26,269 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-22 08:52:26,269 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-22 08:52:26,269 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,296 [IPC Server handler 14 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/73ad851d-99db-49b7-ac3d-75be53f012c8
scm_1               | 2023-06-22 08:52:26,297 [IPC Server handler 14 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 73ad851d-99db-49b7-ac3d-75be53f012c8{ip: 172.24.0.12, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:52:26,301 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,300 [IPC Server handler 18 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/cd5a4933-0666-4f07-ab50-d6c55125d602
scm_1               | 2023-06-22 08:52:26,301 [IPC Server handler 18 on default port 9861] INFO node.SCMNodeManager: Registered Data node : cd5a4933-0666-4f07-ab50-d6c55125d602{ip: 172.24.0.15, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-22 08:52:26,302 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-22 08:52:26,351 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9284cc47-3eb5-4374-917c-8d2e22216578 to datanode:31613e4d-b990-42be-9377-00c7a7ba3288
scm_1               | 2023-06-22 08:52:27,419 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9284cc47-3eb5-4374-917c-8d2e22216578, Nodes: 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:26.349Z[UTC]].
scm_1               | 2023-06-22 08:52:27,428 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:27,649 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 to datanode:cd5a4933-0666-4f07-ab50-d6c55125d602
scm_1               | 2023-06-22 08:52:27,668 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 to datanode:5cccec7c-2f8e-4410-a445-f064595c8b02
scm_1               | 2023-06-22 08:52:27,669 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=62317552-41b7-4512-b6c8-d0b46c7c7cf5 to datanode:1c635413-c797-403f-bedc-bdd0587a593c
scm_1               | 2023-06-22 08:52:27,854 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 62317552-41b7-4512-b6c8-d0b46c7c7cf5, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:27.649Z[UTC]].
scm_1               | 2023-06-22 08:52:27,855 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:27,873 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3e62672c-dd20-4d49-a904-674198ef2f07 to datanode:5cccec7c-2f8e-4410-a445-f064595c8b02
scm_1               | 2023-06-22 08:52:28,019 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3e62672c-dd20-4d49-a904-674198ef2f07, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:27.873Z[UTC]].
scm_1               | 2023-06-22 08:52:28,020 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,021 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 to datanode:5cccec7c-2f8e-4410-a445-f064595c8b02
scm_1               | 2023-06-22 08:52:28,032 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 to datanode:73ad851d-99db-49b7-ac3d-75be53f012c8
scm_1               | 2023-06-22 08:52:28,032 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=60e32c8c-5acc-46ba-bd41-61be3005bf20 to datanode:1c635413-c797-403f-bedc-bdd0587a593c
scm_1               | 2023-06-22 08:52:28,089 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 60e32c8c-5acc-46ba-bd41-61be3005bf20, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.021Z[UTC]].
scm_1               | 2023-06-22 08:52:28,089 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,099 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5141abe2-7210-4ceb-8a28-61d23aaffff1 to datanode:1c635413-c797-403f-bedc-bdd0587a593c
scm_1               | 2023-06-22 08:52:28,199 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5141abe2-7210-4ceb-8a28-61d23aaffff1, Nodes: 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.099Z[UTC]].
scm_1               | 2023-06-22 08:52:28,226 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,239 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c to datanode:cd5a4933-0666-4f07-ab50-d6c55125d602
scm_1               | 2023-06-22 08:52:28,239 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c to datanode:31613e4d-b990-42be-9377-00c7a7ba3288
scm_1               | 2023-06-22 08:52:28,239 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0d8ca18e-9239-4aef-9aff-d771ba859f6c to datanode:73ad851d-99db-49b7-ac3d-75be53f012c8
scm_1               | 2023-06-22 08:52:28,266 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0d8ca18e-9239-4aef-9aff-d771ba859f6c, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.239Z[UTC]].
scm_1               | 2023-06-22 08:52:28,266 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,273 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0769956d-ace7-4d84-9f3b-53d02bde8d21 to datanode:73ad851d-99db-49b7-ac3d-75be53f012c8
scm_1               | 2023-06-22 08:52:28,289 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0769956d-ace7-4d84-9f3b-53d02bde8d21, Nodes: 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.273Z[UTC]].
scm_1               | 2023-06-22 08:52:28,292 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,296 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-06-22 08:52:28,299 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=63562320-eb1c-4218-9f26-857f748a2e10 to datanode:cd5a4933-0666-4f07-ab50-d6c55125d602
scm_1               | 2023-06-22 08:52:28,309 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 63562320-eb1c-4218-9f26-857f748a2e10, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:52:28.299Z[UTC]].
scm_1               | 2023-06-22 08:52:28,312 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:28,317 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-06-22 08:52:59,386 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9284cc47-3eb5-4374-917c-8d2e22216578, Nodes: 31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:31613e4d-b990-42be-9377-00c7a7ba3288, CreationTimestamp2023-06-22T08:52:26.349Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:52:59,414 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:52:59,481 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:53:00,104 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:53:05,156 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:53:06,959 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 62317552-41b7-4512-b6c8-d0b46c7c7cf5, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.649Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:06,991 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-22 08:53:07,012 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-22 08:53:07,015 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-22 08:53:07,015 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-22 08:53:07,016 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-22 08:53:07,016 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-22 08:53:07,016 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:53:07,016 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-22 08:53:07,016 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-22 08:53:07,083 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-22 08:53:07,083 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-22 08:53:07,380 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63562320-eb1c-4218-9f26-857f748a2e10, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:cd5a4933-0666-4f07-ab50-d6c55125d602, CreationTimestamp2023-06-22T08:52:28.299Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:07,517 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 60e32c8c-5acc-46ba-bd41-61be3005bf20, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.021Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:07,712 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3e62672c-dd20-4d49-a904-674198ef2f07, Nodes: 5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5cccec7c-2f8e-4410-a445-f064595c8b02, CreationTimestamp2023-06-22T08:52:27.873Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:07,882 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5141abe2-7210-4ceb-8a28-61d23aaffff1, Nodes: 1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1c635413-c797-403f-bedc-bdd0587a593c, CreationTimestamp2023-06-22T08:52:28.099Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:09,822 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0769956d-ace7-4d84-9f3b-53d02bde8d21, Nodes: 73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.273Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:14,028 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0d8ca18e-9239-4aef-9aff-d771ba859f6c, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:73ad851d-99db-49b7-ac3d-75be53f012c8, CreationTimestamp2023-06-22T08:52:28.239Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:53:49,698 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-22 08:53:49,784 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-22 08:53:49,803 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-22 08:54:10,730 [3c18caf3-4ce6-429a-9998-d643fc26f105@group-81C09D0D73E5-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 377e4664-757a-4590-b6ac-f03f1d47a3e1, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:54:10.716Z[UTC]].
scm_1               | 2023-06-22 08:54:10,742 [IPC Server handler 0 on default port 9863] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 377e4664-757a-4590-b6ac-f03f1d47a3e1, Nodes: cd5a4933-0666-4f07-ab50-d6c55125d602(xcompat_datanode_2.xcompat_default/172.24.0.15)31613e4d-b990-42be-9377-00c7a7ba3288(xcompat_datanode_4.xcompat_default/172.24.0.8)73ad851d-99db-49b7-ac3d-75be53f012c8(xcompat_datanode_3.xcompat_default/172.24.0.12)1c635413-c797-403f-bedc-bdd0587a593c(xcompat_datanode_1.xcompat_default/172.24.0.13)5cccec7c-2f8e-4410-a445-f064595c8b02(xcompat_datanode_5.xcompat_default/172.24.0.11), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-22T08:54:10.716Z[UTC]] moved to OPEN state
scm_1               | 2023-06-22 08:54:28,341 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-06-22 08:56:28,346 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-06-22 08:56:43,643 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm_1               | 2023-06-22 08:57:14,941 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-22 08:58:28,347 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
scm_1               | 2023-06-22 09:00:28,349 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 4.
