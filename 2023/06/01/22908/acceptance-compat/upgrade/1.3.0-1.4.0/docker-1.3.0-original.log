Attaching to ha_dn2_1, ha_scm3_1, ha_om3_1, ha_dn5_1, ha_recon_1, ha_s3g_1, ha_om2_1, ha_om1_1, ha_dn1_1, ha_scm2_1, ha_scm1_1, ha_dn3_1, ha_dn4_1
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-06-01 19:20:44,567 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 7cbc9667dc25/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | ************************************************************/
dn3_1    | 2023-06-01 19:20:44,649 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-01 19:20:45,136 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-06-01 19:20:46,207 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-06-01 19:20:47,962 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-06-01 19:20:47,963 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-06-01 19:20:49,314 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7cbc9667dc25 ip:10.9.0.19
dn3_1    | 2023-06-01 19:20:51,988 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-06-01 19:20:54,064 [main] INFO reflections.Reflections: Reflections took 1706 ms to scan 2 urls, producing 92 keys and 204 values 
dn3_1    | 2023-06-01 19:20:55,223 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-06-01 19:20:56,621 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn3_1    | 2023-06-01 19:20:56,709 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-06-01 19:20:56,769 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-06-01 19:20:56,784 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-06-01 19:20:56,888 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-06-01 19:20:57,057 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-01 19:20:57,118 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn3_1    | 2023-06-01 19:20:57,157 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-06-01 19:20:57,157 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-06-01 19:20:57,158 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-06-01 19:20:57,521 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-06-01 19:20:57,522 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-06-01 19:21:12,902 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-06-01 19:21:13,931 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-01 19:21:15,052 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-06-01 19:21:16,375 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-01 19:21:16,411 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-06-01 19:21:16,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-01 19:21:16,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-06-01 19:21:16,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-06-01 19:21:16,431 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-06-01 19:21:16,442 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-06-01 19:21:16,442 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:21:16,462 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-06-01 19:21:16,472 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-01 19:21:16,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-01 19:21:16,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-06-01 19:21:16,982 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-06-01 19:21:21,563 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-06-01 19:21:21,604 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-06-01 19:21:21,621 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-06-01 19:21:21,627 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:21:21,627 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-01 19:21:21,693 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-01 19:21:22,004 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-06-01 19:21:23,209 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-06-01 19:21:23,326 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-06-01 19:21:23,651 [main] INFO util.log: Logging initialized @54789ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-06-01 19:21:25,171 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-06-01 19:21:25,253 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-06-01 19:21:25,392 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-06-01 19:21:25,404 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-06-01 19:21:25,457 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-06-01 19:21:25,458 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-06-01 19:21:26,387 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-06-01 19:20:46,668 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 1b2b0bf6d958/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | ************************************************************/
dn1_1    | 2023-06-01 19:20:46,727 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-06-01 19:20:47,379 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-06-01 19:20:48,337 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-06-01 19:20:50,005 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-06-01 19:20:50,005 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-06-01 19:20:51,684 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1b2b0bf6d958 ip:10.9.0.17
dn1_1    | 2023-06-01 19:20:54,555 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-06-01 19:20:56,453 [main] INFO reflections.Reflections: Reflections took 1380 ms to scan 2 urls, producing 92 keys and 204 values 
dn1_1    | 2023-06-01 19:20:57,826 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-06-01 19:20:59,384 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn1_1    | 2023-06-01 19:20:59,427 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-06-01 19:20:59,475 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-06-01 19:20:59,546 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-06-01 19:20:59,729 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-06-01 19:20:59,863 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-01 19:20:59,874 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn1_1    | 2023-06-01 19:20:59,885 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-06-01 19:20:59,886 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-06-01 19:20:59,890 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-06-01 19:21:00,274 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-06-01 19:21:00,280 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-06-01 19:21:14,755 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-06-01 19:21:15,602 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-01 19:21:16,307 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-06-01 19:21:17,420 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-01 19:21:17,456 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-06-01 19:21:17,462 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-01 19:21:17,467 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-06-01 19:21:17,469 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-06-01 19:21:17,475 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-06-01 19:21:17,478 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-06-01 19:21:17,492 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-01 19:21:17,569 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-06-01 19:21:17,570 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-01 19:21:17,748 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-01 19:21:17,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-06-01 19:21:17,819 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-06-01 19:21:22,176 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-06-01 19:21:22,268 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-06-01 19:21:22,278 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-06-01 19:21:22,285 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-01 19:21:22,303 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-01 19:21:22,328 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-01 19:21:22,601 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-06-01 19:21:24,772 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-06-01 19:21:24,898 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-06-01 19:21:25,256 [main] INFO util.log: Logging initialized @54726ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-06-01 19:21:27,223 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-06-01 19:21:27,327 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-06-01 19:21:27,504 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-06-01 19:21:27,507 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-06-01 19:21:27,544 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-06-01 19:21:27,547 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-01 19:21:28,685 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-06-01 19:21:28,692 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-06-01 19:21:29,058 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-06-01 19:21:29,058 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-06-01 19:21:29,174 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-06-01 19:21:29,396 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bf9ea6b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-06-01 19:21:29,429 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2dfeb141{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-06-01 19:21:31,292 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@756b2d90{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-5512564156728454483/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn1_1    | 2023-06-01 19:21:31,383 [main] INFO server.AbstractConnector: Started ServerConnector@241fbec{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-06-01 19:21:31,384 [main] INFO server.Server: Started @60854ms
dn1_1    | 2023-06-01 19:21:31,427 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-06-01 19:21:31,427 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-06-01 19:21:31,436 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-06-01 19:21:31,493 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-06-01 19:21:31,691 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1d4dc59e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-06-01 19:21:32,155 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-06-01 19:21:32,461 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-06-01 19:21:34,985 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:34,986 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:34,986 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:34,986 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:35,986 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:35,987 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:35,989 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:35,989 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:36,995 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:36,995 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:36,996 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:37,997 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:37,998 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:37,999 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:38,999 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:39,000 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:39,001 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:40,000 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:40,001 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:40,002 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:41,001 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:41,002 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:41,003 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:41,045 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 1b2b0bf6d958/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:49112 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-01 19:20:43,180 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = c31458b395e2/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.3.0
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | ************************************************************/
dn4_1    | 2023-06-01 19:20:43,264 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-06-01 19:20:43,767 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-06-01 19:20:44,844 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-06-01 19:20:46,230 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-06-01 19:20:46,237 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-06-01 19:20:47,603 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c31458b395e2 ip:10.9.0.20
dn4_1    | 2023-06-01 19:20:50,058 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-06-01 19:20:51,766 [main] INFO reflections.Reflections: Reflections took 1431 ms to scan 2 urls, producing 92 keys and 204 values 
dn4_1    | 2023-06-01 19:20:53,011 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-06-01 19:20:54,247 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn4_1    | 2023-06-01 19:20:54,344 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-06-01 19:20:54,450 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-06-01 19:20:54,451 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-06-01 19:20:54,637 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-06-01 19:20:54,900 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-01 19:20:54,901 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn4_1    | 2023-06-01 19:20:54,949 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-06-01 19:20:54,951 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-06-01 19:20:54,951 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-06-01 19:20:55,265 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-06-01 19:20:55,281 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-06-01 19:21:09,008 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-06-01 19:21:09,925 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-01 19:21:26,428 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-06-01 19:21:27,025 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-06-01 19:21:27,040 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-06-01 19:21:27,049 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-06-01 19:21:27,249 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@15986dd5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-06-01 19:21:27,250 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a1a3468{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-06-01 19:21:30,010 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1c98b4eb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-15165961461901396559/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn3_1    | 2023-06-01 19:21:30,098 [main] INFO server.AbstractConnector: Started ServerConnector@1a21f43f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-06-01 19:21:30,129 [main] INFO server.Server: Started @61274ms
dn3_1    | 2023-06-01 19:21:30,166 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-06-01 19:21:30,169 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-06-01 19:21:30,172 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-06-01 19:21:30,188 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-06-01 19:21:30,362 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@276bcf4d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-06-01 19:21:31,027 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-06-01 19:21:31,382 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-06-01 19:21:33,760 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:33,766 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:33,767 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:33,768 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:34,761 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:34,768 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:34,769 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:34,770 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:35,763 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:35,769 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:35,770 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:35,771 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:36,764 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:36,770 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:36,770 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:37,765 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:37,770 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:37,771 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:38,766 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:38,771 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:38,772 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:39,767 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:39,772 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:39,773 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:40,769 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:40,773 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:40,773 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:40,809 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 7cbc9667dc25/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:37680 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-06-01 19:20:47,134 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 8b2845d7a78b/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | ************************************************************/
dn2_1    | 2023-06-01 19:20:47,189 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-06-01 19:20:47,794 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-06-01 19:20:48,714 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-06-01 19:20:50,417 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-06-01 19:20:50,418 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-06-01 19:20:51,898 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:8b2845d7a78b ip:10.9.0.18
dn2_1    | 2023-06-01 19:20:54,672 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-06-01 19:20:56,493 [main] INFO reflections.Reflections: Reflections took 1452 ms to scan 2 urls, producing 92 keys and 204 values 
dn2_1    | 2023-06-01 19:20:57,838 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-06-01 19:20:59,299 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn2_1    | 2023-06-01 19:20:59,523 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-06-01 19:20:59,539 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-06-01 19:20:59,547 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-06-01 19:20:59,716 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-06-01 19:20:59,814 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-01 19:20:59,862 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn2_1    | 2023-06-01 19:20:59,908 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-06-01 19:20:59,909 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-06-01 19:20:59,910 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-06-01 19:21:00,204 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-06-01 19:21:00,207 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-06-01 19:21:14,940 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-06-01 19:21:16,017 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-01 19:21:16,648 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-06-01 19:21:17,424 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:37680 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-01 19:21:41,770 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:41,773 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:41,774 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:42,774 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:42,776 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:43,776 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:43,776 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:17,455 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-06-01 19:21:17,460 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-01 19:21:17,461 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-06-01 19:21:17,468 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-06-01 19:21:17,470 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-06-01 19:21:17,471 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-06-01 19:21:17,479 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-01 19:21:17,490 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-06-01 19:21:17,493 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-01 19:21:17,583 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-01 19:21:17,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-06-01 19:21:17,675 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-06-01 19:21:22,003 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-01 19:21:22,064 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-06-01 19:21:22,133 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-06-01 19:21:22,153 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-01 19:21:22,170 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-01 19:21:22,245 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-01 19:21:22,566 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-06-01 19:21:24,067 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-06-01 19:21:24,204 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-06-01 19:21:24,435 [main] INFO util.log: Logging initialized @52162ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-06-01 19:21:25,407 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-06-01 19:21:25,490 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-06-01 19:21:25,561 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-01 19:21:25,575 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-06-01 19:21:25,601 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-01 19:21:25,606 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-06-01 19:21:26,232 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-06-01 19:21:26,279 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-06-01 19:21:26,684 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-01 19:21:26,684 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-06-01 19:21:26,702 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-06-01 19:21:26,904 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-01 19:21:26,921 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4525e9e8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-06-01 19:21:30,415 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8c43966{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-392201531399978797/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn2_1    | 2023-06-01 19:21:30,515 [main] INFO server.AbstractConnector: Started ServerConnector@3e26482{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-06-01 19:21:30,527 [main] INFO server.Server: Started @58265ms
dn2_1    | 2023-06-01 19:21:30,544 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-06-01 19:21:30,545 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-01 19:21:30,548 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-06-01 19:21:30,581 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-06-01 19:21:31,006 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@74c9076a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-06-01 19:21:31,565 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-06-01 19:21:31,882 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-06-01 19:21:34,317 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:34,323 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:34,324 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:34,326 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:35,323 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:35,325 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:35,325 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:35,333 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:36,325 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:36,325 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:36,333 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:37,326 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:37,327 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:37,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:38,327 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:38,328 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:38,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:39,333 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:39,334 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:10,810 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-06-01 19:21:12,260 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-01 19:21:12,262 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-06-01 19:21:12,262 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-01 19:21:12,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-06-01 19:21:12,299 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-06-01 19:21:12,299 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-06-01 19:21:12,299 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-06-01 19:21:12,300 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-01 19:21:12,301 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-06-01 19:21:12,326 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-01 19:21:12,541 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-01 19:21:12,629 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-01 19:21:12,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-01 19:21:16,455 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-01 19:21:16,519 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-01 19:21:16,552 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-01 19:21:16,557 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-01 19:21:16,560 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-01 19:21:16,586 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-01 19:21:16,782 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-06-01 19:21:18,214 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-06-01 19:21:18,460 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-06-01 19:21:18,744 [main] INFO util.log: Logging initialized @50781ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-06-01 19:21:20,785 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-06-01 19:21:20,828 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-06-01 19:21:21,002 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-06-01 19:21:21,004 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-06-01 19:21:21,005 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-06-01 19:21:21,005 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-01 19:21:21,678 [main] INFO http.HttpServer2: Jetty bound to port 9882
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-01 19:20:45,838 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = ac36eb5b7616/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--init]
om1_1    | STARTUP_MSG:   version = 1.3.0
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-06-01 19:20:45,923 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-01 19:20:58,297 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-01 19:21:03,401 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-01 19:21:04,425 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-01 19:21:04,433 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-01 19:21:04,470 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-01 19:21:05,819 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-01 19:21:10,404 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:12,408 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:14,411 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:16,414 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:18,416 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:20,417 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:22,419 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
dn3_1    | 2023-06-01 19:21:44,778 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:44,778 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:45,780 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:45,780 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:46,780 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 7cbc9667dc25/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:46110 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:46110 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-01 19:21:46,782 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:46,781 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:47,150 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-cc5a0e90-48eb-48cf-98c1-3674e4537020/container.db to cache
dn3_1    | 2023-06-01 19:21:47,151 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-cc5a0e90-48eb-48cf-98c1-3674e4537020/container.db for volume DS-cc5a0e90-48eb-48cf-98c1-3674e4537020
dn3_1    | 2023-06-01 19:21:47,152 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-06-01 19:21:47,156 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn3_1    | 2023-06-01 19:21:47,498 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn3_1    | 2023-06-01 19:21:47,665 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start RPC server
dn3_1    | 2023-06-01 19:21:47,685 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: GrpcService started, listening on 9858
dn3_1    | 2023-06-01 19:21:47,690 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: GrpcService started, listening on 9856
dn3_1    | 2023-06-01 19:21:47,692 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: GrpcService started, listening on 9857
dn3_1    | 2023-06-01 19:21:47,725 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1d53eca5-989b-4da6-8a3f-f976f64c30fa is started using port 9858 for RATIS
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:49112 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-06-01 19:21:42,003 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:42,004 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:42,004 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:43,004 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:43,005 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:44,005 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:44,006 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:45,006 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:45,007 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:46,008 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:46,008 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:47,726 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1d53eca5-989b-4da6-8a3f-f976f64c30fa is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-06-01 19:21:47,726 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1d53eca5-989b-4da6-8a3f-f976f64c30fa is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-06-01 19:21:47,737 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1d53eca5-989b-4da6-8a3f-f976f64c30fa: Started
dn3_1    | 2023-06-01 19:21:47,783 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:47,784 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 7cbc9667dc25/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-06-01 19:21:47,785 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:47,807 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 7cbc9667dc25/10.9.0.19 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-06-01 19:21:48,807 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:47,009 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:47,013 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:47,219 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-eed83fec-afa5-43dd-92a9-8de8a503f83b/container.db to cache
dn1_1    | 2023-06-01 19:21:47,219 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-eed83fec-afa5-43dd-92a9-8de8a503f83b/container.db for volume DS-eed83fec-afa5-43dd-92a9-8de8a503f83b
dn1_1    | 2023-06-01 19:21:47,230 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-06-01 19:21:47,244 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn1_1    | 2023-06-01 19:21:47,540 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 71839cc9-95e0-4895-b82f-6f2805ce4ff9
dn1_1    | 2023-06-01 19:21:47,655 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: start RPC server
dn1_1    | 2023-06-01 19:21:47,680 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: GrpcService started, listening on 9858
dn1_1    | 2023-06-01 19:21:47,692 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: GrpcService started, listening on 9856
dn1_1    | 2023-06-01 19:21:47,723 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: GrpcService started, listening on 9857
dn1_1    | 2023-06-01 19:21:47,723 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-01 19:21:47,745 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-71839cc9-95e0-4895-b82f-6f2805ce4ff9: Started
dn1_1    | 2023-06-01 19:21:47,745 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 71839cc9-95e0-4895-b82f-6f2805ce4ff9 is started using port 9858 for RATIS
dn1_1    | 2023-06-01 19:21:47,758 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 71839cc9-95e0-4895-b82f-6f2805ce4ff9 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-06-01 19:21:47,758 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 71839cc9-95e0-4895-b82f-6f2805ce4ff9 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-06-01 19:21:48,010 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:48,014 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:49,012 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:49,013 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 1b2b0bf6d958/10.9.0.17 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 2023-06-01 19:21:48,822 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:49,809 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:49,823 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:50,438 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-01 19:21:50,811 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:50,824 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:51,812 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:51,824 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:52,813 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:52,827 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:53,815 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:53,828 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:54,816 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:54,829 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:55,818 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:55,831 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:56,821 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:56,834 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:57,826 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:57,834 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:58,827 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-01 19:21:49,015 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:49,015 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 1b2b0bf6d958/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 2023-06-01 19:21:58,835 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:59,828 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:21:59,836 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:00,829 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:00,836 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:01,831 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:01,837 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:02,835 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:02,838 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:03,837 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:03,841 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:04,838 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:04,841 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:05,839 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:05,842 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:06,840 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:06,843 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:07,841 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:07,843 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:08,842 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:08,844 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:09,843 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:09,844 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:10,844 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:10,846 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:11,846 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:11,847 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:12,847 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:12,848 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:21,687 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-06-01 19:21:21,929 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-01 19:21:21,936 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-06-01 19:21:21,949 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-06-01 19:21:22,039 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32a72c4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-06-01 19:21:22,059 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e243175{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-06-01 19:21:24,491 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7f27f59b{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-7543115488409313098/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn4_1    | 2023-06-01 19:21:24,633 [main] INFO server.AbstractConnector: Started ServerConnector@9df564f{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-06-01 19:21:24,633 [main] INFO server.Server: Started @56702ms
dn4_1    | 2023-06-01 19:21:24,696 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-06-01 19:21:24,696 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-06-01 19:21:24,714 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-06-01 19:21:24,737 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-06-01 19:21:25,033 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2b8dc37] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-06-01 19:21:25,945 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-06-01 19:21:26,707 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-06-01 19:21:28,609 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:28,624 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:28,625 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:28,625 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:29,610 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:29,625 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:29,626 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:29,626 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:30,611 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:30,626 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:30,626 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:30,627 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:31,613 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:31,626 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:31,627 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:31,628 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:32,614 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:32,627 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:32,628 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:32,629 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-01 19:20:45,052 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = e833eff4d53a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--init]
om2_1    | STARTUP_MSG:   version = 1.3.0
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-06-01 19:20:45,099 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-01 19:20:55,934 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-01 19:21:00,578 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-01 19:21:01,523 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-01 19:21:01,533 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-01 19:21:01,597 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:21:03,946 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-01 19:21:08,537 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:10,541 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:12,543 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:14,545 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:16,547 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:18,549 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:20,553 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:22,555 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:24,557 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:26,560 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:28,564 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:30,566 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:32,569 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:34,572 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:36,576 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:38,585 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:40,588 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:43,063 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4db01c28-2341-445f-9eff-7a55fcb1aabf is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:45,068 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-01 19:21:47,070 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e833eff4d53a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-01 19:21:49,743 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-01 19:21:49,744 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 2023-06-01 19:21:39,337 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:40,334 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:40,334 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:40,338 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:40,388 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 8b2845d7a78b/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:36126 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:36126 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-01 19:21:41,335 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:41,336 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:41,339 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:42,337 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-01 19:21:50,015 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:50,018 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:51,022 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:51,025 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:51,746 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-01 19:21:52,023 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:52,026 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:53,029 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:53,030 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:54,031 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:54,031 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:55,033 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:55,044 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:56,045 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:56,045 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:57,046 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:57,046 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:42,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:43,340 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:43,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:44,341 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:44,341 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:45,342 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:21:24,421 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:26,424 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:28,426 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:30,428 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:32,430 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:34,433 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:36,438 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:38,443 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:40,447 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:42,450 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:44,531 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4db01c28-2341-445f-9eff-7a55fcb1aabf is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:46,534 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:21:48,536 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ac36eb5b7616/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9;layoutVersion=3
om1_1    | 2023-06-01 19:21:50,670 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1    | /************************************************************
om1_1    | SHUTDOWN_MSG: Shutting down OzoneManager at ac36eb5b7616/10.9.0.11
om1_1    | ************************************************************/
dn3_1    | 2023-06-01 19:22:13,849 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:13,849 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:14,850 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:14,851 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:15,851 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:16,852 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:17,861 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:18,863 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:19,866 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:20,439 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-01 19:22:20,871 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:21,872 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:22,874 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:23,875 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:24,876 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:25,783 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-01 19:22:25,877 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:26,877 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:27,879 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:28,880 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:29,881 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:30,882 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-01 19:21:58,752 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = ac36eb5b7616/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--]
om1_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | 2023-06-01 19:21:58,049 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:58,049 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:59,050 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:21:59,050 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:00,072 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:00,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:01,106 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:01,119 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:02,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:02,120 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:03,109 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:03,121 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:04,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:04,122 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:05,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:05,123 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:06,119 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:06,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:07,121 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:07,126 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:08,122 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:08,126 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:09,123 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:09,127 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:10,124 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:10,128 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:11,125 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:11,129 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:33,616 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:33,628 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:33,629 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:33,629 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:34,617 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:34,629 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:34,630 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:34,630 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:35,618 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:35,630 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:35,631 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:35,631 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:36,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:36,631 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:36,637 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:37,620 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:37,639 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:37,644 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:38,621 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:38,641 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:38,644 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:39,622 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:39,642 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:39,645 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:40,626 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:40,642 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:40,646 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:40,680 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From c31458b395e2/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:51768 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:51768 remote=recon/10.9.0.22:9891]
dn2_1    | 2023-06-01 19:21:45,342 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:46,343 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:46,344 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:46,346 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 8b2845d7a78b/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:60748 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:60748 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-01 19:21:47,285 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-28dea954-b3e3-4571-92a4-43187f604ae8/container.db to cache
dn2_1    | 2023-06-01 19:21:47,285 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-28dea954-b3e3-4571-92a4-43187f604ae8/container.db for volume DS-28dea954-b3e3-4571-92a4-43187f604ae8
dn2_1    | 2023-06-01 19:21:47,320 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-06-01 19:21:47,329 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn2_1    | 2023-06-01 19:21:47,345 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:47,346 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:47,586 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 69bc1289-e9f0-444d-ab3d-1548ab888401
dn2_1    | 2023-06-01 19:21:47,635 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 69bc1289-e9f0-444d-ab3d-1548ab888401: start RPC server
dn2_1    | 2023-06-01 19:21:47,638 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 69bc1289-e9f0-444d-ab3d-1548ab888401: GrpcService started, listening on 9858
dn2_1    | 2023-06-01 19:21:47,639 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 69bc1289-e9f0-444d-ab3d-1548ab888401: GrpcService started, listening on 9856
dn2_1    | 2023-06-01 19:21:47,640 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 69bc1289-e9f0-444d-ab3d-1548ab888401: GrpcService started, listening on 9857
dn2_1    | 2023-06-01 19:21:47,651 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 69bc1289-e9f0-444d-ab3d-1548ab888401 is started using port 9858 for RATIS
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-06-01 19:21:41,631 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:41,643 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:41,646 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:42,632 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:42,635 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From c31458b395e2/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-06-01 19:20:46,055 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = dd0e15ce78c9/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.3.0
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn5_1    | ************************************************************/
dn5_1    | 2023-06-01 19:20:46,088 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-06-01 19:20:46,525 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-06-01 19:20:47,544 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-01 19:20:49,351 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-06-01 19:20:49,353 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-06-01 19:20:50,939 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:dd0e15ce78c9 ip:10.9.0.21
dn5_1    | 2023-06-01 19:20:53,787 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-06-01 19:20:55,856 [main] INFO reflections.Reflections: Reflections took 1684 ms to scan 2 urls, producing 92 keys and 204 values 
dn5_1    | 2023-06-01 19:20:57,121 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-06-01 19:20:58,860 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn5_1    | 2023-06-01 19:20:59,115 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-06-01 19:20:59,192 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-06-01 19:20:59,194 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-06-01 19:20:59,518 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-06-01 19:20:59,741 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-01 19:20:59,742 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn5_1    | 2023-06-01 19:20:59,798 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-06-01 19:20:59,798 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
om2_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9;layoutVersion=3
om2_1    | 2023-06-01 19:21:49,177 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1    | /************************************************************
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at e833eff4d53a/10.9.0.12
om2_1    | ************************************************************/
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-01 19:21:55,754 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = e833eff4d53a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--]
om2_1    | STARTUP_MSG:   version = 1.3.0
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-06-01 19:21:55,847 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-01 19:22:00,885 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-01 19:22:02,998 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-01 19:22:03,422 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-01 19:22:03,429 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-01 19:22:03,437 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:22:03,501 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-06-01 19:22:04,724 [main] INFO reflections.Reflections: Reflections took 928 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om2_1    | 2023-06-01 19:22:04,812 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:22:06,029 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-01 19:22:06,218 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-01 19:22:09,684 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:22:10,375 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-01 19:22:10,378 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-01 19:22:11,207 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-06-01 19:22:11,386 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om2_1    | 2023-06-01 19:22:11,656 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-01 19:22:11,674 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-06-01 19:22:11,734 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-06-01 19:22:12,561 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-06-01 19:22:12,609 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-01 19:22:13,053 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-06-01 19:22:13,263 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1    | 2023-06-01 19:22:13,601 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-06-01 19:22:14,134 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-01 19:22:14,143 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-01 19:22:14,153 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-01 19:22:14,157 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-01 19:22:14,157 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-06-01 19:22:14,158 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-06-01 19:22:14,158 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:31,901 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:32,902 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:33,904 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:34,905 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:35,907 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:36,908 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:37,909 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:38,910 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:39,912 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:40,913 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:41,914 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:42,916 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:47,118 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-01 19:22:51,498 [Command processor thread] INFO server.RaftServer: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: addNew group-A7FD084CD2E7:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-A7FD084CD2E7:java.util.concurrent.CompletableFuture@2db5b664[Not completed]
dn3_1    | 2023-06-01 19:22:51,516 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: new RaftServerImpl for group-A7FD084CD2E7:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-01 19:22:51,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-01 19:22:51,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-01 19:22:51,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-01 19:22:51,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:51,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-01 19:22:51,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-01 19:22:51,532 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: ConfigurationManager, init=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-01 19:22:51,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-01 19:22:51,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-01 19:22:51,542 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-01 19:22:51,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:51,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-01 19:22:51,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-01 19:22:51,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-01 19:22:51,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-01 19:22:51,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-01 19:22:51,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-01 19:22:51,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-01 19:22:51,612 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7 does not exist. Creating ...
dn3_1    | 2023-06-01 19:22:51,619 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7/in_use.lock acquired by nodename 7@7cbc9667dc25
dn3_1    | 2023-06-01 19:22:51,626 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7 has been successfully formatted.
dn3_1    | 2023-06-01 19:22:51,641 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-A7FD084CD2E7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-01 19:22:51,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-01 19:22:51,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-01 19:22:14,182 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-01 19:22:14,182 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-06-01 19:22:14,183 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-01 19:22:14,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-01 19:22:14,286 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-06-01 19:22:14,307 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-06-01 19:22:15,628 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-06-01 19:22:15,698 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-06-01 19:22:15,700 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-06-01 19:22:15,710 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-01 19:22:15,710 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-01 19:22:15,723 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-01 19:22:15,778 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@5e180aaf[Not completed]
om2_1    | 2023-06-01 19:22:15,785 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-06-01 19:22:15,935 [pool-26-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-06-01 19:22:15,979 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-06-01 19:22:15,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-06-01 19:22:15,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-06-01 19:22:15,981 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-01 19:22:15,982 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-01 19:22:15,982 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-06-01 19:22:15,971 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-06-01 19:22:16,078 [pool-26-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-06-01 19:22:16,083 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-01 19:22:16,177 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-06-01 19:22:16,189 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-06-01 19:22:16,301 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-06-01 19:22:16,329 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-06-01 19:22:16,375 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-06-01 19:22:17,307 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-01 19:22:17,372 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-06-01 19:22:17,377 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-06-01 19:22:17,406 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-06-01 19:22:17,431 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-06-01 19:22:20,786 [main] INFO reflections.Reflections: Reflections took 4559 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om2_1    | 2023-06-01 19:22:21,611 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-06-01 19:22:21,721 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-06-01 19:22:22,785 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-06-01 19:22:22,892 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-01 19:22:22,893 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-06-01 19:22:23,321 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-06-01 19:22:23,323 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-06-01 19:22:23,349 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1    | 2023-06-01 19:22:23,384 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@e833eff4d53a
om2_1    | 2023-06-01 19:22:23,565 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1    | 2023-06-01 19:22:23,584 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-06-01 19:22:23,654 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-01 19:22:23,655 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-01 19:22:23,659 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-06-01 19:22:23,664 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-06-01 19:22:23,667 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-01 19:22:23,696 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-01 19:22:51,679 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:51,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-01 19:22:51,682 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-01 19:22:51,685 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:51,721 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-01 19:22:51,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-01 19:22:51,737 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7
dn3_1    | 2023-06-01 19:22:51,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-01 19:22:51,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-01 19:22:51,744 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:51,745 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-01 19:22:51,745 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-01 19:22:51,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-01 19:22:51,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-01 19:22:51,779 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-01 19:22:51,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:51,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:22:51,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:22:51,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-01 19:22:51,931 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-01 19:22:51,941 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-01 19:22:51,966 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: start as a follower, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-01 19:22:51,967 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-01 19:22:51,986 [pool-22-thread-1] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState
dn3_1    | 2023-06-01 19:22:52,018 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A7FD084CD2E7,id=1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn3_1    | 2023-06-01 19:22:52,026 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-01 19:22:52,026 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-01 19:22:52,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-01 19:22:52,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-01 19:22:52,037 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-01 19:22:52,038 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-01 19:22:52,266 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7
dn3_1    | 2023-06-01 19:22:52,267 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7.
dn3_1    | 2023-06-01 19:22:52,274 [Command processor thread] INFO server.RaftServer: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: addNew group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-A4B05FDCBDE0:java.util.concurrent.CompletableFuture@20145709[Not completed]
dn3_1    | 2023-06-01 19:22:52,281 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: new RaftServerImpl for group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-01 19:22:52,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-01 19:22:52,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-01 19:22:52,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-01 19:22:52,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:52,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-01 19:22:52,342 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-01 19:22:52,343 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: ConfigurationManager, init=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-01 19:22:52,343 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-01 19:22:52,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-01 19:22:52,349 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-01 19:22:52,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:52,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-01 19:22:52,392 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-01 19:22:52,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-01 19:22:52,421 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-01 19:22:52,421 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-01 19:22:52,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-01 19:22:52,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-01 19:22:52,424 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 does not exist. Creating ...
dn3_1    | 2023-06-01 19:22:52,457 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/in_use.lock acquired by nodename 7@7cbc9667dc25
dn3_1    | 2023-06-01 19:22:52,469 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 has been successfully formatted.
dn3_1    | 2023-06-01 19:22:52,474 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-A4B05FDCBDE0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-01 19:22:52,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-01 19:22:52,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-01 19:22:52,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:52,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-01 19:22:52,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-01 19:22:52,476 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:52,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-01 19:22:52,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-01 19:22:52,550 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
dn3_1    | 2023-06-01 19:22:52,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-01 19:22:52,551 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-01 19:22:52,551 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:52,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-01 19:22:52,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-01 19:22:52,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-01 19:22:52,556 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-01 19:22:52,557 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-01 19:22:52,586 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:52,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:22:52,587 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:22:52,588 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-01 19:22:52,598 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-01 19:22:52,599 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-01 19:22:52,617 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: start as a follower, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-01 19:22:52,619 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-01 19:22:52,620 [pool-22-thread-1] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState
dn3_1    | 2023-06-01 19:22:52,625 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A4B05FDCBDE0,id=1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:20:59,798 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-06-01 19:21:00,196 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
om2_1    | 2023-06-01 19:22:23,700 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-06-01 19:21:47,651 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 69bc1289-e9f0-444d-ab3d-1548ab888401 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-06-01 19:21:47,651 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 69bc1289-e9f0-444d-ab3d-1548ab888401 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-06-01 19:21:47,652 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-69bc1289-e9f0-444d-ab3d-1548ab888401: Started
dn2_1    | 2023-06-01 19:21:48,347 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:48,348 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 8b2845d7a78b/10.9.0.18 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om1_1    | ************************************************************/
om1_1    | 2023-06-01 19:21:58,780 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-06-01 19:22:12,127 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 2023-06-01 19:21:00,215 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-06-01 19:22:52,627 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-01 19:22:12,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:13,128 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:13,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
om1_1    | 2023-06-01 19:22:04,256 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-01 19:22:06,212 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn5_1    | 2023-06-01 19:21:14,434 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-06-01 19:20:45,139 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = c8d8490e7f62/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.3.0
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
recon_1  | ************************************************************/
recon_1  | 2023-06-01 19:20:45,258 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
dn3_1    | 2023-06-01 19:22:52,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-01 19:22:52,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-01 19:22:14,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:14,131 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:15,131 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:15,132 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:16,133 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:17,134 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:18,161 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:19,162 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:20,164 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:21,165 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:21,748 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om2_1    | 2023-06-01 19:22:23,762 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-06-01 19:22:23,778 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-06-01 19:22:23,795 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | 2023-06-01 19:22:06,656 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-01 19:22:06,701 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-01 19:22:06,734 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-01 19:22:06,833 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-06-01 19:22:23,798 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn5_1    | 2023-06-01 19:21:15,192 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-01 19:21:15,795 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-06-01 19:21:17,188 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-01 19:21:17,236 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-06-01 19:21:17,237 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-01 19:21:17,242 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-06-01 19:21:17,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-06-01 19:21:17,293 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-06-01 19:21:17,295 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-06-01 19:21:17,304 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-01 19:21:17,358 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-06-01 19:21:17,362 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-01 19:21:17,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-01 19:21:17,594 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-06-01 19:21:17,623 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-06-01 19:21:21,380 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-06-01 19:21:21,426 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-01 19:21:21,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-01 19:21:21,427 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-01 19:21:21,427 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-01 19:21:21,464 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-01 19:21:21,714 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-06-01 19:21:22,998 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-06-01 19:21:23,121 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-01 19:21:23,349 [main] INFO util.log: Logging initialized @51708ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-06-01 19:21:24,308 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-06-01 19:21:24,376 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-06-01 19:21:24,453 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-06-01 19:21:24,477 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-06-01 19:21:24,501 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-06-01 19:21:24,501 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-06-01 19:21:25,230 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-06-01 19:21:25,245 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-06-01 19:21:25,668 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-01 19:21:25,668 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-06-01 19:21:25,696 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-06-01 19:21:25,864 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-06-01 19:21:25,906 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c33da7a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-06-01 19:21:28,235 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@11a3a45f{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-749234703125904657/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn5_1    | 2023-06-01 19:21:28,331 [main] INFO server.AbstractConnector: Started ServerConnector@6b37df8e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-06-01 19:21:28,333 [main] INFO server.Server: Started @56692ms
dn5_1    | 2023-06-01 19:21:28,354 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-06-01 19:21:28,354 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-01 19:21:28,357 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-06-01 19:21:28,428 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-06-01 19:21:28,685 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@39786c0d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-06-01 19:21:29,751 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-01 19:22:23,805 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | Waiting for the service scm1:9894
om1_1    | 2023-06-01 19:22:08,523 [main] INFO reflections.Reflections: Reflections took 1416 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | Waiting for the service scm2:9894
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 2023-06-01 19:21:42,644 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:42,645 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From c31458b395e2/10.9.0.20 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om2_1    | 2023-06-01 19:22:23,806 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1    | 2023-06-01 19:20:43,604 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | 2023-06-01 19:22:08,609 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-01 19:20:46,294 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om3_1    | 2023-06-01 19:20:45,850 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
om2_1    | 2023-06-01 19:22:23,822 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
s3g_1    | 2023-06-01 19:20:43,653 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-01 19:22:10,214 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-01 19:22:10,415 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 7f68a8a5d5cc/10.9.0.13
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om2_1    | 2023-06-01 19:22:23,823 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
s3g_1    | 2023-06-01 19:20:44,002 [main] INFO util.log: Logging initialized @14114ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-06-01 19:21:25,111 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om1_1    | 2023-06-01 19:22:14,017 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-01 19:22:14,602 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1   | 2023-06-01 19:22:24,446 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
om3_1    | STARTUP_MSG:   args = [--init]
om3_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
recon_1  | 2023-06-01 19:20:52,110 [main] INFO reflections.Reflections: Reflections took 791 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1  | 2023-06-01 19:20:58,949 [main] INFO recon.ReconServer: Initializing Recon server...
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-06-01 19:22:23,828 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
s3g_1    | 2023-06-01 19:20:45,736 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | /************************************************************
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 2023-06-01 19:22:52,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1    | 2023-06-01 19:20:45,991 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
om1_1    | 2023-06-01 19:22:14,606 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-01 19:22:15,471 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
scm2_1   | STARTUP_MSG:   host = fe03b6a14f2f/10.9.0.15
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | 2023-06-01 19:21:01,579 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1    | 2023-06-01 19:20:46,071 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-06-01 19:22:15,645 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om1_1    | 2023-06-01 19:22:15,732 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | STARTUP_MSG:   args = [--bootstrap]
scm3_1   | STARTUP_MSG:   host = 330dd2bcef66/10.9.0.16
recon_1  | 2023-06-01 19:21:12,711 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 2023-06-01 19:20:46,100 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
om1_1    | 2023-06-01 19:22:15,733 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-06-01 19:22:15,765 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
scm2_1   | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   java = 11.0.14.1
recon_1  | 2023-06-01 19:21:16,073 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
scm3_1   | STARTUP_MSG:   args = [--bootstrap]
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 2023-06-01 19:22:52,630 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-01 19:22:52,655 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
s3g_1    | 2023-06-01 19:20:46,136 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-01 19:22:16,386 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-01 19:22:16,433 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
recon_1  | 2023-06-01 19:21:16,078 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
scm3_1   | STARTUP_MSG:   version = 1.3.0
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 2023-06-01 19:22:52,692 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-01 19:22:57,169 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0.
dn5_1    | 2023-06-01 19:21:30,111 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-06-01 19:21:32,164 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
s3g_1    | 2023-06-01 19:20:46,137 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-06-01 19:22:16,569 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-06-01 19:22:16,634 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
recon_1  | 2023-06-01 19:21:16,129 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-06-01 19:22:57,177 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: new RaftServerImpl for group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-01 19:22:57,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-01 19:21:32,170 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-01 19:20:46,799 [main] INFO s3.Gateway: STARTUP_MSG: 
scm1_1   | STARTUP_MSG:   host = f0aca9c847b3/10.9.0.14
scm1_1   | STARTUP_MSG:   args = [--init]
om1_1    | 2023-06-01 19:22:17,003 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1  | 2023-06-01 19:21:16,317 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
om2_1    | 2023-06-01 19:22:23,882 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 	... 1 more
dn3_1    | 2023-06-01 19:22:57,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-01 19:22:57,188 [Command processor thread] INFO server.RaftServer: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: addNew group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1C4731943483:java.util.concurrent.CompletableFuture@5812cccb[Not completed]
dn5_1    | 2023-06-01 19:21:32,171 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:32,180 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | STARTUP_MSG:   version = 1.3.0
s3g_1    | /************************************************************
om1_1    | 2023-06-01 19:22:18,357 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | ************************************************************/
recon_1  | 2023-06-01 19:21:16,318 [main] INFO recon.ReconServer: Creating Recon Schema.
om2_1    | 2023-06-01 19:22:23,885 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 2023-06-01 19:22:22,166 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:33,165 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:33,171 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:18,446 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
s3g_1    | STARTUP_MSG: Starting Gateway
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
om3_1    | 2023-06-01 19:20:45,944 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-01 19:22:23,893 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 2023-06-01 19:22:57,193 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-01 19:22:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-01 19:21:33,171 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-06-01 19:22:18,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1    | STARTUP_MSG:   host = e4f63a0daf14/10.9.0.23
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
om3_1    | 2023-06-01 19:20:57,595 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-01 19:22:23,894 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | ************************************************************/
dn3_1    | 2023-06-01 19:22:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-01 19:22:57,194 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: ConfigurationManager, init=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 2023-06-01 19:22:23,168 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:33,180 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | ************************************************************/
om1_1    | 2023-06-01 19:22:18,478 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
s3g_1    | STARTUP_MSG:   args = []
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
om3_1    | 2023-06-01 19:21:03,038 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-01 19:22:23,962 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-01 19:22:24,456 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-01 19:22:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-01 19:22:57,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 2023-06-01 19:22:24,169 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:34,166 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:18,478 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-06-01 19:21:25,237 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | STARTUP_MSG:   version = 1.3.0
scm1_1   | ************************************************************/
scm1_1   | 2023-06-01 19:20:46,430 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-06-01 19:20:47,830 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-01 19:20:48,507 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-06-01 19:20:48,836 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-06-01 19:20:49,969 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
recon_1  | 2023-06-01 19:21:21,903 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
dn1_1    | 2023-06-01 19:22:25,170 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
scm1_1   | 2023-06-01 19:20:49,973 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-06-01 19:20:52,828 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-06-01 19:20:55,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-01 19:20:55,153 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-01 19:20:55,233 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-01 19:20:55,234 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1  | 2023-06-01 19:21:26,783 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-06-01 19:21:26,939 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-06-01 19:21:43,640 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
scm1_1   | 2023-06-01 19:20:55,234 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-06-01 19:20:55,234 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-06-01 19:20:55,235 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-06-01 19:20:55,263 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-01 19:20:55,264 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-01 19:20:55,305 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1  | 2023-06-01 19:21:27,030 [main] INFO util.log: Logging initialized @55027ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-06-01 19:21:27,993 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-06-01 19:21:43,647 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:20:55,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-01 19:20:55,660 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-06-01 19:20:55,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-01 19:21:01,187 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-06-01 19:21:01,210 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | 2023-06-01 19:21:28,059 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-06-01 19:21:28,135 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-06-01 19:21:28,154 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-06-01 19:21:28,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-01 19:22:18,493 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn1_1    | 2023-06-01 19:22:25,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-01 19:21:44,641 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:21:03,869 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-01 19:21:03,870 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-01 19:21:03,877 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-01 19:21:05,716 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-01 19:21:09,999 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:12,019 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | 2023-06-01 19:22:26,171 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:44,647 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:21:14,065 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:16,067 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:18,079 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:20,081 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:22,083 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om1_1    | 2023-06-01 19:22:18,498 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn5_1    | 2023-06-01 19:21:34,172 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:25,798 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-01 19:22:24,747 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | ************************************************************/
dn1_1    | 2023-06-01 19:22:27,172 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:45,642 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-01 19:22:57,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-01 19:22:57,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-01 19:22:57,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-01 19:22:57,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-01 19:21:24,086 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:18,523 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-01 19:21:34,172 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:34,181 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:24,918 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
s3g_1    | 2023-06-01 19:20:46,839 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-01 19:22:57,204 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-06-01 19:22:23,963 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-01 19:21:28,161 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-01 19:21:01,361 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-01 19:21:45,648 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:21:26,089 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:28,092 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:18,529 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-06-01 19:22:18,556 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 2023-06-01 19:21:35,168 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:24,919 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 2023-06-01 19:22:57,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
s3g_1    | 2023-06-01 19:20:47,067 [main] INFO s3.Gateway: Starting Ozone S3 gateway
om2_1    | 2023-06-01 19:22:23,977 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-01 19:22:23,978 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-06-01 19:21:01,369 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 2023-06-01 19:21:46,643 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:21:30,095 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:32,100 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:18,621 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-06-01 19:21:26,024 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3_1   | 2023-06-01 19:22:25,128 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 2023-06-01 19:22:57,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1    | 2023-06-01 19:20:47,975 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-06-01 19:22:23,982 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-01 19:22:24,013 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-06-01 19:22:24,015 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-01 19:22:24,024 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-01 19:21:34,102 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:18,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-06-01 19:22:28,173 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:29,174 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:30,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:31,176 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:25,129 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 2023-06-01 19:22:57,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
s3g_1    | 2023-06-01 19:20:49,569 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-01 19:22:24,025 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-06-01 19:22:24,040 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
scm1_1   | 2023-06-01 19:21:01,386 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-01 19:21:46,649 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:21:36,105 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:18,697 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-06-01 19:22:32,177 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:33,181 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:34,182 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:26,039 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-06-01 19:22:25,599 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 2023-06-01 19:22:57,216 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO impl.FollowerState: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5231207487ns, electionTimeout:5179ms
s3g_1    | 2023-06-01 19:20:49,569 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
recon_1  | 2023-06-01 19:21:30,006 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
om2_1    | 2023-06-01 19:22:24,041 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-06-01 19:22:24,044 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
scm1_1   | 2023-06-01 19:21:01,519 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om3_1    | 2023-06-01 19:21:38,109 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:20,783 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-06-01 19:22:35,185 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:36,186 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:37,187 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:26,403 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
scm3_1   | 2023-06-01 19:22:26,990 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9, SCMID 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03
dn3_1    | 2023-06-01 19:22:57,217 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 does not exist. Creating ...
s3g_1    | 2023-06-01 19:20:49,906 [main] INFO http.HttpServer2: Jetty bound to port 9878
recon_1  | 2023-06-01 19:21:31,698 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-06-01 19:21:31,762 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
om2_1    | 2023-06-01 19:22:24,085 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
scm1_1   | 2023-06-01 19:21:01,818 [main] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: addNew group-477B239F9CA9:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] returns group-477B239F9CA9:java.util.concurrent.CompletableFuture@7e242b4d[Not completed]
om3_1    | 2023-06-01 19:21:40,120 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:20,800 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-06-01 19:22:38,193 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:35,172 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:26,403 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
dn2_1    | 	... 12 more
dn3_1    | 2023-06-01 19:22:57,219 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState
scm3_1   | 2023-06-01 19:22:26,991 [main] INFO server.StorageContainerManager: Primary SCM Node ID 4db01c28-2341-445f-9eff-7a55fcb1aabf
s3g_1    | 2023-06-01 19:20:49,949 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-06-01 19:21:31,812 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-06-01 19:21:31,859 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-01 19:22:24,458 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
scm1_1   | 2023-06-01 19:21:02,517 [pool-2-thread-1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf: new RaftServerImpl for group-477B239F9CA9:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
om3_1    | 2023-06-01 19:21:42,122 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-01 19:22:20,801 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-06-01 19:22:39,194 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:40,195 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:48,349 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:27,280 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm3_1   | 2023-06-01 19:22:27,051 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
recon_1  | 2023-06-01 19:21:31,863 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-06-01 19:21:33,474 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 2023-06-01 19:20:50,232 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-06-01 19:22:24,475 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm1_1   | 2023-06-01 19:21:02,542 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om3_1    | 2023-06-01 19:21:44,165 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4db01c28-2341-445f-9eff-7a55fcb1aabf is not the leader. Could not determine the leader node.
om1_1    | 2023-06-01 19:22:20,814 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn1_1    | 2023-06-01 19:22:41,197 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:35,173 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:48,350 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | 2023-06-01 19:22:57,222 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm2_1   | 2023-06-01 19:21:30,944 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1   | /************************************************************
recon_1  | 2023-06-01 19:21:33,848 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-01 19:21:33,991 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
s3g_1    | 2023-06-01 19:20:50,232 [main] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-06-01 19:22:24,501 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
scm1_1   | 2023-06-01 19:21:02,570 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-01 19:21:02,587 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-06-01 19:22:20,815 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-01 19:22:42,198 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:35,182 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | java.net.ConnectException: Call From 8b2845d7a78b/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 2023-06-01 19:22:57,230 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-01 19:22:57,231 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1
recon_1  | 2023-06-01 19:21:33,998 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-06-01 19:21:34,187 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 330dd2bcef66/10.9.0.16
s3g_1    | 2023-06-01 19:20:50,267 [main] INFO server.session: node0 Scavenging every 660000ms
om2_1    | 2023-06-01 19:22:24,750 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-06-01 19:22:24,750 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-06-01 19:22:24,990 [Listener at om2/9862] INFO util.log: Logging initialized @35354ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-06-01 19:22:25,995 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-06-01 19:22:26,008 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-06-01 19:22:26,065 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-06-01 19:22:26,071 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
dn3_1    | 2023-06-01 19:22:57,231 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/in_use.lock acquired by nodename 7@7cbc9667dc25
dn3_1    | 2023-06-01 19:22:57,236 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 has been successfully formatted.
dn3_1    | 2023-06-01 19:22:57,241 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1C4731943483: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-01 19:22:57,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-01 19:22:57,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-01 19:22:57,241 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:57,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-01 19:22:57,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-01 19:22:57,244 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:57,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 2023-06-01 19:21:34,497 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-06-01 19:21:34,670 [main] INFO reflections.Reflections: Reflections took 162 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 2023-06-01 19:21:34,803 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-06-01 19:21:34,877 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-06-01 19:21:34,898 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1  | 2023-06-01 19:21:34,911 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-06-01 19:21:34,974 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 18 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:46,167 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 19 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-01 19:21:48,170 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7f68a8a5d5cc/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9;layoutVersion=3
om3_1    | 2023-06-01 19:21:50,251 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1    | /************************************************************
om3_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 7f68a8a5d5cc/10.9.0.13
om3_1    | ************************************************************/
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 2023-06-01 19:22:57,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-06-01 19:22:26,071 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-06-01 19:21:35,022 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn4_1    | 2023-06-01 19:21:46,654 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From c31458b395e2/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:52904 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1    | 2023-06-01 19:20:50,521 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-01 19:21:57,916 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om1_1    | 2023-06-01 19:22:20,931 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-06-01 19:21:02,588 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn3_1    | 2023-06-01 19:22:57,256 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483
dn3_1    | 2023-06-01 19:22:57,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om2_1    | 2023-06-01 19:22:26,071 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 2023-06-01 19:21:36,169 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:32,947 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | STARTUP_MSG:   host = 7f68a8a5d5cc/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--]
om3_1    | STARTUP_MSG:   version = 1.3.0
s3g_1    | 2023-06-01 19:20:50,523 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-06-01 19:22:21,054 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@5e180aaf[Not completed]
om1_1    | 2023-06-01 19:22:21,056 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
dn3_1    | 2023-06-01 19:22:57,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 2023-06-01 19:21:35,115 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
om2_1    | 2023-06-01 19:22:26,424 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 2023-06-01 19:21:36,173 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:34,950 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 2023-06-01 19:22:43,199 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | ************************************************************/
s3g_1    | WARNING: An illegal reflective access operation has occurred
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | 2023-06-01 19:22:21,328 [main] INFO om.OzoneManager: Creating RPC Server
scm1_1   | 2023-06-01 19:21:02,593 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-01 19:22:57,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-06-01 19:21:35,172 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om2_1    | 2023-06-01 19:22:26,430 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 2023-06-01 19:21:36,174 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:21:36,955 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 2023-06-01 19:22:47,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | 2023-06-01 19:22:21,358 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
scm1_1   | 2023-06-01 19:21:02,593 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-01 19:22:57,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 2023-06-01 19:21:35,343 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
om2_1    | 2023-06-01 19:22:26,652 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
scm2_1   | 2023-06-01 19:21:38,957 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 2023-06-01 19:22:53,350 [Command processor thread] INFO server.RaftServer: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: addNew group-6286EE24275C:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-6286EE24275C:java.util.concurrent.CompletableFuture@1602c124[Not completed]
dn5_1    | 2023-06-01 19:21:37,171 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:34,463 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-06-01 19:22:21,412 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-01 19:21:02,669 [pool-2-thread-1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: ConfigurationManager, init=-1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-01 19:22:57,258 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-01 19:22:57,263 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-06-01 19:22:26,652 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
scm2_1   | 2023-06-01 19:21:40,960 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 2023-06-01 19:22:53,518 [pool-22-thread-1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: new RaftServerImpl for group-6286EE24275C:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-01 19:21:37,173 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:37,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
om1_1    | 2023-06-01 19:22:21,449 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-01 19:22:21,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-01 19:22:57,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-01 19:22:57,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-06-01 19:22:26,675 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
scm2_1   | 2023-06-01 19:21:42,977 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
dn1_1    | 2023-06-01 19:22:53,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-01 19:21:38,172 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:38,174 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
om3_1    | ************************************************************/
scm3_1   | STARTUP_MSG:   host = 330dd2bcef66/10.9.0.16
om1_1    | 2023-06-01 19:22:21,450 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
scm1_1   | 2023-06-01 19:21:02,742 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn3_1    | 2023-06-01 19:22:57,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-01 19:22:57,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-06-01 19:22:26,796 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@26874f2c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 2023-06-01 19:22:53,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-01 19:22:53,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-01 19:21:38,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
om3_1    | 2023-06-01 19:21:57,974 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | STARTUP_MSG:   args = []
om1_1    | 2023-06-01 19:22:21,451 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-01 19:22:21,457 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-01 19:22:57,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:22:57,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om2_1    | 2023-06-01 19:22:26,805 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e68be2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 2023-06-01 19:21:39,175 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
om3_1    | 2023-06-01 19:22:03,239 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
scm3_1   | STARTUP_MSG:   version = 1.3.0
om1_1    | 2023-06-01 19:22:21,534 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-06-01 19:22:21,563 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-01 19:22:57,303 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-01 19:21:35,343 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
om2_1    | 2023-06-01 19:22:28,503 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4a6a6a69{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-5130474197439943385/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 2023-06-01 19:21:39,176 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
om3_1    | 2023-06-01 19:22:05,057 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-01 19:22:21,673 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-06-01 19:21:02,894 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
dn3_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-01 19:21:35,466 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-06-01 19:22:28,546 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@23e2c1ca{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-06-01 19:21:39,175 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:40,176 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
om3_1    | 2023-06-01 19:22:05,326 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om1_1    | 2023-06-01 19:22:21,674 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-06-01 19:21:02,903 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn3_1    | 2023-06-01 19:22:57,307 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:21:35,494 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-01 19:22:28,548 [Listener at om2/9862] INFO server.Server: Started @38912ms
dn1_1    | 2023-06-01 19:22:53,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-01 19:22:53,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-01 19:22:53,550 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-01 19:22:53,599 [pool-22-thread-1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: ConfigurationManager, init=-1: peers:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
s3g_1    | WARNING: All illegal access operations will be denied in a future release
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
om3_1    | 2023-06-01 19:22:05,333 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om1_1    | 2023-06-01 19:22:21,922 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-06-01 19:22:22,134 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | 2023-06-01 19:22:57,313 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-06-01 19:22:57,315 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1
recon_1  | 2023-06-01 19:21:35,494 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-06-01 19:21:36,113 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-06-01 19:21:36,115 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-06-01 19:21:40,177 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:40,178 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:40,225 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
om3_1    | 2023-06-01 19:22:05,383 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-01 19:22:22,141 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-01 19:21:03,548 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn3_1    | 2023-06-01 19:22:57,315 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: start as a follower, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | ************************************************************/
scm3_1   | 2023-06-01 19:22:34,509 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | 2023-06-01 19:22:34,740 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-01 19:22:34,880 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om2_1    | 2023-06-01 19:22:28,562 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | java.net.SocketTimeoutException: Call From dd0e15ce78c9/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:36994 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
s3g_1    | Jun 01, 2023 7:21:30 PM org.glassfish.jersey.internal.Errors logErrors
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
om3_1    | 2023-06-01 19:22:05,539 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-06-01 19:22:23,196 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-01 19:21:03,616 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn3_1    | 2023-06-01 19:22:57,319 [pool-22-thread-1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 2023-06-01 19:22:34,924 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1  | 2023-06-01 19:21:36,187 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-06-01 19:21:36,187 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-06-01 19:21:36,190 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 2023-06-01 19:22:28,562 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
dn2_1    | 	... 12 more
om3_1    | 2023-06-01 19:22:07,028 [main] INFO reflections.Reflections: Reflections took 1246 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om1_1    | 2023-06-01 19:22:23,215 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-01 19:21:03,623 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-01 19:22:57,319 [pool-22-thread-1] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState
dn1_1    | 2023-06-01 19:22:53,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-01 19:22:53,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-01 19:22:53,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-06-01 19:22:35,102 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
om2_1    | 2023-06-01 19:22:28,571 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 2023-06-01 19:21:49,352 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:22:07,157 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-01 19:22:23,231 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-01 19:21:04,631 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-06-01 19:21:08,271 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-01 19:21:08,281 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-06-01 19:22:35,102 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
om2_1    | 2023-06-01 19:22:28,573 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 2023-06-01 19:21:49,364 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:22:08,347 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-01 19:22:23,253 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-01 19:21:08,282 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-01 19:21:08,315 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-01 19:21:08,325 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-06-01 19:21:36,250 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-06-01 19:22:37,058 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:22:28,613 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
om3_1    | 2023-06-01 19:22:08,603 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-01 19:22:12,418 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-01 19:22:12,990 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn3_1    | 2023-06-01 19:22:57,321 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-06-01 19:21:08,336 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 does not exist. Creating ...
dn1_1    | 2023-06-01 19:22:53,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-06-01 19:21:36,251 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:52904 remote=scm1/10.9.0.14:9861]
scm3_1   | 2023-06-01 19:22:37,464 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-01 19:22:28,898 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
s3g_1    | 
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
om3_1    | 2023-06-01 19:22:12,995 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn2_1    | 2023-06-01 19:21:50,353 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:50,365 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,338 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A7FD084CD2E7 with new leaderId: 1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm1_1   | 2023-06-01 19:21:08,638 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/in_use.lock acquired by nodename 13@f0aca9c847b3
dn1_1    | 2023-06-01 19:22:53,731 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1  | 2023-06-01 19:21:40,134 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@643a73fa{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-14630620855705499333/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm3_1   | 2023-06-01 19:22:38,119 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
om2_1    | 2023-06-01 19:22:28,988 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@21527b8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
s3g_1    | 2023-06-01 19:21:30,655 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-13600730205535472488/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
om3_1    | 2023-06-01 19:22:13,730 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn2_1    | 2023-06-01 19:21:51,355 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:51,366 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,339 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: change Leader from null to 1d53eca5-989b-4da6-8a3f-f976f64c30fa at term 1 for becomeLeader, leader elected after 5782ms
scm1_1   | 2023-06-01 19:21:08,789 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 has been successfully formatted.
scm1_1   | 2023-06-01 19:21:08,885 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 2023-06-01 19:21:40,150 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm3_1   | 2023-06-01 19:22:38,122 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om2_1    | 2023-06-01 19:22:29,190 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208239398ns, electionTimeout:5165ms
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
s3g_1    | 2023-06-01 19:21:30,755 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
om3_1    | 2023-06-01 19:22:13,965 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
dn2_1    | 2023-06-01 19:21:52,359 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:52,366 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,356 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-01 19:21:09,107 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-01 19:22:53,732 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-01 19:22:54,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-01 19:22:38,296 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-06-01 19:22:29,192 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
s3g_1    | 2023-06-01 19:21:30,758 [main] INFO server.Server: Started @60871ms
om3_1    | 2023-06-01 19:22:14,089 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-01 19:21:53,360 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:53,369 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,344 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1C4731943483,id=1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm1_1   | 2023-06-01 19:21:09,110 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-06-01 19:21:40,150 [Listener at 0.0.0.0/9891] INFO server.Server: Started @68147ms
dn1_1    | 2023-06-01 19:22:54,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-01 19:22:54,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-01 19:22:54,252 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-01 19:22:54,261 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-06-01 19:22:38,343 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:5e8a32ef-e4cd-4aae-aa6b-c9f950180f03
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om3_1    | 2023-06-01 19:22:14,106 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
dn2_1    | 2023-06-01 19:21:54,361 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:54,370 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,370 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-01 19:21:09,118 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 2023-06-01 19:21:40,159 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-06-01 19:21:40,159 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-06-01 19:21:40,161 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
om2_1    | 2023-06-01 19:22:29,193 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm3_1   | 2023-06-01 19:22:38,519 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-06-01 19:22:54,264 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/77894454-4515-4db1-a815-6286ee24275c does not exist. Creating ...
s3g_1    | 2023-06-01 19:21:30,775 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
s3g_1    | 2023-06-01 19:21:30,776 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-01 19:21:55,363 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:55,371 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,370 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-01 19:21:09,130 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 2023-06-01 19:21:40,161 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
scm2_1   | 2023-06-01 19:21:45,027 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4db01c28-2341-445f-9eff-7a55fcb1aabf is not the leader. Could not determine the leader node.
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 2023-06-01 19:22:29,198 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1    | 2023-06-01 19:22:29,199 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
scm3_1   | 2023-06-01 19:22:38,669 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 2023-06-01 19:22:57,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-06-01 19:21:09,159 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1  | 2023-06-01 19:21:40,181 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 2023-06-01 19:22:29,226 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-01 19:22:29,419 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-01 19:22:38,674 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-06-01 19:22:57,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-01 19:22:57,372 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-01 19:21:09,342 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-01 19:21:09,342 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-01 19:21:09,454 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9
om2_1    | 2023-06-01 19:22:29,419 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-01 19:22:29,439 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
scm3_1   | 2023-06-01 19:22:38,675 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:36994 remote=recon/10.9.0.22:9891]
dn2_1    | 2023-06-01 19:21:56,364 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:56,372 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:57,366 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,378 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483
dn3_1    | 2023-06-01 19:22:57,452 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-01 19:21:09,472 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-01 19:21:09,492 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-01 19:21:09,510 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-01 19:22:29,447 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
dn1_1    | 2023-06-01 19:22:54,322 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/77894454-4515-4db1-a815-6286ee24275c/in_use.lock acquired by nodename 7@1b2b0bf6d958
scm3_1   | 2023-06-01 19:22:38,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-01 19:22:14,190 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 2023-06-01 19:21:57,372 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:23,257 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-06-01 19:22:25,369 [main] INFO reflections.Reflections: Reflections took 3694 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
recon_1  | 2023-06-01 19:21:40,194 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
dn3_1    | 2023-06-01 19:22:57,517 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-06-01 19:21:09,533 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-01 19:21:09,557 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-01 19:21:09,566 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-06-01 19:22:32,991 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.825646530s. [buffered_nanos=2701156478, remote_addr=om3/10.9.0.13:9872]
om2_1    | 2023-06-01 19:22:33,242 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.841710647s. [buffered_nanos=2387437193, remote_addr=om1/10.9.0.11:9872]
scm3_1   | 2023-06-01 19:22:38,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-06-01 19:22:15,120 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 2023-06-01 19:21:58,368 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:21:58,373 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:40,194 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
s3g_1    | 2023-06-01 19:21:30,777 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
dn3_1    | 2023-06-01 19:22:57,518 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-06-01 19:21:09,581 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-01 19:21:09,585 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-01 19:21:09,824 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-06-01 19:21:09,890 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-01 19:21:09,914 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-01 19:21:09,937 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-06-01 19:22:15,286 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 2023-06-01 19:21:59,369 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:26,237 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-06-01 19:21:40,195 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 2023-06-01 19:24:02,901 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn3_1    | 2023-06-01 19:22:57,543 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-06-01 19:21:10,065 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-01 19:22:54,380 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/77894454-4515-4db1-a815-6286ee24275c has been successfully formatted.
dn1_1    | 2023-06-01 19:22:54,440 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6286EE24275C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-01 19:22:54,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-06-01 19:22:33,247 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
scm3_1   | 2023-06-01 19:22:38,676 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om3_1    | 2023-06-01 19:22:15,507 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1  | 2023-06-01 19:21:40,195 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-06-01 19:21:40,199 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
scm1_1   | 2023-06-01 19:21:10,065 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-01 19:21:10,355 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: start as a follower, conf=-1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:21:10,356 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-06-01 19:21:10,379 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState
om2_1    | 2023-06-01 19:22:33,251 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.825646530s. [buffered_nanos=2701156478, remote_addr=om3/10.9.0.13:9872]
scm3_1   | 2023-06-01 19:22:38,677 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om3_1    | 2023-06-01 19:22:15,556 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1  | 2023-06-01 19:21:42,388 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c8d8490e7f62/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
dn3_1    | 2023-06-01 19:22:57,546 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-01 19:21:10,501 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-477B239F9CA9,id=4db01c28-2341-445f-9eff-7a55fcb1aabf
scm1_1   | 2023-06-01 19:21:10,521 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-01 19:21:10,534 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-01 19:21:10,631 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-06-01 19:22:33,251 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.841710647s. [buffered_nanos=2387437193, remote_addr=om1/10.9.0.11:9872]
scm3_1   | 2023-06-01 19:22:38,681 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-01 19:22:15,869 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
recon_1  | 2023-06-01 19:21:44,390 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c8d8490e7f62/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
dn3_1    | 2023-06-01 19:22:57,549 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-01 19:21:10,632 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn1_1    | 2023-06-01 19:22:54,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-01 19:22:54,474 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-01 19:22:54,486 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-06-01 19:22:33,251 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
scm3_1   | 2023-06-01 19:22:38,683 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-06-01 19:22:16,194 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 2023-06-01 19:21:46,423 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:4db01c28-2341-445f-9eff-7a55fcb1aabf is not the leader. Could not determine the leader node.
dn3_1    | 2023-06-01 19:22:57,587 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-06-01 19:21:10,662 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-01 19:21:10,674 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-01 19:21:10,729 [main] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start RPC server
scm1_1   | 2023-06-01 19:21:11,703 [main] INFO server.GrpcService: 4db01c28-2341-445f-9eff-7a55fcb1aabf: GrpcService started, listening on 9894
om2_1    | 2023-06-01 19:22:33,253 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
scm3_1   | 2023-06-01 19:22:38,685 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om3_1    | 2023-06-01 19:22:16,206 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn3_1    | 2023-06-01 19:22:57,594 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-06-01 19:21:11,738 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4db01c28-2341-445f-9eff-7a55fcb1aabf: Started
scm1_1   | 2023-06-01 19:21:15,570 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.FollowerState: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5196609449ns, electionTimeout:5019ms
dn1_1    | 2023-06-01 19:22:54,487 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-01 19:22:54,502 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om2_1    | 2023-06-01 19:22:33,256 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-06-01 19:22:33,257 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-01 19:22:33,284 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn3_1    | 2023-06-01 19:22:57,639 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderStateImpl
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om2_1    | 2023-06-01 19:22:33,284 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-01 19:22:33,478 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 1, (t:0, i:~))
scm3_1   | 2023-06-01 19:22:38,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-01 19:22:26,347 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-06-01 19:22:27,293 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-06-01 19:22:27,466 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-06-01 19:22:27,466 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm1_1   | 2023-06-01 19:21:15,586 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om2_1    | 2023-06-01 19:22:33,484 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: reject ELECTION from om3: already has voted for om2 at current term 1
om2_1    | 2023-06-01 19:22:33,614 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t1. Peer's state: om2@group-D66704EFC61C:t1, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:27,820 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-06-01 19:22:27,826 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-06-01 19:22:27,840 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1    | 2023-06-01 19:22:27,881 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@ac36eb5b7616
scm3_1   | 2023-06-01 19:22:38,725 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-06-01 19:21:15,653 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
dn3_1    | 2023-06-01 19:22:57,718 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO impl.FollowerState: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5098402886ns, electionTimeout:5014ms
dn1_1    | 2023-06-01 19:22:54,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-06-01 19:22:34,437 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om3 at term 1 for appendEntries, leader elected after 18135ms
om2_1    | 2023-06-01 19:22:34,491 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:27,996 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1    | 2023-06-01 19:22:28,010 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-01 19:22:28,073 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-06-01 19:22:28,081 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-01 19:22:38,726 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-01 19:21:15,771 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 2023-06-01 19:22:57,733 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState
dn1_1    | 2023-06-01 19:22:54,524 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-06-01 19:22:34,524 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om3_1    | 2023-06-01 19:22:16,206 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-01 19:22:28,091 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-06-01 19:22:28,105 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-01 19:22:28,123 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-01 19:22:28,179 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-06-01 19:22:39,084 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-06-01 19:21:15,797 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 2023-06-01 19:22:57,733 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-01 19:22:54,554 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/77894454-4515-4db1-a815-6286ee24275c
om2_1    | 2023-06-01 19:22:35,229 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om2_1    | 2023-06-01 19:22:37,927 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | 2023-06-01 19:22:28,180 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-01 19:21:59,374 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:00,372 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:00,374 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,087 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-06-01 19:21:15,958 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.LeaderElection: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:21:41,178 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om2_1    | [id: "om1"
om3_1    | 2023-06-01 19:22:16,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
dn2_1    | 2023-06-01 19:22:01,373 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:01,375 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,088 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-06-01 19:22:28,297 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm1_1   | 2023-06-01 19:21:15,960 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.LeaderElection: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-06-01 19:21:41,179 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-01 19:21:48,426 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c8d8490e7f62/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-01 19:21:50,428 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From c8d8490e7f62/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-01 19:21:52,907 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-06-01 19:21:52,908 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1  | 2023-06-01 19:21:52,910 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6614ab6a-e6f2-43c2-8c89-156762bb27ab from SCM.
recon_1  | 2023-06-01 19:21:52,938 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]].
recon_1  | 2023-06-01 19:21:52,944 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 from SCM.
recon_1  | 2023-06-01 19:21:52,950 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]].
recon_1  | 2023-06-01 19:21:52,951 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7 from SCM.
recon_1  | 2023-06-01 19:21:52,952 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]].
recon_1  | 2023-06-01 19:21:52,952 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=77894454-4515-4db1-a815-6286ee24275c from SCM.
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
om3_1    | 2023-06-01 19:22:16,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-06-01 19:22:16,209 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-06-01 19:22:16,212 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
recon_1  | 2023-06-01 19:21:52,953 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]].
dn3_1    | 2023-06-01 19:22:57,733 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-01 19:22:57,734 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2
dn3_1    | 2023-06-01 19:22:57,768 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-01 19:22:57,821 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483.
dn3_1    | 2023-06-01 19:22:57,822 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 69bc1289-e9f0-444d-ab3d-1548ab888401
dn3_1    | 2023-06-01 19:22:57,828 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-01 19:22:02,375 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:02,376 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:03,376 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:03,377 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:04,383 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:04,384 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:41,179 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:42,180 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,822 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | address: "om1:9872"
scm1_1   | 2023-06-01 19:21:15,960 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1
recon_1  | 2023-06-01 19:21:52,953 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=3a3b4b03-96c4-47b7-9040-e9508d8d4862 from SCM.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn1_1    | 2023-06-01 19:22:54,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-06-01 19:21:47,300 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-4c9f4a67-bd50-4bee-9d5b-22e08a1d2d10/container.db to cache
dn4_1    | 2023-06-01 19:21:47,301 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-4c9f4a67-bd50-4bee-9d5b-22e08a1d2d10/container.db for volume DS-4c9f4a67-bd50-4bee-9d5b-22e08a1d2d10
dn4_1    | 2023-06-01 19:21:47,328 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-06-01 19:21:47,334 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
scm3_1   | 2023-06-01 19:22:39,088 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn5_1    | 2023-06-01 19:21:42,181 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:57,854 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-01 19:21:15,960 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-01 19:21:52,954 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]].
dn5_1    | 2023-06-01 19:21:43,181 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,088 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-01 19:22:39,093 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | , id: "om3"
scm1_1   | 2023-06-01 19:21:16,004 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: change Leader from null to 4db01c28-2341-445f-9eff-7a55fcb1aabf at term 1 for becomeLeader, leader elected after 12508ms
dn2_1    | 2023-06-01 19:22:05,385 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:05,385 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:06,386 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:06,386 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:07,387 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:07,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:08,389 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:08,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,955 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=54391af0-7339-4d61-8cba-4312d758e1fc from SCM.
dn5_1    | 2023-06-01 19:21:43,182 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,105 [main] INFO server.RaftServer: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: addNew group-477B239F9CA9:[] returns group-477B239F9CA9:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm3_1   | 2023-06-01 19:22:39,128 [pool-16-thread-1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: new RaftServerImpl for group-477B239F9CA9:[] with SCMStateMachine:uninitialized
om2_1    | address: "om3:9872"
scm1_1   | 2023-06-01 19:21:16,170 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-01 19:22:09,390 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:09,390 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:10,391 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:10,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:11,391 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:11,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:12,392 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:12,393 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,956 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]].
dn5_1    | 2023-06-01 19:21:44,183 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,130 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn3_1    | 2023-06-01 19:22:57,917 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 79bd37f0-b225-4c38-b396-23c942162c92
om2_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-01 19:21:16,425 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn2_1    | 2023-06-01 19:22:13,394 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,957 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 from SCM.
om3_1    | 2023-06-01 19:22:16,217 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:57,983 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-LeaderElection1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7: set configuration 0: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:39,130 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
dn5_1    | 2023-06-01 19:21:44,183 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:28,298 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn3_1    | 2023-06-01 19:22:58,096 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-06-01 19:22:39,131 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-06-01 19:21:16,481 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | startupRole: FOLLOWER
dn1_1    | 2023-06-01 19:22:54,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-01 19:22:54,565 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-01 19:22:54,566 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-01 19:22:54,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-06-01 19:22:16,218 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn2_1    | 2023-06-01 19:22:13,394 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,958 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]].
dn5_1    | 2023-06-01 19:21:45,184 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:58,099 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-69bc1289-e9f0-444d-ab3d-1548ab888401#0:OK-t1
scm1_1   | 2023-06-01 19:21:16,635 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm3_1   | 2023-06-01 19:22:39,131 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-06-01 19:22:39,131 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-01 19:22:54,594 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-01 19:22:54,598 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-01 19:22:54,601 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-01 19:22:54,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om3_1    | 2023-06-01 19:22:16,220 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn2_1    | 2023-06-01 19:22:14,395 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,958 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
dn5_1    | 2023-06-01 19:21:45,184 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:58,102 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2 ELECTION round 0: result PASSED
scm1_1   | 2023-06-01 19:21:16,649 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 2023-06-01 19:22:39,132 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-01 19:22:39,141 [pool-16-thread-1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-01 19:22:54,701 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-01 19:22:54,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-01 19:22:54,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-01 19:22:54,749 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-01 19:22:16,258 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-01 19:22:14,395 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,958 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
dn3_1    | 2023-06-01 19:22:58,102 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2
dn5_1    | 2023-06-01 19:21:46,185 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:16,652 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-06-01 19:22:39,145 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | ]
dn1_1    | 2023-06-01 19:22:54,767 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-01 19:22:54,788 [pool-22-thread-1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: start as a follower, conf=-1: peers:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-06-01 19:24:02,965 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-06-01 19:24:02,991 [qtp384515747-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-01 19:22:16,293 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-06-01 19:22:15,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:15,398 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:58,103 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om1_1    | 2023-06-01 19:22:28,298 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-01 19:21:16,911 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm3_1   | 2023-06-01 19:22:39,150 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-06-01 19:23:33,303 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
scm2_1   | 2023-06-01 19:21:47,029 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From fe03b6a14f2f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-01 19:21:49,195 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9, SCMID 5dcb1cf7-eeac-4c03-a034-92235b5458e4
s3g_1    | 2023-06-01 19:24:02,992 [qtp384515747-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-06-01 19:24:04,412 [qtp384515747-20] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
om3_1    | 2023-06-01 19:22:16,296 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-06-01 19:22:16,399 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:52,961 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-06-01 19:22:58,103 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A4B05FDCBDE0 with new leaderId: 1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:21:46,187 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
scm3_1   | 2023-06-01 19:22:39,151 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-01 19:21:49,195 [main] INFO server.StorageContainerManager: Primary SCM Node ID 4db01c28-2341-445f-9eff-7a55fcb1aabf
scm2_1   | 2023-06-01 19:21:49,227 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
om2_1    | 2023-06-01 19:23:36,952 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
om3_1    | 2023-06-01 19:22:17,131 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-01 19:22:17,400 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-01 19:22:58,116 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: change Leader from null to 1d53eca5-989b-4da6-8a3f-f976f64c30fa at term 1 for becomeLeader, leader elected after 5737ms
dn3_1    | 2023-06-01 19:22:58,117 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-01 19:22:58,117 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-01 19:22:58,117 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | java.net.ConnectException: Call From dd0e15ce78c9/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
scm1_1   | 2023-06-01 19:21:16,960 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-06-01 19:22:39,163 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | /************************************************************
scm2_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at fe03b6a14f2f/10.9.0.15
om2_1    | 2023-06-01 19:23:49,076 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
om3_1    | 2023-06-01 19:22:17,168 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-06-01 19:22:18,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:19,404 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:28,299 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-01 19:22:28,300 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-06-01 19:22:17,169 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
s3g_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
s3g_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
dn3_1    | 2023-06-01 19:22:58,118 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-01 19:22:58,118 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-01 19:22:58,119 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-01 19:22:58,119 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om1_1    | 2023-06-01 19:22:28,300 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-01 19:22:28,342 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-01 19:22:20,405 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:22:17,173 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-01 19:22:17,173 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-01 19:22:17,202 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-01 19:22:17,277 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@c3719e5[Not completed]
scm2_1   | ************************************************************/
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | 2023-06-01 19:22:58,119 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-01 19:22:58,290 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-01 19:22:58,295 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:22:58,296 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
om1_1    | 2023-06-01 19:22:28,342 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-01 19:22:28,343 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-01 19:22:21,056 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om3_1    | 2023-06-01 19:22:17,278 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-06-01 19:21:54,701 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | /************************************************************
dn3_1    | 2023-06-01 19:22:58,307 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-06-01 19:22:58,308 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-01 19:22:58,309 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-01 19:22:58,309 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-01 19:22:28,459 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-06-01 19:22:28,475 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
om3_1    | 2023-06-01 19:22:17,706 [pool-26-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm2_1   | STARTUP_MSG:   host = fe03b6a14f2f/10.9.0.15
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | ************************************************************/
dn4_1    | 2023-06-01 19:21:47,644 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:47,651 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:47,720 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3d6edd59-5add-4b7b-8003-44199ccc7b59
dn4_1    | 2023-06-01 19:21:47,841 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3d6edd59-5add-4b7b-8003-44199ccc7b59: start RPC server
dn4_1    | 2023-06-01 19:21:47,870 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3d6edd59-5add-4b7b-8003-44199ccc7b59: GrpcService started, listening on 9858
dn4_1    | 2023-06-01 19:21:47,874 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3d6edd59-5add-4b7b-8003-44199ccc7b59: GrpcService started, listening on 9856
dn4_1    | 2023-06-01 19:21:47,882 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3d6edd59-5add-4b7b-8003-44199ccc7b59: GrpcService started, listening on 9857
dn4_1    | 2023-06-01 19:21:47,914 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3d6edd59-5add-4b7b-8003-44199ccc7b59 is started using port 9858 for RATIS
dn4_1    | 2023-06-01 19:21:47,914 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3d6edd59-5add-4b7b-8003-44199ccc7b59 is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-06-01 19:21:47,914 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3d6edd59-5add-4b7b-8003-44199ccc7b59 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-06-01 19:21:47,929 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3d6edd59-5add-4b7b-8003-44199ccc7b59: Started
dn4_1    | 2023-06-01 19:21:48,645 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:48,651 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:49,649 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:49,652 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:50,650 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:50,653 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:51,651 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:51,653 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:52,652 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:52,655 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:53,654 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:53,655 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:54,655 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:54,656 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:55,673 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:55,674 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:56,676 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:56,682 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:57,678 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:57,682 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:58,684 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:58,693 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:59,686 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:21:59,694 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:00,687 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:00,695 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:01,688 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:01,696 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:02,689 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:02,696 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:03,690 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:03,697 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:04,692 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:04,698 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:05,693 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-01 19:22:54,788 [pool-22-thread-1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-01 19:22:54,790 [pool-22-thread-1] INFO impl.RoleInfo: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: start 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState
dn1_1    | 2023-06-01 19:22:54,817 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-01 19:22:17,734 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-01 19:21:17,115 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl
dn3_1    | 2023-06-01 19:22:58,309 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-06-01 19:22:58,315 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-01 19:22:58,316 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
s3g_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
s3g_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
s3g_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
s3g_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
s3g_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
s3g_1    | , while invoking $Proxy118.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
s3g_1    | 2023-06-01 19:24:04,793 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/old1-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1    | 2023-06-01 19:24:06,039 [qtp384515747-16] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-06-01 19:24:06,383 [qtp384515747-16] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-06-01 19:22:54,817 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-01 19:22:54,839 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6286EE24275C,id=71839cc9-95e0-4895-b82f-6f2805ce4ff9
dn1_1    | 2023-06-01 19:22:54,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-01 19:22:54,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-01 19:22:54,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-01 19:22:54,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-01 19:22:55,000 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=77894454-4515-4db1-a815-6286ee24275c
dn1_1    | 2023-06-01 19:22:55,002 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=77894454-4515-4db1-a815-6286ee24275c.
dn1_1    | 2023-06-01 19:22:59,948 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO impl.FollowerState: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5158490518ns, electionTimeout:5129ms
dn1_1    | 2023-06-01 19:22:59,950 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO impl.RoleInfo: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: shutdown 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState
dn1_1    | 2023-06-01 19:22:59,950 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-01 19:22:59,958 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-01 19:22:59,961 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-FollowerState] INFO impl.RoleInfo: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: start 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om1_1    | 2023-06-01 19:22:28,476 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-01 19:22:59,975 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO impl.LeaderElection: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-01 19:22:59,976 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO impl.LeaderElection: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-06-01 19:22:59,976 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO impl.RoleInfo: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: shutdown 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1
om3_1    | 2023-06-01 19:22:17,737 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-01 19:21:17,730 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-01 19:22:05,699 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:06,695 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:06,699 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:07,696 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:07,700 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:08,697 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:28,476 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1  | 2023-06-01 19:21:53,193 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
om2_1    | 2023-06-01 19:24:04,851 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 2023-06-01 19:22:17,738 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-06-01 19:21:19,099 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-01 19:21:54,740 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-06-01 19:21:55,247 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-01 19:21:55,579 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-06-01 19:21:55,686 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-06-01 19:21:55,982 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm2_1   | 2023-06-01 19:21:55,988 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-06-01 19:21:59,438 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-01 19:22:00,295 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-01 19:22:01,280 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm2_1   | 2023-06-01 19:22:01,294 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-06-01 19:22:01,635 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | 2023-06-01 19:22:01,739 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:5dcb1cf7-eeac-4c03-a034-92235b5458e4
scm2_1   | 2023-06-01 19:22:02,188 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-06-01 19:22:17,739 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-01 19:22:17,741 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-01 19:22:17,873 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-06-01 19:22:17,873 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-06-01 19:22:18,119 [pool-26-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-06-01 19:24:32,435 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 Received prepare request with log index 25
om2_1    | 2023-06-01 19:24:32,438 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om2 waiting for index 25 to flush to OM DB and index 26 to flush to Ratis state machine.
om2_1    | 2023-06-01 19:24:37,447 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:26)
om2_1    | 2023-06-01 19:24:37,449 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 26 -> 26
om2_1    | 2023-06-01 19:24:37,449 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 26
om2_1    | 2023-06-01 19:24:37,450 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 26
om2_1    | 2023-06-01 19:24:37,453 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 26
om2_1    | 2023-06-01 19:24:37,459 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-26
om3_1    | 2023-06-01 19:22:18,145 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-01 19:22:18,411 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-06-01 19:22:18,413 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-06-01 19:22:18,659 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-06-01 19:22:18,753 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-06-01 19:22:18,753 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-06-01 19:22:20,126 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-01 19:22:02,590 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-01 19:22:02,608 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-06-01 19:22:02,628 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-01 19:22:02,629 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-06-01 19:22:02,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-06-01 19:22:02,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-06-01 19:22:02,646 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1   | 2023-06-01 19:22:02,650 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-01 19:22:02,667 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1   | 2023-06-01 19:22:02,681 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-06-01 19:22:02,755 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-06-01 19:22:02,785 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-06-01 19:22:02,789 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1   | 2023-06-01 19:22:04,283 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2_1   | 2023-06-01 19:22:04,298 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2_1   | 2023-06-01 19:22:04,298 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1   | 2023-06-01 19:22:04,298 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-06-01 19:22:04,298 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-01 19:22:04,327 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-01 19:22:04,387 [main] INFO server.RaftServer: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: addNew group-477B239F9CA9:[] returns group-477B239F9CA9:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm2_1   | 2023-06-01 19:22:04,527 [pool-16-thread-1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: new RaftServerImpl for group-477B239F9CA9:[] with SCMStateMachine:uninitialized
scm2_1   | 2023-06-01 19:22:04,546 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2_1   | 2023-06-01 19:22:04,551 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-01 19:22:04,552 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-01 19:22:04,553 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-06-01 19:22:04,555 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-01 19:22:28,525 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-01 19:22:28,525 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-01 19:22:59,977 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-06-01 19:22:58,316 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 2023-06-01 19:22:39,166 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-06-01 19:21:20,208 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0
dn4_1    | 2023-06-01 19:22:08,702 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:04,559 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 2023-06-01 19:22:20,126 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om1_1    | 2023-06-01 19:22:28,551 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:21:53,777 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
dn1_1    | 2023-06-01 19:22:59,977 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6286EE24275C with new leaderId: 71839cc9-95e0-4895-b82f-6f2805ce4ff9
dn3_1    | 2023-06-01 19:22:58,316 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm3_1   | 2023-06-01 19:22:39,167 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-01 19:21:21,784 [main] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: close
dn4_1    | 2023-06-01 19:22:09,698 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:04,633 [pool-16-thread-1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om3_1    | 2023-06-01 19:22:20,126 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
om2_1    | 2023-06-01 19:24:37,494 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 25 to file /data/metadata/current/prepareMarker
om2_1    | 2023-06-01 19:24:37,497 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 prepared at log index 25. Returning response txnID: 25
om1_1    | 2023-06-01 19:22:28,552 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 2023-06-01 19:21:53,778 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
dn1_1    | 2023-06-01 19:22:59,978 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: change Leader from null to 71839cc9-95e0-4895-b82f-6f2805ce4ff9 at term 1 for becomeLeader, leader elected after 6284ms
dn3_1    | 2023-06-01 19:22:58,316 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 2023-06-01 19:22:39,358 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-01 19:21:21,788 [main] INFO server.GrpcService: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown server GrpcServerProtocolService now
dn4_1    | 2023-06-01 19:22:09,702 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:04,642 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om3_1    | 2023-06-01 19:22:20,155 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
om2_1    |  with log index 25
om1_1    | 2023-06-01 19:22:28,562 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
recon_1  | 2023-06-01 19:21:53,920 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
dn1_1    | 2023-06-01 19:23:00,040 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-01 19:22:58,317 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-01 19:22:39,359 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-01 19:21:21,788 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: shutdown
dn4_1    | 2023-06-01 19:22:10,699 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:04,714 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om3_1    | 2023-06-01 19:22:20,168 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
om1_1    | 2023-06-01 19:22:28,577 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
recon_1  | 2023-06-01 19:21:53,921 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
dn1_1    | 2023-06-01 19:23:00,055 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-01 19:22:58,317 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-06-01 19:22:39,359 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-01 19:21:21,793 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-477B239F9CA9,id=4db01c28-2341-445f-9eff-7a55fcb1aabf
dn4_1    | 2023-06-01 19:22:10,703 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:04,720 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
om3_1    | 2023-06-01 19:22:22,889 [main] INFO reflections.Reflections: Reflections took 4320 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
om1_1    | 2023-06-01 19:22:28,579 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-06-01 19:21:54,047 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn1_1    | 2023-06-01 19:23:00,056 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-01 19:22:58,317 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 2023-06-01 19:22:39,360 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-01 19:21:21,793 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl
dn4_1    | 2023-06-01 19:22:11,701 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
scm2_1   | 2023-06-01 19:22:04,821 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-06-01 19:22:04,835 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2_1   | 2023-06-01 19:22:04,839 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-06-01 19:22:28,580 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-01 19:22:23,617 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-06-01 19:23:00,081 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-01 19:22:58,324 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderStateImpl
scm3_1   | 2023-06-01 19:22:39,360 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-06-01 19:21:21,868 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO impl.PendingRequests: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-06-01 19:22:11,704 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
om1_1    | 2023-06-01 19:22:28,580 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-01 19:21:54,167 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 226 milliseconds.
scm2_1   | 2023-06-01 19:22:06,074 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-01 19:22:23,691 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
dn1_1    | 2023-06-01 19:23:00,081 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-01 19:22:58,325 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-06-01 19:22:39,362 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1_1   | 2023-06-01 19:21:21,958 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO impl.StateMachineUpdater: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater: set stopIndex = 0
scm1_1   | 2023-06-01 19:21:21,960 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO impl.StateMachineUpdater: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater: Took a snapshot at index 0
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-06-01 19:22:28,592 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
recon_1  | 2023-06-01 19:21:55,561 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:49128: output error
scm2_1   | 2023-06-01 19:22:06,076 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-06-01 19:22:24,681 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-06-01 19:23:00,084 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-01 19:22:58,369 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-LeaderElection2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0: set configuration 0: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:39,362 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1_1   | 2023-06-01 19:21:22,024 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO impl.StateMachineUpdater: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
dn4_1    | 2023-06-01 19:22:12,702 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
om1_1    | 2023-06-01 19:22:28,593 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
recon_1  | 2023-06-01 19:21:55,576 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:40462: output error
scm2_1   | 2023-06-01 19:22:06,077 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-06-01 19:22:24,772 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-06-01 19:23:00,099 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-01 19:22:58,448 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A4B05FDCBDE0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/current/log_inprogress_0
scm3_1   | 2023-06-01 19:22:39,363 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1_1   | 2023-06-01 19:21:22,036 [main] INFO server.GrpcService: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown server GrpcServerProtocolService successfully
dn4_1    | 2023-06-01 19:22:12,705 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om1_1    | 2023-06-01 19:22:28,639 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
recon_1  | 2023-06-01 19:21:55,577 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:51770: output error
recon_1  | 2023-06-01 19:21:55,579 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:51768: output error
om3_1    | 2023-06-01 19:22:24,772 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
dn1_1    | 2023-06-01 19:23:00,111 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-01 19:22:58,457 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-A7FD084CD2E7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7/current/log_inprogress_0
scm3_1   | 2023-06-01 19:22:39,386 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1_1   | 2023-06-01 19:21:22,141 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: closes. applyIndex: 0
scm1_1   | 2023-06-01 19:21:22,163 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om1_1    | 2023-06-01 19:22:28,673 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
scm2_1   | 2023-06-01 19:22:06,086 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-06-01 19:22:25,089 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
dn1_1    | 2023-06-01 19:23:00,116 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO impl.RoleInfo: 71839cc9-95e0-4895-b82f-6f2805ce4ff9: start 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderStateImpl
dn3_1    | 2023-06-01 19:23:02,506 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO impl.FollowerState: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5187307978ns, electionTimeout:5134ms
scm3_1   | 2023-06-01 19:22:39,635 [main] INFO reflections.Reflections: Reflections took 206 ms to scan 3 urls, producing 112 keys and 252 values 
scm1_1   | 2023-06-01 19:21:22,169 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker close()
dn4_1    | 2023-06-01 19:22:13,704 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:55,585 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:54694: output error
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om1_1    | 2023-06-01 19:22:29,102 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
scm2_1   | 2023-06-01 19:22:06,110 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-06-01 19:22:25,089 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
dn1_1    | 2023-06-01 19:23:00,141 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-01 19:23:02,507 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState
scm3_1   | 2023-06-01 19:22:39,709 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3_1   | 2023-06-01 19:22:39,709 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
dn4_1    | 2023-06-01 19:22:13,711 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:55,586 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:49112: output error
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-06-01 19:22:29,138 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm2_1   | 2023-06-01 19:22:06,131 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om3_1    | 2023-06-01 19:22:25,115 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
dn1_1    | 2023-06-01 19:23:00,232 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-LeaderElection1] INFO server.RaftServer$Division: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C: set configuration 0: peers:[71839cc9-95e0-4895-b82f-6f2805ce4ff9|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-01 19:23:02,508 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-01 19:21:22,220 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4db01c28-2341-445f-9eff-7a55fcb1aabf: Stopped
scm3_1   | 2023-06-01 19:22:39,712 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3_1   | 2023-06-01 19:22:39,713 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-06-01 19:21:55,589 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:37680: output error
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om1_1    | 2023-06-01 19:22:29,151 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
scm2_1   | 2023-06-01 19:22:06,131 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-01 19:22:25,147 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@7f68a8a5d5cc
dn1_1    | 2023-06-01 19:23:00,280 [71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 71839cc9-95e0-4895-b82f-6f2805ce4ff9@group-6286EE24275C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/77894454-4515-4db1-a815-6286ee24275c/current/log_inprogress_0
dn3_1    | 2023-06-01 19:23:02,508 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-01 19:23:02,509 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3
dn4_1    | 2023-06-01 19:22:14,706 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:22,221 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-01 19:21:22,256 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9; layoutVersion=4; scmId=4db01c28-2341-445f-9eff-7a55fcb1aabf
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-06-01 19:22:29,473 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm2_1   | 2023-06-01 19:22:06,134 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-06-01 19:22:25,310 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
dn3_1    | 2023-06-01 19:23:02,519 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:39,751 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn4_1    | 2023-06-01 19:22:14,712 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:15,707 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:55,589 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:37686: output error
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 2023-06-01 19:22:29,475 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-06-01 19:22:25,327 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-01 19:23:02,523 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-01 19:22:39,764 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
dn4_1    | 2023-06-01 19:22:16,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:17,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:55,594 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:34482: output error
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 2023-06-01 19:22:29,759 [Listener at om1/9862] INFO util.log: Logging initialized @38281ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-06-01 19:22:06,216 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
om3_1    | 2023-06-01 19:22:25,414 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-01 19:23:02,523 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-01 19:22:39,765 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 2023-06-01 19:22:18,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:22,366 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:36998: output error
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 	... 1 more
om1_1    | 2023-06-01 19:22:30,490 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-06-01 19:22:07,604 [main] INFO reflections.Reflections: Reflections took 918 ms to scan 3 urls, producing 112 keys and 252 values 
om3_1    | 2023-06-01 19:22:25,415 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-01 19:23:02,577 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-06-01 19:22:39,774 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3_1   | 2023-06-01 19:22:39,803 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | /************************************************************
scm1_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at f0aca9c847b3/10.9.0.14
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 2023-06-01 19:22:21,407 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:30,552 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm2_1   | 2023-06-01 19:22:08,029 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
om3_1    | 2023-06-01 19:22:25,424 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-01 19:23:02,580 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.LeaderElection:   Response 0: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-69bc1289-e9f0-444d-ab3d-1548ab888401#0:FAIL-t1
dn4_1    | 2023-06-01 19:22:19,712 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,804 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | ************************************************************/
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:40448: output error
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 2023-06-01 19:22:22,408 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:30,598 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | 2023-06-01 19:22:08,048 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
om3_1    | 2023-06-01 19:22:25,450 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-01 19:23:02,581 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.LeaderElection: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3 ELECTION round 0: result REJECTED
dn4_1    | 2023-06-01 19:22:20,713 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,810 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:36126: output error
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:36138: output error
dn2_1    | 2023-06-01 19:22:23,410 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:30,611 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm2_1   | 2023-06-01 19:22:08,068 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
om3_1    | 2023-06-01 19:22:25,467 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn3_1    | 2023-06-01 19:23:02,582 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn4_1    | 2023-06-01 19:22:21,116 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm3_1   | 2023-06-01 19:22:39,811 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:36994: output error
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 2023-06-01 19:22:24,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:30,611 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-06-01 19:22:08,087 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
om3_1    | 2023-06-01 19:22:25,524 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 2023-06-01 19:23:02,582 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3
scm3_1   | 2023-06-01 19:22:39,814 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | 2023-06-01 19:21:34,591 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:45554: output error
dn2_1    | 2023-06-01 19:22:25,412 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:30,612 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-06-01 19:22:08,361 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om3_1    | 2023-06-01 19:22:25,552 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
scm3_1   | 2023-06-01 19:22:39,814 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | /************************************************************
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
recon_1  | 2023-06-01 19:21:55,597 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:34494: output error
dn2_1    | 2023-06-01 19:22:25,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om1_1    | 2023-06-01 19:22:31,144 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm2_1   | 2023-06-01 19:22:08,415 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 2023-06-01 19:23:02,591 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-LeaderElection3] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState
dn3_1    | 2023-06-01 19:23:02,638 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-01 19:22:39,820 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
recon_1  | 2023-06-01 19:21:55,603 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:55008: output error
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
om1_1    | 2023-06-01 19:22:31,146 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-06-01 19:22:08,416 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-01 19:22:08,458 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
dn3_1    | 2023-06-01 19:23:02,638 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-01 19:22:25,673 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm3_1   | 2023-06-01 19:22:39,820 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | STARTUP_MSG:   host = f0aca9c847b3/10.9.0.14
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-06-01 19:22:26,414 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:31,511 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-06-01 19:22:31,511 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 2023-06-01 19:23:07,692 [grpc-default-executor-2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: receive requestVote(ELECTION, 69bc1289-e9f0-444d-ab3d-1548ab888401, group-1C4731943483, 2, (t:0, i:0))
om3_1    | 2023-06-01 19:22:25,698 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-01 19:22:39,870 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | STARTUP_MSG:   args = []
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-06-01 19:22:27,414 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:31,513 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
scm2_1   | 2023-06-01 19:22:08,626 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 2023-06-01 19:23:07,694 [grpc-default-executor-2] INFO impl.VoteContext: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FOLLOWER: accept ELECTION from 69bc1289-e9f0-444d-ab3d-1548ab888401: our priority 0 <= candidate's priority 1
om3_1    | 2023-06-01 19:22:25,724 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-06-01 19:22:39,895 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1   | STARTUP_MSG:   version = 1.3.0
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om1_1    | 2023-06-01 19:22:31,831 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-06-01 19:22:08,626 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 2023-06-01 19:23:07,694 [grpc-default-executor-2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:69bc1289-e9f0-444d-ab3d-1548ab888401
om3_1    | 2023-06-01 19:22:25,738 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-01 19:22:39,954 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn2_1    | 2023-06-01 19:22:28,416 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:31,835 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-06-01 19:22:08,649 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 2023-06-01 19:23:07,694 [grpc-default-executor-2] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: shutdown 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState
om3_1    | 2023-06-01 19:22:25,746 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-06-01 19:22:39,977 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
dn5_1    | 	... 12 more
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | 2023-06-01 19:22:29,417 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:33,317 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 1, (t:0, i:~))
scm2_1   | 2023-06-01 19:22:08,649 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 2023-06-01 19:23:07,694 [grpc-default-executor-2] INFO impl.RoleInfo: 1d53eca5-989b-4da6-8a3f-f976f64c30fa: start 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState
dn3_1    | 2023-06-01 19:23:07,695 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO impl.FollowerState: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState was interrupted
dn3_1    | 2023-06-01 19:23:07,723 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-01 19:22:25,752 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | ************************************************************/
dn2_1    | 2023-06-01 19:22:30,419 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:33,342 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
scm2_1   | 2023-06-01 19:22:08,675 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-06-01 19:23:07,724 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-01 19:22:39,980 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
om3_1    | 2023-06-01 19:22:25,792 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-01 19:21:46,189 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-01 19:21:34,608 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-06-01 19:22:31,420 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:22:33,369 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: accept ELECTION from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-01 19:22:33,373 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om3
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 2023-06-01 19:22:32,422 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,989 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
om3_1    | 2023-06-01 19:22:25,793 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | java.net.SocketTimeoutException: Call From dd0e15ce78c9/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39478 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-01 19:21:34,715 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-01 19:22:08,706 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2_1   | 2023-06-01 19:22:08,712 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn3_1    | 2023-06-01 19:23:07,803 [grpc-default-executor-2] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483 replies to ELECTION vote request: 69bc1289-e9f0-444d-ab3d-1548ab888401<-1d53eca5-989b-4da6-8a3f-f976f64c30fa#0:OK-t2. Peer's state: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483:t2, leader=null, voted=69bc1289-e9f0-444d-ab3d-1548ab888401, raftlog=Memoized:1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:33,375 [grpc-default-executor-1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
dn2_1    | 2023-06-01 19:22:33,423 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:39,995 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:39,997 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-01 19:22:42,069 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-01 19:22:42,236 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 2023-06-01 19:23:08,153 [1d53eca5-989b-4da6-8a3f-f976f64c30fa-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1C4731943483 with new leaderId: 69bc1289-e9f0-444d-ab3d-1548ab888401
om1_1    | 2023-06-01 19:22:33,378 [grpc-default-executor-1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
dn2_1    | 2023-06-01 19:22:34,429 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:08,741 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om3_1    | 2023-06-01 19:22:25,805 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm3_1   | 2023-06-01 19:22:42,409 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
dn3_1    | 2023-06-01 19:23:08,154 [1d53eca5-989b-4da6-8a3f-f976f64c30fa-server-thread1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: change Leader from null to 69bc1289-e9f0-444d-ab3d-1548ab888401 at term 2 for appendEntries, leader elected after 10957ms
om1_1    | 2023-06-01 19:22:33,379 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-01 19:22:35,430 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:08,928 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-06-01 19:21:34,782 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om3_1    | 2023-06-01 19:22:25,900 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm3_1   | 2023-06-01 19:22:42,915 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn3_1    | 2023-06-01 19:23:08,203 [1d53eca5-989b-4da6-8a3f-f976f64c30fa-server-thread1] INFO server.RaftServer$Division: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483: set configuration 0: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:33,382 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState was interrupted
dn2_1    | 2023-06-01 19:22:36,431 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:09,042 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-06-01 19:21:34,809 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om3_1    | 2023-06-01 19:22:25,900 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
om1_1    | 2023-06-01 19:22:33,401 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-01 19:22:33,515 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:OK-t1. Peer's state: om1@group-D66704EFC61C:t1, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-01 19:22:09,279 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 2023-06-01 19:22:25,901 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-01 19:23:08,207 [1d53eca5-989b-4da6-8a3f-f976f64c30fa-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-01 19:23:08,213 [1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1d53eca5-989b-4da6-8a3f-f976f64c30fa@group-1C4731943483-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/current/log_inprogress_0
om1_1    | 2023-06-01 19:22:33,515 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om3 at current term 1
dn2_1    | 2023-06-01 19:22:37,433 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:09,422 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
dn4_1    | 	... 1 more
scm3_1   | 2023-06-01 19:22:42,958 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-01 19:21:34,897 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 2023-06-01 19:22:25,933 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
om1_1    | 2023-06-01 19:22:33,533 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t1. Peer's state: om1@group-D66704EFC61C:t1, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:34,234 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-6326735908231919716/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om1_1    | 2023-06-01 19:22:34,317 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-06-01 19:22:34,318 [Listener at om1/9862] INFO server.Server: Started @42840ms
om1_1    | 2023-06-01 19:22:34,368 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-06-01 19:22:34,371 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-01 19:22:34,379 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-06-01 19:22:34,380 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-01 19:22:34,404 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-06-01 19:22:34,487 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om3 at term 1 for appendEntries, leader elected after 12564ms
om1_1    | 2023-06-01 19:22:34,552 [om1-server-thread2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-01 19:22:34,570 [om1-server-thread2] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om1_1    | 2023-06-01 19:22:35,534 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-06-01 19:22:35,623 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2084e65a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-06-01 19:22:35,708 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om1_1    | 2023-06-01 19:22:38,468 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-06-01 19:23:33,287 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om1_1    | 2023-06-01 19:23:36,949 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om1_1    | 2023-06-01 19:23:49,079 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om1_1    | 2023-06-01 19:24:04,843 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-06-01 19:22:09,425 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm2_1   | 2023-06-01 19:22:09,475 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2_1   | 2023-06-01 19:22:09,525 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:09,530 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1   | 2023-06-01 19:22:13,904 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-01 19:22:14,150 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-01 19:22:14,474 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1   | 2023-06-01 19:22:14,900 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-01 19:22:14,975 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-01 19:22:14,977 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-06-01 19:22:15,466 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-01 19:22:15,492 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-01 19:22:15,493 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 2023-06-01 19:22:15,712 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-06-01 19:22:15,721 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
scm2_1   | Running                        true
scm2_1   | Container Balancer Configuration values:
scm3_1   | 2023-06-01 19:22:42,960 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3_1   | 2023-06-01 19:22:43,102 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-01 19:22:43,122 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-06-01 19:22:43,125 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3_1   | 2023-06-01 19:22:43,258 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3_1   | 2023-06-01 19:22:43,274 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3_1   | Container Balancer status:
scm3_1   | Key                            Value
scm3_1   | Running                        true
scm3_1   | Container Balancer Configuration values:
dn2_1    | 2023-06-01 19:22:38,435 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-06-01 19:24:32,444 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 Received prepare request with log index 25
om1_1    | 2023-06-01 19:24:32,449 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om1 waiting for index 25 to flush to OM DB and index 26 to flush to Ratis state machine.
om1_1    | 2023-06-01 19:24:37,450 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:26)
om1_1    | 2023-06-01 19:24:37,452 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 26 -> 26
dn2_1    | 2023-06-01 19:22:39,436 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-01 19:24:37,452 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 26
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:34,898 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-06-01 19:21:36,210 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-01 19:21:36,582 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-01 19:21:37,178 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
dn2_1    | 2023-06-01 19:22:40,437 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
recon_1  | 2023-06-01 19:21:55,813 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
scm1_1   | 2023-06-01 19:21:37,180 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-06-01 19:21:37,345 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-06-01 19:21:37,372 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:4db01c28-2341-445f-9eff-7a55fcb1aabf
scm1_1   | 2023-06-01 19:21:37,490 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-06-01 19:21:37,712 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-01 19:24:37,452 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 26
dn2_1    | 2023-06-01 19:22:41,445 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:21,714 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:22,715 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | Max Size to Move per Iteration                     500GB
dn2_1    | 2023-06-01 19:22:42,446 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 
scm2_1   | 2023-06-01 19:22:15,721 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
dn2_1    | 2023-06-01 19:22:47,113 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om3_1    | 2023-06-01 19:22:25,970 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-01 19:22:25,970 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-06-01 19:21:37,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-01 19:21:37,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-01 19:22:52,569 [Command processor thread] INFO server.RaftServer: 69bc1289-e9f0-444d-ab3d-1548ab888401: addNew group-E9508D8D4862:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-E9508D8D4862:java.util.concurrent.CompletableFuture@1693002a[Not completed]
scm3_1   | Key                                                Value
scm3_1   | Threshold                                          10
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
scm3_1   | Max Size Leaving Source per Iteration              26GB
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-06-01 19:21:37,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-01 19:21:37,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-06-01 19:22:52,805 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401: new RaftServerImpl for group-E9508D8D4862:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm3_1   | 
scm3_1   | 2023-06-01 19:22:43,275 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-06-01 19:22:43,284 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1   | 2023-06-01 19:22:43,296 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3_1   | 2023-06-01 19:22:43,306 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | 2023-06-01 19:22:43,321 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-06-01 19:21:37,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-06-01 19:21:37,719 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
dn2_1    | 2023-06-01 19:22:52,824 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-01 19:22:52,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-01 19:24:37,467 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-26
om1_1    | 2023-06-01 19:24:37,502 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 26
om1_1    | 2023-06-01 19:24:37,510 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 25 to file /data/metadata/current/prepareMarker
om1_1    | 2023-06-01 19:24:37,524 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 prepared at log index 25. Returning response txnID: 25
om1_1    |  with log index 25
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-06-01 19:21:37,723 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-01 19:21:37,724 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-01 19:21:37,725 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn2_1    | 2023-06-01 19:22:52,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-01 19:21:37,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-01 19:21:37,758 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 2023-06-01 19:22:52,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-01 19:22:15,722 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-06-01 19:22:15,733 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn4_1    | 2023-06-01 19:22:23,715 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:24,717 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-01 19:21:37,759 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3_1   | 2023-06-01 19:22:43,337 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/in_use.lock acquired by nodename 7@330dd2bcef66
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 2023-06-01 19:22:52,837 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-01 19:22:25,993 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:25,997 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1    | 2023-06-01 19:22:26,013 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-01 19:22:26,021 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 2023-06-01 19:21:38,345 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-06-01 19:22:43,390 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 has been successfully formatted.
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 2023-06-01 19:22:52,838 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-01 19:22:52,909 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: ConfigurationManager, init=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 2023-06-01 19:21:38,352 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-06-01 19:22:43,402 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-01 19:22:52,921 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-01 19:22:26,022 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-01 19:22:26,023 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-06-01 19:22:26,034 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-06-01 19:21:38,353 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-06-01 19:22:43,430 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-01 19:22:43,434 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-01 19:22:25,719 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:25,796 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-01 19:22:26,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:27,723 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:22:26,034 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-06-01 19:22:26,042 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-01 19:21:38,357 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-06-01 19:22:43,441 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-06-01 19:22:43,445 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-01 19:22:28,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:29,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-01 19:22:26,042 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-06-01 19:22:26,133 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
scm1_1   | 2023-06-01 19:21:38,357 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-01 19:22:43,466 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-01 19:22:43,492 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-01 19:22:15,751 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2_1   | 2023-06-01 19:22:15,753 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 does not exist. Creating ...
scm2_1   | 2023-06-01 19:22:15,783 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/in_use.lock acquired by nodename 6@fe03b6a14f2f
scm2_1   | 2023-06-01 19:22:15,845 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9 has been successfully formatted.
om3_1    | 2023-06-01 19:22:26,398 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-06-01 19:22:26,410 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm1_1   | 2023-06-01 19:21:38,363 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-01 19:22:43,497 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-06-01 19:22:43,525 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9
scm2_1   | 2023-06-01 19:22:15,859 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-06-01 19:22:15,888 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-06-01 19:22:15,889 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-01 19:22:15,890 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-06-01 19:22:26,412 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-06-01 19:21:38,368 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: found a subdirectory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9
scm3_1   | 2023-06-01 19:22:43,535 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-01 19:22:43,536 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2_1   | 2023-06-01 19:22:15,892 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-06-01 19:22:15,900 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-01 19:22:15,919 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-01 19:22:15,920 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-06-01 19:22:26,744 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-06-01 19:22:26,747 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-06-01 19:21:38,376 [main] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: addNew group-477B239F9CA9:[] returns group-477B239F9CA9:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
scm3_1   | 2023-06-01 19:22:43,551 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-01 19:22:43,558 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-06-01 19:22:15,941 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9
om3_1    | 2023-06-01 19:22:26,997 [Listener at om3/9862] INFO util.log: Logging initialized @35933ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-06-01 19:22:27,788 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-06-01 19:21:38,404 [pool-16-thread-1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf: new RaftServerImpl for group-477B239F9CA9:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-06-01 19:22:43,558 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-06-01 19:22:43,562 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-06-01 19:22:27,824 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm2_1   | 2023-06-01 19:22:15,945 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-06-01 19:22:15,949 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-06-01 19:22:43,563 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-01 19:22:53,022 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om3_1    | 2023-06-01 19:22:27,895 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-01 19:22:53,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-01 19:22:53,124 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-01 19:22:30,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:31,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-06-01 19:22:53,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-01 19:22:53,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-01 19:22:32,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:33,733 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:22:53,673 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-01 19:22:53,701 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-01 19:22:34,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:35,736 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
dn2_1    | 2023-06-01 19:22:53,706 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-01 19:22:53,712 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-01 19:22:36,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-06-01 19:22:53,724 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-01 19:22:53,733 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3a3b4b03-96c4-47b7-9040-e9508d8d4862 does not exist. Creating ...
dn4_1    | 2023-06-01 19:22:37,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:38,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn2_1    | 2023-06-01 19:22:53,789 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a3b4b03-96c4-47b7-9040-e9508d8d4862/in_use.lock acquired by nodename 7@8b2845d7a78b
dn2_1    | 2023-06-01 19:22:53,864 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3a3b4b03-96c4-47b7-9040-e9508d8d4862 has been successfully formatted.
dn4_1    | 2023-06-01 19:22:39,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:40,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn2_1    | 2023-06-01 19:22:53,931 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-E9508D8D4862: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-01 19:22:53,936 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-01 19:22:41,746 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:42,748 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn2_1    | 2023-06-01 19:22:54,029 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-01 19:22:54,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-01 19:22:47,118 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn2_1    | 2023-06-01 19:22:54,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-01 19:22:54,043 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-01 19:22:52,526 [Command processor thread] INFO server.RaftServer: 3d6edd59-5add-4b7b-8003-44199ccc7b59: addNew group-4312D758E1FC:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-4312D758E1FC:java.util.concurrent.CompletableFuture@3614c508[Not completed]
dn4_1    | 2023-06-01 19:22:52,761 [pool-22-thread-1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59: new RaftServerImpl for group-4312D758E1FC:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn2_1    | 2023-06-01 19:22:54,053 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-01 19:22:54,115 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-01 19:22:52,806 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-06-01 19:22:27,923 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm2_1   | 2023-06-01 19:22:15,954 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-01 19:22:15,954 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-06-01 19:22:15,955 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-01 19:22:54,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-06-01 19:22:15,963 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-01 19:21:38,406 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn2_1    | 2023-06-01 19:22:54,177 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3a3b4b03-96c4-47b7-9040-e9508d8d4862
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-06-01 19:22:52,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-01 19:22:52,820 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-06-01 19:22:43,571 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-06-01 19:22:27,928 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-06-01 19:22:15,964 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-01 19:21:38,410 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-01 19:22:54,181 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:39478 remote=scm1/10.9.0.14:9861]
dn4_1    | 2023-06-01 19:22:52,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-01 19:22:52,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-01 19:22:43,642 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-06-01 19:22:27,932 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-06-01 19:22:15,965 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-01 19:21:38,412 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-01 19:22:54,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-06-01 19:22:52,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-01 19:22:43,646 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-01 19:21:38,412 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn2_1    | 2023-06-01 19:22:54,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn4_1    | 2023-06-01 19:22:52,894 [pool-22-thread-1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: ConfigurationManager, init=-1: peers:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-06-01 19:22:16,004 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-06-01 19:22:28,102 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
scm3_1   | 2023-06-01 19:22:43,656 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-01 19:21:38,413 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-01 19:22:54,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn4_1    | 2023-06-01 19:22:52,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 2023-06-01 19:22:16,005 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-06-01 19:22:28,105 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm3_1   | 2023-06-01 19:22:43,658 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-06-01 19:21:38,413 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-01 19:22:54,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
dn4_1    | 2023-06-01 19:22:53,024 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1   | 2023-06-01 19:22:16,010 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-06-01 19:22:16,024 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-06-01 19:22:43,693 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-01 19:21:38,437 [pool-16-thread-1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-01 19:22:54,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1  | java.nio.channels.ClosedChannelException
dn4_1    | 2023-06-01 19:22:53,038 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-01 19:22:16,048 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-01 19:22:28,423 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-06-01 19:22:43,693 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-01 19:21:38,448 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn2_1    | 2023-06-01 19:22:54,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-06-01 19:22:53,106 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm2_1   | 2023-06-01 19:22:16,048 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-01 19:22:28,428 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-06-01 19:22:43,706 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm1_1   | 2023-06-01 19:21:38,452 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-06-01 19:22:43,715 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: changes role from      null to FOLLOWER at term 0 for startInitializing
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-06-01 19:22:53,210 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-06-01 19:22:16,063 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:28,430 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-06-01 19:22:54,202 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-06-01 19:22:43,724 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-477B239F9CA9,id=5e8a32ef-e4cd-4aae-aa6b-c9f950180f03
scm1_1   | 2023-06-01 19:21:38,453 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn4_1    | 2023-06-01 19:22:53,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-01 19:22:16,064 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: changes role from      null to FOLLOWER at term 0 for startInitializing
om3_1    | 2023-06-01 19:22:28,501 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-01 19:22:54,264 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | 2023-06-01 19:22:43,734 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-01 19:21:38,482 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn4_1    | 2023-06-01 19:22:53,446 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-01 19:22:16,068 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-477B239F9CA9,id=5dcb1cf7-eeac-4c03-a034-92235b5458e4
om3_1    | 2023-06-01 19:22:28,513 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-06-01 19:22:54,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-06-01 19:22:43,735 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-06-01 19:21:38,486 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn4_1    | 2023-06-01 19:22:53,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-06-01 19:22:16,073 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-06-01 19:22:30,709 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-611805428188627735/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
dn2_1    | 2023-06-01 19:22:54,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-06-01 19:22:43,740 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-06-01 19:22:43,753 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-06-01 19:22:43,780 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: start RPC server
scm1_1   | 2023-06-01 19:21:38,489 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-06-01 19:22:30,761 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn4_1    | 2023-06-01 19:22:53,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-06-01 19:22:16,078 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
om3_1    | 2023-06-01 19:22:30,762 [Listener at om3/9862] INFO server.Server: Started @39698ms
dn2_1    | 2023-06-01 19:22:54,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-01 19:22:54,329 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-01 19:22:54,331 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-01 19:21:38,827 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-01 19:22:53,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-06-01 19:22:16,081 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 2023-06-01 19:22:54,342 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: start as a follower, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-06-01 19:22:44,057 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: GrpcService started, listening on 9894
om3_1    | 2023-06-01 19:22:30,764 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-06-01 19:21:38,827 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-01 19:22:53,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-01 19:22:16,084 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-01 19:22:54,344 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-06-01 19:22:44,086 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: Started
om3_1    | 2023-06-01 19:22:30,764 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-06-01 19:21:38,828 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-01 19:22:53,473 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/54391af0-7339-4d61-8cba-4312d758e1fc does not exist. Creating ...
scm2_1   | 2023-06-01 19:22:16,091 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: start RPC server
dn5_1    | 2023-06-01 19:21:46,190 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:54,348 [pool-22-thread-1] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm3_1   | 2023-06-01 19:22:44,166 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
om3_1    | 2023-06-01 19:22:30,775 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm1_1   | 2023-06-01 19:21:38,832 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-01 19:22:53,541 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/54391af0-7339-4d61-8cba-4312d758e1fc/in_use.lock acquired by nodename 6@c31458b395e2
scm2_1   | 2023-06-01 19:22:16,258 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: GrpcService started, listening on 9894
dn5_1    | 2023-06-01 19:21:46,191 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From dd0e15ce78c9/10.9.0.21 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm3_1   | 2023-06-01 19:22:45,367 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: receive installSnapshot: 4db01c28-2341-445f-9eff-7a55fcb1aabf->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0-t2,notify:(t:1, i:0)
om3_1    | 2023-06-01 19:22:30,780 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-01 19:21:38,833 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-01 19:22:53,581 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/54391af0-7339-4d61-8cba-4312d758e1fc has been successfully formatted.
scm2_1   | 2023-06-01 19:22:16,296 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5dcb1cf7-eeac-4c03-a034-92235b5458e4: Started
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 2023-06-01 19:22:54,354 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1   | 2023-06-01 19:22:45,383 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
om3_1    | 2023-06-01 19:22:30,868 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm1_1   | 2023-06-01 19:21:38,838 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
dn4_1    | 2023-06-01 19:22:53,614 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-4312D758E1FC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm2_1   | 2023-06-01 19:22:16,389 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 2023-06-01 19:22:54,356 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm3_1   | 2023-06-01 19:22:45,383 [grpc-default-executor-0] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: change Leader from null to 4db01c28-2341-445f-9eff-7a55fcb1aabf at term 2 for installSnapshot, leader elected after 6220ms
om3_1    | 2023-06-01 19:22:30,928 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
scm1_1   | 2023-06-01 19:21:38,838 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-01 19:22:53,708 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-06-01 19:22:21,185 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: receive installSnapshot: 4db01c28-2341-445f-9eff-7a55fcb1aabf->5dcb1cf7-eeac-4c03-a034-92235b5458e4#0-t2,notify:(t:1, i:0)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm3_1   | 2023-06-01 19:22:45,388 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: Received notification to install snapshot at index 0
om3_1    | 2023-06-01 19:22:31,071 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057744162ns, electionTimeout:5046ms
scm1_1   | 2023-06-01 19:21:38,839 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn4_1    | 2023-06-01 19:22:53,821 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-01 19:22:53,829 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-01 19:22:54,380 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E9508D8D4862,id=69bc1289-e9f0-444d-ab3d-1548ab888401
scm2_1   | 2023-06-01 19:22:21,228 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1   | 2023-06-01 19:22:45,397 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
om3_1    | 2023-06-01 19:22:31,119 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
scm1_1   | 2023-06-01 19:21:38,894 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-06-01 19:22:53,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 2023-06-01 19:22:54,385 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-06-01 19:22:45,671 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set new configuration index: 19
scm2_1   | 2023-06-01 19:22:21,228 [grpc-default-executor-0] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: change Leader from null to 4db01c28-2341-445f-9eff-7a55fcb1aabf at term 2 for installSnapshot, leader elected after 16412ms
om3_1    | 2023-06-01 19:22:31,134 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-01 19:21:39,338 [main] INFO reflections.Reflections: Reflections took 338 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-06-01 19:22:53,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 2023-06-01 19:22:54,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | configurationEntry {
scm2_1   | 2023-06-01 19:22:21,237 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: Received notification to install snapshot at index 0
scm2_1   | 2023-06-01 19:22:21,238 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm1_1   | 2023-06-01 19:21:39,517 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn4_1    | 2023-06-01 19:22:53,867 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 2023-06-01 19:22:54,393 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   |   peers {
scm2_1   | 2023-06-01 19:22:21,789 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set new configuration index: 1
om3_1    | 2023-06-01 19:22:31,158 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-01 19:21:39,518 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn4_1    | 2023-06-01 19:22:53,878 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-01 19:22:53,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   |     id: "5dcb1cf7-eeac-4c03-a034-92235b5458e4"
scm2_1   | configurationEntry {
om3_1    | 2023-06-01 19:22:31,173 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-06-01 19:21:39,522 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
dn4_1    | 2023-06-01 19:22:53,919 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/54391af0-7339-4d61-8cba-4312d758e1fc
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 2023-06-01 19:22:54,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   |     address: "scm2:9894"
scm2_1   |   peers {
om3_1    | 2023-06-01 19:22:31,276 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2084e65a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-06-01 19:21:39,523 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1_1   | 2023-06-01 19:21:39,576 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 2023-06-01 19:22:54,515 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3a3b4b03-96c4-47b7-9040-e9508d8d4862
scm3_1   |     startupRole: FOLLOWER
scm2_1   |     id: "4db01c28-2341-445f-9eff-7a55fcb1aabf"
om3_1    | 2023-06-01 19:22:31,281 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:31,616 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
dn4_1    | 2023-06-01 19:22:53,925 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 2023-06-01 19:22:54,518 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3a3b4b03-96c4-47b7-9040-e9508d8d4862.
scm3_1   |   }
scm2_1   |     address: "scm1:9894"
om3_1    | 2023-06-01 19:22:31,626 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-01 19:21:39,606 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
dn4_1    | 2023-06-01 19:22:53,926 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 2023-06-01 19:22:54,522 [Command processor thread] INFO server.RaftServer: 69bc1289-e9f0-444d-ab3d-1548ab888401: addNew group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-A4B05FDCBDE0:java.util.concurrent.CompletableFuture@2a2db069[Not completed]
scm3_1   |   peers {
scm2_1   |     startupRole: FOLLOWER
om3_1    | 2023-06-01 19:22:31,639 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-01 19:21:39,607 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 2023-06-01 19:22:53,930 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 2023-06-01 19:22:54,547 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401: new RaftServerImpl for group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm3_1   |     id: "4db01c28-2341-445f-9eff-7a55fcb1aabf"
scm3_1   |     address: "scm1:9894"
om3_1    | 2023-06-01 19:22:31,641 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
scm1_1   | 2023-06-01 19:21:39,611 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
dn4_1    | 2023-06-01 19:22:53,932 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 2023-06-01 19:22:54,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm2_1   |   }
om3_1    | 2023-06-01 19:22:33,321 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
scm1_1   | 2023-06-01 19:21:39,641 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn4_1    | 2023-06-01 19:22:53,932 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   |     startupRole: FOLLOWER
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 2023-06-01 19:22:54,557 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | }
om3_1    | 2023-06-01 19:22:33,323 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 1
scm1_1   | 2023-06-01 19:21:39,642 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 2023-06-01 19:22:53,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   |   }
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 2023-06-01 19:22:54,558 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   |  from snapshot
om3_1    | 2023-06-01 19:22:33,385 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t1. Peer's state: om3@group-D66704EFC61C:t1, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:21:39,649 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn4_1    | 2023-06-01 19:22:53,940 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | }
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-06-01 19:22:54,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 2023-06-01 19:22:21,811 [grpc-default-executor-0] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:33,627 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm1_1   | 2023-06-01 19:21:39,650 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn4_1    | 2023-06-01 19:22:53,940 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   |  from snapshot
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-06-01 19:22:54,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 2023-06-01 19:22:21,840 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: reply installSnapshot: 4db01c28-2341-445f-9eff-7a55fcb1aabf<-5dcb1cf7-eeac-4c03-a034-92235b5458e4#0:OK-t0,ALREADY_INSTALLED
om3_1    | 2023-06-01 19:22:33,627 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:OK-t1
scm1_1   | 2023-06-01 19:21:39,653 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn4_1    | 2023-06-01 19:22:54,012 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | 2023-06-01 19:22:45,689 [grpc-default-executor-0] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 19: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-06-01 19:22:54,563 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 2023-06-01 19:22:21,933 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: Completed INSTALL_SNAPSHOT, lastRequest: 4db01c28-2341-445f-9eff-7a55fcb1aabf->5dcb1cf7-eeac-4c03-a034-92235b5458e4#0-t2,notify:(t:1, i:0)
om3_1    | 2023-06-01 19:22:33,627 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
scm1_1   | 2023-06-01 19:21:39,654 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
dn4_1    | 2023-06-01 19:22:54,029 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-06-01 19:22:45,702 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: reply installSnapshot: 4db01c28-2341-445f-9eff-7a55fcb1aabf<-5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0:OK-t0,ALREADY_INSTALLED
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-06-01 19:22:54,567 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: ConfigurationManager, init=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om3_1    | 2023-06-01 19:22:33,627 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
scm1_1   | 2023-06-01 19:21:39,661 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn4_1    | 2023-06-01 19:22:54,031 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-06-01 19:22:45,723 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: Completed INSTALL_SNAPSHOT, lastRequest: 4db01c28-2341-445f-9eff-7a55fcb1aabf->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-06-01 19:22:45,823 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread1] INFO impl.RoleInfo: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: start 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-FollowerState
scm3_1   | 2023-06-01 19:22:45,846 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 2023-06-01 19:22:22,375 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO impl.RoleInfo: 5dcb1cf7-eeac-4c03-a034-92235b5458e4: start 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-FollowerState
scm1_1   | 2023-06-01 19:21:39,662 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
dn4_1    | 2023-06-01 19:22:54,036 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm3_1   | 2023-06-01 19:22:45,853 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread1] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: inconsistency entries. Reply:4db01c28-2341-445f-9eff-7a55fcb1aabf<-5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
dn2_1    | 2023-06-01 19:22:54,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 2023-06-01 19:22:22,398 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
om3_1    | 2023-06-01 19:22:33,644 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-06-01 19:21:39,708 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn4_1    | 2023-06-01 19:22:54,127 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-01 19:22:45,855 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
dn2_1    | 2023-06-01 19:22:54,569 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-06-01 19:22:22,401 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: inconsistency entries. Reply:4db01c28-2341-445f-9eff-7a55fcb1aabf<-5dcb1cf7-eeac-4c03-a034-92235b5458e4#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om3_1    | 2023-06-01 19:22:33,644 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om3 at term 1 for becomeLeader, leader elected after 14985ms
scm1_1   | 2023-06-01 19:21:39,743 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn4_1    | 2023-06-01 19:22:54,128 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-01 19:22:45,855 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: inconsistency entries. Reply:4db01c28-2341-445f-9eff-7a55fcb1aabf<-5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
dn2_1    | 2023-06-01 19:22:54,573 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-06-01 19:22:22,476 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
om3_1    | 2023-06-01 19:22:33,688 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-01 19:21:39,808 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1   | 2023-06-01 19:21:39,854 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-06-01 19:22:45,879 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-01 19:22:54,575 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
scm2_1   | 2023-06-01 19:22:22,476 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: inconsistency entries. Reply:4db01c28-2341-445f-9eff-7a55fcb1aabf<-5dcb1cf7-eeac-4c03-a034-92235b5458e4#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om3_1    | 2023-06-01 19:22:33,712 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-06-01 19:21:39,858 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn4_1    | 2023-06-01 19:22:54,147 [pool-22-thread-1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: start as a follower, conf=-1: peers:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:45,879 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
scm2_1   | 2023-06-01 19:22:22,543 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:33,713 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-01 19:21:39,873 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn4_1    | 2023-06-01 19:22:54,154 [pool-22-thread-1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-01 19:22:54,575 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm3_1   | 2023-06-01 19:22:45,880 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 17: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
scm2_1   | 2023-06-01 19:22:22,544 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:33,735 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1_1   | 2023-06-01 19:21:39,885 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-01 19:22:54,158 [pool-22-thread-1] INFO impl.RoleInfo: 3d6edd59-5add-4b7b-8003-44199ccc7b59: start 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState
dn2_1    | 2023-06-01 19:22:54,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-06-01 19:22:45,881 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 19: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
scm2_1   | 2023-06-01 19:22:22,549 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: Starting segment from index:0
om3_1    | 2023-06-01 19:22:33,741 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-01 19:21:39,889 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
dn4_1    | 2023-06-01 19:22:54,170 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
scm1_1   | 2023-06-01 19:21:40,970 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-01 19:22:22,622 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn2_1    | 2023-06-01 19:22:54,580 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-01 19:22:45,905 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
om3_1    | 2023-06-01 19:22:33,745 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-01 19:22:54,176 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-01 19:21:40,993 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-06-01 19:22:54,582 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-06-01 19:22:45,961 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
dn5_1    | 	... 12 more
dn5_1    | 2023-06-01 19:21:47,120 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-04a10b92-c348-4a3b-b44c-dbfb15b56b0e/container.db to cache
dn4_1    | 2023-06-01 19:22:54,202 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4312D758E1FC,id=3d6edd59-5add-4b7b-8003-44199ccc7b59
scm2_1   | 2023-06-01 19:22:22,913 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0
scm1_1   | 2023-06-01 19:21:41,037 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
dn2_1    | 2023-06-01 19:22:54,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-01 19:22:46,190 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0
om3_1    | 2023-06-01 19:22:33,812 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn5_1    | 2023-06-01 19:21:47,148 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-ed78d68c-b0d3-43e0-aab9-477b239f9ca9/DS-04a10b92-c348-4a3b-b44c-dbfb15b56b0e/container.db for volume DS-04a10b92-c348-4a3b-b44c-dbfb15b56b0e
dn4_1    | 2023-06-01 19:22:54,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-06-01 19:22:22,945 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0 to /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_0-0
scm1_1   | 2023-06-01 19:21:41,163 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn2_1    | 2023-06-01 19:22:54,583 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-01 19:22:46,223 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0 to /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_0-0
om3_1    | 2023-06-01 19:22:33,834 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-06-01 19:21:47,149 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-06-01 19:22:54,215 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm2_1   | 2023-06-01 19:22:23,022 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_1
scm1_1   | 2023-06-01 19:21:41,169 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-06-01 19:22:54,589 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm3_1   | 2023-06-01 19:22:46,282 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_1
om3_1    | 2023-06-01 19:22:33,873 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1    | 2023-06-01 19:22:33,874 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-01 19:22:54,221 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm2_1   | 2023-06-01 19:22:23,157 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:21:41,170 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
dn2_1    | 2023-06-01 19:22:54,593 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | 2023-06-01 19:22:46,301 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om3_1    | 2023-06-01 19:22:33,875 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn5_1    | 2023-06-01 19:21:47,162 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn4_1    | 2023-06-01 19:22:54,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-06-01 19:22:23,190 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-06-01 19:21:41,214 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn2_1    | 2023-06-01 19:22:54,597 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/in_use.lock acquired by nodename 7@8b2845d7a78b
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-06-01 19:22:46,303 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
om3_1    | 2023-06-01 19:22:33,899 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-06-01 19:21:47,190 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-01 19:22:54,312 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=54391af0-7339-4d61-8cba-4312d758e1fc
scm2_1   | 2023-06-01 19:22:23,205 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1_1   | 2023-06-01 19:21:41,223 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-06-01 19:22:54,605 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 has been successfully formatted.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-06-01 19:22:46,312 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
om3_1    | 2023-06-01 19:22:33,900 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 2023-06-01 19:21:47,193 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:47,567 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 79bd37f0-b225-4c38-b396-23c942162c92
scm2_1   | 2023-06-01 19:22:23,217 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-06-01 19:21:41,224 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn2_1    | 2023-06-01 19:22:54,617 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-A4B05FDCBDE0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-06-01 19:22:46,312 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
om3_1    | 2023-06-01 19:22:33,900 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-01 19:22:54,314 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=54391af0-7339-4d61-8cba-4312d758e1fc.
dn5_1    | 2023-06-01 19:21:47,767 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 79bd37f0-b225-4c38-b396-23c942162c92: start RPC server
dn5_1    | 2023-06-01 19:21:47,793 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 79bd37f0-b225-4c38-b396-23c942162c92: GrpcService started, listening on 9858
scm2_1   | 2023-06-01 19:22:23,242 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn2_1    | 2023-06-01 19:22:54,617 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om3_1    | 2023-06-01 19:22:33,911 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-01 19:21:41,268 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn5_1    | 2023-06-01 19:21:47,799 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 79bd37f0-b225-4c38-b396-23c942162c92: GrpcService started, listening on 9856
scm2_1   | 2023-06-01 19:22:23,353 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 2023-06-01 19:22:54,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-06-01 19:22:33,911 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-06-01 19:22:33,940 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-06-01 19:21:47,805 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 79bd37f0-b225-4c38-b396-23c942162c92: GrpcService started, listening on 9857
scm2_1   | 2023-06-01 19:22:23,611 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 17: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn4_1    | 2023-06-01 19:22:59,288 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO impl.FollowerState: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5131366096ns, electionTimeout:5105ms
dn4_1    | 2023-06-01 19:22:59,292 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO impl.RoleInfo: 3d6edd59-5add-4b7b-8003-44199ccc7b59: shutdown 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState
dn4_1    | 2023-06-01 19:22:59,292 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-06-01 19:22:54,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-01 19:22:33,953 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-01 19:22:33,954 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn5_1    | 2023-06-01 19:21:47,834 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 79bd37f0-b225-4c38-b396-23c942162c92 is started using port 9858 for RATIS
scm2_1   | 2023-06-01 19:22:23,782 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 19: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-01 19:22:23,953 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861: skipped Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:36390
scm2_1   | 2023-06-01 19:22:24,140 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-477B239F9CA9:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-01 19:22:24,141 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
dn2_1    | 2023-06-01 19:22:54,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-01 19:21:41,274 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
dn5_1    | 2023-06-01 19:21:47,834 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 79bd37f0-b225-4c38-b396-23c942162c92 is started using port 9857 for RATIS_ADMIN
om3_1    | 2023-06-01 19:22:33,956 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 2023-06-01 19:22:24,539 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,335 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-01 19:22:46,336 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-06-01 19:22:46,470 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 21: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn2_1    | 2023-06-01 19:22:54,622 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-01 19:22:54,623 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-01 19:21:47,834 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 79bd37f0-b225-4c38-b396-23c942162c92 is started using port 9856 for RATIS_SERVER
om3_1    | 2023-06-01 19:22:33,957 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm2_1   | 2023-06-01 19:22:24,586 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:46,505 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-server-thread2] INFO server.RaftServer$Division: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9: set configuration 23: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:46,680 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-477B239F9CA9:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-06-01 19:22:46,685 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
dn2_1    | 2023-06-01 19:22:54,646 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-01 19:22:54,665 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-01 19:21:47,843 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-79bd37f0-b225-4c38-b396-23c942162c92: Started
scm2_1   | 2023-06-01 19:22:24,597 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3_1   | 2023-06-01 19:22:46,756 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,764 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:46,764 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn2_1    | 2023-06-01 19:22:54,666 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
dn2_1    | 2023-06-01 19:22:54,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-01 19:22:54,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | Container Balancer status:
dn5_1    | 2023-06-01 19:21:48,194 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,601 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
dn2_1    | 2023-06-01 19:22:54,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-01 19:22:54,667 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-01 19:22:54,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | Key                            Value
dn5_1    | 2023-06-01 19:21:48,202 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,763 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]].
dn4_1    | 2023-06-01 19:22:59,300 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-01 19:22:59,300 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-FollowerState] INFO impl.RoleInfo: 3d6edd59-5add-4b7b-8003-44199ccc7b59: start 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1
dn2_1    | 2023-06-01 19:22:54,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-01 19:22:54,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-01 19:22:54,669 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | Running                        true
dn5_1    | 2023-06-01 19:21:49,194 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,768 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-01 19:22:59,322 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO impl.LeaderElection: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-01 19:22:59,323 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO impl.LeaderElection: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-06-01 19:22:54,671 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | 2023-06-01 19:22:46,764 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3_1   | 2023-06-01 19:22:46,811 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]].
dn4_1    | 2023-06-01 19:22:59,324 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO impl.RoleInfo: 3d6edd59-5add-4b7b-8003-44199ccc7b59: shutdown 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1
dn4_1    | 2023-06-01 19:22:59,324 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-06-01 19:22:54,676 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-06-01 19:22:46,811 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:46,812 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,812 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om3_1    | 2023-06-01 19:22:33,957 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-01 19:22:33,960 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-06-01 19:22:33,972 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-06-01 19:22:54,680 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-01 19:22:54,683 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | Container Balancer Configuration values:
scm1_1   | Key                                                Value
scm3_1   | 2023-06-01 19:22:46,834 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,834 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:46,835 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,835 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn2_1    | 2023-06-01 19:22:54,683 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-01 19:22:54,683 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | Threshold                                          10
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | Max Size to Move per Iteration                     500GB
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-06-01 19:22:54,685 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: start as a follower, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:21:49,202 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:46,836 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,836 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:46,849 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]].
scm3_1   | 2023-06-01 19:22:46,857 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn2_1    | 2023-06-01 19:22:54,685 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-01 19:22:54,687 [pool-22-thread-1] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState
scm1_1   | Max Size Entering Target per Iteration             26GB
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 2023-06-01 19:22:47,158 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | 2023-06-01 19:22:47,244 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3_1   | 2023-06-01 19:22:47,249 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-06-01 19:22:47,905 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
dn5_1    | 2023-06-01 19:21:50,196 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:54,701 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A4B05FDCBDE0,id=69bc1289-e9f0-444d-ab3d-1548ab888401
dn2_1    | 2023-06-01 19:22:54,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-01 19:22:54,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
scm3_1   | 2023-06-01 19:22:47,910 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-01 19:22:47,916 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn5_1    | 2023-06-01 19:21:50,204 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:54,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-01 19:22:54,705 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-06-01 19:22:59,324 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4312D758E1FC with new leaderId: 3d6edd59-5add-4b7b-8003-44199ccc7b59
scm3_1   | 2023-06-01 19:22:48,021 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
dn5_1    | 2023-06-01 19:21:51,197 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,790 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]].
dn2_1    | 2023-06-01 19:22:54,705 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-01 19:21:41,275 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn4_1    | 2023-06-01 19:22:59,326 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: change Leader from null to 3d6edd59-5add-4b7b-8003-44199ccc7b59 at term 1 for becomeLeader, leader elected after 6247ms
scm3_1   | 2023-06-01 19:22:48,028 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
dn5_1    | 2023-06-01 19:21:51,205 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:54,725 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-01 19:22:54,728 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
dn2_1    | 2023-06-01 19:22:57,270 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0.
scm1_1   | 2023-06-01 19:21:41,275 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
dn4_1    | 2023-06-01 19:22:59,359 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 2023-06-01 19:22:48,029 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
dn5_1    | 2023-06-01 19:21:52,198 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:57,292 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401: new RaftServerImpl for group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-01 19:22:57,292 [Command processor thread] INFO server.RaftServer: 69bc1289-e9f0-444d-ab3d-1548ab888401: addNew group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1C4731943483:java.util.concurrent.CompletableFuture@256b2139[Not completed]
dn2_1    | 2023-06-01 19:22:57,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-01 19:21:41,280 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn4_1    | 2023-06-01 19:22:59,368 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-06-01 19:22:48,029 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 2023-06-01 19:21:52,205 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,793 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn2_1    | 2023-06-01 19:22:57,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm1_1   | 2023-06-01 19:21:41,284 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn4_1    | 2023-06-01 19:22:59,379 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm3_1   | 2023-06-01 19:22:48,129 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6c5ca0b6] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-06-01 19:21:53,199 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,794 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]].
scm2_1   | 2023-06-01 19:22:24,834 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-01 19:22:33,992 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderStateImpl
om3_1    | 2023-06-01 19:22:34,049 [om3@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
scm1_1   | 2023-06-01 19:21:41,291 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/in_use.lock acquired by nodename 7@f0aca9c847b3
dn4_1    | 2023-06-01 19:22:59,418 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-06-01 19:22:48,173 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
dn5_1    | 2023-06-01 19:21:53,206 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,904 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]].
om3_1    | 2023-06-01 19:22:34,197 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-01 19:22:35,073 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om3_1    | 2023-06-01 19:22:35,752 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm3_1   | 2023-06-01 19:22:48,173 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-01 19:21:54,200 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,904 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
scm3_1   | 2023-06-01 19:22:48,232 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @20409ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-06-01 19:22:48,483 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-06-01 19:22:24,938 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]].
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
scm3_1   | 2023-06-01 19:22:48,500 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
dn5_1    | 2023-06-01 19:21:54,207 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,941 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-01 19:21:41,305 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=4db01c28-2341-445f-9eff-7a55fcb1aabf} from /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/raft-meta
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
scm3_1   | 2023-06-01 19:22:48,521 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-06-01 19:22:48,525 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn5_1    | 2023-06-01 19:21:55,201 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:24,978 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]].
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 2023-06-01 19:21:41,359 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:21:41,364 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-01 19:22:59,421 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-06-01 19:22:59,432 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-06-01 19:22:48,525 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-06-01 19:22:48,526 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-06-01 19:21:55,226 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-01 19:22:25,025 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om3_1    | ]
scm1_1   | 2023-06-01 19:21:41,372 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-01 19:22:59,442 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-01 19:22:59,447 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-06-01 19:22:48,693 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
dn5_1    | 2023-06-01 19:21:56,202 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:48,695 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-06-01 19:22:25,209 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-06-01 19:22:43,953 [qtp1099892020-48] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
scm1_1   | 2023-06-01 19:21:41,372 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-01 19:22:59,458 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO impl.RoleInfo: 3d6edd59-5add-4b7b-8003-44199ccc7b59: start 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderStateImpl
dn4_1    | 2023-06-01 19:22:59,478 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-01 19:22:59,561 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-LeaderElection1] INFO server.RaftServer$Division: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC: set configuration 0: peers:[3d6edd59-5add-4b7b-8003-44199ccc7b59|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-01 19:22:59,669 [3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3d6edd59-5add-4b7b-8003-44199ccc7b59@group-4312D758E1FC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/54391af0-7339-4d61-8cba-4312d758e1fc/current/log_inprogress_0
scm3_1   | 2023-06-01 19:22:48,757 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-06-01 19:22:25,474 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-06-01 19:22:43,976 [qtp1099892020-48] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1685647363954 in 21 milliseconds
om3_1    | 2023-06-01 19:22:44,056 [qtp1099892020-48] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 78 milliseconds
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: ConfigurationManager, init=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-01 19:22:57,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-01 19:21:56,226 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:57,203 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:57,227 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:58,204 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:58,233 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:59,206 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:21:59,234 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:00,207 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:00,236 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:01,214 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:01,237 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:02,217 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:02,239 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:57,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-01 19:22:57,302 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-01 19:22:57,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-01 19:22:48,757 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-06-01 19:22:48,759 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm3_1   | 2023-06-01 19:22:48,788 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@49ced9c7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-06-01 19:22:48,793 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b09ce57{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-01 19:21:41,374 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-01 19:21:41,375 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-01 19:21:41,382 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn2_1    | 2023-06-01 19:22:57,324 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-01 19:22:57,325 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-01 19:22:57,328 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-01 19:22:57,328 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-06-01 19:21:41,390 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-01 19:21:41,392 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-01 19:21:41,399 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9
dn5_1    | 2023-06-01 19:22:03,218 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:03,240 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:04,219 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:49,535 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3ed7dd70{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-16802978232989400341/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm1_1   | 2023-06-01 19:21:41,400 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-01 19:21:41,400 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-01 19:21:41,402 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-01 19:21:41,403 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-01 19:21:41,408 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-01 19:21:41,409 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-01 19:21:41,413 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-01 19:21:41,413 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-06-01 19:22:44,057 [qtp1099892020-48] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1685647363954
scm2_1   | 2023-06-01 19:22:25,501 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 2023-06-01 19:22:25,683 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#16 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:47990: output error
om3_1    | 2023-06-01 19:23:33,306 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
scm2_1   | 2023-06-01 19:22:25,707 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#15 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:34438: output error
om3_1    | 2023-06-01 19:23:36,957 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om3_1    | 2023-06-01 19:23:49,059 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
scm2_1   | 2023-06-01 19:22:25,708 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#15 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:43438: output error
scm2_1   | 2023-06-01 19:22:25,708 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:47986: output error
scm2_1   | 2023-06-01 19:22:25,720 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:34856: output error
scm2_1   | 2023-06-01 19:22:25,724 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#16 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:51106: output error
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-01 19:21:41,441 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-06-01 19:22:25,724 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#16 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:36404: output error
scm2_1   | 2023-06-01 19:22:25,785 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:41,442 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
scm1_1   | 2023-06-01 19:21:41,443 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1    | 2023-06-01 19:24:04,830 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-01 19:21:41,444 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
dn5_1    | 2023-06-01 19:22:04,241 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:05,220 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:41,496 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:49,575 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@38089dae{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3_1   | 2023-06-01 19:22:49,576 [Listener at 0.0.0.0/9860] INFO server.Server: Started @21753ms
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
scm1_1   | 2023-06-01 19:21:41,502 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0
scm3_1   | 2023-06-01 19:22:49,585 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3_1   | 2023-06-01 19:22:49,586 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
scm1_1   | 2023-06-01 19:21:41,527 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm3_1   | 2023-06-01 19:22:49,588 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3_1   | 2023-06-01 19:22:50,456 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1d53eca5-989b-4da6-8a3f-f976f64c30fa
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-06-01 19:21:41,528 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-06-01 19:21:41,609 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: start as a follower, conf=0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn2_1    | 2023-06-01 19:22:57,337 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 does not exist. Creating ...
om3_1    | 2023-06-01 19:24:32,422 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 Received prepare request with log index 25
om3_1    | 2023-06-01 19:24:32,427 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om3 waiting for index 25 to flush to OM DB and index 26 to flush to Ratis state machine.
dn5_1    | 2023-06-01 19:22:05,242 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,458 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:21:41,610 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from      null to FOLLOWER at term 1 for startAsFollower
dn2_1    | 2023-06-01 19:22:57,349 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/in_use.lock acquired by nodename 7@8b2845d7a78b
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om3_1    | 2023-06-01 19:24:37,429 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:26)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 2023-06-01 19:22:06,221 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,472 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-06-01 19:21:41,614 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState
dn2_1    | 2023-06-01 19:22:57,372 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 has been successfully formatted.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om3_1    | 2023-06-01 19:24:37,430 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 26 -> 26
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 2023-06-01 19:22:06,243 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,490 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-01 19:21:41,617 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-01 19:22:57,399 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1C4731943483: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om3_1    | 2023-06-01 19:24:37,430 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 26
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn5_1    | 2023-06-01 19:22:07,224 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,678 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/79bd37f0-b225-4c38-b396-23c942162c92
scm1_1   | 2023-06-01 19:21:41,617 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-01 19:22:57,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om3_1    | 2023-06-01 19:24:37,431 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 26
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn5_1    | 2023-06-01 19:22:07,244 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,680 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-06-01 19:22:57,405 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-01 19:21:41,622 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-477B239F9CA9,id=4db01c28-2341-445f-9eff-7a55fcb1aabf
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om3_1    | 2023-06-01 19:24:37,434 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 26
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn5_1    | 2023-06-01 19:22:08,226 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,681 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn2_1    | 2023-06-01 19:22:57,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-01 19:21:41,626 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om3_1    | 2023-06-01 19:24:37,435 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-26
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn5_1    | 2023-06-01 19:22:08,245 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:50,683 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn2_1    | 2023-06-01 19:22:57,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-01 19:22:57,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-01 19:21:41,627 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om3_1    | 2023-06-01 19:24:37,439 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 25 to file /data/metadata/current/prepareMarker
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn5_1    | 2023-06-01 19:22:09,227 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:51,070 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/69bc1289-e9f0-444d-ab3d-1548ab888401
scm3_1   | 2023-06-01 19:22:51,071 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-01 19:22:51,072 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-01 19:21:41,627 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-06-01 19:24:37,472 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 prepared at log index 25. Returning response txnID: 25
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn5_1    | 2023-06-01 19:22:09,246 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:51,072 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-06-01 19:22:51,073 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3_1   | 2023-06-01 19:22:51,073 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3_1   | 2023-06-01 19:22:51,073 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-06-01 19:22:51,080 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3_1   | 2023-06-01 19:22:51,083 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-01 19:22:51,133 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3d6edd59-5add-4b7b-8003-44199ccc7b59
dn2_1    | 2023-06-01 19:22:57,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-01 19:22:57,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-01 19:22:57,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-01 19:22:57,427 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483
dn2_1    | 2023-06-01 19:22:57,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-06-01 19:21:41,628 [4db01c28-2341-445f-9eff-7a55fcb1aabf-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-01 19:22:57,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-01 19:22:57,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-01 19:22:57,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-01 19:22:57,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-01 19:22:57,433 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-01 19:22:57,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-01 19:22:57,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-01 19:22:57,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-01 19:22:57,446 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:10,228 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:10,247 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:11,230 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:22:57,446 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-01 19:22:57,449 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-01 19:22:57,449 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-01 19:22:57,449 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-01 19:21:41,637 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start RPC server
scm1_1   | 2023-06-01 19:21:41,692 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 4db01c28-2341-445f-9eff-7a55fcb1aabf: GrpcService started, listening on 9894
scm1_1   | 2023-06-01 19:21:41,704 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4db01c28-2341-445f-9eff-7a55fcb1aabf: Started
dn2_1    | 2023-06-01 19:22:57,453 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: start as a follower, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-01 19:22:57,453 [pool-22-thread-1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-01 19:22:57,453 [pool-22-thread-1] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:22:11,248 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-01 19:22:51,134 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-01 19:22:51,134 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om3_1    |  with log index 25
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn2_1    | 2023-06-01 19:22:57,454 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-01 19:22:57,455 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1C4731943483,id=69bc1289-e9f0-444d-ab3d-1548ab888401
dn2_1    | 2023-06-01 19:22:57,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-01 19:22:57,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-01 19:22:57,456 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-01 19:22:57,457 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-01 19:22:57,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-01 19:22:57,459 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483
dn2_1    | 2023-06-01 19:22:58,007 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn5_1    | 2023-06-01 19:22:12,231 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:12,248 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-06-01 19:22:51,680 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]] moved to OPEN state
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn2_1    | 2023-06-01 19:22:58,034 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: receive requestVote(ELECTION, 1d53eca5-989b-4da6-8a3f-f976f64c30fa, group-A4B05FDCBDE0, 1, (t:0, i:0))
dn5_1    | 2023-06-01 19:22:13,233 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:13,250 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1   | 2023-06-01 19:22:51,800 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/71839cc9-95e0-4895-b82f-6f2805ce4ff9
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
dn2_1    | 2023-06-01 19:22:58,038 [grpc-default-executor-0] INFO impl.VoteContext: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FOLLOWER: accept ELECTION from 1d53eca5-989b-4da6-8a3f-f976f64c30fa: our priority 0 <= candidate's priority 1
dn2_1    | 2023-06-01 19:22:58,039 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn2_1    | 2023-06-01 19:22:58,040 [grpc-default-executor-0] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState
dn2_1    | 2023-06-01 19:22:58,040 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState] INFO impl.FollowerState: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState was interrupted
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:22:58,047 [grpc-default-executor-0] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState
scm1_1   | 2023-06-01 19:21:41,709 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-06-01 19:22:51,814 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn5_1    | 2023-06-01 19:22:14,234 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn2_1    | 2023-06-01 19:22:58,048 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-01 19:22:14,251 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
scm3_1   | 2023-06-01 19:22:51,960 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-01 19:22:25,785 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-06-01 19:22:51,973 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:21:41,709 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-06-01 19:21:41,814 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-01 19:22:15,235 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-01 19:22:52,565 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn2_1    | 2023-06-01 19:22:58,050 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-01 19:22:58,065 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0 replies to ELECTION vote request: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-69bc1289-e9f0-444d-ab3d-1548ab888401#0:OK-t1. Peer's state: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0:t1, leader=null, voted=1d53eca5-989b-4da6-8a3f-f976f64c30fa, raftlog=Memoized:69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-01 19:22:58,522 [69bc1289-e9f0-444d-ab3d-1548ab888401-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A4B05FDCBDE0 with new leaderId: 1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn2_1    | 2023-06-01 19:22:58,522 [69bc1289-e9f0-444d-ab3d-1548ab888401-server-thread1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: change Leader from null to 1d53eca5-989b-4da6-8a3f-f976f64c30fa at term 1 for appendEntries, leader elected after 3947ms
dn5_1    | 2023-06-01 19:22:15,252 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn2_1    | 2023-06-01 19:22:58,554 [69bc1289-e9f0-444d-ab3d-1548ab888401-server-thread1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0: set configuration 0: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-01 19:22:58,630 [69bc1289-e9f0-444d-ab3d-1548ab888401-server-thread1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-01 19:22:58,948 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-A4B05FDCBDE0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/current/log_inprogress_0
scm2_1   | java.nio.channels.ClosedChannelException
dn5_1    | 2023-06-01 19:22:16,253 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | 2023-06-01 19:22:53,238 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:79bd37f0-b225-4c38-b396-23c942162c92, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]] moved to OPEN state
dn2_1    | 2023-06-01 19:22:59,371 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO impl.FollowerState: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5023083553ns, electionTimeout:5014ms
scm1_1   | 2023-06-01 19:21:41,829 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-01 19:22:17,256 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm3_1   | 2023-06-01 19:22:53,314 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn2_1    | 2023-06-01 19:22:59,372 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState
dn5_1    | 2023-06-01 19:22:18,257 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:41,829 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn2_1    | 2023-06-01 19:22:59,372 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-06-01 19:22:59,378 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm3_1   | 2023-06-01 19:22:53,715 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3d6edd59-5add-4b7b-8003-44199ccc7b59, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:21:42,260 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
dn5_1    | 2023-06-01 19:22:19,258 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm3_1   | 2023-06-01 19:22:53,810 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:21:42,261 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-01 19:21:42,267 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn5_1    | 2023-06-01 19:22:20,259 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:20,666 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm3_1   | 2023-06-01 19:22:53,851 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
scm3_1   | 2023-06-01 19:22:53,981 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:21:42,353 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-06-01 19:21:42,354 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-06-01 19:21:42,357 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm3_1   | 2023-06-01 19:22:54,081 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:21:42,397 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1_1   | 2023-06-01 19:21:42,459 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@30b3d899] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-06-01 19:21:42,482 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
scm3_1   | 2023-06-01 19:22:54,417 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:71839cc9-95e0-4895-b82f-6f2805ce4ff9, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-01 19:22:54,539 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-01 19:22:54,649 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-01 19:22:57,289 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-01 19:22:57,389 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-06-01 19:22:57,410 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-01 19:22:57,415 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-01 19:22:58,149 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-01 19:22:58,153 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-01 19:22:58,283 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm3_1   | 2023-06-01 19:22:58,831 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm3_1   | 2023-06-01 19:22:58,831 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn2_1    | 2023-06-01 19:22:59,378 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-FollowerState] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1
dn2_1    | 2023-06-01 19:22:59,388 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO impl.LeaderElection: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-01 19:22:58,832 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 2023-06-01 19:21:42,483 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-06-01 19:21:42,508 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @16103ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | 2023-06-01 19:22:58,832 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3_1   | 2023-06-01 19:22:58,832 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-06-01 19:21:42,876 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-06-01 19:22:58,832 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-06-01 19:22:58,832 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-06-01 19:21:42,892 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-06-01 19:21:42,922 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-06-01 19:21:42,927 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-06-01 19:22:59,389 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO impl.LeaderElection: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm3_1   | 2023-06-01 19:23:07,754 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:21:42,927 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-01 19:22:59,393 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn2_1    | 2023-06-01 19:22:59,397 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-06-01 19:22:59,400 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E9508D8D4862 with new leaderId: 69bc1289-e9f0-444d-ab3d-1548ab888401
dn2_1    | 2023-06-01 19:22:59,401 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: change Leader from null to 69bc1289-e9f0-444d-ab3d-1548ab888401 at term 1 for becomeLeader, leader elected after 6275ms
dn2_1    | 2023-06-01 19:22:59,430 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-01 19:22:59,451 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-01 19:22:59,452 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-06-01 19:22:59,494 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-06-01 19:21:42,927 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-01 19:21:43,003 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-06-01 19:21:43,006 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-06-01 19:21:43,086 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-01 19:22:59,497 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-06-01 19:22:59,503 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-01 19:22:59,531 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-06-01 19:23:40,868 [5e8a32ef-e4cd-4aae-aa6b-c9f950180f03@group-477B239F9CA9-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:22:59,553 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-06-01 19:22:59,561 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderStateImpl
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn2_1    | 2023-06-01 19:22:59,689 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn2_1    | 2023-06-01 19:22:59,691 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a3b4b03-96c4-47b7-9040-e9508d8d4862/current/log_inprogress_0
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-06-01 19:22:59,703 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862-LeaderElection1] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-E9508D8D4862: set configuration 0: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-01 19:23:02,549 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: receive requestVote(ELECTION, 1d53eca5-989b-4da6-8a3f-f976f64c30fa, group-1C4731943483, 1, (t:0, i:0))
scm1_1   | 2023-06-01 19:21:43,086 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-06-01 19:21:43,088 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | 2023-06-01 19:21:43,103 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7b3cbe6e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1   | 2023-06-01 19:21:43,104 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@57b9389f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-01 19:22:21,260 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:02,550 [grpc-default-executor-0] INFO impl.VoteContext: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FOLLOWER: reject ELECTION from 1d53eca5-989b-4da6-8a3f-f976f64c30fa: our priority 1 > candidate's priority 0
scm1_1   | 2023-06-01 19:21:43,518 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4c063cb9{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-12384838132393753085/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm1_1   | 2023-06-01 19:21:43,532 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@60b616c8{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
dn2_1    | 2023-06-01 19:23:02,550 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:22:22,261 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:23,262 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:24,263 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-01 19:21:43,533 [Listener at 0.0.0.0/9860] INFO server.Server: Started @17128ms
dn2_1    | 2023-06-01 19:23:02,550 [grpc-default-executor-0] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:22:25,264 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 2023-06-01 19:21:43,536 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-06-01 19:23:02,551 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO impl.FollowerState: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState was interrupted
dn5_1    | 2023-06-01 19:22:25,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-01 19:22:26,265 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:27,267 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-01 19:21:43,536 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-01 19:23:02,551 [grpc-default-executor-0] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:22:28,267 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-01 19:21:43,538 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn2_1    | 2023-06-01 19:23:02,571 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-01 19:22:29,280 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:46,644 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.FollowerState: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5030041094ns, electionTimeout:5025ms
dn2_1    | 2023-06-01 19:23:02,572 [grpc-default-executor-0] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483 replies to ELECTION vote request: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-69bc1289-e9f0-444d-ab3d-1548ab888401#0:FAIL-t1. Peer's state: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483:t1, leader=null, voted=null, raftlog=Memoized:69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:30,282 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 2023-06-01 19:22:25,781 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm1_1   | 2023-06-01 19:21:46,646 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState
dn2_1    | 2023-06-01 19:23:02,584 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-01 19:23:07,650 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO impl.FollowerState: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099348197ns, electionTimeout:5066ms
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-01 19:21:46,648 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn2_1    | 2023-06-01 19:23:07,652 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:22:31,285 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-06-01 19:21:46,651 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-06-01 19:23:07,652 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn5_1    | 2023-06-01 19:22:32,287 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-01 19:21:46,652 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-FollowerState] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1
dn2_1    | 2023-06-01 19:23:07,653 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-06-01 19:23:07,653 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2
dn2_1    | 2023-06-01 19:23:07,657 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.LeaderElection: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn5_1    | 2023-06-01 19:22:33,288 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,663 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-06-01 19:21:46,683 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.LeaderElection: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
dn5_1    | 2023-06-01 19:22:34,290 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:35,291 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-06-01 19:21:46,684 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.LeaderElection: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1 ELECTION round 0: result PASSED (term=2)
recon_1  | java.nio.channels.ClosedChannelException
dn5_1    | 2023-06-01 19:22:36,292 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,663 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-06-01 19:21:46,685 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: shutdown 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-01 19:22:37,293 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,666 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-01 19:21:46,685 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1_1   | 2023-06-01 19:21:46,686 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
dn5_1    | 2023-06-01 19:22:38,294 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-01 19:22:39,295 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-01 19:21:46,686 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-06-01 19:22:40,296 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,682 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 79bd37f0-b225-4c38-b396-23c942162c92
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 2023-06-01 19:22:41,297 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,735 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.LeaderElection: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 2023-06-01 19:21:46,688 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: change Leader from null to 4db01c28-2341-445f-9eff-7a55fcb1aabf at term 2 for becomeLeader, leader elected after 8208ms
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 2023-06-01 19:22:42,298 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,736 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.LeaderElection:   Response 0: 69bc1289-e9f0-444d-ab3d-1548ab888401<-79bd37f0-b225-4c38-b396-23c942162c92#0:OK-t2
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 2023-06-01 19:21:46,697 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn5_1    | 2023-06-01 19:22:43,299 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-01 19:23:07,736 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.LeaderElection: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2 ELECTION round 0: result PASSED
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 2023-06-01 19:21:46,702 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn5_1    | 2023-06-01 19:22:47,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-01 19:23:07,736 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: shutdown 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-06-01 19:21:46,703 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn5_1    | 2023-06-01 19:22:51,973 [Command processor thread] INFO server.RaftServer: 79bd37f0-b225-4c38-b396-23c942162c92: addNew group-156762BB27AB:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-156762BB27AB:java.util.concurrent.CompletableFuture@5b414717[Not completed]
dn2_1    | 2023-06-01 19:23:07,737 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-06-01 19:21:46,708 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn5_1    | 2023-06-01 19:22:52,080 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92: new RaftServerImpl for group-156762BB27AB:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-01 19:23:07,737 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1C4731943483 with new leaderId: 69bc1289-e9f0-444d-ab3d-1548ab888401
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-01 19:21:46,708 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn5_1    | 2023-06-01 19:22:52,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-01 19:23:07,737 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: change Leader from null to 69bc1289-e9f0-444d-ab3d-1548ab888401 at term 2 for becomeLeader, leader elected after 10431ms
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-01 19:21:46,709 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn5_1    | 2023-06-01 19:22:52,103 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-01 19:23:07,761 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:46,713 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn5_1    | 2023-06-01 19:22:52,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-01 19:22:52,109 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-01 19:22:25,781 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm1_1   | 2023-06-01 19:21:46,718 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
dn5_1    | 2023-06-01 19:22:52,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-01 19:23:07,761 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-01 19:21:46,722 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO impl.RoleInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf: start 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn5_1    | 2023-06-01 19:22:52,110 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-01 19:23:07,762 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-06-01 19:21:46,730 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-06-01 19:22:52,148 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: ConfigurationManager, init=-1: peers:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-06-01 19:21:46,741 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_0 to /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_0-0
dn5_1    | 2023-06-01 19:22:52,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-01 19:22:52,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-01 19:22:52,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-06-01 19:21:46,743 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderElection1] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 1: peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:52,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:23:07,763 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-06-01 19:21:46,762 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/ed78d68c-b0d3-43e0-aab9-477b239f9ca9/current/log_inprogress_1
dn5_1    | 2023-06-01 19:22:52,324 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn2_1    | 2023-06-01 19:23:07,768 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-06-01 19:21:46,773 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
dn5_1    | 2023-06-01 19:22:52,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-01 19:21:46,776 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1_1   | 2023-06-01 19:21:46,792 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn5_1    | 2023-06-01 19:22:52,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-06-01 19:23:07,768 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-01 19:21:46,795 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-06-01 19:21:46,797 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
dn5_1    | 2023-06-01 19:22:52,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-06-01 19:23:07,769 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-01 19:23:07,769 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-01 19:23:07,882 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn5_1    | 2023-06-01 19:22:52,763 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn2_1    | 2023-06-01 19:23:07,885 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-01 19:23:07,886 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm1_1   | 2023-06-01 19:21:46,801 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn5_1    | 2023-06-01 19:22:52,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn2_1    | 2023-06-01 19:23:07,916 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-06-01 19:21:46,806 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn5_1    | 2023-06-01 19:22:52,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn2_1    | 2023-06-01 19:23:07,916 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-01 19:23:07,917 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn5_1    | 2023-06-01 19:22:52,786 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6614ab6a-e6f2-43c2-8c89-156762bb27ab does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn2_1    | 2023-06-01 19:23:07,917 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-01 19:21:46,806 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
dn5_1    | 2023-06-01 19:22:52,822 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6614ab6a-e6f2-43c2-8c89-156762bb27ab/in_use.lock acquired by nodename 7@dd0e15ce78c9
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn2_1    | 2023-06-01 19:23:07,917 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-01 19:21:46,881 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:46110
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn5_1    | 2023-06-01 19:22:52,942 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6614ab6a-e6f2-43c2-8c89-156762bb27ab has been successfully formatted.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn2_1    | 2023-06-01 19:23:07,929 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-01 19:21:46,927 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:60748: output error
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-06-01 19:22:53,029 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-156762BB27AB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
dn2_1    | 2023-06-01 19:23:07,933 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-01 19:21:46,929 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-06-01 19:22:53,150 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
dn2_1    | 2023-06-01 19:23:07,933 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm1_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn5_1    | 2023-06-01 19:22:53,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn2_1    | 2023-06-01 19:23:07,934 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn5_1    | 2023-06-01 19:22:53,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-06-01 19:22:25,774 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
dn2_1    | 2023-06-01 19:23:07,938 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-06-01 19:22:53,317 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | java.nio.channels.ClosedChannelException
dn2_1    | 2023-06-01 19:23:07,938 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-06-01 19:22:53,327 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-06-01 19:23:07,939 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-01 19:22:53,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn2_1    | 2023-06-01 19:23:07,939 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-06-01 19:22:53,381 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
dn2_1    | 2023-06-01 19:23:07,944 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO impl.RoleInfo: 69bc1289-e9f0-444d-ab3d-1548ab888401: start 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderStateImpl
dn5_1    | 2023-06-01 19:22:53,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
dn2_1    | 2023-06-01 19:23:07,946 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-01 19:22:53,413 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6614ab6a-e6f2-43c2-8c89-156762bb27ab
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-06-01 19:23:07,957 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/current/log_inprogress_0
dn2_1    | 2023-06-01 19:23:08,001 [69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483-LeaderElection2] INFO server.RaftServer$Division: 69bc1289-e9f0-444d-ab3d-1548ab888401@group-1C4731943483: set configuration 0: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:53,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-01 19:22:25,787 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:43434: output error
scm2_1   | 2023-06-01 19:22:25,895 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-01 19:22:53,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 2023-06-01 19:21:46,928 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:52904: output error
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-06-01 19:22:53,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 2023-06-01 19:21:46,951 [IPC Server handler 37 on default port 9861] WARN ipc.Server: IPC Server handler 37 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:39478: output error
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 2023-06-01 19:22:53,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 2023-06-01 19:22:53,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-01 19:22:53,466 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-01 19:22:53,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-01 19:22:53,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-01 19:22:25,787 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:34870: output error
scm2_1   | 2023-06-01 19:22:25,905 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
dn5_1    | 2023-06-01 19:22:53,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:53,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:53,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:53,546 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-01 19:22:53,583 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:53,583 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:53,588 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: start as a follower, conf=-1: peers:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:53,588 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-06-01 19:22:53,602 [pool-22-thread-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState
dn5_1    | 2023-06-01 19:22:53,609 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-01 19:22:53,609 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:21:55,811 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn5_1    | 2023-06-01 19:22:53,613 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-156762BB27AB,id=79bd37f0-b225-4c38-b396-23c942162c92
dn5_1    | 2023-06-01 19:22:53,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-01 19:22:53,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-01 19:21:46,985 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
dn5_1    | 2023-06-01 19:22:53,621 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-01 19:22:53,629 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-01 19:22:53,726 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6614ab6a-e6f2-43c2-8c89-156762bb27ab
dn5_1    | 2023-06-01 19:22:53,727 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6614ab6a-e6f2-43c2-8c89-156762bb27ab.
dn5_1    | 2023-06-01 19:22:53,728 [Command processor thread] INFO server.RaftServer: 79bd37f0-b225-4c38-b396-23c942162c92: addNew group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-A4B05FDCBDE0:java.util.concurrent.CompletableFuture@2ce7dc5d[Not completed]
dn5_1    | 2023-06-01 19:22:53,756 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92: new RaftServerImpl for group-A4B05FDCBDE0:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-01 19:22:53,757 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-01 19:22:53,757 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-01 19:22:53,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-01 19:22:25,786 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-01 19:22:25,786 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-01 19:22:53,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-01 19:22:53,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-01 19:22:53,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: ConfigurationManager, init=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-06-01 19:21:55,849 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-01 19:22:53,766 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-01 19:22:53,778 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 does not exist. Creating ...
dn5_1    | 2023-06-01 19:22:53,810 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/in_use.lock acquired by nodename 7@dd0e15ce78c9
dn5_1    | 2023-06-01 19:22:53,824 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 has been successfully formatted.
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-A4B05FDCBDE0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-01 19:22:53,825 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:46,986 [IPC Server handler 37 on default port 9861] INFO ipc.Server: IPC Server handler 37 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-01 19:22:20,455 [IPC Server handler 75 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1d53eca5-989b-4da6-8a3f-f976f64c30fa
recon_1  | 2023-06-01 19:22:20,460 [IPC Server handler 75 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:20,467 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1d53eca5-989b-4da6-8a3f-f976f64c30fa to Node DB.
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 2023-06-01 19:22:20,709 [IPC Server handler 93 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/79bd37f0-b225-4c38-b396-23c942162c92
recon_1  | 2023-06-01 19:22:20,709 [IPC Server handler 93 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:20,712 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 79bd37f0-b225-4c38-b396-23c942162c92 to Node DB.
recon_1  | 2023-06-01 19:22:21,113 [IPC Server handler 37 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/69bc1289-e9f0-444d-ab3d-1548ab888401
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 2023-06-01 19:22:21,113 [IPC Server handler 37 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:21,114 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 69bc1289-e9f0-444d-ab3d-1548ab888401 to Node DB.
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:48,473 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-06-01 19:22:27,182 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-5dcb1cf7-eeac-4c03-a034-92235b5458e4: Detected pause in JVM or host machine (eg GC): pause of approximately 159637161ns.
scm2_1   | GC pool 'ParNew' had collection(s): count=1 time=213ms
scm1_1   | 2023-06-01 19:21:48,481 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:21:48,489 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-01 19:21:48,497 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-06-01 19:22:27,939 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-06-01 19:22:27,943 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-01 19:21:48,500 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7 to datanode:1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm1_1   | 2023-06-01 19:21:48,499 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | 2023-06-01 19:21:48,506 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2_1   | 2023-06-01 19:22:28,031 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn5_1    | 2023-06-01 19:22:53,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:53,861 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1   | 2023-06-01 19:22:28,320 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-06-01 19:22:28,322 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
dn5_1    | 2023-06-01 19:22:53,861 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 2023-06-01 19:22:21,129 [IPC Server handler 33 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3d6edd59-5add-4b7b-8003-44199ccc7b59
scm1_1   | 2023-06-01 19:21:48,611 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]].
scm2_1   | 2023-06-01 19:22:28,453 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-01 19:22:28,454 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-06-01 19:22:29,282 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@11826398] INFO util.JvmPauseMonitor: Starting JVM pause monitor
recon_1  | 2023-06-01 19:22:21,129 [IPC Server handler 33 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:21:48,613 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:29,449 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
dn5_1    | 2023-06-01 19:22:53,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-01 19:22:53,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-01 19:22:53,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-01 19:22:53,870 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:53,879 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:53,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:53,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-01 19:22:53,889 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:53,892 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:53,893 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: start as a follower, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:22:21,131 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3d6edd59-5add-4b7b-8003-44199ccc7b59 to Node DB.
recon_1  | 2023-06-01 19:22:21,816 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/71839cc9-95e0-4895-b82f-6f2805ce4ff9
dn5_1    | 2023-06-01 19:22:53,893 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 2023-06-01 19:22:21,818 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:29,453 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-01 19:22:53,893 [pool-22-thread-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState
dn5_1    | 2023-06-01 19:22:53,894 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A4B05FDCBDE0,id=79bd37f0-b225-4c38-b396-23c942162c92
dn5_1    | 2023-06-01 19:22:53,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1  | 2023-06-01 19:22:21,821 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 71839cc9-95e0-4895-b82f-6f2805ce4ff9 to Node DB.
dn5_1    | 2023-06-01 19:22:53,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-01 19:21:48,691 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/79bd37f0-b225-4c38-b396-23c942162c92
recon_1  | 2023-06-01 19:22:40,197 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn5_1    | 2023-06-01 19:22:53,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-01 19:22:53,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-06-01 19:22:29,715 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @40081ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-06-01 19:22:40,198 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
dn5_1    | 2023-06-01 19:22:53,895 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-01 19:21:48,692 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:30,391 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-06-01 19:22:42,105 [pool-27-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:248)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:235)
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:228)
scm1_1   | 2023-06-01 19:21:48,693 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-06-01 19:21:48,739 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
scm1_1   | 2023-06-01 19:21:48,741 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6614ab6a-e6f2-43c2-8c89-156762bb27ab to datanode:79bd37f0-b225-4c38-b396-23c942162c92
scm1_1   | 2023-06-01 19:21:48,752 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]].
scm2_1   | 2023-06-01 19:22:30,472 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
recon_1  | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm1_1   | 2023-06-01 19:21:48,752 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
dn5_1    | 2023-06-01 19:22:53,919 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0
dn5_1    | 2023-06-01 19:22:53,933 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-01 19:21:49,107 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/69bc1289-e9f0-444d-ab3d-1548ab888401
recon_1  | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
dn5_1    | 2023-06-01 19:22:57,196 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0.
scm2_1   | 2023-06-01 19:22:30,518 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-06-01 19:21:49,128 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
dn5_1    | 2023-06-01 19:22:57,197 [Command processor thread] INFO server.RaftServer: 79bd37f0-b225-4c38-b396-23c942162c92: addNew group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1C4731943483:java.util.concurrent.CompletableFuture@31cce952[Not completed]
scm2_1   | 2023-06-01 19:22:30,526 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-06-01 19:21:49,130 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
dn5_1    | 2023-06-01 19:22:57,201 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92: new RaftServerImpl for group-1C4731943483:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-06-01 19:21:49,131 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
dn5_1    | 2023-06-01 19:22:57,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-01 19:21:49,136 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-01 19:22:30,537 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-06-01 19:21:49,137 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-01 19:22:30,544 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-01 19:21:49,137 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-01 19:22:30,893 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-06-01 19:21:49,139 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-01 19:22:30,912 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-06-01 19:21:49,139 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-06-01 19:21:49,133 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3a3b4b03-96c4-47b7-9040-e9508d8d4862 to datanode:69bc1289-e9f0-444d-ab3d-1548ab888401
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
dn5_1    | 2023-06-01 19:22:57,207 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: ConfigurationManager, init=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-06-01 19:21:49,160 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]].
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-06-01 19:22:57,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-06-01 19:21:49,161 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:31,154 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-01 19:22:57,209 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-01 19:21:49,180 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3d6edd59-5add-4b7b-8003-44199ccc7b59
dn5_1    | 2023-06-01 19:22:57,215 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-01 19:21:49,199 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:22:57,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm1_1   | 2023-06-01 19:21:49,200 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-06-01 19:22:31,159 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-06-01 19:22:31,161 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 2023-06-01 19:22:57,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-01 19:22:57,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-01 19:21:49,241 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 to datanode:1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm1_1   | 2023-06-01 19:21:49,257 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 to datanode:79bd37f0-b225-4c38-b396-23c942162c92
scm2_1   | 2023-06-01 19:22:31,248 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c579202{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-06-01 19:22:31,248 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@766b6d02{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-06-01 19:21:49,258 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 to datanode:69bc1289-e9f0-444d-ab3d-1548ab888401
scm1_1   | 2023-06-01 19:21:49,274 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]].
scm1_1   | 2023-06-01 19:21:49,276 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:33,013 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27896d3b{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-2239814611600594049/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm2_1   | 2023-06-01 19:22:33,050 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@99c4993{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-06-01 19:21:49,280 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=54391af0-7339-4d61-8cba-4312d758e1fc to datanode:3d6edd59-5add-4b7b-8003-44199ccc7b59
scm1_1   | 2023-06-01 19:21:49,289 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]].
scm1_1   | 2023-06-01 19:21:49,293 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:33,050 [Listener at 0.0.0.0/9860] INFO server.Server: Started @43416ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-01 19:21:49,295 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 to datanode:79bd37f0-b225-4c38-b396-23c942162c92
dn5_1    | 2023-06-01 19:22:57,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-01 19:22:57,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | , while invoking $Proxy42.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1  | 2023-06-01 19:22:44,189 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1685647360198
dn5_1    | 2023-06-01 19:22:57,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-01 19:21:49,303 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 to datanode:69bc1289-e9f0-444d-ab3d-1548ab888401
dn5_1    | 2023-06-01 19:22:57,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-01 19:22:57,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-01 19:22:33,052 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-06-01 19:22:33,052 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-01 19:22:57,236 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 does not exist. Creating ...
dn5_1    | 2023-06-01 19:22:57,261 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/in_use.lock acquired by nodename 7@dd0e15ce78c9
dn5_1    | 2023-06-01 19:22:57,303 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483 has been successfully formatted.
dn5_1    | 2023-06-01 19:22:57,303 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1C4731943483: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 2023-06-01 19:22:44,196 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-01 19:22:44,200 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 2023-06-01 19:22:44,327 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1685647360198.
recon_1  | 2023-06-01 19:22:44,353 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-01 19:21:49,304 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 to datanode:1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-01 19:22:33,061 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-06-01 19:22:46,456 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 21: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-01 19:21:49,322 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]].
recon_1  | 2023-06-01 19:22:44,365 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
scm2_1   | 2023-06-01 19:22:46,471 [5dcb1cf7-eeac-4c03-a034-92235b5458e4-server-thread1] INFO server.RaftServer$Division: 5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9: set configuration 23: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483
scm1_1   | 2023-06-01 19:21:49,322 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 2023-06-01 19:22:44,378 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
dn5_1    | 2023-06-01 19:22:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-06-01 19:22:50,502 [IPC Server handler 20 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1d53eca5-989b-4da6-8a3f-f976f64c30fa
recon_1  | 2023-06-01 19:22:44,864 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1  | 2023-06-01 19:22:44,865 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1  | 2023-06-01 19:22:44,865 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-06-01 19:22:44,871 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-06-01 19:22:44,888 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm1_1   | 2023-06-01 19:21:49,323 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 contains same datanodes as previous pipelines: PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 nodeIds: 79bd37f0-b225-4c38-b396-23c942162c92, 69bc1289-e9f0-444d-ab3d-1548ab888401, 1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm2_1   | 2023-06-01 19:22:50,510 [IPC Server handler 20 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:44,888 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.023 seconds to process 0 keys.
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 2023-06-01 19:22:44,969 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1  | 2023-06-01 19:22:44,976 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
scm1_1   | 2023-06-01 19:21:49,831 [IPC Server handler 38 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/71839cc9-95e0-4895-b82f-6f2805ce4ff9
scm2_1   | 2023-06-01 19:22:50,521 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-01 19:21:49,831 [IPC Server handler 38 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:50,522 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 2023-06-01 19:22:50,485 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn3_1.ha_net
scm1_1   | 2023-06-01 19:21:49,832 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-06-01 19:22:50,704 [IPC Server handler 94 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/79bd37f0-b225-4c38-b396-23c942162c92
scm1_1   | 2023-06-01 19:21:49,833 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=77894454-4515-4db1-a815-6286ee24275c to datanode:71839cc9-95e0-4895-b82f-6f2805ce4ff9
recon_1  | 2023-06-01 19:22:50,698 [IPC Server handler 93 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn5_1.ha_net
recon_1  | 2023-06-01 19:22:51,110 [IPC Server handler 33 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn2_1.ha_net
scm1_1   | 2023-06-01 19:21:49,838 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]].
dn5_1    | 2023-06-01 19:22:57,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-06-01 19:22:50,705 [IPC Server handler 94 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:50,706 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-01 19:21:49,839 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn5_1    | 2023-06-01 19:22:57,358 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-01 19:22:57,358 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:57,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-01 19:22:57,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-01 19:22:57,381 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:57,381 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-01 19:22:57,382 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: start as a follower, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:22:17,052 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: 4db01c28-2341-445f-9eff-7a55fcb1aabf: Submitting SetConfiguration request to Ratis server with new SCM peers list: [4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-01 19:22:50,710 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn5_1    | 2023-06-01 19:22:57,382 [pool-22-thread-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 2023-06-01 19:22:51,150 [IPC Server handler 37 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn4_1.ha_net
scm2_1   | 2023-06-01 19:22:51,074 [IPC Server handler 14 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/69bc1289-e9f0-444d-ab3d-1548ab888401
scm1_1   | 2023-06-01 19:22:17,086 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: receive setConfiguration SetConfigurationRequest:client-7FCD14621E46->4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
recon_1  | 2023-06-01 19:22:51,657 [IPC Server handler 87 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
recon_1  | 2023-06-01 19:22:51,659 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:22:57,382 [pool-22-thread-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState
recon_1  | 2023-06-01 19:22:51,659 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:22:51,851 [IPC Server handler 15 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn1_1.ha_net
dn5_1    | 2023-06-01 19:22:57,411 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1C4731943483,id=79bd37f0-b225-4c38-b396-23c942162c92
dn5_1    | 2023-06-01 19:22:57,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-01 19:22:57,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-01 19:22:17,087 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-7FCD14621E46->4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
dn5_1    | 2023-06-01 19:22:57,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-01 19:22:57,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-01 19:22:57,415 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-06-01 19:22:51,074 [IPC Server handler 14 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:51,075 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-06-01 19:22:51,076 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-06-01 19:22:51,076 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-06-01 19:22:51,077 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-01 19:22:51,077 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-01 19:22:51,079 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-06-01 19:22:51,080 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-01 19:22:17,130 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-01 19:22:17,131 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-01 19:22:51,133 [IPC Server handler 16 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3d6edd59-5add-4b7b-8003-44199ccc7b59
scm2_1   | 2023-06-01 19:22:51,133 [IPC Server handler 16 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:52,544 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:22:17,133 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-06-01 19:22:17,156 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-06-01 19:22:57,438 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-01 19:22:57,438 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483
recon_1  | 2023-06-01 19:22:53,083 [IPC Server handler 33 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
scm1_1   | 2023-06-01 19:22:17,159 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn5_1    | 2023-06-01 19:22:57,889 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483.
dn5_1    | 2023-06-01 19:22:58,087 [grpc-default-executor-0] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: receive requestVote(ELECTION, 1d53eca5-989b-4da6-8a3f-f976f64c30fa, group-A4B05FDCBDE0, 1, (t:0, i:0))
scm2_1   | 2023-06-01 19:22:51,133 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-01 19:22:51,715 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-01 19:22:51,995 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-5dcb1cf7-eeac-4c03-a034-92235b5458e4: Detected pause in JVM or host machine (eg GC): pause of approximately 183543137ns.
recon_1  | 2023-06-01 19:22:53,085 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6614ab6a-e6f2-43c2-8c89-156762bb27ab reported by 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:53,086 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:79bd37f0-b225-4c38-b396-23c942162c92, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]] moved to OPEN state
scm2_1   | GC pool 'ParNew' had collection(s): count=1 time=190ms
scm2_1   | 2023-06-01 19:22:52,006 [IPC Server handler 14 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/71839cc9-95e0-4895-b82f-6f2805ce4ff9
scm2_1   | 2023-06-01 19:22:52,017 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 2023-06-01 19:22:53,709 [IPC Server handler 94 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
recon_1  | 2023-06-01 19:22:53,711 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=54391af0-7339-4d61-8cba-4312d758e1fc reported by 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:52,046 [IPC Server handler 14 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:52,046 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-01 19:22:52,548 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-06-01 19:22:53,713 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3d6edd59-5add-4b7b-8003-44199ccc7b59, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:22:53,830 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:53,981 [IPC Server handler 33 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-06-01 19:22:53,984 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=3a3b4b03-96c4-47b7-9040-e9508d8d4862 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:53,984 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-01 19:22:53,161 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:79bd37f0-b225-4c38-b396-23c942162c92, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-01 19:22:53,286 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:53,657 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3d6edd59-5add-4b7b-8003-44199ccc7b59, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:22:54,438 [IPC Server handler 74 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
recon_1  | 2023-06-01 19:22:54,440 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=77894454-4515-4db1-a815-6286ee24275c reported by 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:54,441 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:71839cc9-95e0-4895-b82f-6f2805ce4ff9, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:22:54,640 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:53,793 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:17,159 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-01 19:22:58,091 [grpc-default-executor-0] INFO impl.VoteContext: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FOLLOWER: accept ELECTION from 1d53eca5-989b-4da6-8a3f-f976f64c30fa: our priority 0 <= candidate's priority 1
scm2_1   | 2023-06-01 19:22:53,836 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-06-01 19:22:57,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:22:58,093 [grpc-default-executor-0] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1d53eca5-989b-4da6-8a3f-f976f64c30fa
scm2_1   | 2023-06-01 19:22:53,995 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:22:57,274 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:57,332 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:57,332 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-01 19:22:54,067 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:17,161 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-01 19:22:17,161 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-01 19:22:17,182 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm2_1   | 2023-06-01 19:22:54,407 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:71839cc9-95e0-4895-b82f-6f2805ce4ff9, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-01 19:22:54,522 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:54,646 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:57,277 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:57,352 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:57,360 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:57,392 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:17,490 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-GrpcLogAppender: send 4db01c28-2341-445f-9eff-7a55fcb1aabf->5dcb1cf7-eeac-4c03-a034-92235b5458e4#0-t2,notify:(t:1, i:0)
scm1_1   | 2023-06-01 19:22:17,552 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 5dcb1cf7-eeac-4c03-a034-92235b5458e4
scm1_1   | 2023-06-01 19:22:22,069 [grpc-default-executor-0] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-InstallSnapshotResponseHandler: received the first reply 4db01c28-2341-445f-9eff-7a55fcb1aabf<-5dcb1cf7-eeac-4c03-a034-92235b5458e4#0:OK-t0,ALREADY_INSTALLED
scm1_1   | 2023-06-01 19:22:22,099 [grpc-default-executor-0] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1   | 2023-06-01 19:22:22,120 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: snapshotIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-01 19:22:22,120 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: matchIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-01 19:22:22,125 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: nextIndex: setUnconditionally 0 -> 1
scm1_1   | 2023-06-01 19:22:22,125 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4 acknowledged installing snapshot
scm1_1   | 2023-06-01 19:22:22,126 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: nextIndex: updateToMax old=1, new=1, updated? false
scm1_1   | 2023-06-01 19:22:22,452 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: nextIndex: updateUnconditionally 17 -> 0
scm1_1   | 2023-06-01 19:22:22,483 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5dcb1cf7-eeac-4c03-a034-92235b5458e4: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-01 19:22:23,260 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 17: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-06-01 19:22:23,715 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 19: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:22:23,819 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 5dcb1cf7-eeac-4c03-a034-92235b5458e4.
dn5_1    | 2023-06-01 19:22:58,094 [grpc-default-executor-0] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState
dn5_1    | 2023-06-01 19:22:58,094 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState] INFO impl.FollowerState: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState was interrupted
dn5_1    | 2023-06-01 19:22:58,095 [grpc-default-executor-0] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState
dn5_1    | 2023-06-01 19:22:58,098 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-01 19:22:58,100 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-01 19:22:58,127 [grpc-default-executor-0] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0 replies to ELECTION vote request: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-79bd37f0-b225-4c38-b396-23c942162c92#0:OK-t1. Peer's state: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0:t1, leader=null, voted=1d53eca5-989b-4da6-8a3f-f976f64c30fa, raftlog=Memoized:79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:58,507 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A4B05FDCBDE0 with new leaderId: 1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:22:58,507 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: change Leader from null to 1d53eca5-989b-4da6-8a3f-f976f64c30fa at term 1 for appendEntries, leader elected after 4740ms
dn5_1    | 2023-06-01 19:22:58,550 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0: set configuration 0: peers:[1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:22:44,479 [IPC Server handler 12 on default port 9863] INFO ha.SCMRatisServerImpl: 4db01c28-2341-445f-9eff-7a55fcb1aabf: Submitting SetConfiguration request to Ratis server with new SCM peers list: [5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-01 19:22:44,479 [IPC Server handler 12 on default port 9863] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: receive setConfiguration SetConfigurationRequest:client-7FCD14621E46->4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9, cid=8, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm2_1   | 2023-06-01 19:22:58,151 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]] moved to OPEN state
dn5_1    | 2023-06-01 19:22:58,565 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker: Starting segment from index:0
scm1_1   | 2023-06-01 19:22:44,480 [IPC Server handler 12 on default port 9863] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-7FCD14621E46->4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9, cid=8, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-06-01 19:22:44,481 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 2023-06-01 19:22:58,154 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn5_1    | 2023-06-01 19:22:58,728 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO impl.FollowerState: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5138998462ns, electionTimeout:5117ms
dn5_1    | 2023-06-01 19:22:58,734 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState
dn5_1    | 2023-06-01 19:22:58,735 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm2_1   | 2023-06-01 19:22:58,274 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2_1   | 2023-06-01 19:22:58,826 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:58,827 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2_1   | 2023-06-01 19:22:58,827 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-06-01 19:22:58,827 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 2023-06-01 19:22:58,828 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 2023-06-01 19:22:58,828 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-01 19:22:58,828 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 2023-06-01 19:23:07,764 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-01 19:23:40,864 [5dcb1cf7-eeac-4c03-a034-92235b5458e4@group-477B239F9CA9-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
dn5_1    | 2023-06-01 19:22:58,759 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-01 19:22:58,759 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-FollowerState] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1
dn5_1    | 2023-06-01 19:22:58,786 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO impl.LeaderElection: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:58,788 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO impl.LeaderElection: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-06-01 19:22:58,788 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1
dn5_1    | 2023-06-01 19:22:58,790 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-06-01 19:22:58,790 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-156762BB27AB with new leaderId: 79bd37f0-b225-4c38-b396-23c942162c92
dn5_1    | 2023-06-01 19:22:58,791 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: change Leader from null to 79bd37f0-b225-4c38-b396-23c942162c92 at term 1 for becomeLeader, leader elected after 6496ms
dn5_1    | 2023-06-01 19:22:58,835 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-06-01 19:22:58,846 [79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-A4B05FDCBDE0-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5b2c3cec-0454-454b-b9bd-a4b05fdcbde0/current/log_inprogress_0
dn5_1    | 2023-06-01 19:22:58,898 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-01 19:22:58,899 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-06-01 19:22:58,908 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-06-01 19:22:58,908 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-06-01 19:22:58,918 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-06-01 19:22:58,943 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-01 19:22:58,964 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-06-01 19:22:59,001 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderStateImpl
dn5_1    | 2023-06-01 19:22:59,077 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-01 19:22:59,093 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-LeaderElection1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB: set configuration 0: peers:[79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:22:59,094 [79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-156762BB27AB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6614ab6a-e6f2-43c2-8c89-156762bb27ab/current/log_inprogress_0
dn5_1    | 2023-06-01 19:23:02,535 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: receive requestVote(ELECTION, 1d53eca5-989b-4da6-8a3f-f976f64c30fa, group-1C4731943483, 1, (t:0, i:0))
dn5_1    | 2023-06-01 19:23:02,535 [grpc-default-executor-1] INFO impl.VoteContext: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FOLLOWER: accept ELECTION from 1d53eca5-989b-4da6-8a3f-f976f64c30fa: our priority 0 <= candidate's priority 0
dn5_1    | 2023-06-01 19:23:02,537 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:1d53eca5-989b-4da6-8a3f-f976f64c30fa
dn5_1    | 2023-06-01 19:23:02,537 [grpc-default-executor-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:23:02,537 [grpc-default-executor-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState
dn5_1    | 2023-06-01 19:23:02,537 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO impl.FollowerState: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState was interrupted
recon_1  | 2023-06-01 19:22:57,352 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:57,353 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-01 19:22:57,378 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:22:44,481 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-01 19:22:44,482 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-06-01 19:22:44,484 [IPC Server handler 12 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1  | 2023-06-01 19:22:57,378 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:22:44,489 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-01 19:22:44,492 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-01 19:23:02,562 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483 replies to ELECTION vote request: 1d53eca5-989b-4da6-8a3f-f976f64c30fa<-79bd37f0-b225-4c38-b396-23c942162c92#0:OK-t1. Peer's state: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483:t1, leader=null, voted=1d53eca5-989b-4da6-8a3f-f976f64c30fa, raftlog=Memoized:79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-01 19:22:44,493 [IPC Server handler 12 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 2023-06-01 19:22:58,139 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:23:02,563 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-01 19:23:02,563 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-01 19:22:58,140 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=5b2c3cec-0454-454b-b9bd-a4b05fdcbde0 reported by 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:23:07,709 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO impl.FollowerState: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5147775088ns, electionTimeout:5143ms
recon_1  | 2023-06-01 19:22:58,140 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:44,493 [IPC Server handler 12 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-06-01 19:23:07,710 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState
recon_1  | 2023-06-01 19:22:58,807 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-01 19:22:44,496 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
dn5_1    | 2023-06-01 19:23:07,710 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn5_1    | 2023-06-01 19:23:07,710 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-01 19:22:44,502 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-GrpcLogAppender: send 4db01c28-2341-445f-9eff-7a55fcb1aabf->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0-t2,notify:(t:1, i:0)
dn5_1    | 2023-06-01 19:23:07,710 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-LeaderElection2
recon_1  | 2023-06-01 19:22:59,419 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:23:07,725 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: receive requestVote(ELECTION, 69bc1289-e9f0-444d-ab3d-1548ab888401, group-1C4731943483, 2, (t:0, i:0))
scm1_1   | 2023-06-01 19:22:44,503 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03
recon_1  | 2023-06-01 19:23:07,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a66d7aff-ae9a-4df1-9b68-1c4731943483 reported by 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-01 19:23:07,726 [grpc-default-executor-1] INFO impl.VoteContext: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-CANDIDATE: accept ELECTION from 69bc1289-e9f0-444d-ab3d-1548ab888401: our priority 0 <= candidate's priority 1
scm1_1   | 2023-06-01 19:22:45,725 [grpc-default-executor-0] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-InstallSnapshotResponseHandler: received the first reply 4db01c28-2341-445f-9eff-7a55fcb1aabf<-5e8a32ef-e4cd-4aae-aa6b-c9f950180f03#0:OK-t0,ALREADY_INSTALLED
recon_1  | 2023-06-01 19:23:07,746 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]] moved to OPEN state
dn5_1    | 2023-06-01 19:23:07,726 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: changes role from CANDIDATE to FOLLOWER at term 2 for candidate:69bc1289-e9f0-444d-ab3d-1548ab888401
scm1_1   | 2023-06-01 19:22:45,726 [grpc-default-executor-0] INFO server.GrpcLogAppender: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
recon_1  | 2023-06-01 19:23:44,105 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn5_1.ha_net.
dn5_1    | 2023-06-01 19:23:07,726 [grpc-default-executor-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: shutdown 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-LeaderElection2
recon_1  | 2023-06-01 19:23:44,108 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn2_1.ha_net.
scm1_1   | 2023-06-01 19:22:45,726 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: snapshotIndex: setUnconditionally 0 -> 0
dn5_1    | 2023-06-01 19:23:07,727 [grpc-default-executor-1] INFO impl.RoleInfo: 79bd37f0-b225-4c38-b396-23c942162c92: start 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState
recon_1  | 2023-06-01 19:23:44,174 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn3_1.ha_net.
scm1_1   | 2023-06-01 19:22:45,726 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: matchIndex: setUnconditionally 0 -> 0
dn5_1    | 2023-06-01 19:23:07,728 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-LeaderElection2] INFO impl.LeaderElection: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-LeaderElection2: skip running since this is already CLOSING
recon_1  | 2023-06-01 19:23:44,268 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm1_1   | 2023-06-01 19:22:45,727 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: nextIndex: setUnconditionally 0 -> 1
dn5_1    | 2023-06-01 19:23:07,730 [grpc-default-executor-1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483 replies to ELECTION vote request: 69bc1289-e9f0-444d-ab3d-1548ab888401<-79bd37f0-b225-4c38-b396-23c942162c92#0:OK-t2. Peer's state: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483:t2, leader=null, voted=69bc1289-e9f0-444d-ab3d-1548ab888401, raftlog=Memoized:79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:23:44,277 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm1_1   | 2023-06-01 19:22:45,727 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03 acknowledged installing snapshot
dn5_1    | 2023-06-01 19:23:07,739 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-01 19:23:44,278 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm1_1   | 2023-06-01 19:22:45,728 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: nextIndex: updateToMax old=1, new=1, updated? false
dn5_1    | 2023-06-01 19:23:07,752 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-01 19:23:45,001 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-06-01 19:23:45,005 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-01 19:22:45,871 [grpc-default-executor-1] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: nextIndex: updateUnconditionally 21 -> 0
dn5_1    | 2023-06-01 19:23:08,117 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1C4731943483 with new leaderId: 69bc1289-e9f0-444d-ab3d-1548ab888401
recon_1  | 2023-06-01 19:23:45,005 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-01 19:22:45,873 [grpc-default-executor-0] INFO leader.FollowerInfo: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9->5e8a32ef-e4cd-4aae-aa6b-c9f950180f03: nextIndex: updateUnconditionally 0 -> 0
dn5_1    | 2023-06-01 19:23:08,118 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread1] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: change Leader from null to 69bc1289-e9f0-444d-ab3d-1548ab888401 at term 2 for appendEntries, leader elected after 10901ms
recon_1  | 2023-06-01 19:23:45,006 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-01 19:22:46,447 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 21: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn5_1    | 2023-06-01 19:23:08,169 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread2] INFO server.RaftServer$Division: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483: set configuration 0: peers:[69bc1289-e9f0-444d-ab3d-1548ab888401|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 1d53eca5-989b-4da6-8a3f-f976f64c30fa|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 79bd37f0-b225-4c38-b396-23c942162c92|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-01 19:23:45,006 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-01 19:22:46,467 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-LeaderStateImpl] INFO server.RaftServer$Division: 4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9: set configuration 23: peers:[5dcb1cf7-eeac-4c03-a034-92235b5458e4|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, 4db01c28-2341-445f-9eff-7a55fcb1aabf|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-01 19:23:08,169 [79bd37f0-b225-4c38-b396-23c942162c92-server-thread2] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 2023-06-01 19:23:45,006 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-01 19:22:46,480 [IPC Server handler 12 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 5e8a32ef-e4cd-4aae-aa6b-c9f950180f03.
dn5_1    | 2023-06-01 19:23:08,172 [79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 79bd37f0-b225-4c38-b396-23c942162c92@group-1C4731943483-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a66d7aff-ae9a-4df1-9b68-1c4731943483/current/log_inprogress_0
scm1_1   | 2023-06-01 19:22:51,691 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 67675d8c-abb5-4f7f-9fc5-a7fd084cd2e7, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:48.499Z[UTC]] moved to OPEN state
recon_1  | 2023-06-01 19:23:45,006 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-01 19:23:45,008 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-06-01 19:23:45,008 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 2 
recon_1  | 2023-06-01 19:23:45,099 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-06-01 19:23:45,118 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 5, SequenceNumber diff: 13, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-01 19:23:45,118 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 13 records
scm1_1   | 2023-06-01 19:22:51,823 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
recon_1  | 2023-06-01 19:23:45,134 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-06-01 19:23:45,137 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-06-01 19:23:45,332 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-06-01 19:23:45,368 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
recon_1  | 2023-06-01 19:23:45,430 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-06-01 19:23:55,747 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: New container #2 got from ha_dn3_1.ha_net.
recon_1  | 2023-06-01 19:23:55,760 [FixedThreadPoolWithAffinityExecutor-9-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
scm1_1   | 2023-06-01 19:22:51,889 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:52,521 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:53,097 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6614ab6a-e6f2-43c2-8c89-156762bb27ab, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:79bd37f0-b225-4c38-b396-23c942162c92, CreationTimestamp2023-06-01T19:21:48.741Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:53,148 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:53,201 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:53,731 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 54391af0-7339-4d61-8cba-4312d758e1fc, Nodes: 3d6edd59-5add-4b7b-8003-44199ccc7b59{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3d6edd59-5add-4b7b-8003-44199ccc7b59, CreationTimestamp2023-06-01T19:21:49.280Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:53,772 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:53,782 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:53,839 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:53,976 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a3b4b03-96c4-47b7-9040-e9508d8d4862, Nodes: 69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.133Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:54,041 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:54,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:54,444 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 77894454-4515-4db1-a815-6286ee24275c, Nodes: 71839cc9-95e0-4895-b82f-6f2805ce4ff9{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:71839cc9-95e0-4895-b82f-6f2805ce4ff9, CreationTimestamp2023-06-01T19:21:49.833Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:54,513 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:54,516 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:54,678 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:57,288 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:57,336 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:57,387 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:57,399 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:58,165 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 5b2c3cec-0454-454b-b9bd-a4b05fdcbde0, Nodes: 1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:1d53eca5-989b-4da6-8a3f-f976f64c30fa, CreationTimestamp2023-06-01T19:21:49.241Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:22:58,166 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:58,190 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1_1   | 2023-06-01 19:22:58,256 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1_1   | 2023-06-01 19:22:58,276 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1_1   | 2023-06-01 19:22:58,291 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1_1   | 2023-06-01 19:23:07,757 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a66d7aff-ae9a-4df1-9b68-1c4731943483, Nodes: 79bd37f0-b225-4c38-b396-23c942162c92{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}69bc1289-e9f0-444d-ab3d-1548ab888401{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}1d53eca5-989b-4da6-8a3f-f976f64c30fa{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:69bc1289-e9f0-444d-ab3d-1548ab888401, CreationTimestamp2023-06-01T19:21:49.295Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-01 19:23:09,856 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:23:09,872 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:23:39,857 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:23:39,873 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:23:40,762 [IPC Server handler 76 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1_1   | 2023-06-01 19:23:40,861 [4db01c28-2341-445f-9eff-7a55fcb1aabf@group-477B239F9CA9-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-06-01 19:23:40,893 [IPC Server handler 76 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm1_1   | 2023-06-01 19:24:09,857 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:24:09,875 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:24:39,858 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-01 19:24:39,875 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
