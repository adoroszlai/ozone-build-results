Attaching to ha_scm2_1, ha_s3g_1, ha_dn3_1, ha_dn5_1, ha_om1_1, ha_scm1_1, ha_dn2_1, ha_dn1_1, ha_om3_1, ha_scm3_1, ha_recon_1, ha_dn4_1, ha_om2_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-06-12 10:18:31,776 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 48a94d757b73/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn1_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-06-12 10:18:31,877 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-06-12 10:18:32,293 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-06-12 10:18:33,099 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-06-12 10:18:34,735 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-06-12 10:18:34,736 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-06-12 10:18:36,099 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:48a94d757b73 ip:10.9.0.17
dn1_1    | 2023-06-12 10:18:37,440 [main] INFO reflections.Reflections: Reflections took 1017 ms to scan 2 urls, producing 105 keys and 226 values 
dn1_1    | 2023-06-12 10:18:40,282 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-06-12 10:18:40,784 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-06-12 10:18:42,109 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-06-12T10:18:02.959Z
dn1_1    | 2023-06-12 10:18:42,208 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-06-12 10:18:42,215 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-06-12 10:18:42,231 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-06-12 10:18:42,355 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-06-12 10:18:42,535 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-12 10:18:42,567 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-06-12T10:18:02.955Z
dn1_1    | 2023-06-12 10:18:42,584 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-06-12 10:18:42,587 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-06-12 10:18:42,631 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-06-12 10:18:46,000 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-a78e4ffe-b068-446e-852a-ab70f58541c9/container.db to cache
dn1_1    | 2023-06-12 10:18:46,000 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-a78e4ffe-b068-446e-852a-ab70f58541c9/container.db for volume DS-a78e4ffe-b068-446e-852a-ab70f58541c9
dn1_1    | 2023-06-12 10:18:46,186 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-06-12 10:18:48,147 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-06-12 10:18:48,148 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 2s
dn1_1    | 2023-06-12 10:18:57,217 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-06-12 10:18:57,664 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-12 10:18:57,993 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-06-12 10:18:58,033 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-12 10:18:58,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-06-12 10:18:58,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-12 10:18:58,050 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-06-12 10:18:58,054 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-06-12 10:18:58,054 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-06-12 10:18:58,058 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-06-12 10:18:58,064 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:18:58,066 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-06-12 10:18:58,068 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:18:58,163 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-12 10:18:58,197 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-06-12 10:18:58,216 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-06-12 10:19:00,318 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-06-12 10:19:00,346 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-06-12 10:19:00,365 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-06-12 10:19:00,365 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:00,365 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:00,417 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-12 10:19:00,449 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: found a subdirectory /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e
dn1_1    | 2023-06-12 10:19:00,534 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: addNew group-DD6E9427813E:[] returns group-DD6E9427813E:java.util.concurrent.CompletableFuture@e74a8bb[Not completed]
dn1_1    | 2023-06-12 10:19:00,534 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: found a subdirectory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn1_1    | 2023-06-12 10:19:00,539 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: addNew group-4675AE3D33CA:[] returns group-4675AE3D33CA:java.util.concurrent.CompletableFuture@7cb18a50[Not completed]
dn1_1    | 2023-06-12 10:19:00,539 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: found a subdirectory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn1_1    | 2023-06-12 10:19:00,541 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: addNew group-E4EF5B9B8C4A:[] returns group-E4EF5B9B8C4A:java.util.concurrent.CompletableFuture@4796c761[Not completed]
dn1_1    | 2023-06-12 10:19:00,858 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-06-12 10:19:01,061 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253: new RaftServerImpl for group-DD6E9427813E:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-12 10:19:01,094 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-12 10:19:01,181 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-12 10:19:01,182 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-12 10:19:01,182 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:01,182 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:01,183 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-12 10:19:01,341 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-12 10:19:01,351 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-12 10:19:01,385 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-06-12 10:19:01,414 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-12 10:19:01,434 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-12 10:19:01,654 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:01,733 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:01,781 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-12 10:19:01,782 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-12 10:19:02,261 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-06-12 10:18:32,558 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 1b90e16c96f4/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn2_1    | STARTUP_MSG:   java = 11.0.19
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-06-12 10:18:32,654 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-06-12 10:18:33,066 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-06-12 10:18:33,806 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-06-12 10:18:35,330 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-06-12 10:18:35,331 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-06-12 10:18:36,478 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1b90e16c96f4 ip:10.9.0.18
dn2_1    | 2023-06-12 10:18:37,998 [main] INFO reflections.Reflections: Reflections took 1210 ms to scan 2 urls, producing 105 keys and 226 values 
dn2_1    | 2023-06-12 10:18:41,337 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-06-12 10:18:41,837 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-06-12 10:18:43,187 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-06-12T10:18:03.098Z
dn2_1    | 2023-06-12 10:18:43,271 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-06-12 10:18:43,344 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-06-12 10:18:43,352 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-06-12 10:18:43,579 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-06-12 10:18:43,705 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-12 10:18:43,794 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-06-12T10:18:03.061Z
dn2_1    | 2023-06-12 10:18:43,797 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-06-12 10:18:43,798 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-06-12 10:18:43,800 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-06-12 10:18:44,685 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-2be9a528-5927-44cc-b6e0-e2d9302f084f/container.db to cache
dn2_1    | 2023-06-12 10:18:44,686 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-2be9a528-5927-44cc-b6e0-e2d9302f084f/container.db for volume DS-2be9a528-5927-44cc-b6e0-e2d9302f084f
dn2_1    | 2023-06-12 10:18:44,767 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-06-12 10:18:44,882 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-06-12 10:18:44,882 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-06-12 10:18:56,736 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-06-12 10:18:57,649 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-12 10:18:58,154 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-06-12 10:18:58,859 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-12 10:18:58,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-06-12 10:18:58,941 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-12 10:18:58,945 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-06-12 10:18:58,949 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-06-12 10:18:58,949 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-06-12 10:18:58,950 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-06-12 10:18:58,957 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:18:58,963 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-06-12 10:18:59,004 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-12 10:18:59,107 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-12 10:18:59,127 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-06-12 10:18:59,139 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-06-12 10:19:01,239 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-12 10:19:01,348 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-06-12 10:19:01,363 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-06-12 10:19:01,367 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:19:01,367 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:19:01,370 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-12 10:19:01,463 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: found a subdirectory /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db
dn2_1    | 2023-06-12 10:19:01,587 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: addNew group-6EC5452C34DB:[] returns group-6EC5452C34DB:java.util.concurrent.CompletableFuture@4dfe05fc[Not completed]
dn2_1    | 2023-06-12 10:19:01,796 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-06-12 10:19:02,070 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: new RaftServerImpl for group-6EC5452C34DB:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-12 10:19:02,098 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-12 10:19:02,104 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:19:02,106 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-06-12 10:18:32,246 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 8ab2498196c6/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn5_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | 2023-06-12 10:19:02,550 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:19:02,604 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-12 10:19:02,604 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,636 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-12 10:19:02,637 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,637 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253: new RaftServerImpl for group-4675AE3D33CA:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:02,639 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:02,640 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-12 10:19:02,640 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-12 10:19:02,673 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-12 10:19:02,674 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-06-12 10:19:02,711 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-06-12 10:19:02,733 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:19:02,733 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-12 10:19:02,733 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,734 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-12 10:19:02,734 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,734 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-12 10:19:02,758 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253: new RaftServerImpl for group-E4EF5B9B8C4A:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-12 10:19:02,771 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-12 10:19:02,782 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-12 10:19:02,784 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-12 10:19:02,784 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:02,786 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:02,786 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-12 10:19:02,786 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-12 10:19:02,787 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-12 10:19:02,826 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-12 10:19:02,831 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-12 10:19:02,831 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:02,833 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:02,833 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-12 10:19:02,834 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-12 10:19:02,836 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-06-12 10:19:02,844 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-06-12 10:19:02,844 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:19:02,877 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-12 10:19:02,877 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,882 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-12 10:19:02,883 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-12 10:19:02,883 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-12 10:19:03,093 [main] INFO util.log: Logging initialized @42924ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-06-12 10:19:03,956 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-06-12 10:19:04,006 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-06-12 10:19:04,070 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-06-12 10:19:04,081 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-06-12 10:19:04,107 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-12 10:19:04,116 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-06-12 10:19:04,383 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-06-12 10:19:04,401 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-06-12 10:19:04,409 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn1_1    | 2023-06-12 10:19:04,609 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-06-12 10:19:04,609 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-06-12 10:19:04,629 [main] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-06-12 10:19:04,689 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@273fa9e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-06-12 10:19:04,696 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3757e8e2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-06-12 10:19:05,418 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@571a663c{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-14350262538557515720/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-06-12 10:19:05,466 [main] INFO server.AbstractConnector: Started ServerConnector@58f254b1{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-06-12 10:19:05,466 [main] INFO server.Server: Started @45297ms
dn1_1    | 2023-06-12 10:19:05,487 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-06-12 10:19:05,487 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-06-12 10:19:05,494 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-06-12 10:19:05,664 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-06-12 10:19:05,899 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-06-12 10:19:07,198 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
dn1_1    | 2023-06-12 10:19:07,262 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 2023-06-12 10:19:07,266 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 2023-06-12 10:19:07,269 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-06-12 10:19:07,287 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 2023-06-12 10:19:07,315 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-06-12 10:19:07,338 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-06-12 10:19:07,340 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-06-12 10:19:07,598 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-06-12 10:19:07,685 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-06-12 10:19:10,679 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:10,733 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:10,733 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:11,680 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:11,735 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:11,735 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-12 10:18:30,113 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 0081bf068fd7/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn4_1    | STARTUP_MSG:   java = 11.0.19
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-06-12 10:18:30,217 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-06-12 10:18:30,578 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-06-12 10:18:31,361 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-06-12 10:18:32,880 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-06-12 10:18:32,880 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-06-12 10:18:34,393 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:0081bf068fd7 ip:10.9.0.20
dn4_1    | 2023-06-12 10:18:35,985 [main] INFO reflections.Reflections: Reflections took 1228 ms to scan 2 urls, producing 105 keys and 226 values 
dn4_1    | 2023-06-12 10:18:39,281 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn4_1    | 2023-06-12 10:18:39,781 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-06-12 10:18:41,050 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8747 at 2023-06-12T10:18:03.075Z
dn4_1    | 2023-06-12 10:18:41,217 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-06-12 10:18:41,230 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-06-12 10:18:41,251 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-06-12 10:18:41,451 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-06-12 10:18:41,798 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-12 10:18:41,844 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-06-12T10:18:03.233Z
dn4_1    | 2023-06-12 10:18:41,845 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-06-12 10:18:41,845 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-06-12 10:18:41,894 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-06-12 10:18:44,971 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-0c54b3a1-c8d3-4ce3-9fe6-761dec1a7331/container.db to cache
dn4_1    | 2023-06-12 10:18:44,978 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-0c54b3a1-c8d3-4ce3-9fe6-761dec1a7331/container.db for volume DS-0c54b3a1-c8d3-4ce3-9fe6-761dec1a7331
dn4_1    | 2023-06-12 10:18:45,205 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-06-12 10:18:47,357 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-06-12 10:18:47,357 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 2s
dn4_1    | 2023-06-12 10:18:56,253 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-06-12 10:18:56,555 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-12 10:18:56,954 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-06-12 10:18:57,032 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-12 10:18:57,052 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-06-12 10:18:57,069 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-12 10:18:57,073 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-06-12 10:18:57,078 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-06-12 10:18:57,078 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-06-12 10:18:57,079 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-06-12 10:18:57,093 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:18:57,096 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-06-12 10:18:57,106 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:18:57,186 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-12 10:18:57,235 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-12 10:18:57,245 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-12 10:18:59,592 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-12 10:18:59,627 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-12 10:18:59,680 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-12 10:18:59,692 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:18:59,692 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:18:59,742 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:18:59,787 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: found a subdirectory /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7
dn4_1    | 2023-06-12 10:18:59,849 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-67DF68C3D0F7:[] returns group-67DF68C3D0F7:java.util.concurrent.CompletableFuture@2347df33[Not completed]
dn4_1    | 2023-06-12 10:18:59,849 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: found a subdirectory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn4_1    | 2023-06-12 10:18:59,858 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-4675AE3D33CA:[] returns group-4675AE3D33CA:java.util.concurrent.CompletableFuture@9a7dd20[Not completed]
dn4_1    | 2023-06-12 10:18:59,859 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: found a subdirectory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn4_1    | 2023-06-12 10:18:59,860 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-E4EF5B9B8C4A:[] returns group-E4EF5B9B8C4A:java.util.concurrent.CompletableFuture@6dc825bd[Not completed]
dn4_1    | 2023-06-12 10:19:00,069 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-06-12 10:19:00,275 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-67DF68C3D0F7:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:19:00,286 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-12 10:19:00,322 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-12 10:19:00,342 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:19:00,343 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:00,343 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:19:00,350 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-12 10:19:00,474 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:19:00,501 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:19:00,543 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn4_1    | 2023-06-12 10:19:00,578 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:19:00,586 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:00,806 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:00,934 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:00,948 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-12 10:19:00,984 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-12 10:19:01,514 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-06-12 10:18:32,370 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-06-12 10:18:32,875 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-06-12 10:18:33,603 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-12 10:18:35,208 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-06-12 10:18:35,208 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-06-12 10:18:36,572 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:8ab2498196c6 ip:10.9.0.21
dn5_1    | 2023-06-12 10:18:38,173 [main] INFO reflections.Reflections: Reflections took 1171 ms to scan 2 urls, producing 105 keys and 226 values 
dn5_1    | 2023-06-12 10:18:40,936 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn5_1    | 2023-06-12 10:18:41,378 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-06-12 10:18:42,719 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-06-12T10:18:02.998Z
dn5_1    | 2023-06-12 10:18:42,891 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-06-12 10:18:42,921 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-06-12 10:18:42,945 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-06-12 10:18:43,128 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-06-12 10:18:43,311 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-12 10:18:43,341 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-06-12T10:18:02.983Z
dn5_1    | 2023-06-12 10:18:43,357 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-06-12 10:18:43,364 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-06-12 10:18:43,394 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-06-12 10:18:44,106 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-86ca9b66-8321-4e52-8bc2-a4b0066fba78/container.db to cache
dn5_1    | 2023-06-12 10:18:44,106 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-86ca9b66-8321-4e52-8bc2-a4b0066fba78/container.db for volume DS-86ca9b66-8321-4e52-8bc2-a4b0066fba78
dn5_1    | 2023-06-12 10:18:44,192 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-06-12 10:18:44,201 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-06-12 10:18:44,201 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-06-12 10:18:55,763 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-06-12 10:18:56,366 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-12 10:18:56,806 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-06-12 10:18:58,026 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-12 10:18:58,057 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-06-12 10:18:58,065 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-12 10:18:58,073 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-06-12 10:18:58,073 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-06-12 10:18:58,084 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-06-12 10:18:58,085 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-06-12 10:18:58,095 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:18:58,105 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-06-12 10:18:58,107 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-12 10:18:58,217 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-12 10:18:58,258 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-06-12 10:18:58,276 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-06-12 10:18:59,521 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-06-12 10:18:59,564 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-12 10:18:59,570 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-12 10:18:59,637 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-12 10:18:59,637 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-12 10:18:59,732 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-12 10:18:59,809 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: found a subdirectory /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c
dn5_1    | 2023-06-12 10:18:59,842 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: addNew group-EA217032BE5C:[] returns group-EA217032BE5C:java.util.concurrent.CompletableFuture@ea982e8[Not completed]
dn5_1    | 2023-06-12 10:19:00,300 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-06-12 10:19:00,660 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: new RaftServerImpl for group-EA217032BE5C:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-12 10:19:00,788 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-12 10:19:00,793 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-12 10:19:00,793 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-12 10:19:00,793 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-12 10:19:00,793 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-12 10:19:00,793 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-12 10:19:00,869 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn5_1    | 2023-06-12 10:19:00,893 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-12 10:19:00,896 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-12 10:19:00,947 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-12 10:19:00,957 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-12 10:19:01,127 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-12 10:19:01,180 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-06-12 10:19:01,235 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-12 10:19:01,264 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-12 10:19:01,455 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-06-12 10:19:01,849 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-06-12 10:19:01,868 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-12 10:19:01,903 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-12 10:19:01,959 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-12 10:19:01,972 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-12 10:19:01,973 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-12 10:19:02,107 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:19:02,112 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:19:02,116 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-12 10:19:02,244 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-12 10:19:02,247 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-12 10:19:02,313 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-12 10:19:02,314 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-12 10:19:02,325 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn2_1    | 2023-06-12 10:19:02,490 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:19:02,574 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-06-12 10:19:02,651 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-12 10:19:02,661 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-12 10:19:03,097 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-06-12 10:19:03,402 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-06-12 10:19:03,525 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-12 10:19:03,590 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-12 10:19:03,599 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-06-12 10:19:03,632 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-12 10:19:03,633 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-12 10:19:03,638 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-12 10:19:03,653 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-12 10:19:03,832 [main] INFO util.log: Logging initialized @42487ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-06-12 10:19:05,055 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-06-12 10:19:05,127 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-06-12 10:19:05,150 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-12 10:19:05,152 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-06-12 10:19:05,152 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-12 10:19:05,152 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-06-12 10:19:05,500 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-06-12 10:19:05,568 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-06-12 10:19:05,591 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn2_1    | 2023-06-12 10:19:06,023 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-12 10:19:06,023 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-06-12 10:19:06,042 [main] INFO server.session: node0 Scavenging every 660000ms
dn2_1    | 2023-06-12 10:19:06,166 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c453c34{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-12 10:19:06,202 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@71936a92{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-06-12 10:19:07,333 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49491770{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-9255690309141678754/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-06-12 10:19:07,405 [main] INFO server.AbstractConnector: Started ServerConnector@28b8f98a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-06-12 10:19:07,406 [main] INFO server.Server: Started @46062ms
dn2_1    | 2023-06-12 10:19:07,418 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-06-12 10:19:07,428 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-12 10:19:07,439 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-06-12 10:19:07,674 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-06-12 10:19:07,927 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn2_1    | 2023-06-12 10:19:08,794 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
dn2_1    | 2023-06-12 10:19:08,827 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn2_1    | 2023-06-12 10:19:08,828 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn2_1    | 2023-06-12 10:19:08,829 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 2023-06-12 10:19:08,845 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn2_1    | 2023-06-12 10:19:08,896 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-06-12 10:19:08,958 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-06-12 10:19:08,959 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-06-12 10:19:09,132 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-06-12 10:19:09,206 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-06-12 10:19:12,103 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:12,105 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:12,120 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:13,106 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:13,107 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:13,121 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:14,106 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:14,107 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:14,121 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:15,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:15,122 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:16,108 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:16,122 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:16,145 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 1b90e16c96f4/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45984 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45984 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-06-12 10:19:17,109 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:17,123 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:18,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:18,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:19,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:19,119 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 1b90e16c96f4/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59298 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 2023-06-12 10:19:01,974 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-12 10:19:02,020 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-12 10:19:02,184 [main] INFO util.log: Logging initialized @40629ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-06-12 10:19:02,965 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-06-12 10:19:03,023 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-06-12 10:19:03,103 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-06-12 10:19:03,122 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-06-12 10:19:03,126 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-06-12 10:19:03,127 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-06-12 10:19:03,606 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-06-12 10:19:03,666 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-06-12 10:19:03,677 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn5_1    | 2023-06-12 10:19:03,864 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-12 10:19:03,867 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-06-12 10:19:03,868 [main] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 2023-06-12 10:19:03,960 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6380e9e9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-06-12 10:19:03,966 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@285a4fe3{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-06-12 10:19:04,668 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@632cf7d3{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-8518055794005516836/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn5_1    | 2023-06-12 10:19:04,749 [main] INFO server.AbstractConnector: Started ServerConnector@4584304{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-06-12 10:19:04,749 [main] INFO server.Server: Started @43195ms
dn5_1    | 2023-06-12 10:19:04,781 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-06-12 10:19:04,782 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-12 10:19:04,789 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-06-12 10:19:04,926 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 2023-06-12 10:19:05,138 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn5_1    | 2023-06-12 10:19:06,834 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
dn5_1    | 2023-06-12 10:19:07,104 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn5_1    | 2023-06-12 10:19:07,104 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn5_1    | 2023-06-12 10:19:07,157 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn5_1    | 2023-06-12 10:19:07,183 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 2023-06-12 10:19:07,231 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-06-12 10:19:07,366 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn5_1    | 2023-06-12 10:19:07,368 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 2023-06-12 10:19:07,536 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-06-12 10:19:07,668 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-06-12 10:19:10,686 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:10,686 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:10,707 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:11,686 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:11,687 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:11,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:12,688 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:12,688 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:12,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:13,688 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:13,689 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:13,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:14,690 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:14,710 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:14,791 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 8ab2498196c6/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44122 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:44122 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-06-12 10:19:15,691 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:15,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:16,692 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:16,714 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:17,694 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:17,715 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:18,695 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:18,697 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 8ab2498196c6/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:59838 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-06-12 10:18:33,074 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 7c95fade90b7/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn3_1    | STARTUP_MSG:   java = 11.0.19
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59298 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn2_1    | 2023-06-12 10:19:19,128 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:20,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:20,129 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:21,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:21,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:22,113 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:22,131 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:23,114 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:24,115 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:25,116 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:26,117 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:26,118 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 1b90e16c96f4/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:59838 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-06-12 10:19:18,716 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:19,696 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:19,717 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:20,696 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:20,717 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:21,698 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:21,718 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:22,719 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:23,723 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:24,723 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:24,724 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 8ab2498196c6/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn2_1    | 	... 12 more
dn2_1    | 2023-06-12 10:19:26,958 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-06-12 10:19:26,965 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-06-12 10:19:27,120 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:27,286 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-06-12 10:19:27,287 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:19:27,388 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/in_use.lock acquired by nodename 7@1b90e16c96f4
dn2_1    | 2023-06-12 10:19:27,407 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29} from /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/raft-meta
dn2_1    | 2023-06-12 10:19:27,590 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: set configuration 3: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:27,660 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO ratis.ContainerStateMachine: group-6EC5452C34DB: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-06-12 10:19:28,122 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:28,212 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-12 10:19:28,241 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-12 10:19:28,243 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:19:28,246 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-12 10:19:28,251 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-12 10:19:28,262 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:19:28,279 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-12 10:19:28,282 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-12 10:19:28,284 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:19:28,305 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db
dn2_1    | 2023-06-12 10:19:28,310 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-12 10:19:28,344 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:19:28,351 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:19:28,355 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-12 10:19:28,369 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-12 10:19:28,373 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-12 10:19:28,377 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-12 10:19:28,377 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-12 10:19:28,435 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-12 10:19:28,443 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:19:28,546 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:19:28,552 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:19:28,555 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-12 10:19:28,817 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:01,639 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-06-12 10:19:01,739 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-06-12 10:19:01,999 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:19:02,040 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:19:02,054 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,054 [main] INFO util.log: Logging initialized @43803ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-06-12 10:19:02,094 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:19:02,098 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,103 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:19:02,107 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-4675AE3D33CA:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:19:02,147 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-12 10:19:02,147 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-12 10:19:02,147 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:19:02,147 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:02,147 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:19:02,148 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-12 10:19:02,148 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:19:02,149 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:19:02,157 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:19:02,164 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:02,165 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:02,204 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:02,209 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-12 10:19:02,211 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-12 10:19:02,212 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-06-12 10:19:02,213 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:19:02,214 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:19:02,214 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,214 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:19:02,218 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,219 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:19:02,285 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-E4EF5B9B8C4A:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:19:02,286 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-12 10:19:02,292 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-12 10:19:02,292 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:19:02,292 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:02,292 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:19:02,294 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-12 10:19:02,295 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:19:02,295 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:19:02,295 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:19:02,295 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:02,296 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:02,304 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:02,305 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-12 10:19:02,305 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-12 10:19:02,306 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-06-12 10:19:02,323 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:19:02,329 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:19:02,330 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,331 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:19:02,336 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:19:02,336 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:19:03,338 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-06-12 10:19:03,377 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-06-12 10:19:03,416 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-06-12 10:19:03,437 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-06-12 10:19:03,449 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-12 10:19:03,451 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-06-12 10:19:03,604 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-06-12 10:19:03,633 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-06-12 10:19:03,639 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn4_1    | 2023-06-12 10:19:03,772 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-12 10:19:03,789 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-06-12 10:19:03,795 [main] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-06-12 10:19:03,849 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d71b500{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-06-12 10:19:03,851 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@437c4b25{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-06-12 10:19:04,807 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6ad6443{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-5071240647930694588/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-06-12 10:19:04,844 [main] INFO server.AbstractConnector: Started ServerConnector@21f7e537{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-06-12 10:19:04,847 [main] INFO server.Server: Started @46596ms
dn4_1    | 2023-06-12 10:19:04,849 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-06-12 10:19:04,855 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-06-12 10:19:04,859 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-06-12 10:19:05,044 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn4_1    | 2023-06-12 10:19:05,301 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn4_1    | 2023-06-12 10:19:06,668 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
dn4_1    | 2023-06-12 10:19:06,739 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn4_1    | 2023-06-12 10:19:06,740 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn4_1    | 2023-06-12 10:19:06,751 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-06-12 10:19:06,752 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn4_1    | 2023-06-12 10:19:06,900 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-06-12 10:19:07,013 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn4_1    | 2023-06-12 10:19:07,017 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-06-12 10:19:07,232 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-06-12 10:19:07,275 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-06-12 10:19:10,236 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:10,242 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:10,242 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:11,238 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:11,244 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:11,245 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:28,830 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_0-0
dn2_1    | 2023-06-12 10:19:28,874 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: set configuration 1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:28,895 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_1-2
dn2_1    | 2023-06-12 10:19:28,918 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: set configuration 3: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:28,927 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_inprogress_3
dn2_1    | 2023-06-12 10:19:28,950 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-06-12 10:19:28,954 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-06-12 10:19:29,126 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:29,606 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: start as a follower, conf=3: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:29,611 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn2_1    | 2023-06-12 10:19:29,621 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState
dn2_1    | 2023-06-12 10:19:29,623 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-12 10:19:29,623 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:19:29,658 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6EC5452C34DB,id=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:19:29,686 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-12 10:19:29,686 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-12 10:19:29,691 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-12 10:19:29,693 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-12 10:19:29,753 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start RPC server
dn2_1    | 2023-06-12 10:19:29,804 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: GrpcService started, listening on 9858
dn2_1    | 2023-06-12 10:19:29,854 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: GrpcService started, listening on 9856
dn2_1    | 2023-06-12 10:19:29,897 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: GrpcService started, listening on 9857
dn2_1    | 2023-06-12 10:19:29,967 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: Started
dn2_1    | 2023-06-12 10:19:29,981 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 is started using port 9858 for RATIS
dn2_1    | 2023-06-12 10:19:29,983 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-06-12 10:19:29,983 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-06-12 10:19:30,054 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-12 10:19:30,119 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-06-12 10:19:30,136 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:31,141 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:32,142 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:33,143 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:34,144 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:34,739 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO impl.FollowerState: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5118068223ns, electionTimeout:5115ms
dn4_1    | 2023-06-12 10:19:12,239 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:12,245 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:12,245 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:13,239 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:13,245 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:13,246 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:14,240 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:14,246 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:14,247 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:14,298 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 0081bf068fd7/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50252 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:50252 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 2023-06-12 10:19:34,740 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState
dn2_1    | 2023-06-12 10:19:34,741 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn2_1    | 2023-06-12 10:19:34,750 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-06-12 10:19:34,753 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1
dn2_1    | 2023-06-12 10:19:34,782 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:34,794 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 2023-06-12 10:19:34,990 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:34,994 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn2_1    | 2023-06-12 10:19:34,995 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1
dn2_1    | 2023-06-12 10:19:35,006 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-06-12 10:19:35,009 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6EC5452C34DB with new leaderId: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:19:35,061 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: change Leader from null to 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 at term 4 for becomeLeader, leader elected after 32519ms
dn2_1    | 2023-06-12 10:19:35,339 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:35,601 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-12 10:19:35,823 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:19:35,845 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-06-12 10:19:35,919 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-12 10:19:35,987 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-06-12 10:19:36,002 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-12 10:19:36,119 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:19:36,152 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-12 10:19:36,182 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderStateImpl
dn2_1    | 2023-06-12 10:19:36,258 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-06-12 10:19:36,295 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_inprogress_3 to /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_3-4
dn2_1    | 2023-06-12 10:19:36,352 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:36,369 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/current/log_inprogress_5
dn2_1    | 2023-06-12 10:19:36,415 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderElection1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: set configuration 5: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:19:37,354 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:38,355 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:39,356 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-06-12 10:18:33,152 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-12 10:18:33,584 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-06-12 10:18:34,399 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-06-12 10:18:35,960 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-06-12 10:18:35,977 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-06-12 10:18:37,425 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:7c95fade90b7 ip:10.9.0.19
dn3_1    | 2023-06-12 10:18:38,794 [main] INFO reflections.Reflections: Reflections took 1055 ms to scan 2 urls, producing 105 keys and 226 values 
dn3_1    | 2023-06-12 10:18:41,240 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn3_1    | 2023-06-12 10:18:41,540 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-06-12 10:18:42,508 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8747 at 2023-06-12T10:18:03.082Z
dn3_1    | 2023-06-12 10:18:42,573 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-06-12 10:18:42,587 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-06-12 10:18:42,602 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-06-12 10:18:42,836 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-06-12 10:18:42,920 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-12 10:18:42,934 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-06-12T10:18:02.996Z
dn3_1    | 2023-06-12 10:18:42,941 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-06-12 10:18:42,941 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-06-12 10:18:42,946 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-06-12 10:18:46,386 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-96770141-e260-428c-abf4-c75d9f60a20a/container.db to cache
dn1_1    | 2023-06-12 10:19:12,681 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:12,737 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:12,738 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:13,682 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:13,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:13,739 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:14,683 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:14,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:14,785 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 48a94d757b73/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:60570 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:60570 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-06-12 10:19:15,685 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:15,740 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:16,686 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:16,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:17,687 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:17,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:18,688 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:18,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:18,750 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 48a94d757b73/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48686 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:48686 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn4_1    | 2023-06-12 10:19:15,241 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:15,247 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:16,243 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:16,248 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:17,244 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-12 10:18:31,494 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 91d1810c0573/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--upgrade]
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:39Z
om3_1    | STARTUP_MSG:   java = 11.0.19
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om3_1    | ************************************************************/
om3_1    | 2023-06-12 10:18:31,577 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-06-12 10:18:41,923 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-06-12 10:18:46,488 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-12 10:18:47,227 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-12 10:18:47,228 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-12 10:18:47,254 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:18:47,842 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
om3_1    | 2023-06-12 10:18:50,354 [main] INFO reflections.Reflections: Reflections took 1998 ms to scan 1 urls, producing 135 keys and 393 values [using 2 cores]
om3_1    | 2023-06-12 10:18:50,599 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:18:52,395 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-12 10:18:52,794 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-12 10:18:59,317 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om3_1    | 2023-06-12 10:19:01,320 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om3_1    | 2023-06-12 10:19:03,322 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om3_1    | 2023-06-12 10:19:05,324 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om3_1    | 2023-06-12 10:19:07,325 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om3_1    | 2023-06-12 10:19:09,327 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om3_1    | 2023-06-12 10:19:11,328 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om3_1    | 2023-06-12 10:19:13,332 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om3_1    | 2023-06-12 10:19:15,333 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om3_1    | 2023-06-12 10:19:17,403 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om3_1    | 2023-06-12 10:19:19,405 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om3_1    | 2023-06-12 10:19:21,407 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om3_1    | 2023-06-12 10:19:23,419 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-12 10:18:32,683 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = cb0461c7989e/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:39Z
om1_1    | STARTUP_MSG:   java = 11.0.19
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-06-12 10:18:32,775 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-12 10:18:42,330 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-12 10:18:45,801 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-12 10:18:46,113 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-12 10:18:46,122 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-12 10:18:46,159 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:18:46,615 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
om1_1    | 2023-06-12 10:18:49,631 [main] INFO reflections.Reflections: Reflections took 2301 ms to scan 1 urls, producing 135 keys and 393 values [using 2 cores]
om1_1    | 2023-06-12 10:18:49,990 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:18:52,118 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-12 10:18:52,599 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-12 10:18:59,449 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om1_1    | 2023-06-12 10:19:01,451 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om1_1    | 2023-06-12 10:19:03,452 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om1_1    | 2023-06-12 10:19:05,454 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn2_1    | 2023-06-12 10:19:40,358 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:41,359 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:42,361 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:43,362 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:44,363 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:45,364 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:46,365 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:19:57,788 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-12 10:20:30,122 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-06-12 10:21:06,019 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-06-12 10:21:06,020 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-06-12 10:21:06,021 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-06-12 10:21:06,022 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn2_1    | 2023-06-12 10:21:06,024 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-06-12 10:21:06,024 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-06-12 10:21:06,025 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-06-12 10:21:06,025 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-06-12 10:21:06,026 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-06-12 10:21:06,026 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-06-12 10:21:06,026 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn2_1    | 2023-06-12 10:21:30,122 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-06-12 10:21:36,019 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-06-12 10:21:36,030 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: remove    LEADER 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB:t4, leader=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, voted=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, raftlog=Memoized:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLog:OPENED:c6, conf=5: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-06-12 10:21:36,032 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: shutdown
dn2_1    | 2023-06-12 10:21:36,033 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-6EC5452C34DB,id=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:21:36,033 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-LeaderStateImpl
dn2_1    | 2023-06-12 10:21:36,045 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-PendingRequests: sendNotLeaderResponses
dn2_1    | 2023-06-12 10:21:36,048 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater: set stopIndex = 6
dn2_1    | 2023-06-12 10:21:36,048 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-6EC5452C34DB: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/sm/snapshot.4_6
dn2_1    | 2023-06-12 10:21:36,050 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-6EC5452C34DB: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db/sm/snapshot.4_6 took: 2 ms
dn2_1    | 2023-06-12 10:21:36,052 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater] INFO impl.StateMachineUpdater: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater: Took a snapshot at index 6
dn2_1    | 2023-06-12 10:21:36,052 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater] INFO impl.StateMachineUpdater: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn2_1    | 2023-06-12 10:21:36,055 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: closes. applyIndex: 6
dn2_1    | 2023-06-12 10:21:36,544 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB-SegmentedRaftLogWorker close()
dn2_1    | 2023-06-12 10:21:36,550 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-6EC5452C34DB: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/e44c9ee0-8daa-4eef-b191-6ec5452c34db
dn2_1    | 2023-06-12 10:21:36,552 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db command on datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29.
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn5_1    | 	... 12 more
dn5_1    | 2023-06-12 10:19:25,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:26,709 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 8ab2498196c6/10.9.0.21 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:53174 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:53174 remote=scm2/10.9.0.15:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-06-12 10:19:26,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:26,841 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-06-12 10:19:26,847 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn5_1    | 2023-06-12 10:19:27,259 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn5_1    | 2023-06-12 10:19:27,261 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn5_1    | 2023-06-12 10:19:27,551 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/in_use.lock acquired by nodename 7@8ab2498196c6
dn5_1    | 2023-06-12 10:19:27,578 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3} from /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/raft-meta
dn5_1    | 2023-06-12 10:19:27,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:27,817 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: set configuration 3: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-12 10:18:31,236 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 8ac830dea21e/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--upgrade]
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | 2023-06-12 10:19:07,462 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om1_1    | 2023-06-12 10:19:09,464 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om1_1    | 2023-06-12 10:19:11,466 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om1_1    | 2023-06-12 10:19:13,467 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om1_1    | 2023-06-12 10:19:15,469 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om1_1    | 2023-06-12 10:19:17,518 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 2023-06-12 10:19:19,519 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 2023-06-12 10:19:21,521 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-06-12 10:19:23,532 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn2_1    | 2023-06-12 10:22:06,072 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: addNew group-D83197229949:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-D83197229949:java.util.concurrent.CompletableFuture@4b80ca8[Not completed]
dn2_1    | 2023-06-12 10:22:06,078 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: new RaftServerImpl for group-D83197229949:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-12 10:22:06,079 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-12 10:22:06,080 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:22:06,080 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-12 10:22:06,080 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:06,080 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:22:06,080 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-12 10:22:06,081 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-12 10:22:06,081 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-12 10:22:06,084 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-12 10:22:06,085 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-12 10:22:06,085 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:06,085 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:06,086 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-12 10:22:06,087 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-12 10:22:06,087 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-06-12 10:22:06,088 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-12 10:22:06,090 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-12 10:22:06,090 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-12 10:22:06,091 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-12 10:22:06,091 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-12 10:22:06,091 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-12 10:22:06,091 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f8e4ee74-e97b-4a76-a7a0-d83197229949 does not exist. Creating ...
dn2_1    | 2023-06-12 10:22:06,097 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f8e4ee74-e97b-4a76-a7a0-d83197229949/in_use.lock acquired by nodename 7@1b90e16c96f4
dn2_1    | 2023-06-12 10:22:06,099 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f8e4ee74-e97b-4a76-a7a0-d83197229949 has been successfully formatted.
dn2_1    | 2023-06-12 10:22:06,103 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO ratis.ContainerStateMachine: group-D83197229949: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-12 10:22:06,105 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-12 10:22:06,105 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-12 10:22:06,114 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,114 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-12 10:22:06,114 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f8e4ee74-e97b-4a76-a7a0-d83197229949
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:06,125 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-12 10:22:06,126 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,131 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,179 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:22:06,179 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:22:06,179 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:06,180 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-12 10:22:06,180 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-12 10:22:06,181 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:06,181 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-12 10:22:06,183 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState
dn2_1    | 2023-06-12 10:22:06,184 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D83197229949,id=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:22:06,185 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:06,185 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-12 10:22:06,185 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-12 10:22:06,185 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-12 10:22:06,186 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-12 10:22:06,186 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:06,188 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=f8e4ee74-e97b-4a76-a7a0-d83197229949
dn2_1    | 2023-06-12 10:22:06,190 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f8e4ee74-e97b-4a76-a7a0-d83197229949.
dn2_1    | 2023-06-12 10:22:06,191 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: addNew group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1B5181454F64:java.util.concurrent.CompletableFuture@75868288[Not completed]
dn2_1    | 2023-06-12 10:22:06,203 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: new RaftServerImpl for group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-12 10:22:06,203 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-12 10:22:06,203 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:22:06,204 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-12 10:22:06,204 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:06,204 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:22:06,204 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-06-12 10:19:19,689 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:19,743 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:20,690 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:20,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:21,691 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:21,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:22,746 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:23,747 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:24,751 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:24,754 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 48a94d757b73/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-12 10:19:25,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:26,699 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 48a94d757b73/10.9.0.17 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:56608 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:56608 remote=scm2/10.9.0.15:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:39Z
om2_1    | STARTUP_MSG:   java = 11.0.19
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | ************************************************************/
om2_1    | 2023-06-12 10:18:31,307 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-12 10:18:41,199 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-12 10:18:44,437 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-12 10:18:45,019 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-12 10:18:45,028 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-12 10:18:45,107 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-12 10:18:46,060 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = FILESYSTEM_SNAPSHOT (version = 5)
om2_1    | 2023-06-12 10:18:48,947 [main] INFO reflections.Reflections: Reflections took 1620 ms to scan 1 urls, producing 135 keys and 393 values [using 2 cores]
om2_1    | 2023-06-12 10:18:49,292 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-12 10:18:51,976 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-12 10:18:52,437 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-12 10:18:59,054 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om2_1    | 2023-06-12 10:19:01,058 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om2_1    | 2023-06-12 10:19:03,059 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om2_1    | 2023-06-12 10:19:05,062 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn3_1    | 2023-06-12 10:18:46,386 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/DS-96770141-e260-428c-abf4-c75d9f60a20a/container.db for volume DS-96770141-e260-428c-abf4-c75d9f60a20a
dn3_1    | 2023-06-12 10:18:46,625 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-06-12 10:18:48,596 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-06-12 10:18:48,632 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 2s
dn3_1    | 2023-06-12 10:18:56,369 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-06-12 10:18:56,886 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-12 10:18:57,289 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-06-12 10:18:57,413 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-12 10:18:57,441 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-06-12 10:18:57,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-12 10:18:57,442 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-06-12 10:18:57,444 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-06-12 10:18:57,445 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-06-12 10:18:57,457 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-06-12 10:18:57,458 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:18:57,458 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-06-12 10:18:57,478 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-12 10:18:57,554 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-12 10:18:57,596 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-06-12 10:18:57,621 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-06-12 10:19:00,037 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-06-12 10:19:00,085 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-06-12 10:19:00,089 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-06-12 10:19:00,090 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:00,094 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-12 10:19:00,157 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-12 10:19:00,189 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: found a subdirectory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn3_1    | 2023-06-12 10:19:00,297 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: addNew group-4675AE3D33CA:[] returns group-4675AE3D33CA:java.util.concurrent.CompletableFuture@e74a8bb[Not completed]
dn3_1    | 2023-06-12 10:19:00,301 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: found a subdirectory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn3_1    | 2023-06-12 10:19:00,301 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: addNew group-E4EF5B9B8C4A:[] returns group-E4EF5B9B8C4A:java.util.concurrent.CompletableFuture@7cb18a50[Not completed]
dn3_1    | 2023-06-12 10:19:00,301 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: found a subdirectory /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6
dn3_1    | 2023-06-12 10:19:00,303 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: addNew group-F138DFAD79A6:[] returns group-F138DFAD79A6:java.util.concurrent.CompletableFuture@4796c761[Not completed]
dn3_1    | 2023-06-12 10:19:00,565 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-06-12 10:19:00,812 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: new RaftServerImpl for group-4675AE3D33CA:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-12 10:19:00,856 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-12 10:19:00,862 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-12 10:19:00,865 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-12 10:19:00,865 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:00,865 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-12 10:19:00,871 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-12 10:19:01,033 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-12 10:19:01,055 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-12 10:19:01,170 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-12 10:19:01,225 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-12 10:19:01,242 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-06-12 10:19:01,494 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:01,593 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-06-12 10:19:01,677 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-12 10:19:01,677 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-12 10:19:01,905 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-06-12 10:19:17,249 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:18,245 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:18,249 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:19,246 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:19,250 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:19,254 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 0081bf068fd7/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:34692 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:34692 remote=scm1/10.9.0.14:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn4_1    | 2023-06-12 10:19:20,247 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:20,251 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:21,248 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:21,252 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:22,249 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:22,256 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:23,257 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:24,258 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:24,260 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 0081bf068fd7/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om1_1    | 2023-06-12 10:19:25,556 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7740519e-fe5b-4936-af00-27e2eba71ce5 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om1_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om1_1    | 2023-06-12 10:19:27,558 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From cb0461c7989e/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om1_1    | 2023-06-12 10:19:34,537 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-06-12 10:19:34,890 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:19:37,192 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1    | 2023-06-12 10:19:38,425 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-06-12 10:19:38,529 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1    | 2023-06-12 10:19:38,605 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:19:38,943 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1    | 2023-06-12 10:19:38,956 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om1_1    | java.io.IOException: Permission denied
om1_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om1_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:140)
om1_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:116)
om1_1    | 	at org.apache.hadoop.hdds.utils.db.managed.ManagedSSTDumpTool.<clinit>(ManagedSSTDumpTool.java:39)
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:268)
om1_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om1_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:654)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:736)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om1_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om1_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om1_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om1_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om1_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om1_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om1_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om1_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om1_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om1_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om1_1    | 2023-06-12 10:19:38,973 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1    | 2023-06-12 10:19:39,930 [main] WARN om.OzoneManager: Prepare marker file index 110 does not match DB prepare index 109. Writing DB index to prepare file and maintaining prepared state.
om1_1    | 2023-06-12 10:19:39,935 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 109 to file /data/metadata/current/prepareMarker
om1_1    | 2023-06-12 10:19:40,433 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-12 10:19:40,434 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-06-12 10:19:40,586 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1    | 2023-06-12 10:19:40,592 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-06-12 10:19:42,010 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-12 10:19:42,181 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-12 10:19:42,676 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-06-12 10:19:42,739 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:110)
om1_1    | 2023-06-12 10:19:43,230 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-06-12 10:19:43,292 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-12 10:19:43,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-12 10:19:43,302 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-12 10:19:43,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-12 10:19:43,308 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-06-12 10:19:43,313 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-06-12 10:19:43,315 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn1_1    | 2023-06-12 10:19:26,772 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:26,862 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-06-12 10:19:26,872 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-06-12 10:19:27,268 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-06-12 10:19:27,271 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3c030830-a72e-4f6f-8f54-144518af6253
dn3_1    | 2023-06-12 10:19:02,511 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-12 10:19:02,529 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-12 10:19:02,555 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,555 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,556 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,563 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,566 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: new RaftServerImpl for group-E4EF5B9B8C4A:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-12 10:19:02,614 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-12 10:19:02,615 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-12 10:19:02,618 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-12 10:19:02,619 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:02,619 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-12 10:19:02,620 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-12 10:19:02,622 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-12 10:19:02,624 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-12 10:19:02,644 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-12 10:19:02,644 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-12 10:19:02,645 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:02,649 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-06-12 10:19:02,650 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-12 10:19:02,651 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-12 10:19:02,651 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-06-12 10:19:02,660 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-12 10:19:02,665 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-12 10:19:02,668 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,669 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,670 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,685 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,733 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: new RaftServerImpl for group-F138DFAD79A6:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-12 10:19:02,734 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-12 10:19:02,735 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-12 10:19:02,738 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-12 10:19:02,756 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:02,758 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-12 10:19:02,758 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-12 10:19:02,759 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-12 10:19:02,759 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-12 10:19:02,761 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-12 10:19:02,761 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-12 10:19:02,762 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:19:02,762 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-06-12 10:19:02,766 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-12 10:19:02,766 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-12 10:19:02,766 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-06-12 10:19:27,962 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO ratis.ContainerStateMachine: group-EA217032BE5C: Setting the last applied index to (t:3, i:4)
dn5_1    | 2023-06-12 10:19:28,547 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-12 10:19:28,582 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-12 10:19:28,583 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:19:28,584 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-12 10:19:28,592 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-12 10:19:28,597 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:19:28,618 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-12 10:19:28,619 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-12 10:19:28,619 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:19:28,666 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c
dn5_1    | 2023-06-12 10:19:28,668 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-12 10:19:28,672 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:19:28,678 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:19:28,679 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-12 10:19:28,680 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-12 10:19:28,683 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-12 10:19:28,684 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-12 10:19:28,685 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-12 10:19:28,736 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:28,762 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-12 10:19:28,775 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:19:28,826 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:19:28,829 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:19:28,835 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-12 10:19:29,117 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: set configuration 0: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:29,129 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_0-0
dn5_1    | 2023-06-12 10:19:29,138 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: set configuration 1: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:29,146 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_1-2
dn5_1    | 2023-06-12 10:19:29,151 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: set configuration 3: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:29,152 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_inprogress_3
dn5_1    | 2023-06-12 10:19:29,161 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn5_1    | 2023-06-12 10:19:29,161 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn5_1    | 2023-06-12 10:19:29,402 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: start as a follower, conf=3: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:29,402 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn5_1    | 2023-06-12 10:19:29,429 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState
dn5_1    | 2023-06-12 10:19:29,511 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA217032BE5C,id=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn3_1    | 2023-06-12 10:19:02,773 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-06-12 10:19:02,777 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-12 10:19:02,800 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-12 10:19:02,800 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,802 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,802 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-12 10:19:02,803 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-12 10:19:02,850 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-06-12 10:19:03,015 [main] INFO util.log: Logging initialized @41507ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-06-12 10:19:03,918 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-06-12 10:19:03,974 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-06-12 10:19:04,039 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-06-12 10:19:04,058 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-06-12 10:19:04,066 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-06-12 10:19:04,067 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-06-12 10:19:04,361 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-06-12 10:19:04,396 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-06-12 10:19:04,403 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn3_1    | 2023-06-12 10:19:04,614 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-06-12 10:19:04,614 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-06-12 10:19:04,632 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-06-12 10:19:04,718 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b435211{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-06-12 10:19:04,752 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@58aa1d72{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-06-12 10:19:05,585 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2d130ac4{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-5035982447944178296/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-06-12 10:19:05,692 [main] INFO server.AbstractConnector: Started ServerConnector@435e416c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-06-12 10:19:05,696 [main] INFO server.Server: Started @44188ms
dn3_1    | 2023-06-12 10:19:05,704 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-06-12 10:19:05,704 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-06-12 10:19:05,713 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-06-12 10:19:05,929 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-06-12 10:19:06,520 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn3_1    | 2023-06-12 10:19:08,006 [Listener at 0.0.0.0/9864] INFO hdds.HddsUtils: Restoring thread name: main
dn3_1    | 2023-06-12 10:19:08,077 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn3_1    | 2023-06-12 10:19:08,077 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-06-12 10:19:08,093 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn3_1    | 2023-06-12 10:19:08,077 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-06-12 10:19:08,132 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-06-12 10:19:08,159 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-06-12 10:19:08,161 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-06-12 10:19:08,496 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-06-12 10:19:08,588 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-06-12 10:19:11,377 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:11,382 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:11,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:12,378 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:12,383 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:12,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-12 10:19:43,337 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-12 10:19:43,338 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-06-12 10:19:43,346 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-12 10:19:43,458 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-12 10:19:43,470 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-06-12 10:19:43,470 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-06-12 10:19:44,971 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-06-12 10:19:44,995 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-06-12 10:19:45,001 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-06-12 10:19:45,005 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-12 10:19:45,008 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-12 10:19:45,027 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-12 10:19:45,062 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-06-12 10:19:45,097 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@6cbb7a7d[Not completed]
om1_1    | 2023-06-12 10:19:45,137 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-06-12 10:19:45,162 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-06-12 10:19:45,262 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-06-12 10:19:45,281 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-06-12 10:19:45,292 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-12 10:19:45,293 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-06-12 10:19:45,294 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-12 10:19:45,298 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-12 10:19:45,300 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-06-12 10:19:45,408 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-06-12 10:19:45,412 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-12 10:19:45,480 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-06-12 10:19:45,485 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-06-12 10:19:45,887 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-06-12 10:19:45,953 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1    | 2023-06-12 10:19:46,085 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-06-12 10:19:46,100 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-06-12 10:19:46,581 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1    | 2023-06-12 10:19:47,078 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-12 10:19:47,127 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-06-12 10:19:47,131 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-06-12 10:19:47,136 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-06-12 10:19:47,152 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-06-12 10:19:47,160 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-06-12 10:19:49,213 [main] INFO reflections.Reflections: Reflections took 3754 ms to scan 8 urls, producing 24 keys and 625 values [using 2 cores]
om1_1    | 2023-06-12 10:19:49,949 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-06-12 10:19:50,006 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-06-12 10:19:50,213 [Listener at om1/9862] INFO hdds.HddsUtils: Restoring thread name: main
om1_1    | 2023-06-12 10:19:52,539 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om1_1    | 2023-06-12 10:19:53,099 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-06-12 10:19:53,155 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-06-12 10:19:53,158 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-06-12 10:19:53,591 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-06-12 10:19:53,595 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-06-12 10:19:53,630 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@cb0461c7989e
om1_1    | 2023-06-12 10:19:53,665 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om1_1    | 2023-06-12 10:19:53,770 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:27,356 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/in_use.lock acquired by nodename 7@48a94d757b73
dn1_1    | 2023-06-12 10:19:27,358 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/in_use.lock acquired by nodename 7@48a94d757b73
dn1_1    | 2023-06-12 10:19:27,357 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/in_use.lock acquired by nodename 7@48a94d757b73
dn1_1    | 2023-06-12 10:19:27,399 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=3c030830-a72e-4f6f-8f54-144518af6253} from /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/raft-meta
dn1_1    | 2023-06-12 10:19:27,453 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-12 10:19:27,417 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=78f8fca8-1c45-4717-8e79-22872958dcce} from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/raft-meta
dn1_1    | 2023-06-12 10:19:27,417 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=d6f449c5-6fac-4083-a27e-b83edf3e8b1c} from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/raft-meta
dn1_1    | 2023-06-12 10:19:27,593 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:27,602 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: set configuration 3: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:27,603 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:27,670 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Setting the last applied index to (t:6, i:20)
dn1_1    | 2023-06-12 10:19:27,685 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Setting the last applied index to (t:5, i:47)
dn1_1    | 2023-06-12 10:19:27,697 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO ratis.ContainerStateMachine: group-DD6E9427813E: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-06-12 10:19:27,775 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:28,181 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-12 10:19:28,181 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-12 10:19:28,228 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-12 10:19:28,231 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-12 10:19:28,232 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,234 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-12 10:19:28,235 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-12 10:19:28,236 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,236 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-12 10:19:28,237 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-12 10:19:28,239 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-12 10:19:28,239 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-12 10:19:53,778 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-12 10:19:53,809 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-06-12 10:19:53,810 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-12 10:19:53,816 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-06-12 10:19:53,820 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-12 10:19:53,829 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-12 10:19:53,868 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-06-12 10:19:53,871 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-06-12 10:19:53,872 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-12 10:19:53,900 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-06-12 10:19:53,903 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-06-12 10:19:53,905 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-06-12 10:19:53,912 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-12 10:19:53,914 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-06-12 10:19:53,917 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-12 10:19:53,920 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-06-12 10:19:53,926 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-12 10:19:53,928 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-06-12 10:19:53,972 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-06-12 10:19:53,975 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-12 10:19:54,032 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-06-12 10:19:54,039 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-12 10:19:54,042 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-06-12 10:19:54,083 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 110
om1_1    | 2023-06-12 10:19:54,084 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-12 10:19:54,094 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:19:54,097 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om1_1    | 2023-06-12 10:19:54,108 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-12 10:19:54,198 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-12 10:19:54,213 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-12 10:19:54,250 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-06-12 10:19:54,271 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-06-12 10:19:54,274 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-06-12 10:19:54,282 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-06-12 10:19:54,293 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-06-12 10:19:54,366 [main] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-06-12 10:19:54,920 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-06-12 10:19:54,950 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-06-12 10:19:54,957 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-06-12 10:19:54,959 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-06-12 10:19:54,963 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-06-12 10:19:55,323 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-06-12 10:19:55,341 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-06-12 10:19:55,607 [main] INFO util.log: Logging initialized @94158ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-06-12 10:19:56,756 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-06-12 10:19:56,807 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-06-12 10:19:56,885 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-06-12 10:19:56,900 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-06-12 10:19:56,901 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-12 10:19:56,901 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn4_1    | 	... 12 more
dn4_1    | 2023-06-12 10:19:25,264 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:26,265 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:26,867 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-06-12 10:19:26,875 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn4_1    | 2023-06-12 10:19:27,258 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-06-12 10:19:27,259 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:19:27,266 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:27,396 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:19:27,402 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:19:27,414 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:19:27,435 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=78f8fca8-1c45-4717-8e79-22872958dcce} from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/raft-meta
dn4_1    | 2023-06-12 10:19:27,451 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=78f8fca8-1c45-4717-8e79-22872958dcce} from /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/raft-meta
dn4_1    | 2023-06-12 10:19:27,442 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=78f8fca8-1c45-4717-8e79-22872958dcce} from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/raft-meta
dn4_1    | 2023-06-12 10:19:27,677 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:27,689 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: set configuration 3: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:27,665 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:19:07,064 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om2_1    | 2023-06-12 10:19:09,065 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om2_1    | 2023-06-12 10:19:11,067 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om2_1    | 2023-06-12 10:19:13,069 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om2_1    | 2023-06-12 10:19:15,070 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om2_1    | 2023-06-12 10:19:17,190 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om2_1    | 2023-06-12 10:19:19,193 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om2_1    | 2023-06-12 10:19:21,197 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om2_1    | 2023-06-12 10:19:23,203 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om2_1    | 2023-06-12 10:19:25,390 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7740519e-fe5b-4936-af00-27e2eba71ce5 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om2_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om2_1    | 2023-06-12 10:19:27,392 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8ac830dea21e/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om2_1    | 2023-06-12 10:19:34,840 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om2_1    | 2023-06-12 10:19:35,672 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-12 10:19:36,796 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-06-12 10:19:37,791 [Thread-35] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om2_1    | 2023-06-12 10:19:37,836 [Thread-38] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] and output files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] are same.
om2_1    | 2023-06-12 10:19:37,838 [Thread-39] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om2_1    | 2023-06-12 10:19:37,873 [Thread-50] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om2_1    | 2023-06-12 10:19:37,874 [Thread-50] ERROR rocksdiff.RocksDBCheckpointDiffer: Unable to append compaction log. Compaction log path is not set. Please check initialization.
om2_1    | Exception in thread "Thread-50" java.lang.RuntimeException: Compaction log path not set
om2_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer.appendToCurrentCompactionLog(RocksDBCheckpointDiffer.java:375)
om2_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer.access$500(RocksDBCheckpointDiffer.java:100)
om2_1    | 	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer$2.onCompactionCompleted(RocksDBCheckpointDiffer.java:588)
om2_1    | 	at org.rocksdb.AbstractEventListener.onCompactionCompletedProxy(AbstractEventListener.java:191)
om2_1    | 2023-06-12 10:19:38,315 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-06-12 10:19:38,532 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1    | 2023-06-12 10:19:38,601 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:19:57,068 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1    | 2023-06-12 10:19:57,080 [main] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-06-12 10:19:57,086 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om1_1    | 2023-06-12 10:19:57,336 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-06-12 10:19:57,337 [main] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-06-12 10:19:57,351 [main] INFO server.session: node0 Scavenging every 600000ms
om1_1    | 2023-06-12 10:19:57,446 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5daad59a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-06-12 10:19:57,447 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@56f2c9e8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-06-12 10:19:58,147 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4449b273{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-6987652913332275531/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1    | 2023-06-12 10:19:58,187 [main] INFO server.AbstractConnector: Started ServerConnector@1eb6037d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-06-12 10:19:58,188 [main] INFO server.Server: Started @96742ms
om1_1    | 2023-06-12 10:19:58,199 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-06-12 10:19:58,199 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-12 10:19:58,204 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-06-12 10:19:58,204 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-12 10:19:58,233 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-06-12 10:19:58,591 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-06-12 10:19:59,080 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om1_1    | 2023-06-12 10:19:59,313 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208895253ns, electionTimeout:5099ms
om1_1    | 2023-06-12 10:19:59,315 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-12 10:19:59,315 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om1_1    | 2023-06-12 10:19:59,318 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-06-12 10:19:59,318 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-06-12 10:19:59,354 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:19:59,419 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-06-12 10:19:59,422 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-12 10:19:59,422 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-12 10:19:59,425 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-06-12 10:20:01,256 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:110))
om1_1    | 2023-06-12 10:20:01,281 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-12 10:20:01,343 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:01,375 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 4, (t:4, i:110))
om1_1    | 2023-06-12 10:20:01,376 [grpc-default-executor-1] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-12 10:20:01,394 [grpc-default-executor-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om1#0:OK-t4. Peer's state: om1@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:01,497 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:110))
om1_1    | 2023-06-12 10:20:01,498 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept ELECTION from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-12 10:20:01,505 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 5 for candidate:om3
om1_1    | 2023-06-12 10:20:01,505 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-06-12 10:20:01,508 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-12 10:20:01,559 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:OK-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:01,590 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=5) received 1 response(s) and 0 exception(s):
om1_1    | 2023-06-12 10:20:01,593 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t5
dn1_1    | 2023-06-12 10:19:28,242 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,243 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-12 10:19:28,243 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-12 10:19:28,249 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,252 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,252 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,290 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-12 10:19:28,291 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:19:28,293 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,293 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-12 10:19:28,293 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-12 10:19:28,296 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:19:28,296 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,296 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:19:28,302 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:28,345 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn1_1    | 2023-06-12 10:19:28,346 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn1_1    | 2023-06-12 10:19:28,352 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-12 10:19:28,353 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:19:28,358 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-12 10:19:28,364 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:19:28,366 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,367 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,371 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-12 10:19:28,378 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-12 10:19:28,373 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e
dn1_1    | 2023-06-12 10:19:28,381 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-12 10:19:28,381 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:19:28,381 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,385 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-12 10:19:28,385 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-12 10:19:28,371 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-12 10:19:28,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-12 10:19:28,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:28,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:28,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:28,387 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:28,392 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-12 10:19:28,394 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-12 10:19:28,395 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:28,400 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:28,400 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-12 10:19:28,440 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-12 10:19:29,518 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-12 10:19:29,518 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-06-12 10:19:29,533 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-12 10:19:29,534 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-12 10:19:29,532 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:19:29,535 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:19:29,611 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start RPC server
dn5_1    | 2023-06-12 10:19:29,669 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: GrpcService started, listening on 9858
dn5_1    | 2023-06-12 10:19:29,735 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: GrpcService started, listening on 9856
dn5_1    | 2023-06-12 10:19:29,740 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:29,742 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: GrpcService started, listening on 9857
dn5_1    | 2023-06-12 10:19:29,785 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: Started
dn5_1    | 2023-06-12 10:19:29,803 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 is started using port 9858 for RATIS
dn5_1    | 2023-06-12 10:19:29,808 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-06-12 10:19:29,808 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-06-12 10:19:30,059 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-12 10:19:30,225 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-06-12 10:19:30,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:31,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:32,743 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:33,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:34,571 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO impl.FollowerState: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5142473985ns, electionTimeout:5035ms
dn5_1    | 2023-06-12 10:19:34,573 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState
dn5_1    | 2023-06-12 10:19:34,574 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-06-12 10:19:34,577 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-06-12 10:19:34,577 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1
dn5_1    | 2023-06-12 10:19:34,610 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:34,617 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn5_1    | 2023-06-12 10:19:34,730 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:34,731 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn5_1    | 2023-06-12 10:19:34,732 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1
dn5_1    | 2023-06-12 10:19:34,733 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn5_1    | 2023-06-12 10:19:34,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-12 10:19:38,987 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om2_1    | 2023-06-12 10:19:39,018 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om2_1    | java.io.IOException: Permission denied
om2_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om2_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:140)
om2_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:116)
om2_1    | 	at org.apache.hadoop.hdds.utils.db.managed.ManagedSSTDumpTool.<clinit>(ManagedSSTDumpTool.java:39)
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:268)
om2_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om2_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:654)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:736)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om2_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om2_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om2_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om2_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om2_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om2_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om2_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om2_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om2_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om2_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om2_1    | 2023-06-12 10:19:39,037 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om2_1    | 2023-06-12 10:19:40,226 [main] WARN om.OzoneManager: Prepare marker file index 110 does not match DB prepare index 109. Writing DB index to prepare file and maintaining prepared state.
om2_1    | 2023-06-12 10:19:40,235 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 109 to file /data/metadata/current/prepareMarker
om2_1    | 2023-06-12 10:19:40,609 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-12 10:19:40,612 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-06-12 10:19:40,724 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om2_1    | 2023-06-12 10:19:40,731 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
dn3_1    | 2023-06-12 10:19:13,380 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:13,384 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:13,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:14,380 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:14,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:15,382 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:15,395 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:15,436 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 7c95fade90b7/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:33266 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:33266 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn3_1    | 2023-06-12 10:19:16,382 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:16,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:17,383 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:17,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:18,384 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:18,395 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 7c95fade90b7/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:43834 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 2023-06-12 10:22:06,204 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-12 10:22:06,205 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-12 10:22:06,206 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-12 10:22:06,207 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-12 10:22:06,207 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:06,207 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:06,208 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-12 10:22:06,208 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-12 10:22:06,209 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-06-12 10:22:06,211 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-12 10:22:06,211 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-12 10:22:06,211 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-12 10:22:06,212 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-12 10:22:06,212 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-12 10:22:06,213 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-12 10:22:06,213 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 does not exist. Creating ...
dn2_1    | 2023-06-12 10:22:06,217 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/in_use.lock acquired by nodename 7@1b90e16c96f4
dn2_1    | 2023-06-12 10:22:06,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 has been successfully formatted.
dn2_1    | 2023-06-12 10:22:06,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO ratis.ContainerStateMachine: group-1B5181454F64: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-12 10:22:06,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-12 10:22:06,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-12 10:22:06,220 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,220 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-12 10:22:06,231 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-12 10:22:06,231 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,234 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-12 10:22:06,234 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-12 10:22:06,234 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,234 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64
dn2_1    | 2023-06-12 10:22:06,235 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-12 10:22:06,235 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:22:06,235 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,238 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-12 10:22:06,238 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-12 10:22:06,238 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-12 10:22:06,238 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:06,238 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-12 10:22:06,239 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:06,241 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:06,314 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:22:06,314 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:22:06,314 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:06,315 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:19:27,782 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO ratis.ContainerStateMachine: group-67DF68C3D0F7: Setting the last applied index to (t:3, i:4)
dn4_1    | 2023-06-12 10:19:27,811 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Setting the last applied index to (t:5, i:47)
dn4_1    | 2023-06-12 10:19:27,818 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Setting the last applied index to (t:6, i:20)
dn4_1    | 2023-06-12 10:19:28,269 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:28,517 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:19:28,522 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:19:28,522 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:19:28,554 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-12 10:19:28,556 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,558 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-12 10:19:28,558 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,559 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:19:28,565 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-12 10:19:28,566 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,566 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:19:28,567 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:19:28,575 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:19:28,569 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:19:28,583 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:19:28,599 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,600 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,607 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,637 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:19:28,638 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:19:28,638 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,639 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:19:28,640 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:19:28,641 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:19:28,641 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:19:28,641 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,641 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,665 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7
dn4_1    | 2023-06-12 10:19:28,667 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:19:28,668 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:19:28,670 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn4_1    | 2023-06-12 10:19:28,692 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:19:28,692 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:19:28,692 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,693 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:19:28,697 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,698 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:19:28,698 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-12 10:19:28,700 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-12 10:19:28,703 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:19:28,706 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:28,704 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:19:28,709 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:19:28,711 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:28,713 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:19:28,718 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn4_1    | 2023-06-12 10:19:28,719 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:19:28,724 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:19:28,725 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,726 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:19:28,727 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-12 10:19:28,727 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:19:28,727 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:28,727 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:19:28,770 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,771 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,778 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,779 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:28,856 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:28,870 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:30,597 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:30,602 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,625 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,632 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:30,740 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,744 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,752 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:30,983 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,985 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:30,989 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:31,261 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: set configuration 0: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:01,594 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=5)
om1_1    | 2023-06-12 10:20:01,658 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 5, (t:4, i:110))
om1_1    | 2023-06-12 10:20:01,658 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: reject ELECTION from om2: already has voted for om3 at current term 5
om1_1    | 2023-06-12 10:20:01,659 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:FAIL-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:02,454 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om3 at term 5 for appendEntries, leader elected after 16564ms
om1_1    | 2023-06-12 10:20:02,524 [om1-server-thread3] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 111: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-12 10:20:02,547 [om1-server-thread3] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:111
om1_1    | 2023-06-12 10:20:02,826 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_111
om1_1    | 2023-06-12 10:20:03,430 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-06-12 10:22:19,090 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om1_1    | 2023-06-12 10:22:19,091 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
om1_1    | 2023-06-12 10:22:19,105 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
om1_1    | 2023-06-12 10:22:19,105 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om1_1    | 2023-06-12 10:22:19,106 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
om1_1    | 2023-06-12 10:22:19,106 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om1_1    | 2023-06-12 10:22:19,106 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om1_1    | 2023-06-12 10:22:19,112 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
om1_1    | 2023-06-12 10:22:49,642 [OmRpcReader-0] ERROR om.OzoneManagerServiceGrpc: Failed to submit request
om1_1    | com.google.protobuf.ServiceException: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:250)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
om1_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerServiceGrpc.submitRequest(OzoneManagerServiceGrpc.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$MethodHandlers.invoke(OzoneManagerServiceGrpc.java:237)
om1_1    | 	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
om1_1    | 	at io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
om1_1    | 	at io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
om1_1    | 	at io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
om1_1    | 	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
om1_1    | 	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
om1_1    | 	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
om1_1    | 	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
om1_1    | 	... 18 more
om1_1    | 2023-06-12 10:23:29,619 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om1_1    | 2023-06-12 10:23:33,854 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om1_1    | 2023-06-12 10:23:45,652 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
om1_1    | 2023-06-12 10:23:54,861 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:344)
om2_1    | 2023-06-12 10:19:41,743 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-06-12 10:19:41,825 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-12 10:19:42,194 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-06-12 10:19:42,434 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:110)
om2_1    | 2023-06-12 10:19:43,188 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-06-12 10:19:43,282 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-12 10:19:43,297 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-12 10:19:43,302 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-12 10:19:43,304 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-12 10:19:43,309 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-06-12 10:19:43,309 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-06-12 10:19:43,310 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-06-12 10:19:43,322 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-12 10:19:43,328 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-06-12 10:19:43,333 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-12 10:19:43,406 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-12 10:19:43,427 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-06-12 10:19:43,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-06-12 10:19:45,007 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-06-12 10:19:45,051 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-06-12 10:19:45,063 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-06-12 10:19:45,065 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-12 10:19:45,073 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-12 10:19:45,096 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-12 10:19:45,145 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-06-12 10:19:45,185 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@20349058[Not completed]
om2_1    | 2023-06-12 10:19:45,185 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-06-12 10:19:45,187 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-06-12 10:19:45,398 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-06-12 10:19:45,426 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-06-12 10:19:45,432 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-06-12 10:19:45,432 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-06-12 10:19:45,432 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-12 10:19:45,436 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-12 10:19:45,448 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-06-12 10:19:45,566 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-06-12 10:19:45,567 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-12 10:19:45,627 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-06-12 10:19:45,637 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-06-12 10:19:45,915 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-06-12 10:19:46,020 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1    | 2023-06-12 10:19:46,133 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-06-12 10:19:46,144 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-06-12 10:19:46,437 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1    | 2023-06-12 10:19:46,907 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-12 10:19:46,961 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-12 10:19:46,963 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-06-12 10:19:46,971 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-06-12 10:19:46,971 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-06-12 10:19:46,975 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-06-12 10:19:49,077 [main] INFO reflections.Reflections: Reflections took 3520 ms to scan 8 urls, producing 24 keys and 625 values [using 2 cores]
om2_1    | 2023-06-12 10:19:49,760 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-06-12 10:19:49,827 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-06-12 10:19:49,951 [Listener at om2/9862] INFO hdds.HddsUtils: Restoring thread name: main
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om3_1    | 2023-06-12 10:19:25,435 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7740519e-fe5b-4936-af00-27e2eba71ce5 is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14220)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
om3_1    | , while invoking $Proxy33.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om3_1    | 2023-06-12 10:19:27,436 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 91d1810c0573/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om3_1    | 2023-06-12 10:19:34,322 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om3_1    | 2023-06-12 10:19:34,606 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:19:36,331 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-06-12 10:19:38,121 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-06-12 10:19:38,311 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1    | 2023-06-12 10:19:38,364 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:19:38,554 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1    | 2023-06-12 10:19:38,594 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
om3_1    | java.io.IOException: Permission denied
om3_1    | 	at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
om3_1    | 	at java.base/java.io.File.createTempFile(File.java:2129)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:140)
om3_1    | 	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:116)
om3_1    | 	at org.apache.hadoop.hdds.utils.db.managed.ManagedSSTDumpTool.<clinit>(ManagedSSTDumpTool.java:39)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initSSTDumpTool(SnapshotDiffManager.java:268)
om3_1    | 	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:232)
om3_1    | 	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:261)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:817)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:654)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:736)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.startAndCancelPrepare(OzoneManagerStarter.java:231)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOmUpgrade(OzoneManagerStarter.java:121)
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
om3_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
om3_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
om3_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
om3_1    | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1972)
om3_1    | 	at picocli.CommandLine.access$1300(CommandLine.java:145)
om3_1    | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2352)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2346)
om3_1    | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2311)
om3_1    | 	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2179)
om3_1    | 	at picocli.CommandLine.execute(CommandLine.java:2078)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:100)
om3_1    | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:91)
om3_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:58)
om3_1    | 2023-06-12 10:19:38,606 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1    | 2023-06-12 10:19:39,456 [main] WARN om.OzoneManager: Prepare marker file index 110 does not match DB prepare index 109. Writing DB index to prepare file and maintaining prepared state.
om3_1    | 2023-06-12 10:19:39,461 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 109 to file /data/metadata/current/prepareMarker
om3_1    | 2023-06-12 10:19:40,057 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-12 10:19:40,085 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-06-12 10:19:40,378 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om3_1    | 2023-06-12 10:19:40,401 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-06-12 10:19:42,254 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-06-12 10:19:42,416 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-12 10:19:42,874 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-06-12 10:19:43,046 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:110)
om3_1    | 2023-06-12 10:19:43,411 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-06-12 10:19:43,595 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-12 10:19:43,611 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-12 10:19:43,611 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-12 10:19:43,630 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-12 10:19:43,631 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-06-12 10:19:43,636 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-06-12 10:19:52,402 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om2_1    | 2023-06-12 10:19:53,008 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-06-12 10:19:53,090 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-12 10:19:53,091 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-06-12 10:19:53,413 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-06-12 10:19:53,413 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-06-12 10:19:53,444 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@8ac830dea21e
om2_1    | 2023-06-12 10:19:53,470 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om2_1    | 2023-06-12 10:19:53,577 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:19:53,583 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-06-12 10:19:53,608 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-12 10:19:53,615 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-12 10:19:53,617 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-06-12 10:19:53,626 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-06-12 10:19:53,641 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-12 10:19:53,677 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-06-12 10:19:53,694 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-06-12 10:19:53,694 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-12 10:19:53,731 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-06-12 10:19:53,735 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-06-12 10:19:53,739 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-06-12 10:19:53,748 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-12 10:19:53,752 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-06-12 10:19:53,754 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-06-12 10:19:53,760 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-06-12 10:19:53,777 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-06-12 10:19:53,790 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-06-12 10:19:53,851 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-06-12 10:19:53,854 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-12 10:19:53,923 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-06-12 10:19:53,931 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-06-12 10:19:53,933 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-06-12 10:19:53,974 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 110
om2_1    | 2023-06-12 10:19:53,974 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-06-12 10:19:53,985 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:19:53,988 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om2_1    | 2023-06-12 10:19:53,994 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-12 10:19:54,014 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-12 10:19:54,026 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-12 10:19:54,033 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-06-12 10:19:54,042 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-06-12 10:19:54,042 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-06-12 10:19:54,043 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-06-12 10:19:54,045 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-06-12 10:19:54,062 [main] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-06-12 10:19:54,226 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-06-12 10:19:54,244 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-06-12 10:19:54,247 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-06-12 10:19:54,253 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om2_1    | 2023-06-12 10:19:54,256 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
dn4_1    | 2023-06-12 10:19:31,269 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_0-0
dn4_1    | 2023-06-12 10:19:31,272 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: set configuration 1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,273 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_1-2
dn4_1    | 2023-06-12 10:19:31,275 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,276 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,287 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: set configuration 3: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,308 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_inprogress_3
dn4_1    | 2023-06-12 10:19:31,318 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_0-7
dn4_1    | 2023-06-12 10:19:31,353 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: set configuration 8: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,355 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn4_1    | 2023-06-12 10:19:31,355 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-06-12 10:19:31,368 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_0-4
dn4_1    | 2023-06-12 10:19:31,371 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: set configuration 5: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,397 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_5-6
dn4_1    | 2023-06-12 10:19:31,398 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,425 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.LogSegment: Successfully read 26 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_8-33
dn4_1    | 2023-06-12 10:19:31,450 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:31,473 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7
dn4_1    | 2023-06-12 10:19:31,495 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn4_1    | 2023-06-12 10:19:31,495 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 6
dn4_1    | 2023-06-12 10:19:31,496 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34
dn4_1    | 2023-06-12 10:19:31,496 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 47
dn4_1    | 2023-06-12 10:19:31,500 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 33
dn5_1    | 2023-06-12 10:19:34,734 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EA217032BE5C with new leaderId: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn5_1    | 2023-06-12 10:19:34,786 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: change Leader from null to d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 at term 4 for becomeLeader, leader elected after 33622ms
dn5_1    | 2023-06-12 10:19:34,864 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-06-12 10:19:34,970 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:19:34,997 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-06-12 10:19:35,302 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-06-12 10:19:35,306 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-06-12 10:19:35,340 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-06-12 10:19:35,763 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:35,764 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:19:35,766 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-06-12 10:19:35,767 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderStateImpl
dn5_1    | 2023-06-12 10:19:35,776 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-06-12 10:19:35,840 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderElection1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: set configuration 5: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:19:36,023 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_inprogress_3 to /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_3-4
dn5_1    | 2023-06-12 10:19:36,091 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/current/log_inprogress_5
dn5_1    | 2023-06-12 10:19:36,765 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:37,766 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:38,767 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:39,768 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:40,770 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:41,771 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:42,772 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:43,773 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:44,774 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:45,775 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:19:57,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-12 10:20:30,226 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-06-12 10:21:05,788 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-06-12 10:21:05,789 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-06-12 10:21:05,790 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-06-12 10:21:05,791 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn5_1    | 2023-06-12 10:21:05,793 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-06-12 10:22:06,315 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-12 10:22:06,315 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:06,316 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-12 10:22:06,316 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState
dn2_1    | 2023-06-12 10:22:06,317 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1B5181454F64,id=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn2_1    | 2023-06-12 10:22:06,317 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:06,317 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-12 10:22:06,317 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-12 10:22:06,317 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-12 10:22:06,319 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-12 10:22:06,323 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:06,323 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64
dn2_1    | 2023-06-12 10:22:07,720 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64.
dn2_1    | 2023-06-12 10:22:07,721 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: addNew group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-F04DA08514BB:java.util.concurrent.CompletableFuture@7e6e180c[Not completed]
dn2_1    | 2023-06-12 10:22:07,723 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: new RaftServerImpl for group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-12 10:22:07,723 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-12 10:22:07,723 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:22:07,723 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-12 10:22:07,723 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-12 10:22:07,724 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-12 10:22:07,725 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-06-12 10:22:07,726 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-06-12 10:24:29,880 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
om1_1    | 2023-06-12 10:24:33,523 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
om1_1    | 2023-06-12 10:24:37,723 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
om1_1    | 2023-06-12 10:24:37,857 [Thread-303] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000097.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000097.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
om1_1    | 2023-06-12 10:24:37,876 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 in 148 milliseconds
om1_1    | 2023-06-12 10:24:37,941 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 61 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 availability.
om1_1    | 2023-06-12 10:24:37,945 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 for snapshot snapshot1
om1_1    | 2023-06-12 10:24:48,197 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
om1_1    | 2023-06-12 10:24:48,259 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df in 60 milliseconds
om1_1    | 2023-06-12 10:24:48,260 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 1 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df availability.
om1_1    | 2023-06-12 10:24:48,261 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df for snapshot snapshot2
om1_1    | 2023-06-12 10:24:55,187 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:24:55,195 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1    | 2023-06-12 10:24:55,319 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-12 10:24:55,322 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1    | 2023-06-12 10:25:13,565 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
om3_1    | 2023-06-12 10:19:43,640 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-06-12 10:19:43,651 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:19:43,656 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-06-12 10:19:43,660 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-06-12 10:19:43,739 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-06-12 10:19:43,774 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-06-12 10:19:43,778 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-06-12 10:19:44,931 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-06-12 10:19:44,949 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-06-12 10:19:44,955 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-06-12 10:19:44,957 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-12 10:19:44,958 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-12 10:19:44,969 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-12 10:19:44,993 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-06-12 10:19:45,023 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@6cbb7a7d[Not completed]
om3_1    | 2023-06-12 10:19:45,023 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-06-12 10:19:45,040 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-06-12 10:19:45,155 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-06-12 10:19:45,175 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-06-12 10:19:45,175 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-06-12 10:19:45,175 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-06-12 10:19:45,177 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-12 10:19:45,177 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-12 10:19:45,180 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-06-12 10:19:45,287 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-06-12 10:19:45,287 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-12 10:19:45,328 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-06-12 10:19:45,333 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-06-12 10:19:45,533 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-06-12 10:19:45,622 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1    | 2023-06-12 10:19:45,686 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-06-12 10:19:45,700 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-06-12 10:19:46,099 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-06-12 10:19:46,594 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-06-12 10:19:46,642 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-12 10:19:46,649 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-06-12 10:19:46,660 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-06-12 10:19:46,663 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-06-12 10:19:46,668 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-06-12 10:19:49,360 [main] INFO reflections.Reflections: Reflections took 4077 ms to scan 8 urls, producing 24 keys and 625 values [using 2 cores]
om3_1    | 2023-06-12 10:19:50,166 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-06-12 10:19:50,225 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-06-12 10:19:50,386 [Listener at om3/9862] INFO hdds.HddsUtils: Restoring thread name: main
om3_1    | 2023-06-12 10:19:52,771 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om3_1    | 2023-06-12 10:19:53,351 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-06-12 10:19:53,427 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-06-12 10:19:53,427 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-06-12 10:19:53,749 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-06-12 10:19:53,753 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-06-12 10:19:53,787 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@91d1810c0573
om3_1    | 2023-06-12 10:19:53,802 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om3} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-06-12 10:19:54,025 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:19:54,033 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-06-12 10:19:54,082 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-06-12 10:19:54,086 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:19:54,095 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-06-12 10:19:54,100 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-06-12 10:19:54,117 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-12 10:19:54,143 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-06-12 10:19:54,147 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-06-12 10:19:54,147 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:19:54,169 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-06-12 10:19:54,171 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-06-12 10:19:54,174 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-06-12 10:19:54,176 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-12 10:19:54,180 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-06-12 10:19:54,180 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-06-12 10:19:54,183 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-06-12 10:19:54,185 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-06-12 10:19:54,186 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-06-12 10:19:54,242 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-06-12 10:19:54,246 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:19:54,347 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-06-12 10:19:54,349 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:21:05,793 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-06-12 10:21:05,794 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-06-12 10:21:05,794 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-06-12 10:21:05,795 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-06-12 10:21:05,795 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn5_1    | 2023-06-12 10:21:05,795 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn5_1    | 2023-06-12 10:21:30,227 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-06-12 10:21:35,787 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    | 2023-06-12 10:21:35,800 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: remove    LEADER d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C:t4, leader=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, voted=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, raftlog=Memoized:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLog:OPENED:c6, conf=5: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-06-12 10:21:35,802 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: shutdown
dn5_1    | 2023-06-12 10:21:35,802 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA217032BE5C,id=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn5_1    | 2023-06-12 10:21:35,802 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-LeaderStateImpl
dn5_1    | 2023-06-12 10:21:35,807 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-PendingRequests: sendNotLeaderResponses
dn5_1    | 2023-06-12 10:21:35,811 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EA217032BE5C: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/sm/snapshot.4_6
dn5_1    | 2023-06-12 10:21:35,812 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater: set stopIndex = 6
dn5_1    | 2023-06-12 10:21:35,813 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-EA217032BE5C: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c/sm/snapshot.4_6 took: 2 ms
dn5_1    | 2023-06-12 10:21:35,815 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater] INFO impl.StateMachineUpdater: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 2023-06-12 10:21:35,815 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater] INFO impl.StateMachineUpdater: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn5_1    | 2023-06-12 10:21:35,818 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: closes. applyIndex: 6
dn5_1    | 2023-06-12 10:21:36,194 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C-SegmentedRaftLogWorker close()
dn5_1    | 2023-06-12 10:21:36,200 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-EA217032BE5C: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/f8f43c49-46f2-454a-ba47-ea217032be5c
dn5_1    | 2023-06-12 10:21:36,202 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c command on datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3.
dn5_1    | 2023-06-12 10:22:05,813 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: addNew group-8C2A955BD013:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-8C2A955BD013:java.util.concurrent.CompletableFuture@1519c491[Not completed]
dn5_1    | 2023-06-12 10:22:05,816 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: new RaftServerImpl for group-8C2A955BD013:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-12 10:22:05,816 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-12 10:22:05,816 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-12 10:22:05,817 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-12 10:22:05,817 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-12 10:22:05,817 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-12 10:22:05,817 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-12 10:22:05,818 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: ConfigurationManager, init=-1: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-12 10:22:05,819 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-12 10:22:05,820 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-12 10:22:05,821 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-12 10:22:05,821 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-12 10:22:05,821 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-06-12 10:22:05,821 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-12 10:22:05,821 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-12 10:22:05,822 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-06-12 10:22:05,823 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-12 10:22:05,823 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-12 10:22:05,824 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,824 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-12 10:22:05,824 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,824 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-12 10:22:05,825 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e4d68ba5-f2e1-460a-8ae0-8c2a955bd013 does not exist. Creating ...
dn5_1    | 2023-06-12 10:22:05,830 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e4d68ba5-f2e1-460a-8ae0-8c2a955bd013/in_use.lock acquired by nodename 7@8ab2498196c6
dn5_1    | 2023-06-12 10:22:05,832 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e4d68ba5-f2e1-460a-8ae0-8c2a955bd013 has been successfully formatted.
dn5_1    | 2023-06-12 10:22:05,833 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO ratis.ContainerStateMachine: group-8C2A955BD013: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-12 10:22:05,833 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-12 10:22:05,833 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-12 10:22:05,833 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:22:05,834 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-12 10:22:05,834 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-12 10:22:05,838 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:05,838 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-12 10:22:05,839 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-12 10:22:05,839 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:22:05,847 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e4d68ba5-f2e1-460a-8ae0-8c2a955bd013
dn5_1    | 2023-06-12 10:22:05,847 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-12 10:22:05,847 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:05,848 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:05,858 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-12 10:22:05,859 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-12 10:22:05,859 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-12 10:22:05,859 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-12 10:22:05,860 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-12 10:22:05,862 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:05,871 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:22:05,902 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:22:05,902 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:22:05,902 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-12 10:22:05,902 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-12 10:22:05,903 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-12 10:22:05,903 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: start as a follower, conf=-1: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:05,904 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-12 10:19:28,443 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,448 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 2023-06-12 10:22:07,726 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-12 10:19:28,443 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:28,452 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:19:54,352 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-06-12 10:18:31,931 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
dn4_1    | 2023-06-12 10:19:31,608 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:32,249 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: start as a follower, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:07,726 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-06-12 10:19:54,398 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 110
recon_1  | STARTUP_MSG:   host = 9ca79353606b/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | 2023-06-12 10:19:54,491 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-06-12 10:19:54,495 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-06-12 10:19:28,453 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:29,156 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:22:07,726 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-12 10:22:07,727 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:19:32,249 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn1_1    | 2023-06-12 10:19:29,169 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,173 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,174 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:29,223 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,224 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,224 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:29,302 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,306 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:29,306 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:29,379 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:29,381 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:29,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_0-0
dn5_1    | 2023-06-12 10:22:05,905 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState
om3_1    | 2023-06-12 10:19:54,399 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-12 10:19:54,404 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:29,386 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:29,406 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: set configuration 1: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:19:54,600 [main] INFO util.log: Logging initialized @95967ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-06-12 10:22:07,727 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-12 10:22:07,731 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb does not exist. Creating ...
dn4_1    | 2023-06-12 10:19:32,250 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState
dn4_1    | 2023-06-12 10:19:32,294 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn5_1    | 2023-06-12 10:22:05,906 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8C2A955BD013,id=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
om3_1    | 2023-06-12 10:19:54,407 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om3_1    | 2023-06-12 10:19:54,416 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
dn1_1    | 2023-06-12 10:19:29,450 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_0-7
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
scm2_1   | Waiting for the service scm1:9894
om2_1    | 2023-06-12 10:19:55,937 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-06-12 10:18:32,831 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
dn4_1    | 2023-06-12 10:19:32,301 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:29,488 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: set configuration 8: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:29,491 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_1-2
dn1_1    | 2023-06-12 10:19:29,501 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_0-4
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-06-12 10:19:17,845 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn1_1    | 2023-06-12 10:19:29,517 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 5: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-12 10:19:54,429 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | 2023-06-12 10:19:32,301 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om2_1    | 2023-06-12 10:19:55,966 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 2023-06-12 10:22:07,733 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/in_use.lock acquired by nodename 7@1b90e16c96f4
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn1_1    | 2023-06-12 10:19:29,522 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_5-6
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om3_1    | 2023-06-12 10:19:54,429 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-12 10:19:54,430 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
dn4_1    | 2023-06-12 10:19:32,302 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om2_1    | 2023-06-12 10:19:56,044 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-06-12 10:19:56,064 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-06-12 10:19:56,064 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-12 10:22:07,734 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb has been successfully formatted.
dn2_1    | 2023-06-12 10:22:07,758 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO ratis.ContainerStateMachine: group-F04DA08514BB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | STARTUP_MSG:   host = e85c32bf6fdc/10.9.0.15
scm3_1   | Waiting for the service scm2:9894
dn1_1    | 2023-06-12 10:19:29,549 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: set configuration 3: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-12 10:19:32,317 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: start as a follower, conf=3: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
s3g_1    | 2023-06-12 10:18:32,864 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-06-12 10:18:33,334 [main] INFO util.log: Logging initialized @12092ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-06-12 10:18:34,479 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-06-12 10:18:34,692 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-06-12 10:18:34,771 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-06-12 10:18:34,805 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-06-12 10:18:34,810 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-06-12 10:18:34,811 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-06-12 10:19:56,067 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-06-12 10:19:56,335 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1    | 2023-06-12 10:19:56,347 [main] INFO http.HttpServer2: Jetty bound to port 9874
om2_1    | 2023-06-12 10:19:56,350 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om2_1    | 2023-06-12 10:19:56,484 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-06-12 10:19:56,484 [main] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-06-12 10:19:56,494 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:05,907 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 2023-06-12 10:19:32,319 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: start as a follower, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-06-12 10:18:35,198 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir9591623524834138191
om2_1    | 2023-06-12 10:19:56,609 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11295cb1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-06-12 10:19:56,620 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2619cb76{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-06-12 10:19:57,276 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3e2772a9{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-18375350275936518311/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1    | 2023-06-12 10:19:57,325 [main] INFO server.AbstractConnector: Started ServerConnector@2444c3df{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-06-12 10:19:57,327 [main] INFO server.Server: Started @98693ms
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:39Z
dn4_1    | 2023-06-12 10:19:33,663 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: changes role from      null to FOLLOWER at term 5 for startAsFollower
s3g_1    | 2023-06-12 10:18:36,344 [main] INFO s3.Gateway: STARTUP_MSG: 
om2_1    | 2023-06-12 10:19:57,335 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-06-12 10:19:29,556 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_inprogress_3
dn1_1    | 2023-06-12 10:19:29,563 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-06-12 10:19:29,567 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-06-12 10:19:29,568 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:24,671 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
recon_1  | STARTUP_MSG:   java = 11.0.19
dn2_1    | 2023-06-12 10:22:07,759 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:19:33,663 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-06-12 10:19:33,702 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1    | /************************************************************
dn1_1    | 2023-06-12 10:19:29,584 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7
dn1_1    | 2023-06-12 10:19:29,592 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn1_1    | 2023-06-12 10:19:29,593 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 6
dn1_1    | 2023-06-12 10:19:29,578 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.LogSegment: Successfully read 26 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_8-33
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
dn5_1    | 2023-06-12 10:22:05,910 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=e4d68ba5-f2e1-460a-8ae0-8c2a955bd013
dn4_1    | 2023-06-12 10:19:33,716 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:19:33,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:33,722 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState
dn4_1    | 2023-06-12 10:19:33,717 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState
scm3_1   | STARTUP_MSG:   host = 200a502423a3/10.9.0.16
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | ************************************************************/
recon_1  | 2023-06-12 10:18:32,009 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-06-12 10:18:37,194 [main] INFO reflections.Reflections: Reflections took 511 ms to scan 1 urls, producing 20 keys and 74 values 
recon_1  | 2023-06-12 10:18:41,549 [main] INFO reflections.Reflections: Reflections took 814 ms to scan 3 urls, producing 130 keys and 282 values 
recon_1  | 2023-06-12 10:18:41,983 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-06-12 10:18:42,110 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1686564385713
dn4_1    | 2023-06-12 10:19:33,722 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-06-12 10:19:57,335 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-06-12 10:19:57,338 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-06-12 10:19:57,340 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | STARTUP_MSG:   args = []
recon_1  | 2023-06-12 10:18:43,879 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-12 10:18:50,362 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
om2_1    | 2023-06-12 10:19:57,351 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1    | 2023-06-12 10:19:57,406 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-06-12 10:19:57,994 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om2_1    | 2023-06-12 10:19:59,139 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5144950970ns, electionTimeout:5110ms
om2_1    | 2023-06-12 10:19:59,140 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-12 10:19:59,141 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om2_1    | 2023-06-12 10:19:59,143 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = bff830a84b9a/10.9.0.23
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
om2_1    | 2023-06-12 10:19:59,143 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
dn4_1    | 2023-06-12 10:19:33,808 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-06-12 10:18:53,943 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-12 10:18:53,959 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:39Z
s3g_1    | STARTUP_MSG:   java = 11.0.19
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir9591623524834138191, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | 2023-06-12 10:19:59,148 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:05,912 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e4d68ba5-f2e1-460a-8ae0-8c2a955bd013.
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
recon_1  | 2023-06-12 10:18:53,963 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-06-12 10:18:57,153 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
recon_1  | 2023-06-12 10:18:57,331 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
recon_1  | 2023-06-12 10:18:57,417 [main] INFO codegen.SqlDbUtils: CONTAINER_COUNT_BY_SIZE table already exists, skipping creation.
dn1_1    | 2023-06-12 10:19:29,624 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
om2_1    | 2023-06-12 10:19:59,188 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:19:33,808 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:19:33,812 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:19:33,816 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:19:33,894 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-67DF68C3D0F7,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:19:29,633 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34
dn1_1    | 2023-06-12 10:19:29,647 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 47
dn1_1    | 2023-06-12 10:19:29,647 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 33
dn1_1    | 2023-06-12 10:19:30,005 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: start as a follower, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-06-12 10:19:59,189 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:18:57,485 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
dn4_1    | 2023-06-12 10:19:33,894 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:19:30,009 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: start as a follower, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | STARTUP_MSG:   java = 11.0.19
om2_1    | 2023-06-12 10:19:59,194 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
recon_1  | 2023-06-12 10:18:57,614 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
dn4_1    | 2023-06-12 10:19:33,911 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:33,911 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:33,912 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:33,912 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | 2023-06-12 10:22:07,761 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-12 10:22:07,767 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om2_1    | 2023-06-12 10:19:59,198 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
recon_1  | 2023-06-12 10:18:57,705 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
dn4_1    | 2023-06-12 10:19:33,912 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:19:33,912 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:33,921 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:43834 remote=scm1/10.9.0.14:9861]
dn1_1    | 2023-06-12 10:19:30,012 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: changes role from      null to FOLLOWER at term 5 for startAsFollower
dn2_1    | 2023-06-12 10:22:07,770 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-12 10:22:08,021 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3_1   | ************************************************************/
om2_1    | 2023-06-12 10:20:01,402 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:110))
recon_1  | 2023-06-12 10:18:58,014 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-06-12 10:18:58,062 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-06-12 10:18:58,122 [main] INFO util.log: Logging initialized @37812ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
om3_1    | 2023-06-12 10:19:54,441 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-12 10:22:05,912 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: addNew group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1B5181454F64:java.util.concurrent.CompletableFuture@47ee82fd[Not completed]
dn1_1    | 2023-06-12 10:19:30,014 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState
om2_1    | 2023-06-12 10:20:01,451 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:110))
scm1_1   | 2023-06-12 10:18:39,131 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1  | 2023-06-12 10:18:58,898 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-06-12 10:19:24,681 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 2023-06-12 10:22:05,915 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: new RaftServerImpl for group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-12 10:19:30,009 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: start as a follower, conf=3: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:30,125 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: changes role from      null to FOLLOWER at term 3 for startAsFollower
s3g_1    | ************************************************************/
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
dn2_1    | 2023-06-12 10:22:08,021 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-12 10:22:08,021 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om2_1    | 2023-06-12 10:20:01,478 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | 2023-06-12 10:19:24,721 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 2023-06-12 10:22:05,915 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-12 10:19:30,094 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=3c030830-a72e-4f6f-8f54-144518af6253
s3g_1    | 2023-06-12 10:18:36,415 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-12 10:20:01,487 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm1_1   | STARTUP_MSG:   host = dfadc871a7c0/10.9.0.14
scm1_1   | STARTUP_MSG:   args = []
scm3_1   | 2023-06-12 10:19:25,089 [main] INFO reflections.Reflections: Reflections took 268 ms to scan 3 urls, producing 130 keys and 282 values 
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 2023-06-12 10:22:05,915 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-12 10:19:30,012 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn1_1    | 2023-06-12 10:19:30,126 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState
dn2_1    | 2023-06-12 10:22:08,022 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-12 10:22:08,022 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
scm2_1   | STARTUP_MSG:   java = 11.0.19
scm1_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.5.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.5.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.5.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm3_1   | 2023-06-12 10:19:25,200 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:517)
dn5_1    | 2023-06-12 10:22:05,916 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-12 10:19:30,130 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
s3g_1    | 2023-06-12 10:18:36,575 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-06-12 10:18:37,314 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-06-12 10:18:38,568 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-12 10:20:01,494 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t4
recon_1  | 2023-06-12 10:18:58,953 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/b4ea2fb100e7edd7246dc2645dc8723746a68907 ; compiled by 'runner' on 2023-06-12T09:24Z
scm1_1   | STARTUP_MSG:   java = 11.0.19
scm3_1   | 2023-06-12 10:19:25,220 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)
dn5_1    | 2023-06-12 10:22:05,916 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:30,191 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1    | 2023-06-12 10:18:38,569 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-06-12 10:18:38,842 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
dn4_1    | 2023-06-12 10:19:33,922 [78f8fca8-1c45-4717-8e79-22872958dcce-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:19:34,018 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: start RPC server
recon_1  | 2023-06-12 10:18:59,018 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm3_1   | 2023-06-12 10:19:25,302 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm1_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.min.gap=15m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.diff.cleanup.service.run.internal=60m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=5000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1204)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1095)
dn5_1    | 2023-06-12 10:22:05,916 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:30,191 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:19:30,191 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1    | 2023-06-12 10:18:38,913 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-06-12 10:18:38,915 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om2_1    | 2023-06-12 10:20:01,494 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
recon_1  | 2023-06-12 10:18:59,046 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
scm2_1   | ************************************************************/
scm3_1   | 2023-06-12 10:19:25,302 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm1_1   | ************************************************************/
dn3_1    | 2023-06-12 10:19:18,397 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:19,385 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:19,399 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:20,386 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:22:08,023 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 2023-06-12 10:18:39,326 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-12 10:19:34,111 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 78f8fca8-1c45-4717-8e79-22872958dcce: GrpcService started, listening on 9858
om2_1    | 2023-06-12 10:20:01,504 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t4. Peer's state: om2@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-12 10:18:59,046 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-06-12 10:18:59,046 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-06-12 10:18:39,255 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-12 10:19:20,400 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:21,387 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:21,400 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:30,191 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:08,024 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: changes role from      null to FOLLOWER at term 0 for startAsFollower
s3g_1    | 2023-06-12 10:18:39,350 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-06-12 10:18:39,353 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 2023-06-12 10:18:59,374 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-06-12 10:18:59,406 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
om2_1    | 2023-06-12 10:20:01,518 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:18:40,205 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:19:54,442 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-06-12 10:19:54,446 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-06-12 10:19:54,449 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
dn1_1    | 2023-06-12 10:19:30,213 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=3c030830-a72e-4f6f-8f54-144518af6253
dn2_1    | 2023-06-12 10:22:08,024 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState
dn4_1    | 2023-06-12 10:19:34,159 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 78f8fca8-1c45-4717-8e79-22872958dcce: GrpcService started, listening on 9856
s3g_1    | 2023-06-12 10:18:39,618 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63fbfaeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-06-12 10:18:39,642 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@76f4b65{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-06-12 10:19:17,854 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-12 10:20:01,584 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-12 10:20:01,619 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:18:43,882 [main] INFO reflections.Reflections: Reflections took 3066 ms to scan 3 urls, producing 130 keys and 282 values 
scm1_1   | 2023-06-12 10:18:45,254 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om3_1    | 2023-06-12 10:19:54,467 [main] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-06-12 10:19:54,734 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
dn1_1    | 2023-06-12 10:19:30,214 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:08,025 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F04DA08514BB,id=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn4_1    | 2023-06-12 10:19:34,174 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 78f8fca8-1c45-4717-8e79-22872958dcce: GrpcService started, listening on 9857
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
scm2_1   | 2023-06-12 10:19:17,889 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-12 10:20:01,611 [grpc-default-executor-2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:110))
scm3_1   | 2023-06-12 10:19:26,664 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-12 10:18:45,335 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn3_1    | 2023-06-12 10:19:22,388 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-12 10:19:54,757 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-06-12 10:19:54,758 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-06-12 10:22:08,025 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-12 10:22:08,025 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:34,206 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 78f8fca8-1c45-4717-8e79-22872958dcce is started using port 9858 for RATIS
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm2_1   | 2023-06-12 10:19:18,097 [main] INFO reflections.Reflections: Reflections took 159 ms to scan 3 urls, producing 130 keys and 282 values 
om2_1    | 2023-06-12 10:20:01,518 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
scm3_1   | 2023-06-12 10:19:27,960 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-12 10:18:46,031 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm1_1   | 2023-06-12 10:18:46,102 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
om3_1    | 2023-06-12 10:19:54,774 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn5_1    | 2023-06-12 10:22:05,916 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-12 10:22:08,025 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-12 10:19:34,206 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 78f8fca8-1c45-4717-8e79-22872958dcce is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-06-12 10:19:34,209 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 78f8fca8-1c45-4717-8e79-22872958dcce is started using port 9856 for RATIS_SERVER
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 2023-06-12 10:19:07,542 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5a06904{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir9591623524834138191/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-945665875894982798/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
scm2_1   | 2023-06-12 10:19:18,154 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om2_1    | 2023-06-12 10:20:01,636 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:28,984 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om3_1    | 2023-06-12 10:19:54,774 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
dn5_1    | 2023-06-12 10:22:05,916 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-12 10:22:05,917 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:19:34,215 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-78f8fca8-1c45-4717-8e79-22872958dcce: Started
dn4_1    | 2023-06-12 10:19:34,369 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
s3g_1    | 2023-06-12 10:19:07,753 [main] INFO server.AbstractConnector: Started ServerConnector@2dcd168a{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-06-12 10:19:07,754 [main] INFO server.Server: Started @46512ms
scm2_1   | 2023-06-12 10:19:18,160 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1  | 2023-06-12 10:18:59,652 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
scm3_1   | 2023-06-12 10:19:28,990 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om3_1    | 2023-06-12 10:19:54,982 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
dn5_1    | 2023-06-12 10:22:05,917 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-12 10:22:05,917 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:34,395 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-06-12 10:19:34,797 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-12 10:19:07,817 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-06-12 10:20:01,637 [grpc-default-executor-2] INFO impl.VoteContext: om2@group-D66704EFC61C-CANDIDATE: reject ELECTION from om3: already has voted for om2 at current term 5
om2_1    | 2023-06-12 10:20:01,639 [grpc-default-executor-2] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:20:02,194 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
scm3_1   | 2023-06-12 10:19:29,530 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-06-12 10:19:54,983 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-06-12 10:22:08,025 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-12 10:22:08,026 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:19:35,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:36,805 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-12 10:19:07,817 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-06-12 10:20:02,194 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t5
recon_1  | 2023-06-12 10:18:59,674 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
scm2_1   | 2023-06-12 10:19:18,187 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
dn3_1    | 2023-06-12 10:19:22,403 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:23,389 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-12 10:19:30,607 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:c790b820-4c4f-44e6-aba9-08b18fc50228
om3_1    | 2023-06-12 10:19:55,065 [main] INFO util.log: Logging initialized @95517ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-06-12 10:19:55,655 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-06-12 10:19:55,686 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
dn4_1    | 2023-06-12 10:19:37,820 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:38,813 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6562975860ns, electionTimeout:5036ms
dn4_1    | 2023-06-12 10:19:38,817 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState
om2_1    | 2023-06-12 10:20:02,194 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t5
om2_1    | 2023-06-12 10:20:02,195 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
scm2_1   | 2023-06-12 10:19:18,188 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-06-12 10:19:18,794 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-12 10:19:18,985 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-12 10:19:31,472 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-06-12 10:19:55,744 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-12 10:22:08,030 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:08,031 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb
dn2_1    | 2023-06-12 10:22:08,527 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb.
dn4_1    | 2023-06-12 10:19:38,820 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn4_1    | 2023-06-12 10:19:38,835 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-12 10:20:02,195 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
om2_1    | 2023-06-12 10:20:02,196 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
scm2_1   | 2023-06-12 10:19:19,179 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn3_1    | 2023-06-12 10:19:24,390 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:25,391 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:22:11,102 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: receive requestVote(PRE_VOTE, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-1B5181454F64, 0, (t:0, i:0))
dn2_1    | 2023-06-12 10:22:11,104 [grpc-default-executor-0] INFO impl.VoteContext: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FOLLOWER: reject PRE_VOTE from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 1 > candidate's priority 0
om3_1    | 2023-06-12 10:19:55,758 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
dn4_1    | 2023-06-12 10:19:38,844 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-06-12 10:19:38,854 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137284426ns, electionTimeout:5031ms
om2_1    | 2023-06-12 10:20:02,198 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-12 10:20:02,494 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om3 at term 5 for appendEntries, leader elected after 16542ms
dn3_1    | 2023-06-12 10:19:25,392 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
s3g_1    | 2023-06-12 10:19:07,821 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-06-12 10:22:47,315 [qtp193388045-22] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-06-12 10:22:47,379 [qtp193388045-22] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-06-12 10:22:47,400 [qtp193388045-22] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn2_1    | 2023-06-12 10:22:11,109 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64 replies to PRE_VOTE vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:FAIL-t0. Peer's state: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64:t0, leader=null, voted=, raftlog=Memoized:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:11,217 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO impl.FollowerState: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5034043904ns, electionTimeout:5031ms
om3_1    | 2023-06-12 10:19:55,758 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-12 10:19:38,860 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState
dn4_1    | 2023-06-12 10:19:38,861 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
om2_1    | 2023-06-12 10:20:02,514 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 111: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-12 10:20:02,537 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:111
dn3_1    | java.net.ConnectException: Call From 7c95fade90b7/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
s3g_1    | 2023-06-12 10:22:47,400 [qtp193388045-22] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-06-12 10:22:48,763 [qtp193388045-22] INFO protocolPB.GrpcOmTransport: GrpcOmTransport: started
s3g_1    | 2023-06-12 10:22:49,677 [qtp193388045-22] ERROR protocolPB.GrpcOmTransport: Failed to submit request
s3g_1    | io.grpc.StatusRuntimeException: INTERNAL: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
dn2_1    | 2023-06-12 10:22:11,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState
dn2_1    | 2023-06-12 10:22:11,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1    | 2023-06-12 10:19:55,767 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-12 10:19:30,218 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:38,867 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 2023-06-12 10:18:59,690 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-06-12 10:18:59,923 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-06-12 10:18:59,923 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-06-12 10:19:03,106 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-12 10:19:03,605 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 	at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:271)
s3g_1    | 	at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:252)
dn2_1    | 2023-06-12 10:22:11,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-06-12 10:22:11,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2
om3_1    | 2023-06-12 10:19:56,391 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
dn4_1    | 2023-06-12 10:19:38,870 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2
recon_1  | 2023-06-12 10:19:03,901 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
s3g_1    | 	at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:165)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om2_1    | 2023-06-12 10:20:03,179 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_111
scm1_1   | 2023-06-12 10:18:57,600 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-12 10:19:00,197 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-12 10:19:02,736 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
dn2_1    | 2023-06-12 10:22:11,220 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:11,221 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
om3_1    | 2023-06-12 10:19:56,440 [main] INFO http.HttpServer2: Jetty bound to port 9874
dn4_1    | 2023-06-12 10:19:38,894 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1
recon_1  | 2023-06-12 10:19:03,919 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om2_1    | 2023-06-12 10:20:03,739 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$OzoneManagerServiceBlockingStub.submitRequest(OzoneManagerServiceGrpc.java:182)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransport.submitRequest(GrpcOmTransport.java:186)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:304)
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1606)
s3g_1    | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:243)
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:248)
dn5_1    | 2023-06-12 10:22:05,917 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-12 10:19:30,221 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-12 10:19:38,934 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5270880344ns, electionTimeout:5096ms
recon_1  | 2023-06-12 10:19:04,881 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
om2_1    | [id: "om1"
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:115)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.<init>(OzoneClientCache.java:83)
om3_1    | 2023-06-12 10:19:56,452 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm1_1   | 2023-06-12 10:19:02,782 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-06-12 10:19:03,849 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-06-12 10:19:06,082 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:9e54e886-87c9-472f-9b2f-e9a516e53bd2
dn5_1    | 2023-06-12 10:22:05,918 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-06-12 10:22:05,918 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
recon_1  | 2023-06-12 10:19:05,142 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-06-12 10:22:11,222 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.getOzoneClientInstance(OzoneClientCache.java:98)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:121)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
dn1_1    | 2023-06-12 10:19:30,224 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:19:38,936 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState
dn4_1    | 2023-06-12 10:19:38,936 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
recon_1  | 2023-06-12 10:19:05,234 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-06-12 10:19:05,338 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn2_1    | 2023-06-12 10:22:11,222 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-06-12 10:22:11,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2
dn2_1    | 2023-06-12 10:22:11,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
s3g_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:95)
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:85)
s3g_1    | 	at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:103)
dn1_1    | 2023-06-12 10:19:30,262 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DD6E9427813E,id=3c030830-a72e-4f6f-8f54-144518af6253
dn4_1    | 2023-06-12 10:19:38,936 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-06-12 10:19:38,936 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3
recon_1  | 2023-06-12 10:19:05,470 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c030830-a72e-4f6f-8f54-144518af6253
recon_1  | 2023-06-12 10:19:05,478 [main] INFO node.SCMNodeManager: Registered Data node : 3c030830-a72e-4f6f-8f54-144518af6253{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-06-12 10:22:11,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D83197229949 with new leaderId: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn5_1    | 2023-06-12 10:22:05,918 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-12 10:19:19,181 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-06-12 10:19:19,267 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1    | 	at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
s3g_1    | 	at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
dn1_1    | 2023-06-12 10:19:30,265 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:38,937 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om2_1    | address: "om1:9872"
dn2_1    | 2023-06-12 10:22:11,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: change Leader from null to 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 at term 1 for becomeLeader, leader elected after 5138ms
dn5_1    | 2023-06-12 10:22:05,918 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2_1   | 2023-06-12 10:19:19,402 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:7740519e-fe5b-4936-af00-27e2eba71ce5
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:794)
s3g_1    | 	at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
dn1_1    | 2023-06-12 10:19:30,265 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:38,953 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:38,973 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
recon_1  | 2023-06-12 10:19:05,496 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
recon_1  | 2023-06-12 10:19:05,497 [main] INFO node.SCMNodeManager: Registered Data node : 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-06-12 10:22:11,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-06-12 10:22:05,919 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-12 10:22:05,920 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-12 10:19:19,526 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2_1   | 2023-06-12 10:19:19,536 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-12 10:19:19,537 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-06-12 10:19:19,539 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-12 10:19:19,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
dn1_1    | 2023-06-12 10:19:30,266 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
s3g_1    | 	at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:345)
dn4_1    | 2023-06-12 10:19:38,992 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 5 for 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-12 10:19:05,512 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/78f8fca8-1c45-4717-8e79-22872958dcce
recon_1  | 2023-06-12 10:19:05,516 [main] INFO node.SCMNodeManager: Registered Data node : 78f8fca8-1c45-4717-8e79-22872958dcce{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-06-12 10:22:11,224 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:05,920 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,920 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-06-12 10:19:19,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3_1   | 2023-06-12 10:19:31,565 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 2023-06-12 10:19:30,266 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
s3g_1    | 	at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:356)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:69)
recon_1  | 2023-06-12 10:19:05,517 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d6f449c5-6fac-4083-a27e-b83edf3e8b1c
recon_1  | 2023-06-12 10:19:05,519 [main] INFO node.SCMNodeManager: Registered Data node : d6f449c5-6fac-4083-a27e-b83edf3e8b1c{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn2_1    | 2023-06-12 10:22:11,224 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om2_1    | startupRole: FOLLOWER
scm2_1   | 2023-06-12 10:19:19,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-06-12 10:19:19,541 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-06-12 10:19:07,298 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-06-12 10:19:07,391 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
dn1_1    | 2023-06-12 10:19:30,274 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:30,275 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:19:39,191 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 3c030830-a72e-4f6f-8f54-144518af6253
dn2_1    | 2023-06-12 10:22:11,225 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-12 10:22:11,225 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1  | 2023-06-12 10:19:05,521 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
recon_1  | 2023-06-12 10:19:05,527 [main] INFO node.SCMNodeManager: Registered Data node : d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-12 10:19:19,542 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:19,544 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-12 10:19:07,441 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-06-12 10:19:31,573 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1530)
dn4_1    | 2023-06-12 10:19:39,194 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:19:39,222 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:19:39,222 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:19:05,553 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
om2_1    | , id: "om3"
scm2_1   | 2023-06-12 10:19:19,545 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-06-12 10:19:19,558 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-12 10:19:07,457 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-06-12 10:19:31,582 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1    | 	at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
dn2_1    | 2023-06-12 10:22:11,225 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-12 10:22:11,225 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 2023-06-12 10:19:05,602 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn5_1    | 2023-06-12 10:22:05,920 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,920 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:19:39,228 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:19:39,249 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm1_1   | 2023-06-12 10:19:07,471 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-06-12 10:19:31,584 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:71)
s3g_1    | 	at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:665)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
recon_1  | 2023-06-12 10:19:08,132 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-12 10:19:08,250 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-06-12 10:22:11,225 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-12 10:22:11,226 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderStateImpl
dn2_1    | 2023-06-12 10:22:11,226 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-12 10:22:11,227 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f8e4ee74-e97b-4a76-a7a0-d83197229949/current/log_inprogress_0
dn2_1    | 2023-06-12 10:22:11,228 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949-LeaderElection2] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-D83197229949: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:07,487 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3_1   | 2023-06-12 10:19:31,586 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
s3g_1    | 	at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:161)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om2_1    | address: "om3:9872"
recon_1  | 2023-06-12 10:19:08,439 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-06-12 10:19:08,650 [Listener at 0.0.0.0/9891] INFO hdds.HddsUtils: Restoring thread name: main
recon_1  | 2023-06-12 10:19:09,090 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1  | 2023-06-12 10:19:09,103 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-06-12 10:19:09,106 [main] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-06-12 10:19:09,363 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-06-12 10:19:07,489 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3_1   | 2023-06-12 10:19:31,588 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
om2_1    | startupRole: FOLLOWER
recon_1  | 2023-06-12 10:19:09,425 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-06-12 10:19:09,425 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-06-12 10:19:11,213 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-06-12 10:19:11,220 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1  | 2023-06-12 10:19:11,306 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-12 10:22:11,425 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO impl.FollowerState: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5109638742ns, electionTimeout:5102ms
scm1_1   | 2023-06-12 10:19:07,493 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3_1   | 2023-06-12 10:19:31,596 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | , id: "om2"
dn4_1    | 2023-06-12 10:19:39,337 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:05,921 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 does not exist. Creating ...
dn5_1    | 2023-06-12 10:22:05,926 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/in_use.lock acquired by nodename 7@8ab2498196c6
om3_1    | 2023-06-12 10:19:56,828 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-06-12 10:19:56,828 [main] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-06-12 10:19:56,918 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-06-12 10:22:11,426 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState
dn2_1    | 2023-06-12 10:22:11,426 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-06-12 10:22:11,426 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-06-12 10:22:11,426 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3
dn2_1    | 2023-06-12 10:22:11,428 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:11,432 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:19:39,338 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2 ELECTION round 0: result PASSED (term=4)
dn4_1    | 2023-06-12 10:19:39,338 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2
dn1_1    | 2023-06-12 10:19:30,275 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-12 10:22:11,432 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | address: "om2:9872"
dn5_1    | 2023-06-12 10:22:05,928 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 has been successfully formatted.
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm1_1   | 2023-06-12 10:19:07,520 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-06-12 10:19:11,306 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-06-12 10:19:11,311 [main] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-06-12 10:19:39,338 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
om3_1    | 2023-06-12 10:19:57,087 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5daad59a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-12 10:22:11,433 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:19:30,279 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-12 10:22:05,930 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO ratis.ContainerStateMachine: group-1B5181454F64: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-12 10:22:05,930 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-12 10:22:05,930 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 2023-06-12 10:19:07,528 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-12 10:19:07,550 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-12 10:19:07,710 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-12 10:19:39,338 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-67DF68C3D0F7 with new leaderId: 78f8fca8-1c45-4717-8e79-22872958dcce
om3_1    | 2023-06-12 10:19:57,091 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@56f2c9e8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | startupRole: FOLLOWER
dn2_1    | 2023-06-12 10:22:11,435 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm3_1   | 2023-06-12 10:19:31,635 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:19,565 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 2023-06-12 10:19:39,339 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: change Leader from null to 78f8fca8-1c45-4717-8e79-22872958dcce at term 4 for becomeLeader, leader elected after 38532ms
om3_1    | 2023-06-12 10:19:58,025 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4449b273{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-12812468207800169439/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1    | ]
dn2_1    | 2023-06-12 10:22:11,495 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-06-12 10:19:31,639 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1   | 2023-06-12 10:19:19,566 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:717)
s3g_1    | 	at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:64)
s3g_1    | 	at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:87)
dn4_1    | 2023-06-12 10:19:39,514 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | 2023-06-12 10:19:58,134 [main] INFO server.AbstractConnector: Started ServerConnector@1eb6037d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1    | 2023-06-12 10:19:58,134 [main] INFO server.Server: Started @98587ms
om3_1    | 2023-06-12 10:19:58,151 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-06-12 10:19:58,151 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-06-12 10:19:58,176 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm3_1   | 2023-06-12 10:19:31,652 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | 2023-06-12 10:19:31,881 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
recon_1  | 2023-06-12 10:19:11,357 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2f9a10df{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-06-12 10:19:11,358 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@28ee0a3c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-06-12 10:19:15,243 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@590f806a{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-10874035038184186213/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
dn4_1    | 2023-06-12 10:19:39,652 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om3_1    | 2023-06-12 10:19:58,191 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-06-12 10:19:58,207 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-06-12 10:19:58,631 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-06-12 10:19:59,037 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om2_1    | 2023-06-12 10:22:19,076 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 2023-06-12 10:19:31,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-06-12 10:19:20,139 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
scm1_1   | 2023-06-12 10:19:07,761 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:129)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:72)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:112)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:46)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:53)
s3g_1    | 	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:129)
s3g_1    | 	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
om2_1    | 2023-06-12 10:22:19,077 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
scm3_1   | 2023-06-12 10:19:31,919 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1   | 2023-06-12 10:19:20,142 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
scm1_1   | 2023-06-12 10:19:07,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-12 10:19:10,166 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-12 10:22:11,496 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection:   Response 0: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0
dn2_1    | 2023-06-12 10:22:11,497 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3 PRE_VOTE round 0: result PASSED
dn2_1    | 2023-06-12 10:22:11,503 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:11,516 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-12 10:22:19,082 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
scm2_1   | 2023-06-12 10:19:20,142 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
scm1_1   | 2023-06-12 10:19:10,175 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-06-12 10:19:10,182 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-12 10:19:39,659 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-06-12 10:19:39,931 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-12 10:22:19,083 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-12 10:22:05,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:30,279 [3c030830-a72e-4f6f-8f54-144518af6253-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-12 10:19:31,258 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: start RPC server
dn1_1    | 2023-06-12 10:19:31,270 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:31,272 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-12 10:22:19,084 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
dn4_1    | 2023-06-12 10:19:39,934 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-78f8fca8-1c45-4717-8e79-22872958dcce: Detected pause in JVM or host machine approximately 0.130s without any GCs.
dn4_1    | 2023-06-12 10:19:40,021 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-06-12 10:22:05,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-06-12 10:19:20,143 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-06-12 10:19:20,143 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-12 10:19:31,285 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:31,308 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 3c030830-a72e-4f6f-8f54-144518af6253: GrpcService started, listening on 9858
dn1_1    | 2023-06-12 10:19:31,338 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 3c030830-a72e-4f6f-8f54-144518af6253: GrpcService started, listening on 9856
dn1_1    | 2023-06-12 10:19:31,357 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 3c030830-a72e-4f6f-8f54-144518af6253: GrpcService started, listening on 9857
om2_1    | 2023-06-12 10:22:19,085 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-06-12 10:22:11,525 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:11,526 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-06-12 10:22:11,527 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection:   Response 0: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t1
dn4_1    | 2023-06-12 10:19:40,026 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-06-12 10:22:05,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-06-12 10:19:20,147 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-12 10:19:20,151 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer: 7740519e-fe5b-4936-af00-27e2eba71ce5: found a subdirectory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
dn1_1    | 2023-06-12 10:19:31,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c030830-a72e-4f6f-8f54-144518af6253 is started using port 9858 for RATIS
om3_1    | 2023-06-12 10:19:59,492 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5075889920ns, electionTimeout:5061ms
om3_1    | 2023-06-12 10:19:59,493 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-12 10:19:59,494 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om2_1    | 2023-06-12 10:22:19,085 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-06-12 10:19:40,036 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-12 10:19:40,257 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:05,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-06-12 10:19:20,156 [main] INFO server.RaftServer: 7740519e-fe5b-4936-af00-27e2eba71ce5: addNew group-FBBF2AB68244:[] returns group-FBBF2AB68244:java.util.concurrent.CompletableFuture@2e807c54[Not completed]
dn1_1    | 2023-06-12 10:19:31,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c030830-a72e-4f6f-8f54-144518af6253 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-06-12 10:19:31,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3c030830-a72e-4f6f-8f54-144518af6253 is started using port 9856 for RATIS_SERVER
om3_1    | 2023-06-12 10:19:59,497 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-06-12 10:19:59,497 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-06-12 10:19:59,526 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:19:59,669 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-06-12 10:22:19,093 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
dn2_1    | 2023-06-12 10:22:11,527 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3 ELECTION round 0: result PASSED
dn2_1    | 2023-06-12 10:22:11,527 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3
dn4_1    | 2023-06-12 10:19:40,449 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-06-12 10:19:40,497 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderStateImpl
dn5_1    | 2023-06-12 10:22:05,940 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:20,177 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
dn1_1    | 2023-06-12 10:19:31,391 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3c030830-a72e-4f6f-8f54-144518af6253: Started
dn1_1    | 2023-06-12 10:19:31,576 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om3_1    | 2023-06-12 10:19:59,682 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-12 10:22:50,033 [OmRpcReader-0] ERROR om.OzoneManagerServiceGrpc: Failed to submit request
dn2_1    | 2023-06-12 10:22:11,527 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-06-12 10:22:11,527 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1B5181454F64 with new leaderId: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn4_1    | 2023-06-12 10:19:40,698 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn4_1    | 2023-06-12 10:19:40,801 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_inprogress_3 to /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_3-4
dn4_1    | 2023-06-12 10:19:40,918 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/current/log_inprogress_5
scm2_1   | 2023-06-12 10:19:20,179 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5: new RaftServerImpl for group-FBBF2AB68244:[] with SCMStateMachine:uninitialized
scm2_1   | 2023-06-12 10:19:20,181 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn1_1    | 2023-06-12 10:19:31,750 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om3_1    | 2023-06-12 10:19:59,683 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:19:34,590 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-06-12 10:19:34,618 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | com.google.protobuf.ServiceException: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om2 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:790)
dn4_1    | 2023-06-12 10:19:40,934 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:41,064 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderElection2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: set configuration 5: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:41,404 [grpc-default-executor-2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-4675AE3D33CA, 5, (t:5, i:47))
dn4_1    | 2023-06-12 10:19:41,410 [grpc-default-executor-4] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 6, (t:6, i:20))
s3g_1    | 	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:46)
dn1_1    | 2023-06-12 10:19:32,288 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-12 10:19:59,691 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
scm3_1   | 2023-06-12 10:19:34,620 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-06-12 10:19:34,623 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:363)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
dn4_1    | 2023-06-12 10:19:41,413 [grpc-default-executor-5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: receive requestVote(ELECTION, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 7, (t:6, i:20))
scm2_1   | 2023-06-12 10:19:20,181 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-12 10:19:10,182 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
s3g_1    | 	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
dn1_1    | 2023-06-12 10:19:33,289 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-12 10:20:01,457 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-06-12 10:19:34,623 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-12 10:19:34,642 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:232)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
dn5_1    | 2023-06-12 10:22:05,991 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:41,418 [grpc-default-executor-3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: receive requestVote(PRE_VOTE, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-4675AE3D33CA, 5, (t:5, i:47))
scm2_1   | 2023-06-12 10:19:20,181 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-12 10:19:20,182 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
dn1_1    | 2023-06-12 10:19:33,692 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om3_1    | 2023-06-12 10:20:01,458 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:OK-t4
om3_1    | 2023-06-12 10:20:01,458 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
recon_1  | 2023-06-12 10:19:15,286 [main] INFO server.AbstractConnector: Started ServerConnector@2cfe272f{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:225)
dn5_1    | 2023-06-12 10:22:05,991 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:41,432 [grpc-default-executor-4] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-CANDIDATE: reject PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 1 > candidate's priority 0
scm2_1   | 2023-06-12 10:19:20,182 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-12 10:19:10,185 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
om3_1    | 2023-06-12 10:20:01,467 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:34,675 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer: c790b820-4c4f-44e6-aba9-08b18fc50228: found a subdirectory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
recon_1  | 2023-06-12 10:19:15,286 [main] INFO server.Server: Started @54976ms
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:177)
om2_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
dn3_1    | 	... 12 more
dn2_1    | 2023-06-12 10:22:11,528 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: change Leader from null to 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 at term 1 for becomeLeader, leader elected after 5320ms
dn2_1    | 2023-06-12 10:22:11,528 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-06-12 10:19:41,460 [grpc-default-executor-7] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-E4EF5B9B8C4A, 6, (t:6, i:20))
scm2_1   | 2023-06-12 10:19:20,182 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-06-12 10:19:20,190 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
dn3_1    | 2023-06-12 10:19:26,394 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:26,784 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-06-12 10:22:05,991 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:41,434 [grpc-default-executor-2] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-CANDIDATE: accept PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
scm2_1   | 2023-06-12 10:19:20,191 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-12 10:19:10,199 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-12 10:19:10,221 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: found a subdirectory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
om3_1    | 2023-06-12 10:20:01,478 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:110))
om3_1    | 2023-06-12 10:20:01,488 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 2023-06-12 10:22:11,528 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:05,991 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-06-12 10:19:20,194 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:19:41,497 [grpc-default-executor-6] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-4675AE3D33CA, 5, (t:5, i:47))
scm1_1   | 2023-06-12 10:19:10,253 [main] INFO server.RaftServer: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: addNew group-FBBF2AB68244:[] returns group-FBBF2AB68244:java.util.concurrent.CompletableFuture@12c60152[Not completed]
scm1_1   | 2023-06-12 10:19:10,395 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
om3_1    | 2023-06-12 10:20:01,488 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1    | 	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn3_1    | 2023-06-12 10:19:26,807 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
recon_1  | 2023-06-12 10:19:15,297 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManagerServiceGrpc.submitRequest(OzoneManagerServiceGrpc.java:87)
dn2_1    | 2023-06-12 10:22:11,529 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-06-12 10:22:05,991 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-12 10:22:05,992 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:20,195 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:41,533 [grpc-default-executor-8] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: receive requestVote(ELECTION, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-4675AE3D33CA, 6, (t:5, i:47))
dn4_1    | 2023-06-12 10:19:41,535 [grpc-default-executor-2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t5. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA:t5, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:20:01,510 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om2, group-D66704EFC61C, 4, (t:4, i:110))
om3_1    | 2023-06-12 10:20:01,521 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
s3g_1    | 	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn3_1    | 2023-06-12 10:19:27,122 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
recon_1  | 2023-06-12 10:19:15,297 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$MethodHandlers.invoke(OzoneManagerServiceGrpc.java:237)
dn2_1    | 2023-06-12 10:22:11,529 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-12 10:22:11,529 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm2_1   | 2023-06-12 10:19:20,207 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn4_1    | 2023-06-12 10:19:41,536 [grpc-default-executor-8] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-CANDIDATE: accept ELECTION from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 0 <= candidate's priority 1
scm3_1   | 2023-06-12 10:19:34,702 [main] INFO server.RaftServer: c790b820-4c4f-44e6-aba9-08b18fc50228: addNew group-FBBF2AB68244:[] returns group-FBBF2AB68244:java.util.concurrent.CompletableFuture@12c60152[Not completed]
scm3_1   | 2023-06-12 10:19:34,926 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 5#72 with transactionInfo term andIndex
scm3_1   | 2023-06-12 10:19:34,932 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228: new RaftServerImpl for group-FBBF2AB68244:[] with SCMStateMachine:uninitialized
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
dn3_1    | 2023-06-12 10:19:27,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:19:27,184 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/in_use.lock acquired by nodename 7@7c95fade90b7
dn3_1    | 2023-06-12 10:19:27,187 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/in_use.lock acquired by nodename 7@7c95fade90b7
dn3_1    | 2023-06-12 10:19:27,229 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/in_use.lock acquired by nodename 7@7c95fade90b7
dn2_1    | 2023-06-12 10:22:11,529 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-12 10:19:41,537 [grpc-default-executor-4] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-78f8fca8-1c45-4717-8e79-22872958dcce#0:FAIL-t6. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A:t6, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:41,543 [grpc-default-executor-7] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-CANDIDATE: reject PRE_VOTE from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 1 > candidate's priority 0
dn4_1    | 2023-06-12 10:19:41,544 [grpc-default-executor-7] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-78f8fca8-1c45-4717-8e79-22872958dcce#0:FAIL-t6. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A:t6, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:41,544 [grpc-default-executor-5] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-CANDIDATE: reject ELECTION from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 1 > candidate's priority 0
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
dn5_1    | 2023-06-12 10:22:05,992 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 2023-06-12 10:19:15,299 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
om2_1    | 	at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
om2_1    | 	at io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-06-12 10:19:10,403 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: new RaftServerImpl for group-FBBF2AB68244:[] with SCMStateMachine:uninitialized
scm1_1   | 2023-06-12 10:19:10,421 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn4_1    | 2023-06-12 10:19:41,546 [grpc-default-executor-5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: changes role from CANDIDATE to FOLLOWER at term 7 for candidate:3c030830-a72e-4f6f-8f54-144518af6253
om3_1    | 2023-06-12 10:20:01,529 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:20:01,532 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-06-12 10:20:01,532 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om2<-om3#0:OK-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=71: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:20:01,626 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
om2_1    | 	at io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
om2_1    | 	at io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)
dn2_1    | 2023-06-12 10:22:11,530 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-12 10:22:11,530 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-06-12 10:19:41,546 [grpc-default-executor-5] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1
dn4_1    | 2023-06-12 10:19:41,546 [grpc-default-executor-5] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState
dn4_1    | 2023-06-12 10:19:41,542 [grpc-default-executor-8] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: changes role from CANDIDATE to FOLLOWER at term 6 for candidate:d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | 2023-06-12 10:19:41,549 [grpc-default-executor-5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A replies to ELECTION vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-78f8fca8-1c45-4717-8e79-22872958dcce#0:FAIL-t7. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A:t7, leader=null, voted=null, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:41,587 [grpc-default-executor-8] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
recon_1  | 2023-06-12 10:19:15,303 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
dn5_1    | 2023-06-12 10:22:05,992 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState
scm1_1   | 2023-06-12 10:19:10,429 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:22:11,621 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-06-12 10:22:11,622 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 	at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn4_1    | 2023-06-12 10:19:41,588 [grpc-default-executor-8] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState
dn3_1    | 2023-06-12 10:19:27,250 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=78f8fca8-1c45-4717-8e79-22872958dcce} from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/raft-meta
dn3_1    | 2023-06-12 10:19:27,253 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=d6f449c5-6fac-4083-a27e-b83edf3e8b1c} from /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/raft-meta
dn3_1    | 2023-06-12 10:19:27,261 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=d6f449c5-6fac-4083-a27e-b83edf3e8b1c} from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/raft-meta
dn3_1    | 2023-06-12 10:19:27,397 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:27,479 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:27,473 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: set configuration 3: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-12 10:19:15,326 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-06-12 10:19:15,338 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-12 10:19:10,430 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-12 10:22:11,622 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-06-12 10:22:11,626 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 	at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 2023-06-12 10:19:41,705 [grpc-default-executor-8] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA replies to ELECTION vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t6. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:27,497 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:27,548 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO ratis.ContainerStateMachine: group-F138DFAD79A6: Setting the last applied index to (t:3, i:4)
dn3_1    | 2023-06-12 10:19:27,552 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Setting the last applied index to (t:6, i:20)
dn3_1    | 2023-06-12 10:19:27,552 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Setting the last applied index to (t:5, i:47)
dn3_1    | 2023-06-12 10:19:28,118 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-12 10:19:28,118 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 2023-06-12 10:19:15,341 [main] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1686564478359
dn5_1    | 2023-06-12 10:22:05,993 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1B5181454F64,id=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm1_1   | 2023-06-12 10:19:10,431 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn2_1    | 2023-06-12 10:22:11,628 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-06-12 10:22:11,628 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-12 10:22:11,628 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-06-12 10:19:41,708 [grpc-default-executor-6] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FOLLOWER: accept PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
dn3_1    | 2023-06-12 10:19:28,118 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-12 10:19:28,154 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-12 10:19:34,991 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-06-12 10:19:34,991 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-06-12 10:19:15,417 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn5_1    | 2023-06-12 10:22:05,993 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-12 10:19:10,431 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-12 10:22:11,629 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn2_1    | 2023-06-12 10:22:11,629 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-12 10:22:11,629 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-12 10:22:11,633 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-06-12 10:19:41,712 [grpc-default-executor-6] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t6. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:28,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
recon_1  | 2023-06-12 10:19:15,751 [main] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1686564478359.
dn5_1    | 2023-06-12 10:22:05,993 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-12 10:19:10,431 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-12 10:22:11,635 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:20:01,630 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:OK-t5
om3_1    | 2023-06-12 10:20:01,630 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om3_1    | 2023-06-12 10:20:01,630 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
dn4_1    | 2023-06-12 10:19:41,714 [grpc-default-executor-3] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FOLLOWER: accept PRE_VOTE from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-12 10:19:41,716 [grpc-default-executor-3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA replies to PRE_VOTE vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t6. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1    | 	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
recon_1  | 2023-06-12 10:19:15,774 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-06-12 10:19:15,785 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
dn5_1    | 2023-06-12 10:22:05,993 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-12 10:22:11,635 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-06-12 10:22:11,635 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-06-12 10:22:11,636 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-06-12 10:22:11,636 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:19:41,941 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:28,159 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-12 10:19:28,161 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:19:28,163 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
recon_1  | 2023-06-12 10:19:22,743 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:9e54e886-87c9-472f-9b2f-e9a516e53bd2 is not the leader. Could not determine the leader node.
dn5_1    | 2023-06-12 10:22:05,993 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-12 10:19:10,468 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-12 10:22:11,636 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om3_1    | 2023-06-12 10:20:01,631 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
scm2_1   | 2023-06-12 10:19:20,210 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-06-12 10:19:20,215 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn4_1    | 2023-06-12 10:19:42,037 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3: PRE_VOTE DISCOVERED_A_NEW_TERM (term=6) received 1 response(s) and 0 exception(s):
scm3_1   | 2023-06-12 10:19:35,002 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
dn5_1    | 2023-06-12 10:22:05,995 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:10,468 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn2_1    | 2023-06-12 10:22:11,636 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om3_1    | 2023-06-12 10:20:01,661 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om3 at term 5 for becomeLeader, leader elected after 16098ms
om3_1    | 2023-06-12 10:20:01,699 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 5, (t:4, i:110))
om3_1    | 2023-06-12 10:20:01,725 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-06-12 10:19:42,047 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection:   Response 0: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6
scm3_1   | 2023-06-12 10:19:35,004 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-06-12 10:19:35,012 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-12 10:19:35,014 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-12 10:19:35,215 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
dn5_1    | 2023-06-12 10:22:05,999 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:05,999 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64
dn2_1    | 2023-06-12 10:22:11,637 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-12 10:19:20,215 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-12 10:19:20,236 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2_1   | 2023-06-12 10:19:20,359 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn4_1    | 2023-06-12 10:19:42,048 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-LeaderElection3 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=6)
dn4_1    | 2023-06-12 10:19:42,066 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=7) received 1 response(s) and 0 exception(s):
dn4_1    | 2023-06-12 10:19:42,067 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 0: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t7
dn4_1    | 2023-06-12 10:19:42,068 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: result DISCOVERED_A_NEW_TERM (term=7)
dn4_1    | 2023-06-12 10:19:42,276 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4675AE3D33CA with new leaderId: d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm3_1   | 2023-06-12 10:19:35,221 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn5_1    | 2023-06-12 10:22:07,629 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64.
dn5_1    | 2023-06-12 10:22:07,630 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: addNew group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-F04DA08514BB:java.util.concurrent.CompletableFuture@1c7741ef[Not completed]
dn2_1    | 2023-06-12 10:22:11,637 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-12 10:22:11,638 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderStateImpl
dn2_1    | 2023-06-12 10:22:11,639 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-12 10:22:11,641 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/current/log_inprogress_0
dn2_1    | 2023-06-12 10:22:11,649 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64-LeaderElection3] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-1B5181454F64: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:13,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO impl.FollowerState: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5193614101ns, electionTimeout:5187ms
dn2_1    | 2023-06-12 10:22:13,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState
dn4_1    | 2023-06-12 10:19:42,280 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: change Leader from null to d6f449c5-6fac-4083-a27e-b83edf3e8b1c at term 6 for appendEntries, leader elected after 40111ms
dn2_1    | 2023-06-12 10:22:13,218 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-06-12 10:19:28,163 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-12 10:19:34,290 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1    | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
dn5_1    | 2023-06-12 10:22:07,635 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: new RaftServerImpl for group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:19:42,325 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread3] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: set configuration 48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:42,328 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread3] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolling segment log-34_47 to index:47
dn4_1    | 2023-06-12 10:19:42,375 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34 to /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_34-47
dn4_1    | 2023-06-12 10:19:42,404 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_48
dn3_1    | 2023-06-12 10:19:28,172 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-12 10:19:28,172 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-12 10:19:28,190 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:19:28,190 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-12 10:22:07,635 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm2_1   | 2023-06-12 10:19:20,365 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
dn5_1    | 2023-06-12 10:22:07,635 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-12 10:22:13,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 2023-06-12 10:20:01,773 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm3_1   | 2023-06-12 10:19:35,323 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
om2_1    | 	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 2023-06-12 10:22:07,635 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:19:42,976 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
dn2_1    | 2023-06-12 10:22:13,219 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4
dn2_1    | 2023-06-12 10:22:13,220 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:28,191 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-06-12 10:20:01,784 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-06-12 10:20:01,825 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:35,263 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5133089324ns, electionTimeout:5071ms
dn1_1    | 2023-06-12 10:19:35,264 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
dn1_1    | 2023-06-12 10:19:35,265 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn5_1    | 2023-06-12 10:22:07,636 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-12 10:19:20,366 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:19:43,978 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-12 10:19:10,480 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
dn2_1    | 2023-06-12 10:22:13,221 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:19:28,201 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-06-12 10:20:01,827 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om3_1    | 2023-06-12 10:20:01,833 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-06-12 10:19:35,329 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-06-12 10:22:07,636 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-12 10:19:20,367 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn2_1    | 2023-06-12 10:22:13,223 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:13,245 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn2_1    | 2023-06-12 10:22:13,246 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection:   Response 0: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0
dn2_1    | 2023-06-12 10:22:13,246 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection:   Response 1: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3#0:FAIL-t0
dn2_1    | 2023-06-12 10:22:13,246 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4 PRE_VOTE round 0: result REJECTED
dn2_1    | 2023-06-12 10:22:13,248 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
scm3_1   | 2023-06-12 10:19:35,615 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
om2_1    | Caused by: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om2 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:245)
dn5_1    | 2023-06-12 10:22:07,636 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-06-12 10:19:10,481 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-12 10:19:20,368 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-06-12 10:22:13,248 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4
dn2_1    | 2023-06-12 10:22:13,249 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-LeaderElection4] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState
dn3_1    | 2023-06-12 10:19:28,212 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:19:35,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:35,336 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-06-12 10:19:35,336 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1
dn5_1    | 2023-06-12 10:22:07,637 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-06-12 10:19:10,518 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-06-12 10:19:20,369 [7740519e-fe5b-4936-af00-27e2eba71ce5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-12 10:19:20,371 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
s3g_1    | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 2023-06-12 10:20:01,859 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn3_1    | 2023-06-12 10:19:28,216 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-12 10:19:28,216 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:07,637 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:19:44,979 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
scm2_1   | 2023-06-12 10:19:20,371 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-06-12 10:19:20,372 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-06-12 10:20:01,865 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-12 10:19:28,247 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-12 10:19:28,252 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:19:45,980 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:19:46,810 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5263879833ns, electionTimeout:5136ms
dn4_1    | 2023-06-12 10:19:46,811 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState
scm2_1   | 2023-06-12 10:19:20,399 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2_1   | 2023-06-12 10:19:20,428 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
om3_1    | 2023-06-12 10:20:01,916 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-12 10:19:28,253 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-12 10:19:28,254 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:19:46,811 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1    | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm3_1   | 2023-06-12 10:19:35,784 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om2_1    | 	... 18 more
om2_1    | 2023-06-12 10:23:29,608 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:19:46,811 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-06-12 10:19:28,257 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
scm2_1   | 2023-06-12 10:19:20,471 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn2_1    | 2023-06-12 10:22:13,303 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: receive requestVote(PRE_VOTE, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-F04DA08514BB, 0, (t:0, i:0))
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-06-12 10:19:35,984 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
om2_1    | 2023-06-12 10:23:33,849 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om2_1    | 2023-06-12 10:23:45,660 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:19:46,812 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4
dn3_1    | 2023-06-12 10:19:28,257 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
scm2_1   | 2023-06-12 10:19:20,485 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn2_1    | 2023-06-12 10:22:13,304 [grpc-default-executor-0] INFO impl.VoteContext: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FOLLOWER: accept PRE_VOTE from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 0 <= candidate's priority 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om2_1    | 2023-06-12 10:23:54,861 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-12 10:22:07,638 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-12 10:22:07,639 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-06-12 10:19:28,261 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-12 10:19:28,267 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:19:35,371 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:35,371 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5356459938ns, electionTimeout:5179ms
dn1_1    | 2023-06-12 10:19:35,387 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState
dn1_1    | 2023-06-12 10:19:35,393 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn1_1    | 2023-06-12 10:19:35,396 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-06-12 10:19:35,397 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2
dn1_1    | 2023-06-12 10:19:35,545 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 5 for 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:35,801 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:19:35,824 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:35,824 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for d6f449c5-6fac-4083-a27e-b83edf3e8b1c
om3_1    | 2023-06-12 10:20:01,916 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:20:01,917 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-06-12 10:19:10,527 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm1_1   | 2023-06-12 10:19:10,550 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-06-12 10:19:10,550 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-06-12 10:19:35,996 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-06-12 10:19:36,286 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3_1   | 2023-06-12 10:19:37,366 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn1_1    | 2023-06-12 10:19:35,873 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
om3_1    | 2023-06-12 10:20:01,939 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-06-12 10:20:01,950 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-12 10:19:10,617 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-06-12 10:19:11,016 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn1_1    | 2023-06-12 10:19:35,919 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-12 10:22:13,306 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB replies to PRE_VOTE vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:OK-t0. Peer's state: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB:t0, leader=null, voted=, raftlog=Memoized:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:28,268 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:20,486 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-12 10:19:20,526 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om3_1    | 2023-06-12 10:20:01,952 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-12 10:19:11,020 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-12 10:19:11,029 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-12 10:19:11,031 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-12 10:19:11,036 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-12 10:19:11,037 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-12 10:19:35,924 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-12 10:22:13,336 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: receive requestVote(ELECTION, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-F04DA08514BB, 1, (t:0, i:0))
dn3_1    | 2023-06-12 10:19:28,294 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn3_1    | 2023-06-12 10:19:28,297 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
om3_1    | 2023-06-12 10:20:01,953 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om3_1    | 2023-06-12 10:20:01,955 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-12 10:19:11,038 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-06-12 10:19:37,386 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:19:46,812 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 7 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:46,854 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-06-12 10:19:36,314 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6187896921ns, electionTimeout:5040ms
dn1_1    | 2023-06-12 10:19:36,318 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState
scm2_1   | 2023-06-12 10:19:20,526 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-12 10:19:20,532 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2_1   | 2023-06-12 10:19:20,532 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
om3_1    | 2023-06-12 10:20:01,956 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-12 10:20:01,960 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-12 10:19:11,038 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-06-12 10:19:37,398 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-12 10:19:36,319 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
recon_1  | 2023-06-12 10:19:25,385 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7740519e-fe5b-4936-af00-27e2eba71ce5 is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
dn3_1    | 2023-06-12 10:19:28,307 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:19:28,298 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:344)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
dn5_1    | 2023-06-12 10:22:07,648 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-06-12 10:19:11,039 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-06-12 10:19:37,405 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-06-12 10:19:20,534 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
scm3_1   | 2023-06-12 10:19:37,413 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-12 10:19:28,308 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn1_1    | 2023-06-12 10:19:36,321 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-06-12 10:19:36,321 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3
dn1_1    | 2023-06-12 10:19:36,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:36,416 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:20,536 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2_1   | 2023-06-12 10:19:20,540 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | 2023-06-12 10:19:11,167 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
scm3_1   | 2023-06-12 10:19:37,418 [c790b820-4c4f-44e6-aba9-08b18fc50228-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-06-12 10:19:37,428 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om3_1    | 2023-06-12 10:20:01,974 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om3_1    | 2023-06-12 10:20:01,978 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:20:01,979 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om3_1    | 2023-06-12 10:20:01,980 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-06-12 10:20:01,980 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1    | 2023-06-12 10:20:01,981 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
scm2_1   | 2023-06-12 10:19:20,540 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2_1   | 2023-06-12 10:19:20,585 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-06-12 10:19:11,248 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
s3g_1    | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
dn3_1    | 2023-06-12 10:19:28,308 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om3_1    | 2023-06-12 10:20:01,982 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-06-12 10:24:29,871 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
om2_1    | 2023-06-12 10:24:33,528 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
scm3_1   | 2023-06-12 10:19:37,429 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-06-12 10:19:20,606 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | 2023-06-12 10:19:20,647 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1   | 2023-06-12 10:19:11,376 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1_1   | 2023-06-12 10:19:11,428 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn3_1    | 2023-06-12 10:19:28,309 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 2023-06-12 10:19:37,431 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
dn1_1    | 2023-06-12 10:19:36,438 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn5_1    | 2023-06-12 10:22:07,649 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb does not exist. Creating ...
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
dn3_1    | 2023-06-12 10:19:28,322 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om2_1    | 2023-06-12 10:24:37,727 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
om2_1    | 2023-06-12 10:24:37,840 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 in 109 milliseconds
om2_1    | 2023-06-12 10:24:37,888 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 44 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 availability.
om2_1    | 2023-06-12 10:24:37,896 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 for snapshot snapshot1
om3_1    | 2023-06-12 10:20:01,984 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm3_1   | 2023-06-12 10:19:37,699 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3_1   | 2023-06-12 10:19:38,013 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
dn5_1    | 2023-06-12 10:22:07,652 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/in_use.lock acquired by nodename 7@8ab2498196c6
dn1_1    | 2023-06-12 10:19:36,670 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:36,670 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn1_1    | 2023-06-12 10:19:36,671 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3
dn3_1    | 2023-06-12 10:19:28,322 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:19:46,854 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Response 0: 78f8fca8-1c45-4717-8e79-22872958dcce<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t7
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
om3_1    | 2023-06-12 10:20:01,985 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-12 10:20:01,985 [om3@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-06-12 10:20:01,987 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderStateImpl
dn5_1    | 2023-06-12 10:22:07,655 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb has been successfully formatted.
dn3_1    | 2023-06-12 10:19:28,323 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-12 10:19:36,679 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-06-12 10:22:13,336 [grpc-default-executor-0] INFO impl.VoteContext: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FOLLOWER: accept ELECTION from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 0 <= candidate's priority 1
om2_1    | 2023-06-12 10:24:48,197 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
om2_1    | 2023-06-12 10:24:48,250 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df in 51 milliseconds
om3_1    | 2023-06-12 10:20:02,043 [om3@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:111
om3_1    | 2023-06-12 10:20:02,159 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 111: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:38,401 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-06-12 10:19:38,502 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn4_1    | 2023-06-12 10:19:46,854 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4 PRE_VOTE round 0: result PASSED
dn4_1    | 2023-06-12 10:19:46,856 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4 ELECTION round 0: submit vote requests at term 8 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:36,700 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DD6E9427813E with new leaderId: 3c030830-a72e-4f6f-8f54-144518af6253
dn1_1    | 2023-06-12 10:19:36,760 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: change Leader from null to 3c030830-a72e-4f6f-8f54-144518af6253 at term 4 for becomeLeader, leader elected after 35047ms
dn1_1    | 2023-06-12 10:19:36,936 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)
om3_1    | 2023-06-12 10:20:02,161 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-LEADER: reject ELECTION from om2: already has voted for om3 at current term 5
om3_1    | 2023-06-12 10:20:02,174 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:FAIL-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=om3, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c110, conf=111: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:46,966 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-06-12 10:19:46,966 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Response 0: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t8
dn2_1    | 2023-06-12 10:22:13,337 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn2_1    | 2023-06-12 10:22:13,337 [grpc-default-executor-0] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: shutdown 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState
dn2_1    | 2023-06-12 10:22:13,337 [grpc-default-executor-0] INFO impl.RoleInfo: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: start 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
dn5_1    | 2023-06-12 10:22:07,659 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO ratis.ContainerStateMachine: group-F04DA08514BB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm3_1   | 2023-06-12 10:19:38,504 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-06-12 10:19:11,430 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4 ELECTION round 0: result PASSED
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4
dn3_1    | 2023-06-12 10:19:28,323 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-06-12 10:20:02,685 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_111
dn2_1    | 2023-06-12 10:22:13,337 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState] INFO impl.FollowerState: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-FollowerState was interrupted
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
scm2_1   | 2023-06-12 10:19:20,668 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2_1   | 2023-06-12 10:19:20,674 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-06-12 10:19:39,039 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | 2023-06-12 10:19:11,495 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | 2023-06-12 10:19:11,495 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4EF5B9B8C4A with new leaderId: 78f8fca8-1c45-4717-8e79-22872958dcce
om3_1    | 2023-06-12 10:20:03,189 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
dn2_1    | 2023-06-12 10:22:13,351 [grpc-default-executor-0] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB replies to ELECTION vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:OK-t1. Peer's state: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB:t1, leader=null, voted=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, raftlog=Memoized:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:13,538 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F04DA08514BB with new leaderId: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn5_1    | 2023-06-12 10:22:07,659 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-06-12 10:19:39,039 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om2_1    | 2023-06-12 10:24:48,251 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 0 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df availability.
om2_1    | 2023-06-12 10:24:48,252 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df for snapshot snapshot2
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: change Leader from null to 78f8fca8-1c45-4717-8e79-22872958dcce at term 8 for becomeLeader, leader elected after 44670ms
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | [id: "om1"
dn2_1    | 2023-06-12 10:22:13,557 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-server-thread1] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: change Leader from null to d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 at term 1 for appendEntries, leader elected after 5813ms
dn1_1    | 2023-06-12 10:19:37,042 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:19:37,057 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-12 10:19:37,161 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | WARNING: An illegal reflective access operation has occurred
dn5_1    | 2023-06-12 10:22:07,661 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-12 10:19:39,115 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3_1   | 2023-06-12 10:19:39,115 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
om2_1    | 2023-06-12 10:24:54,423 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:19:28,325 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | address: "om1:9872"
dn2_1    | 2023-06-12 10:22:13,571 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-server-thread3] INFO server.RaftServer$Division: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-12 10:22:13,577 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29-server-thread3] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
dn5_1    | 2023-06-12 10:22:07,661 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-12 10:22:07,661 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-06-12 10:19:39,152 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
om2_1    | 2023-06-12 10:24:54,448 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn4_1    | 2023-06-12 10:19:46,967 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-12 10:19:28,329 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-06-12 10:19:11,503 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1_1   | 2023-06-12 10:19:11,503 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 2023-06-12 10:19:39,166 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-06-12 10:19:39,215 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
dn4_1    | 2023-06-12 10:19:46,970 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-12 10:19:28,329 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-12 10:22:13,581 [4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29@group-F04DA08514BB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/current/log_inprogress_0
dn2_1    | 2023-06-12 10:22:30,123 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-06-12 10:23:30,123 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-06-12 10:24:30,124 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm2_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm2_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn1_1    | 2023-06-12 10:19:37,165 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-06-12 10:19:37,176 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-06-12 10:19:39,225 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om2_1    | 2023-06-12 10:24:54,521 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-12 10:24:54,522 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn3_1    | 2023-06-12 10:19:28,329 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-12 10:19:28,329 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-12 10:19:28,330 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1  | 2023-06-12 10:19:27,386 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9ca79353606b/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1  | 2023-06-12 10:19:30,844 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-06-12 10:19:30,845 [main] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-06-12 10:19:33,271 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
scm3_1   | 2023-06-12 10:19:39,586 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om2_1    | 2023-06-12 10:25:13,557 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
dn4_1    | 2023-06-12 10:19:46,970 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-12 10:19:28,335 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-12 10:19:28,350 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
dn3_1    | 2023-06-12 10:19:28,351 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-12 10:19:28,361 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:19:28,364 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-12 10:19:28,366 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-12 10:19:28,367 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-06-12 10:19:39,779 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | WARNING: All illegal access operations will be denied in a future release
dn4_1    | 2023-06-12 10:19:46,970 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-12 10:19:28,367 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:37,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:37,424 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:19:37,499 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-06-12 10:19:37,565 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderStateImpl
dn1_1    | 2023-06-12 10:19:37,694 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn1_1    | 2023-06-12 10:19:37,780 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_inprogress_3 to /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_3-4
scm3_1   | 2023-06-12 10:19:40,173 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm2_1   | 2023-06-12 10:19:20,684 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
dn4_1    | 2023-06-12 10:19:46,970 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:19:28,368 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-12 10:19:28,368 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-12 10:19:28,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:28,429 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-12 10:19:28,430 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-12 10:19:28,430 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:19:28,434 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-12 10:19:40,319 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-06-12 10:19:40,326 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | WARNING: An illegal reflective access operation has occurred
s3g_1    | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
dn4_1    | 2023-06-12 10:19:46,970 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-12 10:19:29,262 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
scm1_1   | 2023-06-12 10:19:11,506 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn1_1    | 2023-06-12 10:19:37,926 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/current/log_inprogress_5
dn5_1    | 2023-06-12 10:22:07,662 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
dn4_1    | 2023-06-12 10:19:47,047 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-12 10:19:29,266 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 2023-06-12 10:19:33,299 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
scm2_1   | 2023-06-12 10:19:20,688 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
dn5_1    | 2023-06-12 10:22:07,694 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
dn4_1    | 2023-06-12 10:19:47,047 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:47,047 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1  | 2023-06-12 10:19:33,341 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-06-12 10:19:33,507 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
scm1_1   | 2023-06-12 10:19:11,512 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-06-12 10:19:11,516 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | 2023-06-12 10:19:11,516 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-06-12 10:19:11,594 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm3_1   | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
dn3_1    | 2023-06-12 10:19:29,267 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:47,061 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om3_1    | 2023-06-12 10:22:19,072 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om3_1    | 2023-06-12 10:22:19,078 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
om3_1    | 2023-06-12 10:22:19,083 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
scm2_1   | 2023-06-12 10:19:20,690 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn1_1    | 2023-06-12 10:19:37,935 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderElection3] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: set configuration 5: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:38,413 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-12 10:19:40,411 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-06-12 10:19:40,429 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
s3g_1    | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
s3g_1    | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm2_1   | 2023-06-12 10:19:20,740 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn1_1    | 2023-06-12 10:19:39,436 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:40,442 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-12 10:19:40,448 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-06-12 10:19:11,634 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1   | 2023-06-12 10:19:11,730 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
dn4_1    | 2023-06-12 10:19:47,069 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-06-12 10:19:47,069 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:19:47,069 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm2_1   | 2023-06-12 10:19:21,423 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn1_1    | 2023-06-12 10:19:40,518 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: receive requestVote(PRE_VOTE, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-4675AE3D33CA, 5, (t:5, i:47))
dn1_1    | 2023-06-12 10:19:40,519 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-E4EF5B9B8C4A, 6, (t:6, i:20))
dn1_1    | 2023-06-12 10:19:40,544 [grpc-default-executor-1] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-CANDIDATE: accept PRE_VOTE from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 0 <= candidate's priority 1
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-12 10:19:29,271 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:47,071 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-06-12 10:19:47,071 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
s3g_1    | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
scm2_1   | 2023-06-12 10:19:21,454 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-12 10:19:11,771 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-06-12 10:19:40,848 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn1_1    | 2023-06-12 10:19:40,547 [grpc-default-executor-0] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-CANDIDATE: accept PRE_VOTE from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 0 <= candidate's priority 0
om3_1    | 2023-06-12 10:22:19,088 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:19:47,071 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
s3g_1    | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
scm2_1   | 2023-06-12 10:19:21,541 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-06-12 10:19:11,774 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | 2023-06-12 10:19:45,219 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn1_1    | 2023-06-12 10:19:40,611 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA replies to PRE_VOTE vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t5. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA:t5, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-12 10:22:19,090 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:41310
dn3_1    | 2023-06-12 10:19:29,275 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:19:47,092 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
scm2_1   | 2023-06-12 10:19:21,573 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm1_1   | WARNING: An illegal reflective access operation has occurred
scm3_1   | 2023-06-12 10:19:45,460 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-06-12 10:22:19,090 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn1_1    | 2023-06-12 10:19:40,617 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A:t6, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:60570
dn4_1    | 2023-06-12 10:19:47,093 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
scm2_1   | 2023-06-12 10:19:21,597 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm3_1   | 2023-06-12 10:19:45,747 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
om3_1    | 2023-06-12 10:22:19,090 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om3_1    | 2023-06-12 10:22:19,095 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 5
om3_1    | 2023-06-12 10:23:29,537 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om3_1    | 2023-06-12 10:23:33,854 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
dn4_1    | 2023-06-12 10:19:47,093 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-06-12 10:19:47,093 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1   | 2023-06-12 10:19:46,074 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
om3_1    | 2023-06-12 10:23:45,644 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:33266
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:36398
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:39106
s3g_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 2023-06-12 10:22:49,685 [qtp193388045-22] ERROR protocolPB.GrpcOmTransport: error unwrapping exception from OMResponse {}
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm2_1   | 2023-06-12 10:19:21,602 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-06-12 10:23:54,854 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:36854
recon_1  | 2023-06-12 10:19:34,366 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:37218
recon_1  | 2023-06-12 10:19:35,205 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:52656: output error
s3g_1    | 2023-06-12 10:22:50,045 [qtp193388045-22] ERROR protocolPB.GrpcOmTransport: Failed to submit request
s3g_1    | io.grpc.StatusRuntimeException: INTERNAL: org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om2 is not the leader. Suggested leader is OM:om3[om3/10.9.0.13].
scm1_1   | WARNING: All illegal access operations will be denied in a future release
scm2_1   | 2023-06-12 10:19:21,603 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
recon_1  | 2023-06-12 10:19:35,213 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:37204: output error
recon_1  | 2023-06-12 10:19:35,240 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:45984: output error
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
s3g_1    | 	at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:271)
s3g_1    | 	at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:252)
scm1_1   | 2023-06-12 10:19:11,816 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2_1   | 2023-06-12 10:19:21,613 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
recon_1  | 2023-06-12 10:19:35,240 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:50252: output error
dn1_1    | 2023-06-12 10:19:40,878 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn1_1    | 2023-06-12 10:19:40,879 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
dn1_1    | 2023-06-12 10:19:40,879 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 5 for 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:40,938 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:40,972 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:19:41,027 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: receive requestVote(ELECTION, d6f449c5-6fac-4083-a27e-b83edf3e8b1c, group-4675AE3D33CA, 6, (t:5, i:47))
dn1_1    | 2023-06-12 10:19:41,032 [grpc-default-executor-0] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-CANDIDATE: accept ELECTION from d6f449c5-6fac-4083-a27e-b83edf3e8b1c: our priority 0 <= candidate's priority 1
scm1_1   | 2023-06-12 10:19:11,824 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-06-12 10:19:21,648 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:344)
recon_1  | 2023-06-12 10:19:35,240 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:40046: output error
dn1_1    | 2023-06-12 10:19:41,032 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-06-12 10:19:41,033 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t6
s3g_1    | 	at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:165)
dn5_1    | 2023-06-12 10:22:07,696 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:07,697 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-12 10:19:29,315 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
recon_1  | 2023-06-12 10:19:35,239 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:51318: output error
dn1_1    | 2023-06-12 10:19:41,035 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: result PASSED
s3g_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerServiceGrpc$OzoneManagerServiceBlockingStub.submitRequest(OzoneManagerServiceGrpc.java:182)
scm3_1   | 2023-06-12 10:19:46,335 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-12 10:19:46,375 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-06-12 10:19:29,315 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:19:47,095 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-12 10:19:47,104 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderStateImpl
scm1_1   | 2023-06-12 10:19:11,832 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-06-12 10:19:11,944 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
recon_1  | 2023-06-12 10:19:35,238 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:50306: output error
recon_1  | 2023-06-12 10:19:35,227 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:39104: output error
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransport.submitRequest(GrpcOmTransport.java:186)
dn3_1    | 2023-06-12 10:19:29,321 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1   | 2023-06-12 10:19:21,654 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-12 10:19:21,655 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-06-12 10:19:13,222 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
recon_1  | 2023-06-12 10:19:35,219 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:58242: output error
recon_1  | 2023-06-12 10:19:35,217 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#11 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:40052: output error
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:304)
dn3_1    | 2023-06-12 10:19:29,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-12 10:19:21,659 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm2_1   | 2023-06-12 10:19:21,739 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | 2023-06-12 10:19:13,284 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-12 10:19:13,344 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
recon_1  | 2023-06-12 10:19:35,217 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:50308: output error
recon_1  | 2023-06-12 10:19:35,215 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:36870: output error
s3g_1    | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1606)
dn3_1    | 2023-06-12 10:19:29,412 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-06-12 10:19:21,740 [main] INFO server.StorageContainerManager: 
scm2_1   | Container Balancer status:
scm1_1   | 2023-06-12 10:19:13,383 [Listener at 0.0.0.0/9861] INFO hdds.HddsUtils: Restoring thread name: main
scm1_1   | 2023-06-12 10:19:13,429 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
s3g_1    | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:243)
scm3_1   | 2023-06-12 10:19:46,389 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3_1   | 2023-06-12 10:19:46,428 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
dn5_1    | 2023-06-12 10:22:07,698 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-12 10:22:07,698 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-12 10:22:07,699 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-12 10:19:13,438 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:248)
dn3_1    | 2023-06-12 10:19:29,413 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-06-12 10:19:46,620 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | Key                            Value
scm2_1   | Running                        false
scm2_1   | Container Balancer Configuration values:
scm1_1   | 2023-06-12 10:19:13,450 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 2023-06-12 10:19:41,040 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: changes role from CANDIDATE to FOLLOWER at term 6 for candidate:d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn1_1    | 2023-06-12 10:19:41,040 [grpc-default-executor-0] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2
dn3_1    | 2023-06-12 10:19:29,413 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm3_1   | 2023-06-12 10:19:47,064 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-06-12 10:19:47,079 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3_1   | 2023-06-12 10:19:47,137 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
scm3_1   | 2023-06-12 10:19:47,571 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | 2023-06-12 10:19:13,467 [Listener at 0.0.0.0/9863] INFO hdds.HddsUtils: Restoring thread name: main
scm1_1   | 2023-06-12 10:19:13,567 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-06-12 10:19:41,041 [grpc-default-executor-0] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState
dn1_1    | 2023-06-12 10:19:41,040 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1 ELECTION round 0: submit vote requests at term 7 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:29,644 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: set configuration 0: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:47,600 [main] INFO server.StorageContainerManager: 
scm3_1   | Container Balancer status:
scm3_1   | Key                            Value
scm3_1   | Running                        false
scm1_1   | 2023-06-12 10:19:13,592 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:29,646 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | Container Balancer Configuration values:
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | 2023-06-12 10:19:13,598 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-06-12 10:19:13,611 [Listener at 0.0.0.0/9860] INFO hdds.HddsUtils: Restoring thread name: main
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:19:29,664 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_0-0
scm3_1   | Key                                                Value
scm3_1   | Threshold                                          10
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3_1   | Max Size to Move per Iteration                     500GB
scm1_1   | 2023-06-12 10:19:13,707 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-06-12 10:19:29,710 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | Max Size Entering Target per Iteration             26GB
dn5_1    | 2023-06-12 10:22:07,699 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-12 10:22:07,705 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-12 10:22:07,707 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-12 10:19:13,707 [main] INFO server.StorageContainerManager: 
om3_1    | 2023-06-12 10:24:29,867 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:snapvolume-1 for user:hadoop
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:19:29,719 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: set configuration 1: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:115)
dn5_1    | 2023-06-12 10:22:08,104 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: Detected pause in JVM or host machine approximately 0.112s with 0.395s GC time.
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=21ms
scm2_1   | Max Size to Move per Iteration                     500GB
scm2_1   | Max Size Entering Target per Iteration             26GB
om3_1    | 2023-06-12 10:24:33,517 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: snapbucket-1 of layout FILE_SYSTEM_OPTIMIZED in volume: snapvolume-1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 2023-06-12 10:19:29,737 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_1-2
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 
scm1_1   | Container Balancer status:
scm1_1   | Key                            Value
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-06-12 10:19:29,789 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_0-7
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.<init>(OzoneClientCache.java:83)
scm3_1   | 2023-06-12 10:19:47,607 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
om3_1    | 2023-06-12 10:24:37,712 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-06-12 10:19:47,660 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm1_1   | Running                        false
scm1_1   | Container Balancer Configuration values:
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:19:29,831 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: set configuration 8: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:47,669 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm3_1   | 2023-06-12 10:19:47,673 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn4_1    | 2023-06-12 10:19:47,123 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolling segment log-7_20 to index:20
dn4_1    | 2023-06-12 10:19:47,143 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: set configuration 21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | Max Size Leaving Source per Iteration              26GB
om3_1    | 2023-06-12 10:24:37,865 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 in 149 milliseconds
scm1_1   | Key                                                Value
scm1_1   | Threshold                                          10
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
dn1_1    | 2023-06-12 10:19:41,069 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:19:29,810 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_0-4
dn3_1    | 2023-06-12 10:19:29,863 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 5: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:47,711 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn5_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=374ms
dn5_1    | 2023-06-12 10:22:08,117 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-12 10:22:08,119 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-06-12 10:24:37,867 [Thread-335] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientCache.getOzoneClientInstance(OzoneClientCache.java:98)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:121)
scm1_1   | Max Size to Move per Iteration                     500GB
dn1_1    | 2023-06-12 10:19:41,070 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:19:47,735 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/in_use.lock acquired by nodename 7@200a502423a3
scm3_1   | 2023-06-12 10:19:47,761 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=} from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/raft-meta
scm3_1   | 2023-06-12 10:19:48,000 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:19:47,163 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7 to /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_7-20
dn4_1    | 2023-06-12 10:19:47,173 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_21
dn4_1    | 2023-06-12 10:19:56,180 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-78f8fca8-1c45-4717-8e79-22872958dcce: Detected pause in JVM or host machine approximately 0.127s without any GCs.
dn4_1    | 2023-06-12 10:19:57,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-12 10:20:34,414 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-06-12 10:21:17,969 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-06-12 10:21:17,970 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-06-12 10:21:17,971 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 
scm2_1   | 2023-06-12 10:19:21,740 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-06-12 10:19:21,750 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm2_1   | 2023-06-12 10:19:21,753 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm2_1   | 2023-06-12 10:19:21,753 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-06-12 10:19:21,759 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
om3_1    | 2023-06-12 10:24:37,891 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 22 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 availability.
om3_1    | 2023-06-12 10:24:37,894 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-75bf30b2-e40f-43a2-8512-253691a1e126 for snapshot snapshot1
om3_1    | 2023-06-12 10:24:48,188 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot 'snapshot2' under path 'snapvolume-1/snapbucket-1'
om3_1    | 2023-06-12 10:24:48,262 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df in 72 milliseconds
dn5_1    | 2023-06-12 10:22:08,120 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-12 10:19:41,087 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA replies to ELECTION vote request: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:41,146 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2: PRE_VOTE DISCOVERED_A_NEW_TERM (term=6) received 1 response(s) and 0 exception(s):
dn1_1    | 2023-06-12 10:19:41,146 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection:   Response 0: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:FAIL-t6
dn1_1    | 2023-06-12 10:19:41,146 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-LeaderElection2 PRE_VOTE round 1: result DISCOVERED_A_NEW_TERM (term=6)
s3g_1    | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
om3_1    | 2023-06-12 10:24:48,263 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 0 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df availability.
om3_1    | 2023-06-12 10:24:48,263 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-8499ff87-c85f-4041-b704-61a1ee1f95df for snapshot snapshot2
om3_1    | 2023-06-12 10:24:54,938 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-12 10:24:54,943 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-06-12 10:24:55,038 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 2023-06-12 10:19:29,893 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_5-6
dn3_1    | 2023-06-12 10:19:29,896 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: set configuration 3: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:29,925 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_inprogress_3
om3_1    | 2023-06-12 10:24:55,041 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-06-12 10:24:56,337 [IPC Server handler 4 on default port 9862] INFO snapshot.SnapshotDiffManager: Submitting snap diff report generation request for volume: snapvolume-1, bucket: snapbucket-1, fromSnapshot: snapshot1 and toSnapshot: snapshot2
scm1_1   | Max Size Entering Target per Iteration             26GB
scm3_1   | 2023-06-12 10:19:48,035 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-12 10:22:08,120 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-12 10:19:29,941 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.LogSegment: Successfully read 26 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_8-33
om3_1    | 2023-06-12 10:24:56,340 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotDiffManager: Started snap diff report generation for volume: snapvolume-1 bucket: snapbucket-1, fromSnapshot: snapshot1 and toSnapshot: snapshot2
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm1_1   | 
dn5_1    | 2023-06-12 10:22:08,120 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1    | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1    | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm2_1   | 2023-06-12 10:19:21,766 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/in_use.lock acquired by nodename 6@e85c32bf6fdc
scm2_1   | 2023-06-12 10:19:21,775 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=9e54e886-87c9-472f-9b2f-e9a516e53bd2} from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/raft-meta
dn3_1    | 2023-06-12 10:19:29,935 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:29,971 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: set configuration 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:48,067 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-12 10:19:48,071 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-12 10:19:48,078 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-06-12 10:19:48,084 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-06-12 10:19:48,096 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn5_1    | 2023-06-12 10:22:08,121 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:41,483 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-12 10:21:17,972 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn4_1    | 2023-06-12 10:21:17,972 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-06-12 10:21:17,972 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 2023-06-12 10:19:48,197 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-12 10:22:08,122 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-06-12 10:22:08,122 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState
dn1_1    | 2023-06-12 10:19:41,666 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn1_1    | 2023-06-12 10:19:41,667 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3c030830-a72e-4f6f-8f54-144518af6253<-78f8fca8-1c45-4717-8e79-22872958dcce#0:FAIL-t7
dn1_1    | 2023-06-12 10:19:41,667 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 1: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t7
dn1_1    | 2023-06-12 10:19:41,667 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1 ELECTION round 0: result REJECTED
s3g_1    | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm3_1   | 2023-06-12 10:19:48,200 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-06-12 10:19:21,832 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,835 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm3_1   | 2023-06-12 10:19:48,210 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:21,842 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-12 10:22:08,123 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F04DA08514BB,id=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn5_1    | 2023-06-12 10:22:08,123 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-12 10:22:08,125 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-06-12 10:22:08,125 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-12 10:19:29,977 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm3_1   | 2023-06-12 10:19:48,263 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
scm2_1   | 2023-06-12 10:19:21,842 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-12 10:19:21,844 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-06-12 10:19:21,845 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:95)
scm1_1   | 2023-06-12 10:19:13,708 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-06-12 10:19:13,715 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm3_1   | 2023-06-12 10:19:48,273 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
dn1_1    | 2023-06-12 10:19:41,667 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn1_1    | 2023-06-12 10:19:41,668 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1
s3g_1    | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:85)
s3g_1    | 	at org.jboss.weld.injection.producer.ProducerMethodProducer.produce(ProducerMethodProducer.java:103)
scm1_1   | 2023-06-12 10:19:13,723 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm1_1   | 2023-06-12 10:19:13,727 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3_1   | 2023-06-12 10:19:48,283 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2_1   | 2023-06-12 10:19:21,849 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
s3g_1    | 	at org.jboss.weld.injection.producer.AbstractMemberProducer.produce(AbstractMemberProducer.java:161)
s3g_1    | 	at org.jboss.weld.bean.AbstractProducerBean.create(AbstractProducerBean.java:180)
scm1_1   | 2023-06-12 10:19:13,741 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
dn3_1    | 2023-06-12 10:19:30,026 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-06-12 10:19:48,294 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn1_1    | 2023-06-12 10:19:41,668 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
scm2_1   | 2023-06-12 10:19:21,855 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
scm1_1   | 2023-06-12 10:19:13,750 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/in_use.lock acquired by nodename 6@dfadc871a7c0
scm1_1   | 2023-06-12 10:19:13,763 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=5, votedFor=9e54e886-87c9-472f-9b2f-e9a516e53bd2} from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/raft-meta
dn4_1    | 2023-06-12 10:21:17,973 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn3_1    | 2023-06-12 10:19:30,030 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 6
om3_1    | 2023-06-12 10:24:56,346 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-12 10:19:48,309 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn1_1    | 2023-06-12 10:19:41,985 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 6, (t:6, i:20))
scm2_1   | 2023-06-12 10:19:21,857 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-12 10:19:13,838 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:13,849 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-06-12 10:24:56,347 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm3_1   | 2023-06-12 10:19:48,316 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-12 10:19:41,985 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-4675AE3D33CA, 5, (t:5, i:47))
scm2_1   | 2023-06-12 10:19:21,857 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:19:30,026 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
om3_1    | 2023-06-12 10:24:56,498 [snapshot-diff-job-thread-id-0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
scm3_1   | 2023-06-12 10:19:48,319 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-12 10:19:41,985 [grpc-default-executor-0] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FOLLOWER: accept PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 0
scm2_1   | 2023-06-12 10:19:21,863 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
scm1_1   | 2023-06-12 10:19:13,878 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-12 10:19:13,879 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-12 10:24:56,503 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-12 10:19:48,325 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-12 10:19:41,986 [grpc-default-executor-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,863 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-12 10:19:13,886 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-12 10:19:30,065 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
om3_1    | 2023-06-12 10:24:56,504 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm3_1   | 2023-06-12 10:19:48,327 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-12 10:19:41,986 [grpc-default-executor-1] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FOLLOWER: accept PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
scm2_1   | 2023-06-12 10:19:21,864 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn5_1    | 2023-06-12 10:22:08,126 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-12 10:22:08,128 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:08,133 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb
dn5_1    | 2023-06-12 10:22:08,137 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:08,471 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb.
scm1_1   | 2023-06-12 10:19:13,887 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-12 10:19:13,896 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-12 10:25:13,561 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snapshot1' under path 'snapvolume-1/snapbucket-1'
scm3_1   | 2023-06-12 10:19:48,462 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn1_1    | 2023-06-12 10:19:41,986 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t7. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A:t7, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,865 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn5_1    | 2023-06-12 10:22:10,922 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO impl.FollowerState: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5016948013ns, electionTimeout:5014ms
dn5_1    | 2023-06-12 10:22:10,922 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState
scm1_1   | 2023-06-12 10:19:13,918 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-12 10:19:48,469 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:19:42,276 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4675AE3D33CA with new leaderId: d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm2_1   | 2023-06-12 10:19:21,866 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getInjectableReference(BeanManagerImpl.java:794)
s3g_1    | 	at org.jboss.weld.injection.FieldInjectionPoint.inject(FieldInjectionPoint.java:92)
scm1_1   | 2023-06-12 10:19:13,918 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-06-12 10:19:48,642 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:42,293 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: change Leader from null to d6f449c5-6fac-4083-a27e-b83edf3e8b1c at term 6 for appendEntries, leader elected after 39601ms
scm2_1   | 2023-06-12 10:19:21,866 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
s3g_1    | 	at org.jboss.weld.util.Beans.injectBoundFields(Beans.java:345)
s3g_1    | 	at org.jboss.weld.util.Beans.injectFieldsAndInitializers(Beans.java:356)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector$1.proceed(ResourceInjector.java:69)
s3g_1    | 	at org.jboss.weld.injection.InjectionContextImpl.run(InjectionContextImpl.java:48)
dn3_1    | 2023-06-12 10:19:30,076 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.LogSegment: Successfully read 14 entries from segment file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm3_1   | 2023-06-12 10:19:48,654 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:42,462 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: set configuration 48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,869 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-12 10:19:13,920 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1    | 	at org.jboss.weld.injection.producer.ResourceInjector.inject(ResourceInjector.java:71)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-06-12 10:19:48,665 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn1_1    | 2023-06-12 10:19:42,464 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolling segment log-34_47 to index:47
scm2_1   | 2023-06-12 10:19:21,869 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-12 10:19:13,938 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244
s3g_1    | 	at org.jboss.weld.injection.producer.BasicInjectionTarget.inject(BasicInjectionTarget.java:117)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 2023-06-12 10:19:48,860 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 0: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:42,472 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34 to /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_34-47
scm2_1   | 2023-06-12 10:19:21,870 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-12 10:22:10,922 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm3_1   | 2023-06-12 10:19:48,866 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_0-0
dn1_1    | 2023-06-12 10:19:42,480 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_48
scm2_1   | 2023-06-12 10:19:21,880 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm1_1   | 2023-06-12 10:19:13,940 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-12 10:19:48,879 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 1: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:48,920 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 17: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-12 10:19:48,927 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 19: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:10,922 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-06-12 10:22:10,922 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2
dn5_1    | 2023-06-12 10:22:10,923 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:10,923 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-06-12 10:22:10,925 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:10,925 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-06-12 10:19:13,943 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn1_1    | 2023-06-12 10:19:42,484 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-06-12 10:19:48,944 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 31: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-12 10:19:48,946 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 33: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:10,925 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2
scm2_1   | 2023-06-12 10:19:21,881 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-12 10:19:13,946 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn1_1    | 2023-06-12 10:19:43,488 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:22:10,925 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-06-12 10:22:10,925 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8C2A955BD013 with new leaderId: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm3_1   | 2023-06-12 10:19:48,958 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_1-48
scm3_1   | 2023-06-12 10:19:48,971 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 49: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:13,947 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn1_1    | 2023-06-12 10:19:44,489 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:22:10,926 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: change Leader from null to d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 at term 1 for becomeLeader, leader elected after 5104ms
dn5_1    | 2023-06-12 10:22:10,926 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 2023-06-12 10:19:48,979 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_49-60
scm3_1   | 2023-06-12 10:19:48,987 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:13,948 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-12 10:19:45,490 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:46,491 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-12 10:19:46,869 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 7, (t:6, i:20))
scm3_1   | 2023-06-12 10:19:49,353 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61
scm3_1   | 2023-06-12 10:19:49,405 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
scm1_1   | 2023-06-12 10:19:13,951 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-12 10:19:30,085 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 47
dn3_1    | 2023-06-12 10:19:30,086 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 33
dn3_1    | 2023-06-12 10:19:30,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-12 10:19:21,902 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:19:46,870 [grpc-default-executor-1] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FOLLOWER: accept PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
dn1_1    | 2023-06-12 10:19:46,870 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t7. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A:t7, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiComponentProvider$InjectionManagerInjectedCdiTarget.inject(CdiComponentProvider.java:665)
s3g_1    | 	at org.jboss.weld.bean.ManagedBean.create(ManagedBean.java:161)
scm1_1   | 2023-06-12 10:19:13,951 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-12 10:19:30,474 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: start as a follower, conf=3: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:30,475 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: changes role from      null to FOLLOWER at term 3 for startAsFollower
scm3_1   | 2023-06-12 10:19:49,407 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO segmented.SegmentedRaftLogWorker: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
scm3_1   | 2023-06-12 10:19:50,105 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: start as a follower, conf=61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:21:17,985 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: remove    LEADER 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A:t8, leader=78f8fca8-1c45-4717-8e79-22872958dcce, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c22, conf=21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-06-12 10:21:17,987 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: shutdown
dn4_1    | 2023-06-12 10:21:17,987 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:19:13,952 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-12 10:19:30,477 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: start as a follower, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:30,479 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: changes role from      null to FOLLOWER at term 5 for startAsFollower
scm2_1   | 2023-06-12 10:19:21,902 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-06-12 10:19:21,903 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn1_1    | 2023-06-12 10:19:46,951 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: receive requestVote(ELECTION, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 8, (t:6, i:20))
dn1_1    | 2023-06-12 10:19:46,951 [grpc-default-executor-1] INFO impl.VoteContext: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FOLLOWER: accept ELECTION from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
dn1_1    | 2023-06-12 10:19:46,951 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:19:46,952 [grpc-default-executor-1] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
dn1_1    | 2023-06-12 10:19:46,952 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState was interrupted
scm1_1   | 2023-06-12 10:19:13,983 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-06-12 10:19:21,935 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 0: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,936 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_0-0
scm3_1   | 2023-06-12 10:19:50,106 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: changes role from      null to FOLLOWER at term 5 for startAsFollower
scm3_1   | 2023-06-12 10:19:50,113 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO impl.RoleInfo: c790b820-4c4f-44e6-aba9-08b18fc50228: start c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-06-12 10:19:21,940 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 1: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,950 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 17: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-12 10:19:50,126 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:46,952 [grpc-default-executor-1] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
s3g_1    | 	at org.jboss.weld.contexts.unbound.DependentContextImpl.get(DependentContextImpl.java:64)
dn3_1    | 2023-06-12 10:19:30,482 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: start as a follower, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:30,551 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn3_1    | 2023-06-12 10:19:30,555 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState
scm1_1   | 2023-06-12 10:19:13,984 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 2023-06-12 10:19:50,128 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FBBF2AB68244,id=c790b820-4c4f-44e6-aba9-08b18fc50228
dn1_1    | 2023-06-12 10:19:46,958 [grpc-default-executor-1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A replies to ELECTION vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t8. Peer's state: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A:t8, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | 	at org.jboss.weld.bean.ContextualInstanceStrategy$DefaultContextualInstanceStrategy.get(ContextualInstanceStrategy.java:100)
dn4_1    | 2023-06-12 10:21:17,988 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-LeaderStateImpl
scm2_1   | 2023-06-12 10:19:21,951 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 19: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,952 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 31: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn3_1    | 2023-06-12 10:19:30,555 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState
dn5_1    | 2023-06-12 10:22:10,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:10,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-06-12 10:19:47,318 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4EF5B9B8C4A with new leaderId: 78f8fca8-1c45-4717-8e79-22872958dcce
s3g_1    | 	at org.jboss.weld.bean.ContextualInstance.get(ContextualInstance.java:50)
dn4_1    | 2023-06-12 10:21:17,989 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm2_1   | 2023-06-12 10:19:21,952 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 33: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,953 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_1-48
scm2_1   | 2023-06-12 10:19:21,954 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 49: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:30,555 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
dn3_1    | 2023-06-12 10:19:30,563 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:19:31,331 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:47,318 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread1] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: change Leader from null to 78f8fca8-1c45-4717-8e79-22872958dcce at term 8 for appendEntries, leader elected after 44487ms
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:694)
dn4_1    | 2023-06-12 10:21:17,997 [grpc-default-executor-8] INFO server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm2_1   | 2023-06-12 10:19:21,955 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_49-60
scm2_1   | 2023-06-12 10:19:21,956 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:21,988 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61
scm2_1   | 2023-06-12 10:19:21,992 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-06-12 10:19:47,435 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread2] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:19:47,437 [3c030830-a72e-4f6f-8f54-144518af6253-server-thread2] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolling segment log-7_20 to index:20
dn1_1    | 2023-06-12 10:19:47,437 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7 to /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_7-20
dn1_1    | 2023-06-12 10:19:47,444 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_21
scm1_1   | 2023-06-12 10:19:14,279 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-06-12 10:19:21,992 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
scm2_1   | 2023-06-12 10:19:22,042 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: start as a follower, conf=61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:22,042 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: changes role from      null to FOLLOWER at term 5 for startAsFollower
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
s3g_1    | 	at org.jboss.weld.manager.BeanManagerImpl.getReference(BeanManagerImpl.java:717)
dn3_1    | 2023-06-12 10:19:31,332 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:19:57,746 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm1_1   | 2023-06-12 10:19:14,280 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-06-12 10:19:22,044 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO impl.RoleInfo: 7740519e-fe5b-4936-af00-27e2eba71ce5: start 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState
dn5_1    | 2023-06-12 10:22:10,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-06-12 10:19:50,136 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
s3g_1    | 	at org.jboss.weld.util.ForwardingBeanManager.getReference(ForwardingBeanManager.java:64)
dn3_1    | 2023-06-12 10:19:31,336 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:20:31,750 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-06-12 10:19:14,280 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2_1   | 2023-06-12 10:19:22,045 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FBBF2AB68244,id=7740519e-fe5b-4936-af00-27e2eba71ce5
dn5_1    | 2023-06-12 10:22:10,931 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-06-12 10:22:10,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-06-12 10:21:18,000 [grpc-default-executor-8] INFO leader.FollowerInfo: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253: decreaseNextIndex nextIndex: updateUnconditionally 23 -> 22
s3g_1    | 	at org.jboss.weld.bean.builtin.BeanManagerProxy.getReference(BeanManagerProxy.java:87)
dn3_1    | 2023-06-12 10:19:31,338 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-12 10:21:07,709 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:19:14,323 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 0: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:22,047 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-12 10:22:10,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-06-12 10:19:50,138 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.CdiUtil.getBeanReference(CdiUtil.java:129)
dn3_1    | 2023-06-12 10:19:31,338 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:21:07,710 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm1_1   | 2023-06-12 10:19:14,327 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_0-0
scm2_1   | 2023-06-12 10:19:22,048 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn5_1    | 2023-06-12 10:22:10,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier$1.getInstance(AbstractCdiBeanSupplier.java:72)
dn4_1    | 2023-06-12 10:21:18,002 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn1_1    | 2023-06-12 10:21:07,711 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm1_1   | 2023-06-12 10:19:14,328 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 1: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:22,048 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn5_1    | 2023-06-12 10:22:10,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderStateImpl
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-06-12 10:19:31,357 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.AbstractCdiBeanSupplier._provide(AbstractCdiBeanSupplier.java:112)
s3g_1    | 	at org.glassfish.jersey.ext.cdi1x.internal.RequestScopedCdiBeanSupplier.get(RequestScopedCdiBeanSupplier.java:46)
dn1_1    | 2023-06-12 10:21:07,712 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
scm1_1   | 2023-06-12 10:19:14,338 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 17: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-06-12 10:19:22,049 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:10,932 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:31,357 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.InstanceSupplierFactoryBridge.provide(InstanceSupplierFactoryBridge.java:53)
s3g_1    | 	at org.jvnet.hk2.internal.FactoryCreator.create(FactoryCreator.java:129)
dn1_1    | 2023-06-12 10:21:07,712 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm1_1   | 2023-06-12 10:19:14,338 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 19: peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:22,049 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:10,933 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-LeaderElection2] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013: set configuration 0: peers:[d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:19:31,357 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
s3g_1    | 	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
s3g_1    | 	at org.jvnet.hk2.internal.PerLookupContext.findOrCreate(PerLookupContext.java:46)
dn1_1    | 2023-06-12 10:21:07,712 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm1_1   | 2023-06-12 10:19:14,339 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 31: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-06-12 10:19:22,053 [7740519e-fe5b-4936-af00-27e2eba71ce5-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-12 10:22:10,942 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-8C2A955BD013-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e4d68ba5-f2e1-460a-8ae0-8c2a955bd013/current/log_inprogress_0
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-06-12 10:19:31,367 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
s3g_1    | 	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-06-12 10:21:18,015 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-PendingRequests: sendNotLeaderResponses
scm2_1   | 2023-06-12 10:19:22,062 [main] INFO server.RaftServer: 7740519e-fe5b-4936-af00-27e2eba71ce5: start RPC server
dn5_1    | 2023-06-12 10:22:11,042 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO impl.FollowerState: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5050594108ns, electionTimeout:5043ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:19:31,367 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F138DFAD79A6,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
s3g_1    | 	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
dn4_1    | 2023-06-12 10:21:18,015 [grpc-default-executor-8] INFO server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm2_1   | 2023-06-12 10:19:22,111 [main] INFO server.GrpcService: 7740519e-fe5b-4936-af00-27e2eba71ce5: GrpcService started, listening on 9894
dn5_1    | 2023-06-12 10:22:11,043 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-06-12 10:19:31,372 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
s3g_1    | 	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
scm1_1   | 2023-06-12 10:19:14,339 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 33: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:21:18,019 [grpc-default-executor-8] INFO leader.FollowerInfo: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c: decreaseNextIndex nextIndex: updateUnconditionally 23 -> 22
dn4_1    | 2023-06-12 10:21:18,039 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Taking a snapshot at:(t:8, i:22) file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.8_22
dn4_1    | 2023-06-12 10:21:18,040 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater: set stopIndex = 22
s3g_1    | 	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
dn4_1    | 2023-06-12 10:21:18,042 [grpc-default-executor-8] INFO server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn4_1    | 2023-06-12 10:21:18,042 [grpc-default-executor-8] INFO leader.FollowerInfo: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->3c030830-a72e-4f6f-8f54-144518af6253: decreaseNextIndex nextIndex: updateUnconditionally 22 -> 21
dn4_1    | 2023-06-12 10:21:18,047 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Finished taking a snapshot at:(t:8, i:22) file:/data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.8_22 took: 7 ms
s3g_1    | 	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
scm1_1   | 2023-06-12 10:19:14,341 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_1-48
dn4_1    | 2023-06-12 10:21:18,048 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater: Took a snapshot at index 22
dn4_1    | 2023-06-12 10:21:18,049 [78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 22
dn4_1    | 2023-06-12 10:21:18,053 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: closes. applyIndex: 22
s3g_1    | 	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
dn4_1    | 2023-06-12 10:21:18,060 [grpc-default-executor-8] INFO server.GrpcLogAppender: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-06-12 10:19:31,400 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-12 10:19:31,400 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-12 10:19:31,405 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-12 10:19:31,408 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:21:18,060 [grpc-default-executor-8] INFO leader.FollowerInfo: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c: decreaseNextIndex nextIndex: updateUnconditionally 22 -> 21
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
dn3_1    | 2023-06-12 10:19:31,411 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-12 10:19:31,412 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-12 10:19:31,415 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-06-12 10:19:50,140 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-06-12 10:19:50,141 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn4_1    | 2023-06-12 10:21:18,498 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A-SegmentedRaftLogWorker close()
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-06-12 10:19:31,413 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:22:11,043 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-06-12 10:21:18,540 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
dn3_1    | 2023-06-12 10:19:31,490 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-12 10:19:31,490 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-12 10:22:11,044 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-06-12 10:22:11,044 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn4_1    | 2023-06-12 10:21:18,540 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
dn3_1    | 2023-06-12 10:19:31,567 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start RPC server
scm3_1   | 2023-06-12 10:19:50,143 [c790b820-4c4f-44e6-aba9-08b18fc50228-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-06-12 10:19:50,152 [main] INFO server.RaftServer: c790b820-4c4f-44e6-aba9-08b18fc50228: start RPC server
scm1_1   | 2023-06-12 10:19:14,345 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 49: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 2023-06-12 10:21:18,602 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-E4EF5B9B8C4A: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn4_1    | 2023-06-12 10:21:18,604 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a command on datanode 78f8fca8-1c45-4717-8e79-22872958dcce.
s3g_1    | 	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
dn3_1    | 2023-06-12 10:19:31,581 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: GrpcService started, listening on 9858
scm3_1   | 2023-06-12 10:19:50,702 [main] INFO server.GrpcService: c790b820-4c4f-44e6-aba9-08b18fc50228: GrpcService started, listening on 9894
dn5_1    | 2023-06-12 10:22:11,048 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:11,058 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:21:18,605 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: remove    LEADER 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7:t4, leader=78f8fca8-1c45-4717-8e79-22872958dcce, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLog:OPENED:c6, conf=5: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 2023-06-12 10:19:50,733 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c790b820-4c4f-44e6-aba9-08b18fc50228: Started
dn5_1    | 2023-06-12 10:22:11,058 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:11,058 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm2_1   | 2023-06-12 10:19:22,145 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7740519e-fe5b-4936-af00-27e2eba71ce5: Started
scm2_1   | 2023-06-12 10:19:22,153 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-12 10:19:22,153 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
dn3_1    | 2023-06-12 10:19:31,593 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: GrpcService started, listening on 9856
dn3_1    | 2023-06-12 10:19:31,603 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: GrpcService started, listening on 9857
dn4_1    | 2023-06-12 10:21:18,605 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: shutdown
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-06-12 10:19:50,766 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
dn5_1    | 2023-06-12 10:22:11,065 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 78f8fca8-1c45-4717-8e79-22872958dcce
dn5_1    | 2023-06-12 10:22:11,121 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm2_1   | 2023-06-12 10:19:22,159 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm2_1   | 2023-06-12 10:19:22,165 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
dn5_1    | 2023-06-12 10:22:11,122 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection:   Response 0: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:FAIL-t0
dn5_1    | 2023-06-12 10:22:11,122 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection:   Response 1: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0
dn5_1    | 2023-06-12 10:22:11,122 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3 PRE_VOTE round 0: result REJECTED
dn5_1    | 2023-06-12 10:22:11,123 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn5_1    | 2023-06-12 10:22:11,123 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3
scm1_1   | 2023-06-12 10:19:14,346 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_49-60
scm1_1   | 2023-06-12 10:19:14,347 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-06-12 10:19:50,766 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3_1   | 2023-06-12 10:19:50,781 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3_1   | 2023-06-12 10:19:50,789 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3_1   | 2023-06-12 10:19:50,789 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3_1   | 2023-06-12 10:19:51,581 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-12 10:22:11,123 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-LeaderElection3] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState
dn4_1    | 2023-06-12 10:21:18,606 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-67DF68C3D0F7,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:21:18,606 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-LeaderStateImpl
dn4_1    | 2023-06-12 10:21:18,607 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-PendingRequests: sendNotLeaderResponses
scm3_1   | 2023-06-12 10:19:51,790 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm2_1   | 2023-06-12 10:19:22,166 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2_1   | 2023-06-12 10:19:22,272 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2_1   | 2023-06-12 10:19:22,284 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-06-12 10:22:11,490 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: receive requestVote(PRE_VOTE, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-1B5181454F64, 0, (t:0, i:0))
dn5_1    | 2023-06-12 10:22:11,492 [grpc-default-executor-0] INFO impl.VoteContext: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FOLLOWER: accept PRE_VOTE from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 0 <= candidate's priority 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:07,713 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm1_1   | 2023-06-12 10:19:14,427 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61
scm1_1   | 2023-06-12 10:19:14,436 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 72
scm1_1   | 2023-06-12 10:19:14,436 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 60
scm2_1   | 2023-06-12 10:19:22,284 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
s3g_1    | 	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
dn3_1    | 2023-06-12 10:19:31,641 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d6f449c5-6fac-4083-a27e-b83edf3e8b1c is started using port 9858 for RATIS
scm1_1   | 2023-06-12 10:19:14,485 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: start as a follower, conf=61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:14,486 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: changes role from      null to FOLLOWER at term 5 for startAsFollower
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 2023-06-12 10:19:22,686 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1    | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn5_1    | 2023-06-12 10:22:11,496 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64 replies to PRE_VOTE vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3#0:OK-t0. Peer's state: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64:t0, leader=null, voted=, raftlog=Memoized:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:11,576 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: receive requestVote(ELECTION, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-1B5181454F64, 1, (t:0, i:0))
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-06-12 10:19:22,687 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-06-12 10:19:31,644 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d6f449c5-6fac-4083-a27e-b83edf3e8b1c is started using port 9857 for RATIS_ADMIN
scm3_1   | 2023-06-12 10:19:51,790 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-06-12 10:19:53,968 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread2] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1   | 2023-06-12 10:19:53,985 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread2] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: change Leader from null to 9e54e886-87c9-472f-9b2f-e9a516e53bd2 at term 6 for appendEntries, leader elected after 18353ms
scm3_1   | 2023-06-12 10:19:54,012 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread2] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: Failed appendEntries as the first entry (index 0) already exists (snapshotIndex: 72, commitIndex: 72)
scm3_1   | 2023-06-12 10:19:54,324 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread2] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: inconsistency entries. Reply:9e54e886-87c9-472f-9b2f-e9a516e53bd2<-c790b820-4c4f-44e6-aba9-08b18fc50228#85:FAIL-t5,INCONSISTENCY,nextIndex=73,followerCommit=72,matchIndex=-1
scm2_1   | 2023-06-12 10:19:22,694 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn3_1    | 2023-06-12 10:19:31,644 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d6f449c5-6fac-4083-a27e-b83edf3e8b1c is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-06-12 10:19:31,645 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Started
scm3_1   | 2023-06-12 10:19:54,562 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread4] INFO server.RaftServer$Division: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244: set configuration 73: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-06-12 10:19:22,780 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3_1   | 2023-06-12 10:19:54,590 [c790b820-4c4f-44e6-aba9-08b18fc50228-server-thread4] INFO segmented.SegmentedRaftLogWorker: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolling segment log-61_72 to index:72
scm3_1   | 2023-06-12 10:19:54,611 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61 to /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_61-72
scm3_1   | 2023-06-12 10:19:54,814 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_73
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-06-12 10:19:22,781 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
dn4_1    | 2023-06-12 10:21:18,627 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-06-12 10:21:18,628 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-67DF68C3D0F7: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/sm/snapshot.4_6
dn4_1    | 2023-06-12 10:21:18,635 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-67DF68C3D0F7: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7/sm/snapshot.4_6 took: 7 ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn5_1    | 2023-06-12 10:22:11,576 [grpc-default-executor-0] INFO impl.VoteContext: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FOLLOWER: accept ELECTION from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 0 <= candidate's priority 1
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm2_1   | 2023-06-12 10:19:22,781 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-12 10:19:22,781 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-06-12 10:19:22,824 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1   | 2023-06-12 10:19:22,824 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-06-12 10:19:14,487 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState
dn5_1    | 2023-06-12 10:22:11,577 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn5_1    | 2023-06-12 10:22:11,577 [grpc-default-executor-0] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState
dn3_1    | 2023-06-12 10:19:31,833 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-12 10:19:31,946 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-06-12 10:19:32,418 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:33,419 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 2023-06-12 10:21:18,635 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 2023-06-12 10:21:18,635 [78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn4_1    | 2023-06-12 10:21:18,636 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: closes. applyIndex: 6
scm1_1   | 2023-06-12 10:19:14,489 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FBBF2AB68244,id=9e54e886-87c9-472f-9b2f-e9a516e53bd2
scm1_1   | 2023-06-12 10:19:14,491 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-12 10:19:34,420 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn5_1    | 2023-06-12 10:22:11,577 [grpc-default-executor-0] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState
dn5_1    | 2023-06-12 10:22:11,577 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState] INFO impl.FollowerState: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-FollowerState was interrupted
dn5_1    | 2023-06-12 10:22:11,580 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64 replies to ELECTION vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3#0:OK-t1. Peer's state: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64:t1, leader=null, voted=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, raftlog=Memoized:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:11,729 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1B5181454F64 with new leaderId: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn5_1    | 2023-06-12 10:22:11,730 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-server-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: change Leader from null to 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 at term 1 for appendEntries, leader elected after 5812ms
dn5_1    | 2023-06-12 10:22:11,767 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-server-thread1] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:11,772 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3-server-thread1] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-12 10:19:35,433 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1    | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1    | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1    | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
scm2_1   | 2023-06-12 10:19:22,854 [main] INFO util.log: Logging initialized @7453ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-06-12 10:19:23,065 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-06-12 10:19:36,359 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5803633728ns, electionTimeout:5001ms
s3g_1    | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
dn4_1    | 2023-06-12 10:21:19,248 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7-SegmentedRaftLogWorker close()
dn4_1    | 2023-06-12 10:21:19,250 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-67DF68C3D0F7: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7
dn4_1    | 2023-06-12 10:21:19,250 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 command on datanode 78f8fca8-1c45-4717-8e79-22872958dcce.
dn4_1    | 2023-06-12 10:21:19,250 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: remove  FOLLOWER 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA:t6, leader=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c49, conf=48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn4_1    | 2023-06-12 10:21:19,251 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: shutdown
dn4_1    | 2023-06-12 10:21:19,251 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:21:19,251 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState
dn4_1    | 2023-06-12 10:21:19,251 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-FollowerState was interrupted
dn4_1    | 2023-06-12 10:21:19,251 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater: set stopIndex = 49
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1    | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
dn3_1    | 2023-06-12 10:19:36,371 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
dn1_1    | 2023-06-12 10:21:17,992 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastRequest: 78f8fca8-1c45-4717-8e79-22872958dcce->3c030830-a72e-4f6f-8f54-144518af6253#2-t8,previous=(t:8, i:21),leaderCommit=21,initializing? true,entries: size=1, first=(t:8, i:22), METADATAENTRY(c:21)
dn1_1    | 2023-06-12 10:21:17,993 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastReply: null
scm3_1   | 2023-06-12 10:19:54,909 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-06-12 10:19:23,080 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-06-12 10:19:23,097 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | 2023-06-12 10:19:23,102 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | 2023-06-12 10:19:23,102 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-12 10:21:19,251 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Taking a snapshot at:(t:6, i:49) file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49
dn1_1    | 2023-06-12 10:21:17,995 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastRequest: null
dn1_1    | 2023-06-12 10:21:18,038 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastReply: serverReply {
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-06-12 10:19:23,103 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-12 10:19:14,491 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-06-12 10:19:14,492 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-12 10:19:14,493 [9e54e886-87c9-472f-9b2f-e9a516e53bd2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:21:19,253 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Finished taking a snapshot at:(t:6, i:49) file:/data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49 took: 2 ms
dn4_1    | 2023-06-12 10:21:19,253 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater: Took a snapshot at index 49
scm3_1   | 2023-06-12 10:19:54,916 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-06-12 10:19:23,163 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2_1   | 2023-06-12 10:19:23,164 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-06-12 10:19:23,166 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
dn1_1    |   requestorId: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn1_1    |   replyId: "3c030830-a72e-4f6f-8f54-144518af6253"
dn1_1    |   raftGroupId {
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-12 10:19:14,495 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:14,496 [main] INFO server.RaftServer: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start RPC server
scm1_1   | 2023-06-12 10:19:14,500 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:14,635 [main] INFO server.GrpcService: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: GrpcService started, listening on 9894
dn1_1    |     id: "\331\372\206\337\255\224G\225\266\275\344\357[\233\214J"
scm3_1   | 2023-06-12 10:19:54,917 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm3_1   | 2023-06-12 10:19:54,917 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-06-12 10:19:14,640 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-9e54e886-87c9-472f-9b2f-e9a516e53bd2: Started
scm1_1   | 2023-06-12 10:19:14,650 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-12 10:19:14,650 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-06-12 10:19:14,653 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
dn1_1    |   }
scm3_1   | 2023-06-12 10:19:54,999 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-12 10:19:54,999 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:14,654 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1_1   | 2023-06-12 10:19:14,654 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1_1   | 2023-06-12 10:19:15,180 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-06-12 10:19:15,286 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    |   callId: 43
scm3_1   | 2023-06-12 10:19:55,188 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
scm1_1   | 2023-06-12 10:19:15,287 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-06-12 10:19:16,192 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-06-12 10:19:16,198 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-12 10:19:16,200 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn4_1    | 2023-06-12 10:21:19,253 [78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-StateMachineUpdater: snapshotIndex: updateIncreasingly 47 -> 49
dn4_1    | 2023-06-12 10:21:19,255 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: closes. applyIndex: 49
dn4_1    | 2023-06-12 10:21:19,628 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA-SegmentedRaftLogWorker close()
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-12 10:19:16,302 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-06-12 10:19:16,357 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-06-12 10:19:16,358 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
dn4_1    | 2023-06-12 10:21:19,636 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn4_1    | 2023-06-12 10:21:19,636 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn4_1    | 2023-06-12 10:21:19,682 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-4675AE3D33CA: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-12 10:22:11,779 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-1B5181454F64-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/current/log_inprogress_0
dn5_1    | 2023-06-12 10:22:13,236 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: receive requestVote(PRE_VOTE, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-F04DA08514BB, 0, (t:0, i:0))
dn5_1    | 2023-06-12 10:22:13,237 [grpc-default-executor-0] INFO impl.VoteContext: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FOLLOWER: reject PRE_VOTE from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 1 > candidate's priority 0
s3g_1    | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
scm3_1   | 2023-06-12 10:19:55,188 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:19:36,373 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn3_1    | 2023-06-12 10:19:36,393 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1459)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
s3g_1    | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
dn3_1    | 2023-06-12 10:19:36,402 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
dn5_1    | 2023-06-12 10:22:13,237 [grpc-default-executor-0] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB replies to PRE_VOTE vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3#0:FAIL-t0. Peer's state: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB:t0, leader=null, voted=, raftlog=Memoized:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:21:19,688 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca command on datanode 78f8fca8-1c45-4717-8e79-22872958dcce.
scm3_1   | 2023-06-12 10:19:55,805 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-06-12 10:19:55,865 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-12 10:19:55,885 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3_1   | 2023-06-12 10:19:56,577 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1681)
dn4_1    | 2023-06-12 10:21:23,836 [grpc-default-executor-8] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed requestVote 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#0
scm3_1   | 2023-06-12 10:19:56,581 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3_1   | 2023-06-12 10:19:56,583 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-12 10:19:56,583 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-06-12 10:19:23,213 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-12 10:22:13,291 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO impl.FollowerState: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5168522655ns, electionTimeout:5152ms
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    |   success: true
dn1_1    | }
dn3_1    | 2023-06-12 10:19:36,442 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-12 10:22:13,291 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState
dn5_1    | 2023-06-12 10:22:13,291 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm2_1   | 2023-06-12 10:19:23,214 [main] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-06-12 10:19:23,224 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn1_1    | term: 8
dn1_1    | nextIndex: 23
dn3_1    | 2023-06-12 10:19:36,443 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5887851509ns, electionTimeout:5105ms
dn5_1    | 2023-06-12 10:22:13,292 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-06-12 10:22:13,292 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-FollowerState] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-06-12 10:19:36,447 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-06-12 10:19:36,448 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-06-12 10:19:36,451 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
s3g_1    | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1    | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm2_1   | 2023-06-12 10:19:23,259 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23b71d24{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-06-12 10:19:23,261 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@100d0218{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | followerCommit: 22
dn1_1    | matchIndex: 18446744073709551615
dn5_1    | 2023-06-12 10:22:13,294 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:13,313 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:13,313 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:13,326 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-12 10:22:13,327 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection:   Response 0: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:OK-t0
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | isHearbeat: true
dn1_1    | 
dn5_1    | 2023-06-12 10:22:13,327 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-06-12 10:22:13,332 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:13,353 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:13,354 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-06-12 10:21:23,829 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6095936064ns, electionTimeout:5008ms
dn1_1    | 2023-06-12 10:21:23,830 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState
dn3_1    | 2023-06-12 10:19:36,452 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2
scm2_1   | 2023-06-12 10:19:23,546 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1da75dde{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-2682973790603916038/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2_1   | 2023-06-12 10:19:23,566 [main] INFO server.AbstractConnector: Started ServerConnector@3b8507df{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
dn5_1    | 2023-06-12 10:22:13,355 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn1_1    | 2023-06-12 10:21:23,830 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
scm1_1   | 2023-06-12 10:19:16,358 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
dn3_1    | 2023-06-12 10:19:36,521 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:13,356 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection:   Response 0: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29#0:OK-t1
dn1_1    | 2023-06-12 10:21:23,830 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-06-12 10:19:16,765 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-06-12 10:19:16,765 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-12 10:22:13,356 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.LeaderElection: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4 ELECTION round 0: result PASSED
scm3_1   | 2023-06-12 10:19:57,734 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:51458: output error
scm1_1   | 2023-06-12 10:19:16,841 [main] INFO util.log: Logging initialized @58023ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-06-12 10:19:36,543 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5988188992ns, electionTimeout:5143ms
scm2_1   | 2023-06-12 10:19:23,566 [main] INFO server.Server: Started @8166ms
scm2_1   | 2023-06-12 10:19:23,576 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
dn5_1    | 2023-06-12 10:22:13,357 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: shutdown d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4
dn1_1    | 2023-06-12 10:21:23,830 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4
scm3_1   | 2023-06-12 10:19:57,755 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#23 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:57108: output error
scm1_1   | 2023-06-12 10:19:17,027 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-06-12 10:19:17,047 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 2023-06-12 10:19:36,544 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1    | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:600)
dn5_1    | 2023-06-12 10:22:13,357 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-06-12 10:21:23,830 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: change Leader from 78f8fca8-1c45-4717-8e79-22872958dcce to null at term 8 for PRE_VOTE
scm3_1   | 2023-06-12 10:19:57,745 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#24 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:57436: output error
scm2_1   | 2023-06-12 10:19:23,577 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-06-12 10:19:23,581 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-06-12 10:19:17,062 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-06-12 10:19:36,546 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-06-12 10:22:13,357 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F04DA08514BB with new leaderId: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn1_1    | 2023-06-12 10:21:23,831 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 8 for 21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:21:23,867 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 2023-06-12 10:21:23,867 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
dn1_1    | 2023-06-12 10:21:23,867 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t8
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
dn5_1    | 2023-06-12 10:22:13,357 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: change Leader from null to d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 at term 1 for becomeLeader, leader elected after 5718ms
dn1_1    | 2023-06-12 10:21:23,868 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 2023-06-12 10:21:23,868 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 PRE_VOTE round 0: result PASSED
scm3_1   | 2023-06-12 10:19:57,745 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:40788: output error
scm3_1   | 2023-06-12 10:19:57,743 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#18 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:44674: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
dn5_1    | 2023-06-12 10:22:13,358 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn1_1    | 2023-06-12 10:21:23,872 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 ELECTION round 0: submit vote requests at term 9 for 21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:57,737 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#17 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:44670: output error
scm2_1   | 2023-06-12 10:19:25,465 [grpc-default-executor-0] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: receive requestVote(PRE_VOTE, 9e54e886-87c9-472f-9b2f-e9a516e53bd2, group-FBBF2AB68244, 5, (t:5, i:72))
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn5_1    | 2023-06-12 10:22:13,359 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm3_1   | 2023-06-12 10:19:57,737 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:51466: output error
scm3_1   | 2023-06-12 10:19:57,737 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#21 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:40790: output error
dn3_1    | 2023-06-12 10:19:36,547 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-12 10:19:57,737 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#20 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:57422: output error
scm2_1   | 2023-06-12 10:19:25,470 [grpc-default-executor-0] INFO impl.VoteContext: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FOLLOWER: accept PRE_VOTE from 9e54e886-87c9-472f-9b2f-e9a516e53bd2: our priority 0 <= candidate's priority 0
dn1_1    | 2023-06-12 10:21:23,888 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 2023-06-12 10:21:23,903 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4: ELECTION PASSED received 1 response(s) and 1 exception(s):
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1    | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
scm2_1   | 2023-06-12 10:19:25,523 [grpc-default-executor-0] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244 replies to PRE_VOTE vote request: 9e54e886-87c9-472f-9b2f-e9a516e53bd2<-7740519e-fe5b-4936-af00-27e2eba71ce5#0:OK-t5. Peer's state: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244:t5, leader=null, voted=9e54e886-87c9-472f-9b2f-e9a516e53bd2, raftlog=Memoized:7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLog:OPENED:c72, conf=61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:21:23,904 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Response 0: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t9
scm1_1   | 2023-06-12 10:19:17,065 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
s3g_1    | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm2_1   | 2023-06-12 10:19:25,681 [grpc-default-executor-0] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: receive requestVote(ELECTION, 9e54e886-87c9-472f-9b2f-e9a516e53bd2, group-FBBF2AB68244, 6, (t:5, i:72))
dn1_1    | 2023-06-12 10:21:23,904 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:17,065 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-12 10:19:17,066 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-06-12 10:19:17,230 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3_1   | java.nio.channels.ClosedChannelException
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
scm2_1   | 2023-06-12 10:19:25,685 [grpc-default-executor-0] INFO impl.VoteContext: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FOLLOWER: accept ELECTION from 9e54e886-87c9-472f-9b2f-e9a516e53bd2: our priority 0 <= candidate's priority 0
dn1_1    | 2023-06-12 10:21:23,904 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4 ELECTION round 0: result PASSED
scm1_1   | 2023-06-12 10:19:17,231 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-06-12 10:19:17,233 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-06-12 10:22:13,359 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
s3g_1    | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
scm2_1   | 2023-06-12 10:19:25,686 [grpc-default-executor-0] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:9e54e886-87c9-472f-9b2f-e9a516e53bd2
dn1_1    | 2023-06-12 10:21:23,904 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4
scm1_1   | 2023-06-12 10:19:17,279 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-12 10:21:23,876 [grpc-default-executor-8] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed requestVote 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#0
dn3_1    | 2023-06-12 10:19:36,548 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-06-12 10:22:13,359 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
s3g_1    | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
scm2_1   | 2023-06-12 10:19:25,686 [grpc-default-executor-0] INFO impl.RoleInfo: 7740519e-fe5b-4936-af00-27e2eba71ce5: shutdown 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState
dn1_1    | 2023-06-12 10:21:23,905 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: changes role from CANDIDATE to LEADER at term 9 for changeToLeader
scm1_1   | 2023-06-12 10:19:17,279 [main] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-06-12 10:19:17,281 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-06-12 10:19:36,586 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:13,378 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
scm2_1   | 2023-06-12 10:19:25,686 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState] INFO impl.FollowerState: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState was interrupted
dn1_1    | 2023-06-12 10:21:23,905 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4EF5B9B8C4A with new leaderId: 3c030830-a72e-4f6f-8f54-144518af6253
dn1_1    | 2023-06-12 10:21:23,905 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: change Leader from null to 3c030830-a72e-4f6f-8f54-144518af6253 at term 9 for becomeLeader, leader elected after 74ms
dn4_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 2023-06-12 10:19:36,597 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2 PRE_VOTE round 0: result PASSED (term=3)
dn5_1    | 2023-06-12 10:22:13,382 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
s3g_1    | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm2_1   | 2023-06-12 10:19:25,687 [grpc-default-executor-0] INFO impl.RoleInfo: 7740519e-fe5b-4936-af00-27e2eba71ce5: start 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-FollowerState
scm2_1   | 2023-06-12 10:19:25,760 [grpc-default-executor-0] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244 replies to ELECTION vote request: 9e54e886-87c9-472f-9b2f-e9a516e53bd2<-7740519e-fe5b-4936-af00-27e2eba71ce5#0:OK-t6. Peer's state: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244:t6, leader=null, voted=9e54e886-87c9-472f-9b2f-e9a516e53bd2, raftlog=Memoized:7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLog:OPENED:c72, conf=61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:21:23,906 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-06-12 10:19:36,628 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 5 for 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-12 10:22:13,387 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
s3g_1    | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
scm2_1   | 2023-06-12 10:19:26,164 [7740519e-fe5b-4936-af00-27e2eba71ce5-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm1_1   | 2023-06-12 10:19:17,308 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@729cd862{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-06-12 10:21:23,906 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 2023-06-12 10:19:36,840 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:13,410 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
dn1_1    | 2023-06-12 10:21:23,907 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-12 10:21:23,913 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-12 10:19:36,843 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:19:36,846 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 78f8fca8-1c45-4717-8e79-22872958dcce
scm2_1   | 2023-06-12 10:19:26,166 [7740519e-fe5b-4936-af00-27e2eba71ce5-server-thread1] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: change Leader from null to 9e54e886-87c9-472f-9b2f-e9a516e53bd2 at term 6 for appendEntries, leader elected after 5958ms
scm2_1   | 2023-06-12 10:19:26,247 [7740519e-fe5b-4936-af00-27e2eba71ce5-server-thread2] INFO server.RaftServer$Division: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244: set configuration 73: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-12 10:19:26,250 [7740519e-fe5b-4936-af00-27e2eba71ce5-server-thread2] INFO segmented.SegmentedRaftLogWorker: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolling segment log-61_72 to index:72
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 2023-06-12 10:19:17,309 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f8667bb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-06-12 10:19:17,702 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1b1e1f02{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-10674790714425500512/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 2023-06-12 10:19:17,718 [main] INFO server.AbstractConnector: Started ServerConnector@656ec00d{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-06-12 10:19:17,718 [main] INFO server.Server: Started @58902ms
scm1_1   | 2023-06-12 10:19:17,726 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-06-12 10:19:17,726 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:19:36,858 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 3c030830-a72e-4f6f-8f54-144518af6253
dn1_1    | 2023-06-12 10:21:23,913 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-12 10:19:17,729 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 2023-06-12 10:22:13,440 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-06-12 10:22:13,440 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:19:36,886 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2 ELECTION round 0: submit vote requests at term 4 for 3: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:36,888 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2 ELECTION round 0: result PASSED (term=4)
scm1_1   | 2023-06-12 10:19:19,598 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.FollowerState: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5110592700ns, electionTimeout:5096ms
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-06-12 10:22:13,441 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-06-12 10:22:13,443 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm2_1   | 2023-06-12 10:19:26,254 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61 to /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_61-72
scm2_1   | 2023-06-12 10:19:26,292 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_73
dn3_1    | 2023-06-12 10:19:36,888 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2
dn1_1    | 2023-06-12 10:21:23,914 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-06-12 10:21:23,914 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:21:23,914 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-06-12 10:19:36,889 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn3_1    | 2023-06-12 10:19:36,892 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F138DFAD79A6 with new leaderId: d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | 2023-06-12 10:21:24,078 [grpc-default-executor-8] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#1-t9,previous=(t:8, i:22),leaderCommit=22,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-06-12 10:19:19,599 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: shutdown 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState
scm1_1   | 2023-06-12 10:19:19,601 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
scm1_1   | 2023-06-12 10:19:19,604 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-06-12 10:19:19,604 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1
dn3_1    | 2023-06-12 10:19:36,898 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: change Leader from null to d6f449c5-6fac-4083-a27e-b83edf3e8b1c at term 4 for becomeLeader, leader elected after 34129ms
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
dn5_1    | 2023-06-12 10:22:13,444 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-06-12 10:21:23,945 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-06-12 10:21:23,945 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:21:23,945 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-06-12 10:19:26,348 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-06-12 10:19:26,350 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-06-12 10:19:26,351 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn3_1    | 2023-06-12 10:19:36,908 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:13,445 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-06-12 10:19:19,610 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 5 for 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:36,909 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:19:37,037 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-12 10:19:37,171 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-12 10:22:13,445 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-06-12 10:19:37,174 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-12 10:19:37,422 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Detected pause in JVM or host machine approximately 0.128s without any GCs.
scm1_1   | 2023-06-12 10:19:19,652 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for c790b820-4c4f-44e6-aba9-08b18fc50228
dn5_1    | 2023-06-12 10:22:13,445 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-06-12 10:22:13,447 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-12 10:19:26,351 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 2023-06-12 10:19:26,380 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn3_1    | 2023-06-12 10:19:37,433 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
s3g_1    | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
dn1_1    | 2023-06-12 10:21:23,949 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | address: "10.9.0.17:9856"
dn4_1    | clientAddress: "10.9.0.17:9858"
dn4_1    | adminAddress: "10.9.0.17:9857"
dn4_1    | startupRole: FOLLOWER
dn5_1    | 2023-06-12 10:22:13,447 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-06-12 10:19:26,404 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-12 10:19:26,730 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:56608: output error
dn3_1    | 2023-06-12 10:19:37,441 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
s3g_1    | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
dn1_1    | 2023-06-12 10:21:23,957 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-12 10:19:19,661 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-12 10:22:13,452 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm2_1   | 2023-06-12 10:19:26,733 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:19:37,443 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
s3g_1    | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
dn1_1    | 2023-06-12 10:21:23,958 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:21:23,958 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn1_1    | 2023-06-12 10:21:23,958 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1_1   | 2023-06-12 10:19:19,664 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-12 10:22:13,452 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-12 10:19:57,737 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#19 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:57106: output error
dn3_1    | 2023-06-12 10:19:37,447 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
dn1_1    | 2023-06-12 10:21:23,958 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn5_1    | 2023-06-12 10:22:13,452 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
dn3_1    | 2023-06-12 10:19:37,596 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
s3g_1    | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
dn1_1    | 2023-06-12 10:21:23,958 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-12 10:21:23,969 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | address: "10.9.0.20:9856"
dn5_1    | 2023-06-12 10:22:13,453 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:37,653 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
dn1_1    | 2023-06-12 10:21:23,969 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-12 10:19:19,662 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 7740519e-fe5b-4936-af00-27e2eba71ce5
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | priority: 1
dn5_1    | 2023-06-12 10:22:13,453 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 2023-06-12 10:21:23,970 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm1_1   | 2023-06-12 10:19:19,993 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | clientAddress: "10.9.0.20:9858"
dn5_1    | 2023-06-12 10:22:13,453 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 2023-06-12 10:21:23,970 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-12 10:19:20,002 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | adminAddress: "10.9.0.20:9857"
dn5_1    | 2023-06-12 10:22:13,453 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-06-12 10:19:37,693 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderStateImpl
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
s3g_1    | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 2023-06-12 10:21:23,970 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-06-12 10:19:20,002 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm1_1   | 2023-06-12 10:19:20,003 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn5_1    | 2023-06-12 10:22:13,454 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-06-12 10:19:37,864 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
s3g_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | 2023-06-12 10:22:50,047 [qtp193388045-22] ERROR protocolPB.GrpcOmTransport: error unwrapping exception from OMResponse {}
dn1_1    | 2023-06-12 10:21:23,970 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-06-12 10:19:20,003 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | startupRole: FOLLOWER
dn5_1    | 2023-06-12 10:22:13,454 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-12 10:19:37,970 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_inprogress_3 to /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_3-4
dn3_1    | 2023-06-12 10:19:38,024 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderElection2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: set configuration 5: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:38,058 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/current/log_inprogress_5
s3g_1    | 2023-06-12 10:22:50,780 [qtp193388045-22] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 2023-06-12 10:21:23,971 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm1_1   | 2023-06-12 10:19:20,003 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1 PRE_VOTE round 0: result REJECTED
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn5_1    | 2023-06-12 10:22:13,454 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-12 10:22:13,455 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO impl.RoleInfo: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: start d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderStateImpl
dn5_1    | 2023-06-12 10:22:13,455 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-12 10:22:13,457 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/current/log_inprogress_0
s3g_1    | 2023-06-12 10:23:54,808 [qtp193388045-21] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with bucket layout OBJECT_STORE, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
s3g_1    | 2023-06-12 10:23:55,949 [qtp193388045-21] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn5_1    | 2023-06-12 10:22:13,457 [d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB-LeaderElection4] INFO server.RaftServer$Division: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3@group-F04DA08514BB: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:21:23,971 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn1_1    | 2023-06-12 10:21:23,971 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-12 10:19:20,003 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
scm1_1   | 2023-06-12 10:19:20,004 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: shutdown 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-06-12 10:22:30,229 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn1_1    | 2023-06-12 10:21:23,971 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-12 10:19:20,004 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection1] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState
scm1_1   | 2023-06-12 10:19:25,020 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.FollowerState: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5016178117ns, electionTimeout:5014ms
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-06-12 10:23:30,230 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-06-12 10:24:30,231 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-06-12 10:21:23,973 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderStateImpl
dn4_1    | address: "10.9.0.19:9856"
scm1_1   | 2023-06-12 10:19:25,021 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: shutdown 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | 2023-06-12 10:19:38,465 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:39,466 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 2023-06-12 10:21:23,974 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolling segment log-21_22 to index:22
dn4_1    | clientAddress: "10.9.0.19:9858"
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | 2023-06-12 10:19:40,475 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:40,672 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 6, (t:6, i:20))
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 2023-06-12 10:21:23,975 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_21 to /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_21-22
scm1_1   | 2023-06-12 10:19:25,021 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn3_1    | 2023-06-12 10:19:40,682 [grpc-default-executor-1] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-CANDIDATE: accept PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-06-12 10:21:23,979 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_23
scm1_1   | 2023-06-12 10:19:25,021 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | startupRole: FOLLOWER
dn4_1    | , old:)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-06-12 10:21:23,984 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderElection4] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: set configuration 23: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:25,021 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-FollowerState] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 2023-06-12 10:19:40,708 [grpc-default-executor-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-4675AE3D33CA, 5, (t:5, i:47))
scm2_1   | 2023-06-12 10:19:26,791 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:53174: output error
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn1_1    | 2023-06-12 10:21:24,108 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
scm1_1   | 2023-06-12 10:19:25,022 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 5 for 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn3_1    | 2023-06-12 10:19:40,710 [grpc-default-executor-0] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-CANDIDATE: reject PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 1 > candidate's priority 0
scm2_1   | 2023-06-12 10:19:26,792 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:25,024 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn3_1    | 2023-06-12 10:19:40,734 [grpc-default-executor-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:FAIL-t5. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA:t5, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-06-12 10:19:25,622 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:19:40,775 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t6. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t6, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm1_1   | 2023-06-12 10:19:25,623 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection:   Response 0: 9e54e886-87c9-472f-9b2f-e9a516e53bd2<-7740519e-fe5b-4936-af00-27e2eba71ce5#0:OK-t5
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:40,858 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-06-12 10:19:25,623 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-06-12 10:19:40,867 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection:   Response 0: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t5
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-06-12 10:19:40,868 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3 PRE_VOTE round 0: result PASSED
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm1_1   | 2023-06-12 10:19:25,627 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 PRE_VOTE round 0: result PASSED
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-06-12 10:19:40,876 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3 ELECTION round 0: submit vote requests at term 6 for 34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 2023-06-12 10:19:25,663 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 ELECTION round 0: submit vote requests at term 6 for 61: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-06-12 10:19:40,911 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:19:40,912 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:41,104 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-4675AE3D33CA, 5, (t:5, i:47))
dn3_1    | 2023-06-12 10:19:41,112 [grpc-default-executor-1] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-CANDIDATE: reject PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 1 > candidate's priority 0
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn3_1    | 2023-06-12 10:19:41,118 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:FAIL-t6. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA:t6, leader=null, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=34: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:41,134 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
scm1_1   | 2023-06-12 10:19:25,665 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-06-12 10:19:41,135 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection:   Response 0: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-06-12 10:19:25,768 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2: ELECTION PASSED received 1 response(s) and 1 exception(s):
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn3_1    | 2023-06-12 10:19:41,135 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3 ELECTION round 0: result PASSED
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-06-12 10:19:41,137 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3
dn3_1    | 2023-06-12 10:19:41,137 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn3_1    | 2023-06-12 10:19:41,139 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4675AE3D33CA with new leaderId: d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 2023-06-12 10:21:24,111 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:25,357 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-06-12 10:19:25,769 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection:   Response 0: 9e54e886-87c9-472f-9b2f-e9a516e53bd2<-7740519e-fe5b-4936-af00-27e2eba71ce5#0:OK-t6
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-06-12 10:19:41,154 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: change Leader from null to d6f449c5-6fac-4083-a27e-b83edf3e8b1c at term 6 for becomeLeader, leader elected after 39650ms
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 2023-06-12 10:19:25,769 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-06-12 10:19:41,154 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-06-12 10:19:25,769 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.LeaderElection: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2 ELECTION round 0: result PASSED
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 2023-06-12 10:19:41,154 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-06-12 10:19:25,770 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: shutdown 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 2023-06-12 10:19:25,771 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: changes role from CANDIDATE to LEADER at term 6 for changeToLeader
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-06-12 10:19:25,771 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO ha.SCMStateMachine: current SCM becomes leader of term 6.
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-06-12 10:21:25,342 [grpc-default-executor-8] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#4-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
scm2_1   | 2023-06-12 10:19:32,093 [IPC Server handler 8 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm2_1   | 2023-06-12 10:19:32,113 [IPC Server handler 8 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-12 10:19:25,771 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,6>
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | address: "10.9.0.17:9856"
scm2_1   | 2023-06-12 10:19:32,166 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-06-12 10:19:25,776 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: change Leader from null to 9e54e886-87c9-472f-9b2f-e9a516e53bd2 at term 6 for becomeLeader, leader elected after 15254ms
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | clientAddress: "10.9.0.17:9858"
dn4_1    | adminAddress: "10.9.0.17:9857"
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:25,793 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn3_1    | 2023-06-12 10:19:41,155 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm2_1   | 2023-06-12 10:19:32,240 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
scm1_1   | 2023-06-12 10:19:25,814 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm2_1   | 2023-06-12 10:19:32,254 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | address: "10.9.0.20:9856"
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-06-12 10:19:41,265 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(ELECTION, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 7, (t:6, i:20))
dn4_1    | priority: 1
scm1_1   | 2023-06-12 10:19:25,828 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-06-12 10:19:32,243 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
dn4_1    | clientAddress: "10.9.0.20:9858"
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:41,300 [grpc-default-executor-1] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-CANDIDATE: accept ELECTION from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
dn3_1    | 2023-06-12 10:19:41,301 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: changes role from CANDIDATE to FOLLOWER at term 7 for candidate:3c030830-a72e-4f6f-8f54-144518af6253
scm2_1   | 2023-06-12 10:19:32,254 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:19:41,301 [grpc-default-executor-1] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1
dn3_1    | 2023-06-12 10:19:41,301 [grpc-default-executor-1] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
scm1_1   | 2023-06-12 10:19:25,844 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-06-12 10:19:41,341 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to ELECTION vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t7. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t7, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | address: "10.9.0.19:9856"
dn4_1    | clientAddress: "10.9.0.19:9858"
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:19:41,461 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-12 10:19:41,463 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-12 10:19:25,846 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | adminAddress: "10.9.0.19:9857"
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn3_1    | 2023-06-12 10:19:41,466 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-06-12 10:19:41,496 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | startupRole: FOLLOWER
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-06-12 10:19:41,503 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-06-12 10:19:41,503 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | , old:)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-06-12 10:19:41,562 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-06-12 10:19:25,848 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-06-12 10:19:25,869 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 2023-06-12 10:21:25,358 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
scm1_1   | 2023-06-12 10:19:25,880 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-06-12 10:21:26,586 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 2023-06-12 10:19:41,564 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm2_1   | 2023-06-12 10:19:32,269 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-12 10:19:32,271 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-06-12 10:19:32,243 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-06-12 10:19:32,276 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn3_1    | 2023-06-12 10:19:41,566 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-06-12 10:19:41,571 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm1_1   | 2023-06-12 10:19:25,924 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-12 10:19:25,926 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-06-12 10:19:32,278 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm1_1   | 2023-06-12 10:19:25,927 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:19:41,572 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-12 10:19:41,616 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
dn3_1    | 2023-06-12 10:19:41,616 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-06-12 10:19:32,279 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-12 10:19:33,538 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c030830-a72e-4f6f-8f54-144518af6253
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn3_1    | 2023-06-12 10:19:41,619 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-06-12 10:19:33,542 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c030830-a72e-4f6f-8f54-144518af6253{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-12 10:19:33,545 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn3_1    | 2023-06-12 10:19:41,619 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-06-12 10:19:33,565 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-06-12 10:19:33,573 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn3_1    | 2023-06-12 10:19:41,624 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-06-12 10:19:41,625 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-12 10:19:41,626 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
dn3_1    | 2023-06-12 10:19:41,626 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-06-12 10:19:41,627 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-12 10:19:33,573 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 2023-06-12 10:19:25,933 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-12 10:19:25,936 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:19:41,627 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-06-12 10:19:33,573 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm1_1   | 2023-06-12 10:19:25,937 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-06-12 10:19:41,655 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderStateImpl
scm2_1   | 2023-06-12 10:19:33,586 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
scm1_1   | 2023-06-12 10:19:25,939 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-06-12 10:19:33,591 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-06-12 10:19:33,597 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2_1   | 2023-06-12 10:19:33,605 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-06-12 10:19:33,900 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:19:41,696 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolling segment log-34_47 to index:47
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-06-12 10:19:33,900 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d6f449c5-6fac-4083-a27e-b83edf3e8b1c{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-06-12 10:19:41,703 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-06-12 10:19:33,901 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-06-12 10:19:41,703 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 0: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-3c030830-a72e-4f6f-8f54-144518af6253#0:OK-t6
dn3_1    | 2023-06-12 10:19:41,703 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection:   Response 1: d6f449c5-6fac-4083-a27e-b83edf3e8b1c<-78f8fca8-1c45-4717-8e79-22872958dcce#0:FAIL-t6
dn1_1    | 2023-06-12 10:21:26,586 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:27,854 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-06-12 10:19:33,967 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 	... 13 more
dn4_1    | 2023-06-12 10:21:26,574 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#5-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
scm1_1   | 2023-06-12 10:19:25,940 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-06-12 10:19:33,969 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-12 10:19:33,986 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-12 10:19:34,081 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:25,942 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-12 10:19:25,942 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | 2023-06-12 10:19:41,703 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-LeaderElection1 PRE_VOTE round 0: result REJECTED
dn3_1    | 2023-06-12 10:19:41,764 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_34 to /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_34-47
scm1_1   | 2023-06-12 10:19:25,951 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-12 10:19:25,951 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 2023-06-12 10:19:25,954 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-06-12 10:19:25,968 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm2_1   | 2023-06-12 10:19:35,507 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn3_1    | 2023-06-12 10:19:41,767 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/current/log_inprogress_48
scm2_1   | 2023-06-12 10:19:35,716 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-06-12 10:19:35,911 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn3_1    | 2023-06-12 10:19:41,842 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderElection3] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: set configuration 48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | address: "10.9.0.17:9856"
dn4_1    | clientAddress: "10.9.0.17:9858"
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-06-12 10:19:35,931 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/78f8fca8-1c45-4717-8e79-22872958dcce
dn3_1    | 2023-06-12 10:19:42,031 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 6, (t:6, i:20))
dn3_1    | 2023-06-12 10:19:42,036 [grpc-default-executor-1] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FOLLOWER: accept PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
dn3_1    | 2023-06-12 10:19:42,037 [grpc-default-executor-1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t7. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t7, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:42,093 [grpc-default-executor-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-4675AE3D33CA, 5, (t:5, i:47))
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-06-12 10:19:35,931 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 78f8fca8-1c45-4717-8e79-22872958dcce{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-06-12 10:19:42,103 [grpc-default-executor-0] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LEADER: reject PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: this server is the leader and still has leadership
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
scm1_1   | 2023-06-12 10:19:25,969 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:42,104 [grpc-default-executor-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:FAIL-t6. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA:t6, leader=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c47, conf=48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:42,567 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | adminAddress: "10.9.0.17:9857"
dn4_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-12 10:19:25,977 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO impl.RoleInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2: start 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderStateImpl
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-06-12 10:19:35,932 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm1_1   | 2023-06-12 10:19:26,005 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolling segment log-61_72 to index:72
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-06-12 10:19:35,933 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | address: "10.9.0.20:9856"
dn4_1    | priority: 1
dn4_1    | clientAddress: "10.9.0.20:9858"
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn4_1    | address: "10.9.0.19:9856"
dn4_1    | clientAddress: "10.9.0.19:9858"
dn4_1    | adminAddress: "10.9.0.19:9857"
dn4_1    | startupRole: FOLLOWER
dn4_1    | , old:)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-06-12 10:19:35,933 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-06-12 10:19:35,933 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 2023-06-12 10:19:35,933 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm2_1   | 2023-06-12 10:19:35,933 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 2023-06-12 10:20:23,971 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm2_1   | 2023-06-12 10:20:23,971 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:27,855 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
scm1_1   | 2023-06-12 10:19:26,016 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_61 to /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_61-72
scm1_1   | 2023-06-12 10:19:26,054 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/log_inprogress_73
scm1_1   | 2023-06-12 10:19:26,060 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-LeaderElection2] INFO server.RaftServer$Division: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244: set configuration 73: peers:[c790b820-4c4f-44e6-aba9-08b18fc50228|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 7740519e-fe5b-4936-af00-27e2eba71ce5|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:26,149 [grpc-default-executor-0] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:26,178 [grpc-default-executor-0] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 73 -> 0
dn1_1    | 2023-06-12 10:21:29,102 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-06-12 10:19:43,577 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:44,578 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-06-12 10:19:45,579 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-06-12 10:19:46,580 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-12 10:19:46,828 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 7, (t:6, i:20))
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-06-12 10:20:24,373 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm2_1   | 2023-06-12 10:20:24,385 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-06-12 10:20:24,393 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm2_1   | 2023-06-12 10:20:24,395 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-06-12 10:19:46,828 [grpc-default-executor-2] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FOLLOWER: accept PRE_VOTE from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
dn3_1    | 2023-06-12 10:19:46,828 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t7. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t7, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:26,269 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:26,293 [grpc-default-executor-6] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:26,298 [grpc-default-executor-4] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:26,309 [grpc-default-executor-6] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 74 -> 0
scm1_1   | 2023-06-12 10:19:26,326 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1_1   | 2023-06-12 10:19:26,397 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
dn3_1    | 2023-06-12 10:19:46,935 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(ELECTION, 78f8fca8-1c45-4717-8e79-22872958dcce, group-E4EF5B9B8C4A, 8, (t:6, i:20))
dn3_1    | 2023-06-12 10:19:46,936 [grpc-default-executor-2] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FOLLOWER: accept ELECTION from 78f8fca8-1c45-4717-8e79-22872958dcce: our priority 0 <= candidate's priority 1
scm1_1   | 2023-06-12 10:19:26,404 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-06-12 10:20:24,400 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 2023-06-12 10:19:35,438 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:41308: output error
recon_1  | 2023-06-12 10:19:35,462 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
scm2_1   | 2023-06-12 10:20:24,407 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-06-12 10:19:46,936 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:78f8fca8-1c45-4717-8e79-22872958dcce
recon_1  | java.nio.channels.ClosedChannelException
dn4_1    | 2023-06-12 10:21:27,844 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#8-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn4_1    | address: "10.9.0.17:9856"
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:19:46,936 [grpc-default-executor-2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-06-12 10:19:26,407 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-06-12 10:20:24,407 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm2_1   | 2023-06-12 10:20:24,410 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-06-12 10:20:24,415 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 2023-06-12 10:19:26,407 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-06-12 10:19:26,407 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-06-12 10:19:26,411 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:26,411 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.17:9858"
dn4_1    | adminAddress: "10.9.0.17:9857"
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-06-12 10:19:46,936 [grpc-default-executor-2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
scm1_1   | 2023-06-12 10:19:26,417 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:26,428 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db in state CLOSED which uses HEALTHY_READONLY datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn4_1    | startupRole: FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-06-12 10:19:46,937 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState was interrupted
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn4_1    | address: "10.9.0.20:9856"
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:19:46,942 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to ELECTION vote request: 78f8fca8-1c45-4717-8e79-22872958dcce<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t8. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t8, leader=null, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c20, conf=7: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:47,314 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4EF5B9B8C4A with new leaderId: 78f8fca8-1c45-4717-8e79-22872958dcce
dn3_1    | 2023-06-12 10:19:47,314 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: change Leader from null to 78f8fca8-1c45-4717-8e79-22872958dcce at term 8 for appendEntries, leader elected after 44669ms
dn3_1    | 2023-06-12 10:19:47,447 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:19:47,454 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread2] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolling segment log-7_20 to index:20
dn3_1    | 2023-06-12 10:19:47,458 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_7 to /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_7-20
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:19:47,470 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_21
dn3_1    | 2023-06-12 10:19:57,759 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-12 10:20:31,946 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-06-12 10:21:12,157 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:21:12,159 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-06-12 10:21:12,160 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-06-12 10:21:12,160 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn3_1    | 2023-06-12 10:21:12,160 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn3_1    | 2023-06-12 10:21:12,161 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c in state CLOSED which uses HEALTHY_READONLY datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3. This will send close commands for its containers.
scm1_1   | 2023-06-12 10:19:26,441 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-06-12 10:19:26,527 [IPC Server handler 12 on default port 9861] INFO ipc.Server: IPC Server handler 12 on default port 9861: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:59298
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm1_1   | 2023-06-12 10:19:26,539 [IPC Server handler 12 on default port 9861] INFO ipc.Server: IPC Server handler 12 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:48690
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | priority: 1
dn4_1    | clientAddress: "10.9.0.20:9858"
scm1_1   | 2023-06-12 10:19:26,545 [IPC Server handler 13 on default port 9861] INFO ipc.Server: IPC Server handler 13 on default port 9861: skipped Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:43846
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-06-12 10:19:27,315 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:34692: output error
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-06-12 10:19:27,338 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | java.nio.channels.ClosedChannelException
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
recon_1  | 2023-06-12 10:19:35,491 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#11 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:36414: output error
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 2023-06-12 10:19:35,497 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:24,416 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 2023-06-12 10:21:29,103 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:30,363 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | address: "10.9.0.19:9856"
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn4_1    | clientAddress: "10.9.0.19:9858"
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | 2023-06-12 10:21:12,161 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn4_1    | startupRole: FOLLOWER
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | , old:)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm2_1   | 2023-06-12 10:20:24,417 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm2_1   | 2023-06-12 10:20:34,811 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-06-12 10:20:35,050 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-06-12 10:20:36,740 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:20:41,194 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm2_1   | 2023-06-12 10:20:46,994 [IPC Server handler 8 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:20:54,967 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-06-12 10:21:04,841 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:21:05,040 [IPC Server handler 56 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
dn3_1    | 2023-06-12 10:21:18,003 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastRequest: 78f8fca8-1c45-4717-8e79-22872958dcce->d6f449c5-6fac-4083-a27e-b83edf3e8b1c#2-t8,previous=(t:8, i:21),leaderCommit=22,initializing? true,entries: size=1, first=(t:8, i:22), METADATAENTRY(c:21)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-06-12 10:21:06,728 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn3_1    | 2023-06-12 10:21:18,004 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastReply: null
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 2023-06-12 10:19:35,366 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:21:18,018 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastRequest: null
dn3_1    | 2023-06-12 10:21:18,057 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastReply: serverReply {
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn3_1    |   requestorId: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn3_1    |   replyId: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn3_1    |   raftGroupId {
dn3_1    |     id: "\331\372\206\337\255\224G\225\266\275\344\357[\233\214J"
scm2_1   | 2023-06-12 10:21:11,195 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm2_1   | 2023-06-12 10:21:11,546 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:20.514734Z[UTC]] removed.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm2_1   | 2023-06-12 10:21:11,576 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:20.513446Z[UTC]] removed.
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm2_1   | 2023-06-12 10:21:11,618 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:20.515533Z[UTC]] removed.
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-06-12 10:19:27,330 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:59838: output error
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-06-12 10:21:11,633 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:20.515794Z[UTC]] removed.
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    |   }
scm2_1   | 2023-06-12 10:21:11,644 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:20.516060Z[UTC]] removed.
scm3_1   | 2023-06-12 10:19:57,772 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
scm1_1   | 2023-06-12 10:19:27,356 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-06-12 10:21:11,662 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:20.515927Z[UTC]] removed.
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-06-12 10:21:11,683 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:20.512549Z[UTC]] removed.
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    |   callId: 38
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-06-12 10:21:17,001 [IPC Server handler 11 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    |   success: true
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:30,364 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-06-12 10:21:18,538 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | }
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn1_1    | 2023-06-12 10:21:31,611 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-06-12 10:21:18,577 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | term: 8
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-06-12 10:21:18,591 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm2_1   | 2023-06-12 10:21:18,671 [FixedThreadPoolWithAffinityExecutor-0-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-06-12 10:21:19,650 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:21:19,683 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn3_1    | nextIndex: 23
dn3_1    | followerCommit: 22
dn3_1    | matchIndex: 18446744073709551615
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn3_1    | isHearbeat: true
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm2_1   | 2023-06-12 10:21:23,947 [IPC Server handler 9 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:21:35,556 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-06-12 10:19:27,330 [IPC Server handler 9 on default port 9861] WARN ipc.Server: IPC Server handler 9 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:59854: output error
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-06-12 10:21:35,556 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-06-12 10:19:35,472 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:44122: output error
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn3_1    | 
dn3_1    | 2023-06-12 10:21:23,837 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(PRE_VOTE, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 8, (t:8, i:22))
scm1_1   | 2023-06-12 10:19:27,330 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:48686: output error
scm1_1   | 2023-06-12 10:19:27,361 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-06-12 10:21:35,556 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
scm2_1   | 2023-06-12 10:21:35,556 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn3_1    | 2023-06-12 10:21:23,837 [grpc-default-executor-2] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FOLLOWER: accept PRE_VOTE from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-06-12 10:21:35,592 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]].
recon_1  | 2023-06-12 10:19:35,531 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 2023-06-12 10:21:23,838 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to PRE_VOTE vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t8. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t8, leader=78f8fca8-1c45-4717-8e79-22872958dcce, voted=78f8fca8-1c45-4717-8e79-22872958dcce, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c22, conf=21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-06-12 10:21:23,891 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: receive requestVote(ELECTION, 3c030830-a72e-4f6f-8f54-144518af6253, group-E4EF5B9B8C4A, 9, (t:8, i:22))
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 2023-06-12 10:21:23,891 [grpc-default-executor-2] INFO impl.VoteContext: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FOLLOWER: accept ELECTION from 3c030830-a72e-4f6f-8f54-144518af6253: our priority 0 <= candidate's priority 0
scm2_1   | 2023-06-12 10:21:35,631 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]].
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-06-12 10:19:57,821 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-06-12 10:21:23,891 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: change Leader from 78f8fca8-1c45-4717-8e79-22872958dcce to null at term 9 for updateCurrentTerm
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:31,613 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:31,751 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-06-12 10:21:23,891 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: changes role from  FOLLOWER to FOLLOWER at term 9 for candidate:3c030830-a72e-4f6f-8f54-144518af6253
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn1_1    | 2023-06-12 10:21:32,857 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-06-12 10:21:39,124 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 	... 13 more
dn4_1    | 2023-06-12 10:21:29,098 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#10-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm2_1   | 2023-06-12 10:21:39,160 [IPC Server handler 31 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-06-12 10:21:39,632 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | java.nio.channels.ClosedChannelException
dn3_1    | 2023-06-12 10:21:23,891 [grpc-default-executor-2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
dn3_1    | 2023-06-12 10:21:23,891 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState was interrupted
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn4_1    | address: "10.9.0.17:9856"
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm2_1   | 2023-06-12 10:21:39,645 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | clientAddress: "10.9.0.17:9858"
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn3_1    | 2023-06-12 10:21:23,892 [grpc-default-executor-2] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn3_1    | 2023-06-12 10:21:23,896 [grpc-default-executor-2] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A replies to ELECTION vote request: 3c030830-a72e-4f6f-8f54-144518af6253<-d6f449c5-6fac-4083-a27e-b83edf3e8b1c#0:OK-t9. Peer's state: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t9, leader=null, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c22, conf=21: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-12 10:21:24,014 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E4EF5B9B8C4A with new leaderId: 3c030830-a72e-4f6f-8f54-144518af6253
dn3_1    | 2023-06-12 10:21:24,014 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: change Leader from null to 3c030830-a72e-4f6f-8f54-144518af6253 at term 9 for appendEntries, leader elected after 123ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn3_1    | 2023-06-12 10:21:24,015 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: set configuration 23: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
dn4_1    | adminAddress: "10.9.0.17:9857"
scm2_1   | 2023-06-12 10:21:41,200 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:21:24,015 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-server-thread1] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolling segment log-21_22 to index:22
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn4_1    | address: "10.9.0.20:9856"
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-06-12 10:21:43,102 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:21:24,019 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_21 to /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_21-22
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | priority: 1
dn4_1    | clientAddress: "10.9.0.20:9858"
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
dn3_1    | 2023-06-12 10:21:24,021 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/current/log_inprogress_23
dn3_1    | 2023-06-12 10:21:31,947 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-06-12 10:21:38,187 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastRequest: 3c030830-a72e-4f6f-8f54-144518af6253->d6f449c5-6fac-4083-a27e-b83edf3e8b1c#2-t9,previous=(t:9, i:23),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:24), METADATAENTRY(c:23)
dn3_1    | 2023-06-12 10:21:38,187 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastReply: null
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
scm2_1   | 2023-06-12 10:21:43,132 [IPC Server handler 31 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm2_1   | 2023-06-12 10:21:43,731 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn3_1    | 2023-06-12 10:21:38,192 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastRequest: null
scm2_1   | 2023-06-12 10:21:43,758 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | address: "10.9.0.19:9856"
dn4_1    | clientAddress: "10.9.0.19:9858"
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-06-12 10:21:50,557 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm3_1   | 2023-06-12 10:19:57,852 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
dn4_1    | adminAddress: "10.9.0.19:9857"
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | startupRole: FOLLOWER
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-06-12 10:21:38,193 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Completed APPEND_ENTRIES, lastReply: serverReply {
dn3_1    |   requestorId: "3c030830-a72e-4f6f-8f54-144518af6253"
dn4_1    | , old:)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm2_1   | 2023-06-12 10:21:50,557 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-06-12 10:21:32,858 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:34,113 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-06-12 10:21:50,585 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]].
recon_1  | 2023-06-12 10:19:35,907 [IPC Server handler 27 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn3_1    |   replyId: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn3_1    |   raftGroupId {
dn3_1    |     id: "\331\372\206\337\255\224G\225\266\275\344\357[\233\214J"
recon_1  | 2023-06-12 10:19:36,052 [IPC Server handler 90 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 0081bf068fd7
recon_1  | 2023-06-12 10:19:36,721 [IPC Server handler 44 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
recon_1  | 2023-06-12 10:20:03,930 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
scm3_1   | 2023-06-12 10:19:57,856 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-06-12 10:19:58,053 [main] INFO util.log: Logging initialized @34839ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
recon_1  | 2023-06-12 10:20:03,937 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm3_1   | 2023-06-12 10:19:58,525 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
recon_1  | 2023-06-12 10:20:03,943 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm3_1   | 2023-06-12 10:19:58,568 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3_1   | 2023-06-12 10:19:58,623 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-06-12 10:19:58,628 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn3_1    |   }
dn3_1    |   callId: 8
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
recon_1  | 2023-06-12 10:20:03,972 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm3_1   | 2023-06-12 10:19:58,629 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm3_1   | 2023-06-12 10:19:58,630 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-06-12 10:21:50,617 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]].
scm2_1   | 2023-06-12 10:21:50,638 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]].
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
recon_1  | 2023-06-12 10:20:04,007 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm2_1   | 2023-06-12 10:22:05,852 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-12 10:22:06,111 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]] moved to OPEN state
recon_1  | 2023-06-12 10:20:04,027 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 79 milliseconds.
recon_1  | 2023-06-12 10:20:04,178 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 215 milliseconds to process 0 existing database records.
dn3_1    |   success: true
dn3_1    | }
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:27,330 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:34706: output error
dn3_1    | term: 9
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 2023-06-12 10:19:27,362 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm3_1   | 2023-06-12 10:19:58,742 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3_1   | 2023-06-12 10:19:58,754 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-06-12 10:22:11,559 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
recon_1  | 2023-06-12 10:20:04,328 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 149 milliseconds for processing 2 containers.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
scm3_1   | 2023-06-12 10:19:58,763 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2_1   | 2023-06-12 10:22:11,572 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-06-12 10:20:04,839 [IPC Server handler 42 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
dn3_1    | nextIndex: 25
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-06-12 10:19:58,956 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-06-12 10:20:05,063 [IPC Server handler 90 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
recon_1  | 2023-06-12 10:20:15,786 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn3_1    | followerCommit: 24
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-12 10:19:58,957 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn3_1    | matchIndex: 18446744073709551615
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm3_1   | 2023-06-12 10:19:58,970 [main] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | isHearbeat: true
dn3_1    | 
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
scm3_1   | 2023-06-12 10:19:59,043 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@57e4b86c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-06-12 10:19:59,045 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6df791a4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-06-12 10:21:34,114 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:35,360 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
scm2_1   | 2023-06-12 10:22:11,618 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]].
scm2_1   | 2023-06-12 10:22:14,559 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm2_1   | 2023-06-12 10:22:14,560 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-12 10:19:59,748 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6eed46e9{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-7804413956482867038/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm2_1   | 2023-06-12 10:22:14,589 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]].
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm2_1   | 2023-06-12 10:22:20,947 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]] moved to OPEN state
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1  | 2023-06-12 10:20:15,790 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 131 
scm3_1   | 2023-06-12 10:19:59,809 [main] INFO server.AbstractConnector: Started ServerConnector@20b54cfe{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm2_1   | 2023-06-12 10:22:40,789 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]] moved to OPEN state
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 2023-06-12 10:20:16,502 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 9, SequenceNumber diff: 25, SequenceNumber Lag from OM 0.
scm3_1   | 2023-06-12 10:19:59,810 [main] INFO server.Server: Started @36596ms
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-06-12 10:22:44,830 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]] moved to OPEN state
recon_1  | 2023-06-12 10:20:16,502 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 25 records
scm3_1   | 2023-06-12 10:19:59,831 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm2_1   | 2023-06-12 10:23:16,133 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
recon_1  | 2023-06-12 10:20:16,539 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
scm3_1   | 2023-06-12 10:19:59,831 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm2_1   | 2023-06-12 10:23:20,106 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-06-12 10:20:16,543 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
scm3_1   | 2023-06-12 10:19:59,839 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm2_1   | 2023-06-12 10:23:26,201 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
recon_1  | 2023-06-12 10:20:17,233 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-06-12 10:20:17,256 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
recon_1  | 2023-06-12 10:20:17,411 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-06-12 10:21:03,979 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1  | 2023-06-12 10:21:04,016 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm2_1   | 2023-06-12 10:23:38,068 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-06-12 10:20:00,391 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:20:00,394 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:21:04,016 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 37
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 2023-06-12 10:20:04,813 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm2_1   | 2023-06-12 10:23:38,167 [7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
recon_1  | 2023-06-12 10:21:17,460 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm3_1   | 2023-06-12 10:20:04,817 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-12 10:23:40,871 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
recon_1  | 2023-06-12 10:21:17,461 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
dn4_1    | 2023-06-12 10:21:30,353 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#12-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn4_1    | address: "10.9.0.17:9856"
dn4_1    | clientAddress: "10.9.0.17:9858"
dn4_1    | adminAddress: "10.9.0.17:9857"
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | startupRole: FOLLOWER
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn4_1    | address: "10.9.0.20:9856"
scm3_1   | 2023-06-12 10:20:04,836 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn4_1    | priority: 1
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn1_1    | 2023-06-12 10:21:35,361 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:36,611 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
dn4_1    | clientAddress: "10.9.0.20:9858"
dn4_1    | adminAddress: "10.9.0.20:9857"
scm3_1   | 2023-06-12 10:20:04,856 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-06-12 10:19:27,323 [IPC Server handler 11 on default port 9861] WARN ipc.Server: IPC Server handler 11 on default port 9861, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:59302: output error
scm1_1   | 2023-06-12 10:19:27,364 [IPC Server handler 11 on default port 9861] INFO ipc.Server: IPC Server handler 11 on default port 9861 caught an exception
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | startupRole: FOLLOWER
scm3_1   | 2023-06-12 10:20:04,856 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-06-12 10:20:04,862 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-06-12 10:21:42,158 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm1_1   | java.nio.channels.ClosedChannelException
recon_1  | 2023-06-12 10:21:17,461 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 156 
recon_1  | 2023-06-12 10:21:17,480 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 4, SequenceNumber Lag from OM 0.
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
scm2_1   | 2023-06-12 10:23:50,118 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm2_1   | 2023-06-12 10:24:17,066 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
scm2_1   | 2023-06-12 10:24:17,069 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 2023-06-12 10:21:17,480 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 4 records
recon_1  | 2023-06-12 10:21:17,485 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn4_1    | address: "10.9.0.19:9856"
scm3_1   | 2023-06-12 10:20:04,863 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-12 10:20:05,031 [IPC Server handler 20 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm3_1   | 2023-06-12 10:20:05,034 [IPC Server handler 20 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 2023-06-12 10:21:17,485 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-06-12 10:21:17,648 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-06-12 10:21:17,649 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
dn4_1    | clientAddress: "10.9.0.19:9858"
dn4_1    | adminAddress: "10.9.0.19:9857"
scm2_1   | 2023-06-12 10:24:17,078 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
scm2_1   | 2023-06-12 10:24:17,078 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server 7740519e-fe5b-4936-af00-27e2eba71ce5@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-06-12 10:21:17,649 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-06-12 10:21:18,546 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
scm3_1   | 2023-06-12 10:20:05,039 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | startupRole: FOLLOWER
dn4_1    | , old:)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 2023-06-12 10:20:05,039 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm3_1   | 2023-06-12 10:20:05,039 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-06-12 10:24:20,675 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 2023-06-12 10:21:18,592 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm3_1   | 2023-06-12 10:20:05,040 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-12 10:20:05,517 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm3_1   | 2023-06-12 10:20:05,518 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
recon_1  | 2023-06-12 10:21:19,640 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
recon_1  | 2023-06-12 10:21:19,656 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
recon_1  | 2023-06-12 10:22:04,017 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-06-12 10:21:42,178 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: remove    LEADER d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6:t4, leader=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLog:OPENED:c6, conf=5: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm3_1   | 2023-06-12 10:20:06,770 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c030830-a72e-4f6f-8f54-144518af6253
recon_1  | 2023-06-12 10:22:04,017 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 2023-06-12 10:22:05,852 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e4d68ba5-f2e1-460a-8ae0-8c2a955bd013. Trying to get from SCM.
recon_1  | 2023-06-12 10:22:05,892 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]] to Recon pipeline metadata.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
scm3_1   | 2023-06-12 10:20:06,772 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c030830-a72e-4f6f-8f54-144518af6253{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-12 10:20:06,773 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-12 10:20:06,773 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
dn3_1    | 2023-06-12 10:21:42,180 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: shutdown
recon_1  | 2023-06-12 10:22:05,893 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]].
recon_1  | 2023-06-12 10:22:05,945 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64. Trying to get from SCM.
recon_1  | 2023-06-12 10:22:05,973 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-06-12 10:21:42,180 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F138DFAD79A6,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:21:42,180 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-LeaderStateImpl
recon_1  | 2023-06-12 10:22:05,991 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]].
dn3_1    | 2023-06-12 10:21:42,181 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-06-12 10:21:42,183 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater: set stopIndex = 6
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm3_1   | 2023-06-12 10:20:06,774 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 2023-06-12 10:22:05,997 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
dn3_1    | 2023-06-12 10:21:42,184 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F138DFAD79A6: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/sm/snapshot.4_6
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm3_1   | 2023-06-12 10:20:06,775 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-12 10:20:06,783 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
recon_1  | 2023-06-12 10:22:06,116 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=f8e4ee74-e97b-4a76-a7a0-d83197229949. Trying to get from SCM.
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-06-12 10:20:06,784 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
recon_1  | 2023-06-12 10:22:06,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]] to Recon pipeline metadata.
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
dn3_1    | 2023-06-12 10:21:42,186 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-F138DFAD79A6: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6/sm/snapshot.4_6 took: 3 ms
dn3_1    | 2023-06-12 10:21:42,188 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 2023-06-12 10:22:06,135 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]].
scm1_1   | 2023-06-12 10:19:27,319 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:43834: output error
scm1_1   | 2023-06-12 10:19:27,367 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1  | 2023-06-12 10:22:06,235 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
scm3_1   | 2023-06-12 10:20:06,784 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn3_1    | 2023-06-12 10:21:42,188 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-12 10:20:06,785 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-06-12 10:21:42,190 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: closes. applyIndex: 6
dn3_1    | 2023-06-12 10:21:42,206 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6-SegmentedRaftLogWorker close()
dn3_1    | 2023-06-12 10:21:42,212 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-F138DFAD79A6: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d89dff80-6c54-4b7d-89f4-f138dfad79a6
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-12 10:20:06,785 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:36,611 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-06-12 10:22:07,109 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 2023-06-12 10:20:10,711 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
recon_1  | 2023-06-12 10:22:07,663 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
scm3_1   | 2023-06-12 10:20:10,711 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:22:07,664 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb. Trying to get from SCM.
dn3_1    | 2023-06-12 10:21:42,213 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 command on datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c.
scm3_1   | 2023-06-12 10:20:11,225 [IPC Server handler 19 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-06-12 10:22:07,667 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]] to Recon pipeline metadata.
scm3_1   | 2023-06-12 10:20:11,225 [IPC Server handler 19 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d6f449c5-6fac-4083-a27e-b83edf3e8b1c{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-06-12 10:21:42,214 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: remove  FOLLOWER d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A:t9, leader=3c030830-a72e-4f6f-8f54-144518af6253, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c24, conf=23: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-06-12 10:21:42,214 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: shutdown
dn3_1    | 2023-06-12 10:21:42,214 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:21:42,214 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState
recon_1  | 2023-06-12 10:22:07,668 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]].
scm3_1   | 2023-06-12 10:20:11,227 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-06-12 10:22:07,669 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
dn1_1    | 2023-06-12 10:21:37,709 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
dn4_1    | 2023-06-12 10:21:31,602 [grpc-default-executor-10] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#14-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn4_1    | address: "10.9.0.17:9856"
scm3_1   | 2023-06-12 10:20:11,227 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-06-12 10:22:07,753 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
dn4_1    | clientAddress: "10.9.0.17:9858"
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 2023-06-12 10:20:15,884 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:22:07,753 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
dn4_1    | adminAddress: "10.9.0.17:9857"
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-06-12 10:20:15,885 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:22:08,427 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)
scm3_1   | 2023-06-12 10:20:17,042 [IPC Server handler 20 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:21:42,214 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-FollowerState was interrupted
dn3_1    | 2023-06-12 10:21:42,215 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater: set stopIndex = 24
recon_1  | 2023-06-12 10:22:08,428 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm3_1   | 2023-06-12 10:20:17,042 [IPC Server handler 20 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 78f8fca8-1c45-4717-8e79-22872958dcce{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-12 10:22:10,938 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-06-12 10:20:17,043 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn3_1    | 2023-06-12 10:21:42,215 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Taking a snapshot at:(t:9, i:24) file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.9_24
dn3_1    | 2023-06-12 10:21:42,219 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Finished taking a snapshot at:(t:9, i:24) file:/data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.9_24 took: 3 ms
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
recon_1  | 2023-06-12 10:22:10,938 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
scm3_1   | 2023-06-12 10:20:17,044 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn3_1    | 2023-06-12 10:21:42,219 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater: Took a snapshot at index 24
dn3_1    | 2023-06-12 10:21:42,220 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 24
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
recon_1  | 2023-06-12 10:22:11,254 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-06-12 10:22:11,254 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
dn3_1    | 2023-06-12 10:21:42,223 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: closes. applyIndex: 24
dn3_1    | 2023-06-12 10:21:43,065 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A-SegmentedRaftLogWorker close()
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
recon_1  | 2023-06-12 10:22:11,558 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
scm3_1   | 2023-06-12 10:20:17,044 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
recon_1  | 2023-06-12 10:22:11,559 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-12 10:20:17,044 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 2023-06-12 10:22:11,560 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-06-12 10:22:13,362 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb reported by d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)
dn3_1    | 2023-06-12 10:21:43,099 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn3_1    | 2023-06-12 10:21:43,112 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
recon_1  | 2023-06-12 10:22:13,363 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]] moved to OPEN state
recon_1  | 2023-06-12 10:22:17,656 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn3_1    | 2023-06-12 10:21:43,139 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-E4EF5B9B8C4A: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn3_1    | 2023-06-12 10:21:43,150 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a command on datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
recon_1  | 2023-06-12 10:22:17,658 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-06-12 10:22:17,658 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 160 
dn3_1    | 2023-06-12 10:21:43,151 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: remove    LEADER d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA:t6, leader=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c49, conf=48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-06-12 10:21:43,151 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: shutdown
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-06-12 10:22:17,669 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-12 10:22:17,669 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
dn3_1    | 2023-06-12 10:21:43,151 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn3_1    | 2023-06-12 10:21:43,152 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-LeaderStateImpl
dn1_1    | 2023-06-12 10:21:37,710 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
recon_1  | 2023-06-12 10:22:20,949 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=f9a89c0e-7717-47ce-8428-3f47456d10ee. Trying to get from SCM.
recon_1  | 2023-06-12 10:22:20,967 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]] to Recon pipeline metadata.
dn3_1    | 2023-06-12 10:21:43,152 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-06-12 10:21:43,152 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->78f8fca8-1c45-4717-8e79-22872958dcce-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->78f8fca8-1c45-4717-8e79-22872958dcce-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn1_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 2023-06-12 10:22:20,969 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]].
scm3_1   | 2023-06-12 10:20:17,044 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:27,361 [IPC Server handler 9 on default port 9861] INFO ipc.Server: IPC Server handler 9 on default port 9861 caught an exception
recon_1  | 2023-06-12 10:22:20,969 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=f9a89c0e-7717-47ce-8428-3f47456d10ee reported by 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm3_1   | 2023-06-12 10:20:17,044 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1   | 2023-06-12 10:20:21,021 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:20:21,021 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:20:24,040 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm3_1   | 2023-06-12 10:20:24,040 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn1_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm3_1   | 2023-06-12 10:20:24,411 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm3_1   | 2023-06-12 10:20:24,438 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-06-12 10:20:24,439 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm3_1   | 2023-06-12 10:20:24,441 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-06-12 10:20:24,443 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn1_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm3_1   | 2023-06-12 10:20:24,444 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-06-12 10:20:24,445 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm3_1   | 2023-06-12 10:20:24,446 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 2023-06-12 10:20:24,447 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm3_1   | 2023-06-12 10:20:24,447 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db in state CLOSED which uses HEALTHY_READONLY datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29. This will send close commands for its containers.
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm3_1   | 2023-06-12 10:20:24,448 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3721)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1718)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:21:37,722 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: remove    LEADER 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E:t4, leader=3c030830-a72e-4f6f-8f54-144518af6253, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLog:OPENED:c6, conf=5: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm3_1   | 2023-06-12 10:20:24,448 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c in state CLOSED which uses HEALTHY_READONLY datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3. This will send close commands for its containers.
dn1_1    | 2023-06-12 10:21:37,724 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: shutdown
scm3_1   | 2023-06-12 10:20:24,448 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1788)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2897)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1860)
dn1_1    | 2023-06-12 10:21:37,724 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-DD6E9427813E,id=3c030830-a72e-4f6f-8f54-144518af6253
dn1_1    | 2023-06-12 10:21:37,725 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-LeaderStateImpl
scm3_1   | 2023-06-12 10:20:24,451 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1171)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 2023-06-12 10:21:37,725 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-06-12 10:21:37,729 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DD6E9427813E: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/sm/snapshot.4_6
scm3_1   | 2023-06-12 10:20:24,452 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
dn1_1    | 2023-06-12 10:21:37,729 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater: set stopIndex = 6
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1106)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-06-12 10:20:24,452 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
dn1_1    | 2023-06-12 10:21:37,731 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-DD6E9427813E: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e/sm/snapshot.4_6 took: 2 ms
dn1_1    | 2023-06-12 10:21:37,732 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater: Took a snapshot at index 6
dn1_1    | 2023-06-12 10:21:37,733 [3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn4_1    | address: "10.9.0.20:9856"
scm3_1   | 2023-06-12 10:20:24,453 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn3_1    | 2023-06-12 10:21:43,152 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->3c030830-a72e-4f6f-8f54-144518af6253-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->3c030830-a72e-4f6f-8f54-144518af6253-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-06-12 10:21:43,173 [grpc-default-executor-2] INFO server.GrpcLogAppender: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->3c030830-a72e-4f6f-8f54-144518af6253-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-06-12 10:21:37,735 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: closes. applyIndex: 6
dn1_1    | 2023-06-12 10:21:37,863 [grpc-default-executor-1] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: Failed appendEntries
scm3_1   | 2023-06-12 10:20:24,453 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 2023-06-12 10:22:20,970 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]] moved to OPEN state
dn1_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm3_1   | 2023-06-12 10:20:24,453 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
scm3_1   | 2023-06-12 10:20:24,454 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
recon_1  | 2023-06-12 10:22:40,780 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=619a7347-cd8e-4ad4-bd7f-887db72cce2f. Trying to get from SCM.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-06-12 10:20:24,454 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm3_1   | 2023-06-12 10:20:24,454 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
recon_1  | 2023-06-12 10:22:40,803 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]] to Recon pipeline metadata.
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | priority: 1
dn4_1    | clientAddress: "10.9.0.20:9858"
recon_1  | 2023-06-12 10:22:40,805 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]].
dn3_1    | 2023-06-12 10:21:43,174 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Taking a snapshot at:(t:6, i:49) file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49
dn1_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm3_1   | 2023-06-12 10:20:24,454 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm3_1   | 2023-06-12 10:20:24,454 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm3_1   | 2023-06-12 10:20:26,062 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:20:26,063 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:22:40,806 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=619a7347-cd8e-4ad4-bd7f-887db72cce2f reported by 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)
dn3_1    | 2023-06-12 10:21:43,175 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater: set stopIndex = 49
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm3_1   | 2023-06-12 10:20:31,152 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:22:40,806 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-12 10:20:31,152 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:20:34,805 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)
scm1_1   | 2023-06-12 10:19:27,653 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:22:44,821 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d9b35e02-7d2f-4445-a8e2-cc28542bf90d. Trying to get from SCM.
scm3_1   | 2023-06-12 10:20:35,051 [IPC Server handler 20 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-06-12 10:20:36,176 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:27,653 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:27,819 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:27,820 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:22:44,825 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]] to Recon pipeline metadata.
scm3_1   | 2023-06-12 10:20:36,176 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm1_1   | 2023-06-12 10:19:27,837 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:21:43,176 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Finished taking a snapshot at:(t:6, i:49) file:/data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49 took: 3 ms
dn3_1    | 2023-06-12 10:21:43,177 [grpc-default-executor-2] INFO leader.FollowerInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->3c030830-a72e-4f6f-8f54-144518af6253: decreaseNextIndex nextIndex: updateUnconditionally 50 -> 49
recon_1  | 2023-06-12 10:22:44,826 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]].
scm3_1   | 2023-06-12 10:20:36,744 [IPC Server handler 89 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 2023-06-12 10:19:28,905 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:21:43,179 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater: Took a snapshot at index 49
dn3_1    | 2023-06-12 10:21:43,180 [grpc-default-executor-3] INFO server.GrpcLogAppender: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->78f8fca8-1c45-4717-8e79-22872958dcce-AppendLogResponseHandler: follower responses appendEntries COMPLETED
recon_1  | 2023-06-12 10:22:44,826 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d9b35e02-7d2f-4445-a8e2-cc28542bf90d reported by d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)
scm3_1   | 2023-06-12 10:20:41,189 [IPC Server handler 19 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm1_1   | 2023-06-12 10:19:28,905 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:28,982 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:21:43,180 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-StateMachineUpdater: snapshotIndex: updateIncreasingly 47 -> 49
scm3_1   | 2023-06-12 10:20:41,244 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 2023-06-12 10:19:28,982 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:29,006 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:29,049 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:20:41,244 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm1_1   | 2023-06-12 10:19:29,060 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:29,083 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:30,170 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:22:44,826 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-12 10:20:46,282 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn3_1    | 2023-06-12 10:21:43,180 [grpc-default-executor-3] INFO leader.FollowerInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 50 -> 49
dn3_1    | 2023-06-12 10:21:43,182 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: closes. applyIndex: 49
dn3_1    | 2023-06-12 10:21:43,720 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA-SegmentedRaftLogWorker close()
recon_1  | 2023-06-12 10:23:04,018 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm3_1   | 2023-06-12 10:20:46,282 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:21:43,727 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
scm1_1   | 2023-06-12 10:19:30,181 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:23:04,018 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm3_1   | 2023-06-12 10:20:47,035 [IPC Server handler 20 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-06-12 10:20:51,418 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:20:51,418 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:30,367 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:23:17,675 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm3_1   | 2023-06-12 10:20:54,972 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
dn3_1    | 2023-06-12 10:21:43,727 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn3_1    | 2023-06-12 10:21:43,765 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-4675AE3D33CA: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
scm1_1   | 2023-06-12 10:19:30,370 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:23:17,675 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn3_1    | 2023-06-12 10:21:43,765 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca command on datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c.
dn3_1    | 2023-06-12 10:22:12,158 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:22:12,158 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-06-12 10:19:30,382 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:22:12,158 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
recon_1  | 2023-06-12 10:23:17,676 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 160 
scm3_1   | 2023-06-12 10:20:56,514 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-06-12 10:19:31,539 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:31,542 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:31,893 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:20:56,515 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-06-12 10:19:31,894 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:31,895 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:31,972 [IPC Server handler 55 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm3_1   | 2023-06-12 10:21:01,647 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-06-12 10:19:32,039 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:32,042 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:32,043 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
recon_1  | 2023-06-12 10:23:17,687 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 2, SequenceNumber Lag from OM 0.
scm3_1   | 2023-06-12 10:21:01,647 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:32,193 [IPC Server handler 94 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-06-12 10:22:12,159 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | 2023-06-12 10:23:17,687 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 2 records
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn4_1    | address: "10.9.0.19:9856"
scm3_1   | 2023-06-12 10:21:04,838 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn1_1    | 2023-06-12 10:21:37,864 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:38,177 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E-SegmentedRaftLogWorker close()
dn3_1    | 2023-06-12 10:22:12,161 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn4_1    | clientAddress: "10.9.0.19:9858"
scm1_1   | 2023-06-12 10:19:32,198 [IPC Server handler 94 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-12 10:23:17,691 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-06-12 10:23:17,691 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn1_1    | 2023-06-12 10:21:38,182 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-DD6E9427813E: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/348b71fd-34e7-470a-b568-dd6e9427813e
dn1_1    | 2023-06-12 10:21:38,183 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e command on datanode 3c030830-a72e-4f6f-8f54-144518af6253.
dn3_1    | 2023-06-12 10:22:12,162 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm1_1   | 2023-06-12 10:19:32,144 [IPC Server handler 89 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:19:32,256 [IPC Server handler 89 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3c030830-a72e-4f6f-8f54-144518af6253{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-12 10:23:17,772 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | 2023-06-12 10:22:12,163 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-06-12 10:22:12,163 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
scm1_1   | 2023-06-12 10:19:32,220 [IPC Server handler 55 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-12 10:19:32,368 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:32,376 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:23:17,773 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-06-12 10:23:17,773 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn3_1    | 2023-06-12 10:22:12,163 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-06-12 10:22:12,163 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-06-12 10:22:12,163 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | startupRole: FOLLOWER
scm3_1   | 2023-06-12 10:21:05,038 [IPC Server handler 20 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:32,393 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:32,441 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn3_1    | 2023-06-12 10:22:12,164 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:22:12,164 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:22:12,164 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:22:14,745 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-06-12 10:22:31,947 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-06-12 10:22:44,790 [PipelineCommandHandlerThread-0] INFO server.RaftServer: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: addNew group-CC28542BF90D:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-CC28542BF90D:java.util.concurrent.CompletableFuture@5ac4b25b[Not completed]
dn4_1    | , old:)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm1_1   | 2023-06-12 10:19:32,542 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:32,549 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
recon_1  | 2023-06-12 10:23:40,810 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1001 got from ha_dn4_1.ha_net.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm3_1   | 2023-06-12 10:21:06,731 [IPC Server handler 85 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:32,527 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 2023-06-12 10:23:40,907 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1001 to Recon.
dn1_1    | 2023-06-12 10:21:38,184 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: remove    LEADER 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A:t9, leader=3c030830-a72e-4f6f-8f54-144518af6253, voted=3c030830-a72e-4f6f-8f54-144518af6253, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLog:OPENED:c24, conf=23: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-06-12 10:22:44,793 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: new RaftServerImpl for group-CC28542BF90D:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 2023-06-12 10:19:32,573 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 2023-06-12 10:23:56,160 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1002 got from ha_dn4_1.ha_net.
dn1_1    | 2023-06-12 10:21:38,184 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: shutdown
dn1_1    | 2023-06-12 10:21:38,184 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E4EF5B9B8C4A,id=3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:19:32,574 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 2023-06-12 10:23:56,193 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1002 to Recon.
recon_1  | 2023-06-12 10:24:04,067 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-12 10:19:32,576 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-06-12 10:21:06,796 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:24:04,068 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 49
recon_1  | 2023-06-12 10:24:17,058 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm1_1   | 2023-06-12 10:19:32,598 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:21:38,184 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-LeaderStateImpl
recon_1  | 2023-06-12 10:24:17,075 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
recon_1  | 2023-06-12 10:24:17,784 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 2023-06-12 10:19:32,665 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-06-12 10:19:32,672 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
recon_1  | 2023-06-12 10:24:17,785 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-06-12 10:24:17,786 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 162 
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
scm1_1   | 2023-06-12 10:19:32,672 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-06-12 10:21:06,796 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-06-12 10:24:17,802 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 10, SequenceNumber diff: 31, SequenceNumber Lag from OM 0.
dn1_1    | 2023-06-12 10:21:38,184 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->78f8fca8-1c45-4717-8e79-22872958dcce-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm1_1   | 2023-06-12 10:19:32,688 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:32,694 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
recon_1  | 2023-06-12 10:24:17,802 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 31 records
dn1_1    | 2023-06-12 10:21:38,185 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm1_1   | 2023-06-12 10:19:32,696 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:32,576 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 2023-06-12 10:24:17,816 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn1_1    | 2023-06-12 10:21:38,186 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-PendingRequests: sendNotLeaderResponses
scm1_1   | 2023-06-12 10:19:32,705 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3_1   | 2023-06-12 10:21:11,198 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
recon_1  | 2023-06-12 10:24:17,816 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
dn1_1    | 2023-06-12 10:21:38,190 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater: set stopIndex = 24
dn1_1    | 2023-06-12 10:21:38,190 [grpc-default-executor-1] INFO server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm3_1   | 2023-06-12 10:21:11,552 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:38.880612Z[UTC]] removed.
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 2023-06-12 10:24:18,048 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
dn1_1    | 2023-06-12 10:21:38,190 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Taking a snapshot at:(t:9, i:24) file /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.9_24
scm1_1   | 2023-06-12 10:19:32,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
dn3_1    | 2023-06-12 10:22:44,794 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: ConfigurationManager, init=-1: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 2023-06-12 10:21:38,190 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c: decreaseNextIndex nextIndex: updateUnconditionally 25 -> 24
dn3_1    | 2023-06-12 10:22:44,795 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-12 10:22:44,798 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1  | 2023-06-12 10:24:18,086 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
recon_1  | 2023-06-12 10:24:18,121 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-06-12 10:25:04,069 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
scm3_1   | 2023-06-12 10:21:11,574 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:38.868632Z[UTC]] removed.
dn3_1    | 2023-06-12 10:22:44,798 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-06-12 10:21:11,596 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:38.889743Z[UTC]] removed.
dn3_1    | 2023-06-12 10:22:44,800 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 2023-06-12 10:21:38,194 [grpc-default-executor-1] INFO server.GrpcLogAppender: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn1_1    | 2023-06-12 10:21:38,194 [grpc-default-executor-1] INFO leader.FollowerInfo: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A->d6f449c5-6fac-4083-a27e-b83edf3e8b1c: decreaseNextIndex nextIndex: updateUnconditionally 24 -> 23
dn1_1    | 2023-06-12 10:21:38,197 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-E4EF5B9B8C4A: Finished taking a snapshot at:(t:9, i:24) file:/data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a/sm/snapshot.9_24 took: 7 ms
scm3_1   | 2023-06-12 10:21:11,626 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:38.895130Z[UTC]] removed.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn1_1    | 2023-06-12 10:21:38,197 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater: Took a snapshot at index 24
dn1_1    | 2023-06-12 10:21:38,197 [3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 24
dn1_1    | 2023-06-12 10:21:38,198 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: closes. applyIndex: 24
scm3_1   | 2023-06-12 10:21:11,640 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:38.901099Z[UTC]] removed.
dn4_1    | 	... 13 more
dn1_1    | 2023-06-12 10:21:39,065 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A-SegmentedRaftLogWorker close()
dn1_1    | 2023-06-12 10:21:39,109 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn1_1    | 2023-06-12 10:21:39,109 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn1_1    | 2023-06-12 10:21:39,130 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-E4EF5B9B8C4A: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
scm3_1   | 2023-06-12 10:21:11,660 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:38.895293Z[UTC]] removed.
dn3_1    | 2023-06-12 10:22:44,800 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:21:32,853 [grpc-default-executor-10] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#16-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn1_1    | 2023-06-12 10:21:39,166 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a command on datanode 3c030830-a72e-4f6f-8f54-144518af6253.
dn1_1    | 2023-06-12 10:21:39,170 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: remove  FOLLOWER 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA:t6, leader=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, voted=d6f449c5-6fac-4083-a27e-b83edf3e8b1c, raftlog=Memoized:3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLog:OPENED:c49, conf=48: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-06-12 10:21:39,172 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: shutdown
scm3_1   | 2023-06-12 10:21:11,681 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:38.857691Z[UTC]] removed.
dn3_1    | 2023-06-12 10:22:44,801 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | address: "10.9.0.17:9856"
scm1_1   | 2023-06-12 10:19:32,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-06-12 10:19:32,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1   | 2023-06-12 10:19:32,709 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-06-12 10:21:11,977 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:44,801 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-12 10:21:39,173 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-4675AE3D33CA,id=3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:19:32,788 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:11,978 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:25:04,069 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
dn3_1    | 2023-06-12 10:22:44,805 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-06-12 10:22:44,807 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-12 10:21:39,174 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState
scm1_1   | 2023-06-12 10:19:32,836 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:25:04,331 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 2 milliseconds to process 0 existing database records.
dn3_1    | 2023-06-12 10:22:44,807 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | clientAddress: "10.9.0.17:9858"
dn1_1    | 2023-06-12 10:21:39,174 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater: set stopIndex = 49
scm1_1   | 2023-06-12 10:19:32,942 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:16,986 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,807 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | adminAddress: "10.9.0.17:9857"
dn1_1    | 2023-06-12 10:21:39,175 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-FollowerState was interrupted
recon_1  | 2023-06-12 10:25:04,351 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 14 pipelines in house.
scm1_1   | 2023-06-12 10:19:32,945 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:17,063 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:22:44,808 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-12 10:22:44,808 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-06-12 10:25:04,353 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 20 milliseconds for processing 4 containers.
scm1_1   | 2023-06-12 10:19:32,967 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:22:44,808 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-06-12 10:25:04,356 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e from Recon.
scm1_1   | 2023-06-12 10:19:33,134 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:33,140 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,808 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d9b35e02-7d2f-4445-a8e2-cc28542bf90d does not exist. Creating ...
scm3_1   | 2023-06-12 10:21:17,063 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:33,147 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn1_1    | 2023-06-12 10:21:39,175 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Taking a snapshot at:(t:6, i:49) file /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
recon_1  | 2023-06-12 10:25:04,357 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.468Z[UTC]] moved to CLOSED state
dn3_1    | 2023-06-12 10:22:44,814 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d9b35e02-7d2f-4445-a8e2-cc28542bf90d/in_use.lock acquired by nodename 7@7c95fade90b7
scm1_1   | 2023-06-12 10:19:33,211 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 1.
scm1_1   | 2023-06-12 10:19:33,250 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 1.
scm1_1   | 2023-06-12 10:19:33,269 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:18,546 [IPC Server handler 53 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-06-12 10:21:18,577 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-06-12 10:21:18,579 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm3_1   | 2023-06-12 10:21:18,680 [FixedThreadPoolWithAffinityExecutor-0-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn1_1    | 2023-06-12 10:21:39,178 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-4675AE3D33CA: Finished taking a snapshot at:(t:6, i:49) file:/data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca/sm/snapshot.6_49 took: 3 ms
dn1_1    | 2023-06-12 10:21:39,179 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater: Took a snapshot at index 49
dn1_1    | 2023-06-12 10:21:39,179 [3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater] INFO impl.StateMachineUpdater: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-StateMachineUpdater: snapshotIndex: updateIncreasingly 47 -> 49
dn4_1    | address: "10.9.0.20:9856"
scm1_1   | 2023-06-12 10:19:33,273 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | priority: 1
dn3_1    | 2023-06-12 10:22:44,817 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d9b35e02-7d2f-4445-a8e2-cc28542bf90d has been successfully formatted.
dn1_1    | 2023-06-12 10:21:39,184 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: closes. applyIndex: 49
recon_1  | 2023-06-12 10:25:04,358 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.468Z[UTC]] removed.
recon_1  | 2023-06-12 10:25:04,359 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a from Recon.
scm1_1   | 2023-06-12 10:19:33,284 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | clientAddress: "10.9.0.20:9858"
dn3_1    | 2023-06-12 10:22:44,826 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO ratis.ContainerStateMachine: group-CC28542BF90D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-12 10:21:39,617 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA-SegmentedRaftLogWorker close()
recon_1  | 2023-06-12 10:25:04,360 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.471Z[UTC]] moved to CLOSED state
recon_1  | 2023-06-12 10:25:04,360 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.471Z[UTC]] removed.
scm1_1   | 2023-06-12 10:19:33,663 [IPC Server handler 41 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | adminAddress: "10.9.0.20:9857"
dn4_1    | startupRole: FOLLOWER
dn1_1    | 2023-06-12 10:21:39,623 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
recon_1  | 2023-06-12 10:25:04,360 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db from Recon.
recon_1  | 2023-06-12 10:25:04,361 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:11.471Z[UTC]] moved to CLOSED state
scm1_1   | 2023-06-12 10:19:33,663 [IPC Server handler 41 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d6f449c5-6fac-4083-a27e-b83edf3e8b1c{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn4_1    | address: "10.9.0.19:9856"
dn1_1    | 2023-06-12 10:21:39,623 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
scm1_1   | 2023-06-12 10:19:33,664 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn4_1    | clientAddress: "10.9.0.19:9858"
dn3_1    | 2023-06-12 10:22:44,826 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-12 10:21:39,641 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-4675AE3D33CA: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/2345b4fc-4790-4824-ac91-4675ae3d33ca
scm3_1   | 2023-06-12 10:21:19,642 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-06-12 10:21:19,659 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:33,664 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:33,671 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 2.
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | 2023-06-12 10:22:44,827 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-12 10:21:39,649 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca command on datanode 3c030830-a72e-4f6f-8f54-144518af6253.
scm3_1   | 2023-06-12 10:21:19,659 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm3_1   | 2023-06-12 10:21:19,669 [FixedThreadPoolWithAffinityExecutor-0-0] WARN container.IncrementalContainerReportHandler: Failed to process QUASI_CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 2023-06-12 10:19:33,803 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:22:44,827 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:21:43,171 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastRequest: d6f449c5-6fac-4083-a27e-b83edf3e8b1c->3c030830-a72e-4f6f-8f54-144518af6253#2-t6,previous=(t:6, i:48),leaderCommit=48,initializing? true,entries: size=1, first=(t:6, i:49), METADATAENTRY(c:48)
scm3_1   | 2023-06-12 10:21:22,189 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-12 10:25:04,361 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:11.471Z[UTC]] removed.
scm1_1   | 2023-06-12 10:19:33,844 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:34,039 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:21:43,171 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastReply: null
scm3_1   | 2023-06-12 10:21:22,191 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-12 10:25:04,362 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c from Recon.
recon_1  | 2023-06-12 10:25:04,362 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:11.472Z[UTC]] moved to CLOSED state
scm1_1   | 2023-06-12 10:19:34,040 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,827 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-12 10:21:43,178 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3c030830-a72e-4f6f-8f54-144518af6253: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | , old:)
scm1_1   | 2023-06-12 10:19:34,041 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 2023-06-12 10:25:04,362 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:11.472Z[UTC]] removed.
recon_1  | 2023-06-12 10:25:04,363 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 from Recon.
dn1_1    | 2023-06-12 10:21:54,906 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 2023-06-12 10:21:23,954 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:34,061 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-06-12 10:25:04,363 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.469Z[UTC]] moved to CLOSED state
recon_1  | 2023-06-12 10:25:04,363 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.469Z[UTC]] removed.
dn1_1    | 2023-06-12 10:21:54,906 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm3_1   | 2023-06-12 10:21:27,290 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm1_1   | 2023-06-12 10:19:34,061 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:34,062 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn1_1    | 2023-06-12 10:21:54,906 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 2023-06-12 10:21:27,291 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
recon_1  | 2023-06-12 10:25:04,364 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 from Recon.
recon_1  | 2023-06-12 10:25:04,364 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.472Z[UTC]] moved to CLOSED state
recon_1  | 2023-06-12 10:25:04,364 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.472Z[UTC]] removed.
dn1_1    | 2023-06-12 10:21:54,906 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
recon_1  | 2023-06-12 10:25:04,365 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca from Recon.
dn3_1    | 2023-06-12 10:22:44,827 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-12 10:22:44,828 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:21:54,908 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-06-12 10:21:32,437 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:44,828 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-12 10:22:44,845 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:21:54,908 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
recon_1  | 2023-06-12 10:25:04,365 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.458Z[UTC]] moved to CLOSED state
scm3_1   | 2023-06-12 10:21:32,437 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:21:35,445 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn3_1    | 2023-06-12 10:22:44,845 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:21:54,908 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-06-12 10:25:04,366 [PipelineSyncTask] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.458Z[UTC]] removed.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm1_1   | 2023-06-12 10:19:34,106 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:34,108 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:34,129 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,846 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO segmented.SegmentedRaftLogWorker: new d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d9b35e02-7d2f-4445-a8e2-cc28542bf90d
dn1_1    | 2023-06-12 10:21:54,908 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
scm3_1   | 2023-06-12 10:21:35,445 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-12 10:21:35,445 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn3_1    | 2023-06-12 10:22:44,846 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-12 10:22:44,847 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-12 10:22:44,848 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-12 10:22:44,849 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-12 10:22:44,849 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-12 10:22:44,849 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-12 10:19:34,804 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:35,070 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-12 10:19:35,291 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:35,291 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm3_1   | 2023-06-12 10:21:35,445 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-12 10:21:35,599 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]].
scm3_1   | 2023-06-12 10:21:35,624 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]].
dn3_1    | 2023-06-12 10:22:44,849 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-06-12 10:25:04,368 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 38 milliseconds.
recon_1  | 2023-06-12 10:25:18,140 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm3_1   | 2023-06-12 10:21:37,616 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:44,849 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
recon_1  | 2023-06-12 10:25:18,140 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-06-12 10:25:18,140 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 193 
scm3_1   | 2023-06-12 10:21:37,616 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:22:44,850 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 2023-06-12 10:25:18,182 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 7, SequenceNumber diff: 19, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-12 10:25:18,182 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 19 records
scm3_1   | 2023-06-12 10:21:39,106 [IPC Server handler 19 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,853 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-06-12 10:25:18,193 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-06-12 10:25:18,194 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
scm3_1   | 2023-06-12 10:21:39,161 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,893 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-06-12 10:21:54,909 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-06-12 10:25:18,308 [pool-49-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-06-12 10:25:18,317 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 2023-06-12 10:22:10,637 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-06-12 10:22:31,752 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-06-12 10:19:35,536 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:39,630 [IPC Server handler 63 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,894 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
recon_1  | 2023-06-12 10:25:18,339 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn1_1    | 2023-06-12 10:22:40,707 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3c030830-a72e-4f6f-8f54-144518af6253: addNew group-887DB72CCE2F:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-887DB72CCE2F:java.util.concurrent.CompletableFuture@7d7b9a7d[Not completed]
dn1_1    | 2023-06-12 10:22:40,717 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253: new RaftServerImpl for group-887DB72CCE2F:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-06-12 10:19:35,542 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,894 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
scm3_1   | 2023-06-12 10:21:39,642 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:35,566 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:22:44,895 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-12 10:22:44,896 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-12 10:19:35,859 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:21:41,186 [IPC Server handler 14 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-06-12 10:22:44,897 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: start as a follower, conf=-1: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:22:40,718 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-06-12 10:19:35,880 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:35,897 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-06-12 10:22:40,719 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-12 10:22:44,897 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 2023-06-12 10:21:42,680 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-06-12 10:19:35,960 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,897 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState
scm3_1   | 2023-06-12 10:21:42,681 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-06-12 10:19:35,976 [IPC Server handler 62 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/78f8fca8-1c45-4717-8e79-22872958dcce
dn1_1    | 2023-06-12 10:22:40,719 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-12 10:22:44,899 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:21:43,083 [IPC Server handler 19 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:35,978 [IPC Server handler 62 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 78f8fca8-1c45-4717-8e79-22872958dcce{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn1_1    | 2023-06-12 10:22:40,720 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-12 10:22:44,899 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-06-12 10:21:43,141 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:35,980 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn1_1    | 2023-06-12 10:22:40,720 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-12 10:22:44,903 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CC28542BF90D,id=d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 2023-06-12 10:21:43,733 [IPC Server handler 84 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:35,988 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn1_1    | 2023-06-12 10:22:40,721 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-12 10:22:44,904 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-12 10:22:44,905 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | 2023-06-12 10:21:43,748 [IPC Server handler 86 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:19:35,988 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn1_1    | 2023-06-12 10:22:40,721 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: ConfigurationManager, init=-1: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
scm3_1   | 2023-06-12 10:21:47,778 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:35,988 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
dn1_1    | 2023-06-12 10:22:40,722 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:21:34,105 [grpc-default-executor-10] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#18-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
dn3_1    | 2023-06-12 10:22:44,905 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 2023-06-12 10:21:47,778 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:35,988 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn1_1    | 2023-06-12 10:22:40,729 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | address: "10.9.0.17:9856"
dn4_1    | clientAddress: "10.9.0.17:9858"
scm3_1   | 2023-06-12 10:21:50,446 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
scm3_1   | 2023-06-12 10:21:50,447 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-12 10:19:35,993 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-06-12 10:19:35,993 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1_1   | 2023-06-12 10:19:35,995 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1_1   | 2023-06-12 10:19:35,995 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1_1   | 2023-06-12 10:19:36,009 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:19:36,132 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1_1   | 2023-06-12 10:19:36,133 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm1_1   | 2023-06-12 10:19:36,177 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,192 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:36,553 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,554 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,614 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,618 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,625 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:36,659 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,668 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:36,675 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:36,766 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,905 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-12 10:22:40,729 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | adminAddress: "10.9.0.17:9857"
scm3_1   | 2023-06-12 10:21:50,590 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]].
scm3_1   | 2023-06-12 10:21:50,616 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]].
dn3_1    | 2023-06-12 10:22:44,911 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=d9b35e02-7d2f-4445-a8e2-cc28542bf90d
dn1_1    | 2023-06-12 10:22:40,733 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
scm1_1   | 2023-06-12 10:19:36,776 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:44,911 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d9b35e02-7d2f-4445-a8e2-cc28542bf90d.
dn1_1    | 2023-06-12 10:22:40,734 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | address: "10.9.0.20:9856"
scm3_1   | 2023-06-12 10:21:50,642 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]].
scm1_1   | 2023-06-12 10:19:36,779 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:22:46,072 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2a8b33ba] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn1_1    | 2023-06-12 10:22:40,734 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | priority: 1
scm3_1   | 2023-06-12 10:21:52,836 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:36,875 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:46,120 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 1001
dn1_1    | 2023-06-12 10:22:40,736 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | clientAddress: "10.9.0.20:9858"
scm3_1   | 2023-06-12 10:21:52,837 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:36,875 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:50,070 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO impl.FollowerState: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172477057ns, electionTimeout:5171ms
dn4_1    | adminAddress: "10.9.0.20:9857"
dn1_1    | 2023-06-12 10:22:40,736 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm3_1   | 2023-06-12 10:21:57,872 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:36,878 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:22:50,070 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState
dn1_1    | 2023-06-12 10:22:40,743 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 2023-06-12 10:21:57,872 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:22:02,988 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn4_1    | address: "10.9.0.19:9856"
dn3_1    | 2023-06-12 10:22:50,071 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-12 10:22:40,744 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-12 10:22:02,988 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:37,806 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.19:9858"
dn3_1    | 2023-06-12 10:22:50,071 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-06-12 10:22:40,745 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-06-12 10:22:05,840 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-12 10:22:06,107 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]] moved to OPEN state
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | 2023-06-12 10:22:50,071 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-FollowerState] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4
dn1_1    | 2023-06-12 10:22:40,745 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-06-12 10:22:08,127 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:37,810 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:37,901 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:50,076 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:22:40,746 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-06-12 10:22:08,127 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:37,905 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:22:50,076 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-06-12 10:22:40,747 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-06-12 10:22:11,448 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn4_1    | , old:)
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:22:40,748 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/619a7347-cd8e-4ad4-bd7f-887db72cce2f does not exist. Creating ...
scm1_1   | 2023-06-12 10:19:37,921 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:22:11,448 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.LeaderElection: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-06-12 10:22:40,754 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/619a7347-cd8e-4ad4-bd7f-887db72cce2f/in_use.lock acquired by nodename 7@48a94d757b73
scm1_1   | 2023-06-12 10:19:37,985 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:22:11,550 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]] moved to OPEN state
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: shutdown d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-06-12 10:19:37,996 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:22:11,610 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]].
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CC28542BF90D with new leaderId: d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn1_1    | 2023-06-12 10:22:40,767 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/619a7347-cd8e-4ad4-bd7f-887db72cce2f has been successfully formatted.
scm1_1   | 2023-06-12 10:19:38,002 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:38,051 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:38,057 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:50,079 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: change Leader from null to d6f449c5-6fac-4083-a27e-b83edf3e8b1c at term 1 for becomeLeader, leader elected after 5279ms
dn1_1    | 2023-06-12 10:22:40,794 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO ratis.ContainerStateMachine: group-887DB72CCE2F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm1_1   | 2023-06-12 10:19:38,072 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm3_1   | 2023-06-12 10:22:13,195 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:50,107 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-06-12 10:22:40,794 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-06-12 10:19:38,120 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm3_1   | 2023-06-12 10:22:13,196 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:22:13,380 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-12 10:22:14,448 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm3_1   | 2023-06-12 10:22:14,448 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 2023-06-12 10:22:40,794 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-12 10:22:50,110 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-06-12 10:19:38,137 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:22:14,590 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]].
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn1_1    | 2023-06-12 10:22:40,795 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-12 10:22:40,797 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-12 10:22:40,797 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-12 10:22:40,797 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:22:40,799 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-06-12 10:22:18,200 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:50,110 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-12 10:22:40,800 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-12 10:22:40,800 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm3_1   | 2023-06-12 10:22:18,200 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:22:50,111 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-06-12 10:22:40,800 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/619a7347-cd8e-4ad4-bd7f-887db72cce2f
scm1_1   | 2023-06-12 10:19:38,154 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm3_1   | 2023-06-12 10:22:23,383 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:50,111 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-12 10:22:50,111 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-12 10:19:39,059 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:39,060 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-06-12 10:22:50,113 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-12 10:22:40,800 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 2023-06-12 10:19:39,138 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm3_1   | 2023-06-12 10:22:23,383 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:22:50,122 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-06-12 10:22:40,801 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | 2023-06-12 10:19:39,138 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn3_1    | 2023-06-12 10:22:50,123 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO impl.RoleInfo: d6f449c5-6fac-4083-a27e-b83edf3e8b1c: start d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderStateImpl
scm1_1   | 2023-06-12 10:19:39,157 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm3_1   | 2023-06-12 10:22:28,397 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:22:50,124 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-06-12 10:22:40,802 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-12 10:22:40,803 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-06-12 10:22:28,397 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:22:50,125 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d9b35e02-7d2f-4445-a8e2-cc28542bf90d/current/log_inprogress_0
dn1_1    | 2023-06-12 10:22:40,803 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-12 10:19:39,207 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-06-12 10:22:50,134 [d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D-LeaderElection4] INFO server.RaftServer$Division: d6f449c5-6fac-4083-a27e-b83edf3e8b1c@group-CC28542BF90D: set configuration 0: peers:[d6f449c5-6fac-4083-a27e-b83edf3e8b1c|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-12 10:22:40,804 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-12 10:19:39,221 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-06-12 10:22:33,423 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:40,804 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-12 10:19:39,230 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn3_1    | 2023-06-12 10:23:22,126 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2a8b33ba] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),3(1),1001(1)], numOfContainers=2, numOfBlocks=6
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn3_1    | 2023-06-12 10:23:22,127 [DeleteBlocksCommandHandlerThread-1] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 1001
scm1_1   | 2023-06-12 10:19:39,318 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-06-12 10:22:33,423 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,805 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-12 10:23:22,127 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
scm1_1   | 2023-06-12 10:19:39,318 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 2023-06-12 10:22:38,535 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:40,807 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-12 10:23:22,128 [DeleteBlocksCommandHandlerThread-0] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 1001
scm1_1   | 2023-06-12 10:19:39,359 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:22:38,535 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,811 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-12 10:23:22,128 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 1001 <= 1001
scm1_1   | 2023-06-12 10:19:40,310 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm3_1   | 2023-06-12 10:22:40,775 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]] moved to OPEN state
dn1_1    | 2023-06-12 10:22:40,883 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-12 10:23:31,948 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-06-12 10:19:40,310 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	... 13 more
scm3_1   | 2023-06-12 10:22:43,604 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:40,884 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-12 10:24:21,081 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
scm1_1   | 2023-06-12 10:19:40,390 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-06-12 10:21:34,414 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 2023-06-12 10:22:43,604 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,884 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-12 10:24:21,081 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
scm1_1   | 2023-06-12 10:19:40,394 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-06-12 10:21:35,357 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#20-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
scm3_1   | 2023-06-12 10:22:44,831 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]] moved to OPEN state
dn1_1    | 2023-06-12 10:22:40,884 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-12 10:19:40,416 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | address: "10.9.0.17:9856"
scm3_1   | 2023-06-12 10:22:48,800 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:24:21,085 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
dn1_1    | 2023-06-12 10:22:40,884 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-12 10:19:40,498 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.17:9858"
scm3_1   | 2023-06-12 10:22:48,800 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-12 10:24:21,087 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn1_1    | 2023-06-12 10:22:40,885 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: start as a follower, conf=-1: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:40,514 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:22:53,849 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-12 10:24:21,087 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn3_1    | 2023-06-12 10:24:21,102 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 46.
scm1_1   | 2023-06-12 10:19:40,545 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | adminAddress: "10.9.0.17:9857"
scm3_1   | 2023-06-12 10:22:53,849 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,886 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-12 10:22:40,886 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn3_1    | 2023-06-12 10:24:31,950 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 12/4988 blocks from 2 candidate containers.
scm1_1   | 2023-06-12 10:19:40,615 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:22:40,890 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:22:58,866 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:22:58,866 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:23:04,054 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:40,651 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | address: "10.9.0.20:9856"
dn3_1    | 2023-06-12 10:24:32,008 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/1/chunks/111677748019200001.block
scm3_1   | 2023-06-12 10:23:04,056 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,890 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:40,713 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | priority: 1
dn3_1    | 2023-06-12 10:24:32,008 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201001.block
scm3_1   | 2023-06-12 10:23:09,173 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:40,891 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-887DB72CCE2F,id=3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:19:41,577 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.20:9858"
scm3_1   | 2023-06-12 10:23:09,173 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,892 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-12 10:19:41,577 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | adminAddress: "10.9.0.20:9857"
dn3_1    | 2023-06-12 10:24:32,014 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201002.block
dn3_1    | 2023-06-12 10:24:32,015 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201003.block
dn1_1    | 2023-06-12 10:22:40,892 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-12 10:19:41,647 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
dn3_1    | 2023-06-12 10:24:32,016 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200002.block
scm3_1   | 2023-06-12 10:23:14,191 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:40,892 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-06-12 10:19:41,657 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
dn3_1    | 2023-06-12 10:24:32,016 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200003.block
scm3_1   | 2023-06-12 10:23:14,191 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:40,892 [3c030830-a72e-4f6f-8f54-144518af6253-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-12 10:19:41,657 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | address: "10.9.0.19:9856"
dn3_1    | 2023-06-12 10:24:38,955 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-d6f449c5-6fac-4083-a27e-b83edf3e8b1c: Detected pause in JVM or host machine approximately 0.216s with 0.230s GC time.
scm3_1   | 2023-06-12 10:23:16,138 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-06-12 10:23:19,247 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:42,834 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.19:9858"
dn1_1    | 2023-06-12 10:22:40,894 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=619a7347-cd8e-4ad4-bd7f-887db72cce2f
dn1_1    | 2023-06-12 10:22:40,895 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=619a7347-cd8e-4ad4-bd7f-887db72cce2f.
dn1_1    | 2023-06-12 10:22:42,048 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@3fcbc766] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn1_1    | 2023-06-12 10:22:46,037 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO impl.FollowerState: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5150525398ns, electionTimeout:5145ms
scm3_1   | 2023-06-12 10:23:19,247 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:42,835 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | adminAddress: "10.9.0.19:9857"
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=230ms
dn1_1    | 2023-06-12 10:22:46,037 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState
scm3_1   | 2023-06-12 10:23:20,097 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-06-12 10:19:42,940 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
dn1_1    | 2023-06-12 10:22:46,037 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-12 10:22:46,037 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 2023-06-12 10:23:24,354 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:42,957 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | , old:)
dn1_1    | 2023-06-12 10:22:46,037 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-FollowerState] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5
scm3_1   | 2023-06-12 10:23:24,354 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:42,961 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:44,081 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:44,081 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:44,120 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:44,122 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:44,128 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:44,176 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn1_1    | 2023-06-12 10:22:46,039 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:23:26,195 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-06-12 10:19:44,177 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn1_1    | 2023-06-12 10:22:46,039 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
scm3_1   | 2023-06-12 10:23:29,554 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:23:29,555 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn1_1    | 2023-06-12 10:22:46,040 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-12 10:23:34,679 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:44,178 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn1_1    | 2023-06-12 10:22:46,040 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.LeaderElection: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5 ELECTION round 0: result PASSED (term=1)
scm3_1   | 2023-06-12 10:23:34,679 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:45,337 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: shutdown 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5
scm3_1   | 2023-06-12 10:23:38,074 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
scm1_1   | 2023-06-12 10:19:45,346 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-887DB72CCE2F with new leaderId: 3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:19:45,386 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm3_1   | 2023-06-12 10:23:38,159 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm3_1   | 2023-06-12 10:23:39,801 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:45,434 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm3_1   | 2023-06-12 10:23:39,801 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: change Leader from null to 3c030830-a72e-4f6f-8f54-144518af6253 at term 1 for becomeLeader, leader elected after 5308ms
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-12 10:19:45,448 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-06-12 10:23:40,868 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-06-12 10:19:45,469 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:46,596 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm3_1   | 2023-06-12 10:23:44,927 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:23:44,928 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:23:49,954 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:23:49,955 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-06-12 10:19:46,596 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-12 10:19:46,718 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-06-12 10:19:46,724 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:23:50,124 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-06-12 10:23:55,063 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-12 10:19:46,747 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:23:55,064 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:00,180 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:22:46,041 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-06-12 10:19:47,847 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:24:00,180 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:05,190 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:47,847 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:47,927 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:47,940 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn1_1    | 2023-06-12 10:22:46,042 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-06-12 10:19:47,955 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:48,002 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-06-12 10:24:05,190 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:10,209 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:10,209 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:22:46,042 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO impl.RoleInfo: 3c030830-a72e-4f6f-8f54-144518af6253: start 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderStateImpl
scm3_1   | 2023-06-12 10:24:15,256 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-06-12 10:24:15,257 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:17,062 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn1_1    | 2023-06-12 10:22:46,042 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
dn4_1    | 	... 13 more
dn4_1    | 2023-06-12 10:21:36,607 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#22-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
scm1_1   | 2023-06-12 10:19:48,015 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:22:46,047 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/619a7347-cd8e-4ad4-bd7f-887db72cce2f/current/log_inprogress_0
scm3_1   | 2023-06-12 10:24:17,086 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #1 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-06-12 10:24:17,088 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
scm3_1   | 2023-06-12 10:24:17,089 [FixedThreadPoolWithAffinityExecutor-9-0] WARN container.IncrementalContainerReportHandler: Failed to process CLOSED Container #2 due to org.apache.ratis.protocol.exceptions.NotLeaderException: Server c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244 is not the leader 9e54e886-87c9-472f-9b2f-e9a516e53bd2|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 2023-06-12 10:24:20,422 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | address: "10.9.0.17:9856"
dn1_1    | 2023-06-12 10:22:46,068 [3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F-LeaderElection5] INFO server.RaftServer$Division: 3c030830-a72e-4f6f-8f54-144518af6253@group-887DB72CCE2F: set configuration 0: peers:[3c030830-a72e-4f6f-8f54-144518af6253|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-12 10:19:48,030 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:24:20,423 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | clientAddress: "10.9.0.17:9858"
dn1_1    | 2023-06-12 10:23:31,753 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-06-12 10:19:49,099 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,099 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | adminAddress: "10.9.0.17:9857"
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
dn1_1    | 2023-06-12 10:24:17,045 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | address: "10.9.0.20:9856"
dn4_1    | priority: 1
scm3_1   | 2023-06-12 10:24:25,608 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:49,129 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,130 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:24:25,608 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:30,742 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:30,742 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:35,820 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:35,821 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:24:40,344 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn4_1    | clientAddress: "10.9.0.20:9858"
dn4_1    | adminAddress: "10.9.0.20:9857"
scm1_1   | 2023-06-12 10:19:49,131 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,154 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,155 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,158 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:49,180 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,191 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:24:17,046 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
scm1_1   | 2023-06-12 10:19:49,209 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | startupRole: FOLLOWER
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
scm1_1   | 2023-06-12 10:19:49,244 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | address: "10.9.0.19:9856"
dn4_1    | clientAddress: "10.9.0.19:9858"
scm1_1   | 2023-06-12 10:19:49,245 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | adminAddress: "10.9.0.19:9857"
dn4_1    | startupRole: FOLLOWER
dn1_1    | 2023-06-12 10:24:17,050 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
scm1_1   | 2023-06-12 10:19:49,248 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:24:40,959 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:40,959 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:24:17,052 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
scm1_1   | 2023-06-12 10:19:49,279 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:24:45,975 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:45,975 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:49,282 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,293 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:49,367 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:24:17,052 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
scm1_1   | 2023-06-12 10:19:49,367 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,377 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:49,423 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-06-12 10:24:51,085 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:51,086 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | , old:)
scm3_1   | 2023-06-12 10:24:56,273 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:24:56,276 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:25:01,279 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-12 10:24:17,072 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 46.
scm3_1   | 2023-06-12 10:25:01,279 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-12 10:25:06,464 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
dn1_1    | 2023-06-12 10:24:31,754 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 6/4994 blocks from 2 candidate containers.
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm3_1   | 2023-06-12 10:25:06,464 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
dn1_1    | 2023-06-12 10:24:31,826 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201001.block
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-06-12 10:25:11,637 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-12 10:25:11,637 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-12 10:24:31,830 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201002.block
scm3_1   | 2023-06-12 10:25:16,728 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-12 10:19:49,429 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,443 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm3_1   | 2023-06-12 10:25:16,728 [c790b820-4c4f-44e6-aba9-08b18fc50228@group-FBBF2AB68244-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-12 10:19:49,490 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,495 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:24:31,831 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201003.block
scm1_1   | 2023-06-12 10:19:49,502 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:49,557 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,578 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn1_1    | 2023-06-12 10:24:31,832 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200002.block
scm1_1   | 2023-06-12 10:19:49,581 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 2023-06-12 10:24:31,833 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200003.block
scm1_1   | 2023-06-12 10:19:49,628 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-06-12 10:24:31,833 [BlockDeletingService#0] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/1/chunks/111677748019200001.block
scm1_1   | 2023-06-12 10:19:49,635 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-06-12 10:24:34,993 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3c030830-a72e-4f6f-8f54-144518af6253: Detected pause in JVM or host machine approximately 0.142s with 0.224s GC time.
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:49,638 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn1_1    | GC pool 'ParNew' had collection(s): count=1 time=224ms
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-06-12 10:19:49,672 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm1_1   | 2023-06-12 10:19:49,685 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	... 13 more
scm1_1   | 2023-06-12 10:19:49,699 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 2023-06-12 10:21:37,859 [grpc-default-executor-9] WARN server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Failed APPEND_ENTRIES request 3c030830-a72e-4f6f-8f54-144518af6253->78f8fca8-1c45-4717-8e79-22872958dcce#24-t9,previous=(t:8, i:22),leaderCommit=23,initializing? true,entries: size=1, first=(t:9, i:23), CONFIGURATIONENTRY(current:id: "3c030830-a72e-4f6f-8f54-144518af6253"
scm1_1   | 2023-06-12 10:19:49,733 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | address: "10.9.0.17:9856"
scm1_1   | 2023-06-12 10:19:49,734 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.17:9858"
scm1_1   | 2023-06-12 10:19:49,741 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | adminAddress: "10.9.0.17:9857"
scm1_1   | 2023-06-12 10:19:49,762 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-12 10:19:49,766 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | ,id: "78f8fca8-1c45-4717-8e79-22872958dcce"
scm1_1   | 2023-06-12 10:19:49,773 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | address: "10.9.0.20:9856"
scm1_1   | 2023-06-12 10:19:49,798 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | priority: 1
scm1_1   | 2023-06-12 10:19:49,802 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,807 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:49,831 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | clientAddress: "10.9.0.20:9858"
scm1_1   | 2023-06-12 10:19:49,833 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:49,841 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | adminAddress: "10.9.0.20:9857"
scm1_1   | 2023-06-12 10:19:49,864 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-12 10:19:49,865 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | ,id: "d6f449c5-6fac-4083-a27e-b83edf3e8b1c"
scm1_1   | 2023-06-12 10:19:49,874 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | address: "10.9.0.19:9856"
dn4_1    | clientAddress: "10.9.0.19:9858"
scm1_1   | 2023-06-12 10:19:49,970 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | adminAddress: "10.9.0.19:9857"
scm1_1   | 2023-06-12 10:19:49,985 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | startupRole: FOLLOWER
scm1_1   | 2023-06-12 10:19:49,992 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | , old:)
scm1_1   | 2023-06-12 10:19:50,016 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | java.util.concurrent.CompletionException: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:19:50,019 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm1_1   | 2023-06-12 10:19:50,027 [grpc-default-executor-1] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
scm1_1   | 2023-06-12 10:19:50,050 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-12 10:19:50,053 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1099)
scm1_1   | 2023-06-12 10:19:50,055 [grpc-default-executor-3] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:637)
scm1_1   | 2023-06-12 10:19:50,090 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:234)
scm1_1   | 2023-06-12 10:19:50,093 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$1.process(GrpcServerProtocolService.java:231)
scm1_1   | 2023-06-12 10:19:50,100 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService$ServerRequestStreamObserver.onNext(GrpcServerProtocolService.java:135)
scm1_1   | 2023-06-12 10:19:50,354 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
scm1_1   | 2023-06-12 10:19:50,355 [grpc-default-executor-3] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
scm1_1   | 2023-06-12 10:19:50,415 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:333)
scm1_1   | 2023-06-12 10:19:50,415 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: decreaseNextIndex nextIndex: updateUnconditionally 75 -> 0
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:316)
scm1_1   | 2023-06-12 10:19:50,423 [grpc-default-executor-1] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:835)
scm1_1   | 2023-06-12 10:19:54,458 [grpc-default-executor-5] WARN server.GrpcLogAppender: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 73
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-06-12 10:19:54,464 [grpc-default-executor-5] INFO leader.FollowerInfo: 9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244->c790b820-4c4f-44e6-aba9-08b18fc50228: setNextIndex nextIndex: updateUnconditionally 75 -> 73
dn4_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 2023-06-12 10:20:06,030 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 2023-06-12 10:20:23,840 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 2023-06-12 10:20:23,947 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-06-12 10:20:23,947 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn4_1    | Caused by: org.apache.ratis.protocol.exceptions.GroupMismatchException: 78f8fca8-1c45-4717-8e79-22872958dcce: group-E4EF5B9B8C4A not found.
scm1_1   | 2023-06-12 10:20:23,955 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 2023-06-12 10:20:24,056 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.469751Z[UTC]] moved to CLOSED state
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 2023-06-12 10:20:24,081 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.468329Z[UTC]] moved to CLOSED state
dn4_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.appendEntriesAsync(RaftServerProxy.java:636)
scm1_1   | 2023-06-12 10:20:24,118 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a
dn4_1    | 	... 13 more
scm1_1   | 2023-06-12 10:20:24,122 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
scm1_1   | 2023-06-12 10:20:24,142 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.471690Z[UTC]] moved to CLOSED state
dn4_1    | 2023-06-12 10:21:43,170 [grpc-default-executor-9] INFO server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Completed APPEND_ENTRIES, lastRequest: d6f449c5-6fac-4083-a27e-b83edf3e8b1c->78f8fca8-1c45-4717-8e79-22872958dcce#3-t6,previous=(t:6, i:48),leaderCommit=48,initializing? true,entries: size=1, first=(t:6, i:49), METADATAENTRY(c:48)
scm1_1   | 2023-06-12 10:20:24,157 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:11.471972Z[UTC]] moved to CLOSED state
dn4_1    | 2023-06-12 10:21:43,171 [grpc-default-executor-9] INFO server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Completed APPEND_ENTRIES, lastReply: null
scm1_1   | 2023-06-12 10:20:24,182 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.472247Z[UTC]] moved to CLOSED state
dn4_1    | 2023-06-12 10:21:43,180 [grpc-default-executor-9] INFO server.GrpcServerProtocolService: 78f8fca8-1c45-4717-8e79-22872958dcce: Completed APPEND_ENTRIES, lastRequest: null
scm1_1   | 2023-06-12 10:20:24,203 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:11.472111Z[UTC]] moved to CLOSED state
dn4_1    | 2023-06-12 10:21:47,970 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:20:24,231 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca
dn4_1    | 2023-06-12 10:21:47,970 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm1_1   | 2023-06-12 10:20:24,231 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
dn4_1    | 2023-06-12 10:21:47,970 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-06-12 10:21:47,970 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm1_1   | 2023-06-12 10:20:24,253 [IPC Server handler 34 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.458412Z[UTC]] moved to CLOSED state
dn4_1    | 2023-06-12 10:21:47,973 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-06-12 10:21:47,973 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm1_1   | 2023-06-12 10:20:24,253 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
dn4_1    | 2023-06-12 10:21:47,974 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm1_1   |   New pipelines creation will remain frozen until Upgrade is finalized.
dn4_1    | 2023-06-12 10:21:47,974 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-06-12 10:21:47,974 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm1_1   | 2023-06-12 10:20:24,272 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn4_1    | 2023-06-12 10:21:47,974 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-06-12 10:21:47,974 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm1_1   | 2023-06-12 10:20:24,280 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-06-12 10:21:47,975 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:20:24,304 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-06-12 10:21:47,975 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:20:24,305 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-06-12 10:21:47,975 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:20:24,325 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-06-12 10:21:47,975 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-06-12 10:20:24,327 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-06-12 10:22:07,035 [grpc-default-executor-9] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] returns group-1B5181454F64:java.util.concurrent.CompletableFuture@14dada3e[Not completed]
scm1_1   | 2023-06-12 10:20:24,327 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-1B5181454F64:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-06-12 10:20:24,341 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-12 10:20:24,341 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-12 10:20:24,341 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db in state CLOSED which uses HEALTHY_READONLY datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm1_1   | 2023-06-12 10:20:24,341 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn4_1    | 2023-06-12 10:22:07,039 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-12 10:20:24,341 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c in state CLOSED which uses HEALTHY_READONLY datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,040 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
dn4_1    | 2023-06-12 10:22:07,040 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:22:07,040 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 78f8fca8-1c45-4717-8e79-22872958dcce. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,042 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,043 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c. This will send close commands for its containers.
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm1_1   | 2023-06-12 10:20:24,342 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm1_1   | 2023-06-12 10:20:24,344 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm1_1   | 2023-06-12 10:20:24,345 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca in state CLOSED which uses HEALTHY_READONLY datanode 3c030830-a72e-4f6f-8f54-144518af6253. This will send close commands for its containers.
scm1_1   | 2023-06-12 10:20:24,347 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-12 10:20:24,354 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:24,357 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-06-12 10:20:29,354 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:34,354 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:34,815 [IPC Server handler 52 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:20:35,053 [IPC Server handler 65 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:22:07,044 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:22:07,045 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 does not exist. Creating ...
dn4_1    | 2023-06-12 10:22:07,046 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:22:07,048 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64 has been successfully formatted.
dn4_1    | 2023-06-12 10:22:07,062 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO ratis.ContainerStateMachine: group-1B5181454F64: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-12 10:22:07,062 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:22:07,063 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-12 10:20:36,738 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:20:39,355 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:41,196 [IPC Server handler 98 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:20:44,355 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:47,032 [IPC Server handler 58 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:20:49,356 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:54,357 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:20:54,357 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-06-12 10:20:54,959 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm1_1   | 2023-06-12 10:20:54,993 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 2000 to 3000.
scm1_1   | 2023-06-12 10:20:59,358 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-06-12 10:22:07,063 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:07,063 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:22:07,063 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:22:07,071 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:07,073 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:22:07,074 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:22:07,076 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-12 10:21:04,358 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-06-12 10:21:04,852 [IPC Server handler 51 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:05,064 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:06,735 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:09,358 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:11,184 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:22:07,077 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-12 10:21:11,519 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 since it stays at CLOSED stage.
dn4_1    | 2023-06-12 10:22:07,079 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:22:07,079 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-12 10:21:11,520 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 close command to datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c
dn4_1    | 2023-06-12 10:22:07,080 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:22:07,080 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:07,082 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:07,096 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:07,097 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:07,097 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:22:07,097 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:07,097 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:07,100 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:07,100 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-12 10:22:07,100 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState
dn4_1    | 2023-06-12 10:22:07,107 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1B5181454F64,id=78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:11,535 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d89dff80-6c54-4b7d-89f4-f138dfad79a6, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.469751Z[UTC]] removed.
dn4_1    | 2023-06-12 10:22:07,110 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-12 10:21:11,544 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,544 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e close command to datanode 3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:21:11,569 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 348b71fd-34e7-470a-b568-dd6e9427813e, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:19:11.468329Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:11,570 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,570 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a close command to datanode 78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:11,570 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a close command to datanode 3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:21:11,570 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a close command to datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm1_1   | 2023-06-12 10:21:11,588 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17)d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.471690Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:11,588 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,589 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db close command to datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm1_1   | 2023-06-12 10:21:11,611 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: e44c9ee0-8daa-4eef-b191-6ec5452c34db, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:19:11.471972Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:11,613 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,613 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 close command to datanode 78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:11,632 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:19:11.472247Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:11,634 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,634 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c close command to datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm1_1   | 2023-06-12 10:21:11,650 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: f8f43c49-46f2-454a-ba47-ea217032be5c, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:19:11.472111Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:11,651 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca since it stays at CLOSED stage.
scm1_1   | 2023-06-12 10:21:11,651 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca close command to datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm1_1   | 2023-06-12 10:21:11,651 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca close command to datanode 78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:11,651 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca close command to datanode 3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:21:11,672 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Pipeline Pipeline[ Id: 2345b4fc-4790-4824-ac91-4675ae3d33ca, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:19:11.458412Z[UTC]] removed.
scm1_1   | 2023-06-12 10:21:14,359 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:17,003 [IPC Server handler 61 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:17,008 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a is not found
scm1_1   | 2023-06-12 10:21:17,008 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=fd7e2d2a-445e-4a8b-ad50-67df68c3d0f7 is not found
scm1_1   | 2023-06-12 10:21:17,009 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca is not found
scm1_1   | 2023-06-12 10:21:18,529 [IPC Server handler 89 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:18,596 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:18,607 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm1_1   | 2023-06-12 10:21:19,359 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:19,649 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:19,660 [IPC Server handler 38 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:19,661 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) reported QUASI_CLOSED replica.
scm1_1   | 2023-06-12 10:21:23,966 [IPC Server handler 61 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:23,968 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=348b71fd-34e7-470a-b568-dd6e9427813e is not found
scm1_1   | 2023-06-12 10:21:23,968 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a is not found
scm1_1   | 2023-06-12 10:21:23,969 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca is not found
scm1_1   | 2023-06-12 10:21:24,359 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:24,359 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm1_1   | 2023-06-12 10:21:29,359 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:34,360 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:34,849 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=f8f43c49-46f2-454a-ba47-ea217032be5c is not found
scm1_1   | 2023-06-12 10:21:35,036 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=e44c9ee0-8daa-4eef-b191-6ec5452c34db is not found
scm1_1   | 2023-06-12 10:21:35,564 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm1_1   | 2023-06-12 10:21:35,564 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-12 10:21:35,565 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
dn4_1    | 2023-06-12 10:22:07,110 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:22:07,110 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-12 10:22:07,111 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:22:07,116 [78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:22:07,125 [78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:22:08,384 [grpc-default-executor-9] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-F04DA08514BB:java.util.concurrent.CompletableFuture@37dc9883[Not completed]
dn4_1    | 2023-06-12 10:22:08,394 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-F04DA08514BB:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:22:08,395 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-12 10:22:08,396 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: ConfigurationManager, init=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:22:08,396 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-12 10:22:08,398 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-12 10:22:08,399 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-06-12 10:22:08,403 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:22:08,403 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:22:08,403 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:22:08,403 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:22:08,404 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:22:08,404 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:22:08,405 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb does not exist. Creating ...
dn4_1    | 2023-06-12 10:22:08,407 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:22:08,410 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb has been successfully formatted.
dn4_1    | 2023-06-12 10:22:08,422 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO ratis.ContainerStateMachine: group-F04DA08514BB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-12 10:22:08,422 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:22:08,423 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-12 10:22:08,423 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:08,423 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:22:08,424 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:22:08,424 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:08,433 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:22:08,436 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:22:08,440 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:08,440 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-12 10:22:08,441 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:22:08,442 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-12 10:22:08,442 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:22:08,443 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:08,445 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:08,464 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:08,465 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:08,465 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:22:08,465 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:08,465 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:08,472 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: start as a follower, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:08,473 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-12 10:22:08,473 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState
dn4_1    | 2023-06-12 10:22:08,479 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F04DA08514BB,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:22:08,479 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-12 10:22:08,480 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:22:08,480 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-12 10:22:08,480 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:22:08,481 [78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:22:08,488 [78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:22:11,090 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: receive requestVote(PRE_VOTE, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-1B5181454F64, 0, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:11,091 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FOLLOWER: accept PRE_VOTE from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 0 <= candidate's priority 0
scm1_1   | 2023-06-12 10:21:35,565 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-12 10:21:35,569 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f8e4ee74-e97b-4a76-a7a0-d83197229949 to datanode:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm1_1   | 2023-06-12 10:21:35,590 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]].
scm1_1   | 2023-06-12 10:21:35,603 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm1_1   | 2023-06-12 10:21:35,604 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e4d68ba5-f2e1-460a-8ae0-8c2a955bd013 to datanode:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm1_1   | 2023-06-12 10:21:35,619 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]].
scm1_1   | 2023-06-12 10:21:35,622 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 2.
scm1_1   | 2023-06-12 10:21:39,118 [IPC Server handler 91 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:39,140 [IPC Server handler 86 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:39,360 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:39,628 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:39,642 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:41,187 [IPC Server handler 95 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:41,188 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d89dff80-6c54-4b7d-89f4-f138dfad79a6 is not found
scm1_1   | 2023-06-12 10:21:41,189 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a is not found
scm1_1   | 2023-06-12 10:21:41,189 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca is not found
scm1_1   | 2023-06-12 10:21:43,102 [IPC Server handler 81 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:43,145 [IPC Server handler 86 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:43,739 [IPC Server handler 28 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:43,748 [IPC Server handler 24 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-06-12 10:21:44,360 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:49,361 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:50,566 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
scm1_1   | 2023-06-12 10:21:50,566 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-12 10:21:50,567 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=f9a89c0e-7717-47ce-8428-3f47456d10ee to datanode:78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:50,582 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]].
scm1_1   | 2023-06-12 10:21:50,595 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 to datanode:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm1_1   | 2023-06-12 10:21:50,596 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 to datanode:78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:50,596 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 to datanode:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm1_1   | 2023-06-12 10:21:50,607 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]].
scm1_1   | 2023-06-12 10:21:50,610 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb to datanode:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
scm1_1   | 2023-06-12 10:21:50,620 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb to datanode:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
scm1_1   | 2023-06-12 10:21:50,620 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb to datanode:78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:50,629 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]].
scm1_1   | 2023-06-12 10:21:50,632 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=ef429986-30ce-4e4f-bbbc-f04da08514bb contains same datanodes as previous pipelines: PipelineID=4789d6b1-e59e-49f4-be63-1b5181454f64 nodeIds: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, 78f8fca8-1c45-4717-8e79-22872958dcce
scm1_1   | 2023-06-12 10:21:50,635 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm1_1   | 2023-06-12 10:21:54,361 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:21:59,363 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:22:04,363 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:22:05,859 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4d68ba5-f2e1-460a-8ae0-8c2a955bd013, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:35.604Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:06,106 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f8e4ee74-e97b-4a76-a7a0-d83197229949, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:35.567Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:09,363 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 2023-06-12 10:22:11,542 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4789d6b1-e59e-49f4-be63-1b5181454f64, Nodes: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20)4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, CreationTimestamp2023-06-12T10:21:50.595Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:11,570 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
scm1_1   | 2023-06-12 10:22:11,571 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-12 10:22:11,571 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=619a7347-cd8e-4ad4-bd7f-887db72cce2f to datanode:3c030830-a72e-4f6f-8f54-144518af6253
scm1_1   | 2023-06-12 10:22:11,601 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]].
scm1_1   | 2023-06-12 10:22:11,603 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
scm1_1   | 2023-06-12 10:22:13,362 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ef429986-30ce-4e4f-bbbc-f04da08514bb, Nodes: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29(ha_dn2_1.ha_net/10.9.0.18)d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3(ha_dn5_1.ha_net/10.9.0.21)78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, CreationTimestamp2023-06-12T10:21:50.610Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:14,364 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
scm1_1   | 2023-06-12 10:22:14,375 [IPC Server handler 34 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm1_1   | 2023-06-12 10:22:14,570 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm1_1   | 2023-06-12 10:22:14,571 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-12 10:22:14,572 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d9b35e02-7d2f-4445-a8e2-cc28542bf90d to datanode:d6f449c5-6fac-4083-a27e-b83edf3e8b1c
scm1_1   | 2023-06-12 10:22:14,583 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]].
scm1_1   | 2023-06-12 10:22:14,584 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:22:16,769 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 18 blocks to be deleted for 3 datanodes, task elapsed time: 2ms
scm1_1   | 2023-06-12 10:22:20,982 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: f9a89c0e-7717-47ce-8428-3f47456d10ee, Nodes: 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:78f8fca8-1c45-4717-8e79-22872958dcce, CreationTimestamp2023-06-12T10:21:50.567Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:40,290 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn4_1    | 2023-06-12 10:22:11,091 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64 replies to PRE_VOTE vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64:t0, leader=null, voted=, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:11,482 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: receive requestVote(PRE_VOTE, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-1B5181454F64, 0, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:11,482 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FOLLOWER: accept PRE_VOTE from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-12 10:22:11,483 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64 replies to PRE_VOTE vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64:t0, leader=null, voted=, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:11,512 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: receive requestVote(ELECTION, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-1B5181454F64, 1, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:11,512 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FOLLOWER: accept ELECTION from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-12 10:22:11,512 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn4_1    | 2023-06-12 10:22:11,513 [grpc-default-executor-9] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState
dn4_1    | 2023-06-12 10:22:11,513 [78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState was interrupted
dn4_1    | 2023-06-12 10:22:11,513 [grpc-default-executor-9] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-FollowerState
dn4_1    | 2023-06-12 10:22:11,519 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64 replies to ELECTION vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t1. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64:t1, leader=null, voted=4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:11,700 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1B5181454F64 with new leaderId: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29
dn4_1    | 2023-06-12 10:22:11,700 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: change Leader from null to 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29 at term 1 for appendEntries, leader elected after 4658ms
dn4_1    | 2023-06-12 10:22:11,756 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread2] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:11,759 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread2] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-12 10:22:11,761 [78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-1B5181454F64-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4789d6b1-e59e-49f4-be63-1b5181454f64/current/log_inprogress_0
dn4_1    | 2023-06-12 10:22:13,225 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: receive requestVote(PRE_VOTE, 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29, group-F04DA08514BB, 0, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:13,226 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FOLLOWER: accept PRE_VOTE from 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29: our priority 0 <= candidate's priority 0
dn4_1    | 2023-06-12 10:22:13,226 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB replies to PRE_VOTE vote request: 4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB:t0, leader=null, voted=, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:13,316 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: receive requestVote(PRE_VOTE, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-F04DA08514BB, 0, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:13,316 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FOLLOWER: accept PRE_VOTE from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 0 <= candidate's priority 1
scm1_1   | 2023-06-12 10:22:40,801 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 619a7347-cd8e-4ad4-bd7f-887db72cce2f, Nodes: 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3c030830-a72e-4f6f-8f54-144518af6253, CreationTimestamp2023-06-12T10:22:11.571Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:22:44,588 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:22:44,849 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d9b35e02-7d2f-4445-a8e2-cc28542bf90d, Nodes: d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:d6f449c5-6fac-4083-a27e-b83edf3e8b1c, CreationTimestamp2023-06-12T10:22:14.572Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-12 10:23:00,734 [IPC Server handler 3 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
scm1_1   | 2023-06-12 10:23:14,590 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:23:15,569 [IPC Server handler 20 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
scm1_1   | 2023-06-12 10:23:16,769 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 12 blocks to be deleted for 2 datanodes, task elapsed time: 3ms
scm1_1   | 2023-06-12 10:23:38,062 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 1000.
scm1_1   | 2023-06-12 10:23:38,080 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 1000 to 2000.
scm1_1   | 2023-06-12 10:23:38,149 [9e54e886-87c9-472f-9b2f-e9a516e53bd2@group-FBBF2AB68244-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm1_1   | 2023-06-12 10:23:38,186 [IPC Server handler 2 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
scm1_1   | 2023-06-12 10:23:44,591 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:24:04,213 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
scm1_1   | 2023-06-12 10:24:11,816 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1 with BCSID 18, which is in QUASI_CLOSED state.
scm1_1   | 2023-06-12 10:24:11,819 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:18.618272Z, pipelineID=PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, owner=om2} to 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1686566021817 and scm deadline 1686566051817
scm1_1   | 2023-06-12 10:24:11,819 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:18.618272Z, pipelineID=PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, owner=om2} to d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1686566021819 and scm deadline 1686566051819
scm1_1   | 2023-06-12 10:24:11,819 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:18.618272Z, pipelineID=PipelineID=d9fa86df-ad94-4795-b6bd-e4ef5b9b8c4a, owner=om2} to 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1686566021819 and scm deadline 1686566051819
scm1_1   | 2023-06-12 10:24:11,820 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #2 with BCSID 46, which is in QUASI_CLOSED state.
scm1_1   | 2023-06-12 10:24:11,820 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:19.668337Z, pipelineID=PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, owner=om2} to 78f8fca8-1c45-4717-8e79-22872958dcce(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1686566021820 and scm deadline 1686566051820
scm1_1   | 2023-06-12 10:24:11,820 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:19.668337Z, pipelineID=PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, owner=om2} to d6f449c5-6fac-4083-a27e-b83edf3e8b1c(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1686566021820 and scm deadline 1686566051820
scm1_1   | 2023-06-12 10:24:11,821 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-06-12T10:21:19.668337Z, pipelineID=PipelineID=2345b4fc-4790-4824-ac91-4675ae3d33ca, owner=om2} to 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) with datanode deadline 1686566021821 and scm deadline 1686566051821
scm1_1   | 2023-06-12 10:24:11,823 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 34 milliseconds for processing 4 containers.
scm1_1   | 2023-06-12 10:24:14,594 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:24:17,079 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
scm1_1   | 2023-06-12 10:24:17,104 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode 3c030830-a72e-4f6f-8f54-144518af6253(ha_dn1_1.ha_net/10.9.0.17) reported CLOSED replica.
scm1_1   | 2023-06-12 10:24:44,597 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm1_1   | 2023-06-12 10:25:14,598 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn4_1    | 2023-06-12 10:22:13,317 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB replies to PRE_VOTE vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t0. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB:t0, leader=null, voted=, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:13,390 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: receive requestVote(ELECTION, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, group-F04DA08514BB, 1, (t:0, i:0))
dn4_1    | 2023-06-12 10:22:13,390 [grpc-default-executor-9] INFO impl.VoteContext: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FOLLOWER: accept ELECTION from d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-12 10:22:13,391 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn4_1    | 2023-06-12 10:22:13,391 [grpc-default-executor-9] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState
dn4_1    | 2023-06-12 10:22:13,391 [grpc-default-executor-9] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState
dn4_1    | 2023-06-12 10:22:13,391 [78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-FollowerState was interrupted
dn4_1    | 2023-06-12 10:22:13,395 [grpc-default-executor-9] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB replies to ELECTION vote request: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3<-78f8fca8-1c45-4717-8e79-22872958dcce#0:OK-t1. Peer's state: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB:t1, leader=null, voted=d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3, raftlog=Memoized:78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:13,525 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F04DA08514BB with new leaderId: d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3
dn4_1    | 2023-06-12 10:22:13,525 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: change Leader from null to d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3 at term 1 for appendEntries, leader elected after 5126ms
dn4_1    | 2023-06-12 10:22:13,544 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB: set configuration 0: peers:[4e75acb7-ab47-4e4e-b8a9-bad2ecf80c29|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:0|startupRole:FOLLOWER, d7cb11b5-fdfc-4a25-a96f-ccccb7dedae3|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:13,545 [78f8fca8-1c45-4717-8e79-22872958dcce-server-thread1] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-12 10:22:13,547 [78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-F04DA08514BB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ef429986-30ce-4e4f-bbbc-f04da08514bb/current/log_inprogress_0
dn4_1    | 2023-06-12 10:22:20,709 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce: new RaftServerImpl for group-3F47456D10EE:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-12 10:22:20,710 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-12 10:22:20,711 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: ConfigurationManager, init=-1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-12 10:22:20,712 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-12 10:22:20,713 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-12 10:22:20,714 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-06-12 10:22:20,716 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-12 10:22:20,717 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-12 10:22:20,717 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-12 10:22:20,717 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-12 10:22:20,718 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-12 10:22:20,718 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-12 10:22:20,721 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 78f8fca8-1c45-4717-8e79-22872958dcce: addNew group-3F47456D10EE:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns      null 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn4_1    | 2023-06-12 10:22:20,721 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/f9a89c0e-7717-47ce-8428-3f47456d10ee does not exist. Creating ...
dn4_1    | 2023-06-12 10:22:20,724 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/f9a89c0e-7717-47ce-8428-3f47456d10ee/in_use.lock acquired by nodename 7@0081bf068fd7
dn4_1    | 2023-06-12 10:22:20,738 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/f9a89c0e-7717-47ce-8428-3f47456d10ee has been successfully formatted.
dn4_1    | 2023-06-12 10:22:20,744 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO ratis.ContainerStateMachine: group-3F47456D10EE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-12 10:22:20,745 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-12 10:22:20,746 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-12 10:22:20,746 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:20,746 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-12 10:22:20,746 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-12 10:22:20,747 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:20,747 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-12 10:22:20,747 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/f9a89c0e-7717-47ce-8428-3f47456d10ee
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:20,748 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-12 10:22:20,749 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-12 10:22:20,749 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-12 10:22:20,749 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-12 10:22:20,749 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-12 10:22:20,750 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-12 10:22:20,751 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-12 10:22:20,939 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:20,939 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-12 10:22:20,939 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-12 10:22:20,940 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:20,940 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-12 10:22:20,947 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: start as a follower, conf=-1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:20,949 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-12 10:22:20,950 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState
dn4_1    | 2023-06-12 10:22:20,950 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3F47456D10EE,id=78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:22:20,950 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-12 10:22:20,951 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-12 10:22:20,951 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-12 10:22:20,951 [78f8fca8-1c45-4717-8e79-22872958dcce-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-12 10:22:20,952 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-12 10:22:20,952 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=f9a89c0e-7717-47ce-8428-3f47456d10ee
dn4_1    | 2023-06-12 10:22:20,975 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-12 10:22:20,976 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=f9a89c0e-7717-47ce-8428-3f47456d10ee.
dn4_1    | 2023-06-12 10:22:26,164 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO impl.FollowerState: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5214198502ns, electionTimeout:5188ms
dn4_1    | 2023-06-12 10:22:26,164 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState
dn4_1    | 2023-06-12 10:22:26,164 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-06-12 10:22:26,165 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-06-12 10:22:26,165 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-FollowerState] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5
dn4_1    | 2023-06-12 10:22:26,167 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:26,167 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
dn4_1    | 2023-06-12 10:22:26,169 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:26,169 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.LeaderElection: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-06-12 10:22:26,169 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: shutdown 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5
dn4_1    | 2023-06-12 10:22:26,169 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-06-12 10:22:26,169 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3F47456D10EE with new leaderId: 78f8fca8-1c45-4717-8e79-22872958dcce
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: change Leader from null to 78f8fca8-1c45-4717-8e79-22872958dcce at term 1 for becomeLeader, leader elected after 5455ms
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-06-12 10:22:26,170 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-12 10:22:26,171 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-12 10:22:26,171 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-06-12 10:22:26,171 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO impl.RoleInfo: 78f8fca8-1c45-4717-8e79-22872958dcce: start 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderStateImpl
dn4_1    | 2023-06-12 10:22:26,171 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-12 10:22:26,173 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f9a89c0e-7717-47ce-8428-3f47456d10ee/current/log_inprogress_0
dn4_1    | 2023-06-12 10:22:26,185 [78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE-LeaderElection5] INFO server.RaftServer$Division: 78f8fca8-1c45-4717-8e79-22872958dcce@group-3F47456D10EE: set configuration 0: peers:[78f8fca8-1c45-4717-8e79-22872958dcce|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-12 10:22:34,415 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-06-12 10:22:39,947 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@67fb5025] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),3(0),1001(0)], numOfContainers=2, numOfBlocks=6
dn4_1    | 2023-06-12 10:23:27,982 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@67fb5025] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),3(1),1001(1)], numOfContainers=2, numOfBlocks=6
dn4_1    | 2023-06-12 10:23:27,984 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 1001
dn4_1    | 2023-06-12 10:23:27,985 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
dn4_1    | 2023-06-12 10:23:27,985 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 3 <= 1001
dn4_1    | 2023-06-12 10:23:27,986 [DeleteBlocksCommandHandlerThread-1] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 1001 <= 1001
dn4_1    | 2023-06-12 10:23:34,416 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-06-12 10:24:27,097 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-06-12 10:24:27,097 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-06-12 10:24:27,102 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
dn4_1    | 2023-06-12 10:24:27,102 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn4_1    | 2023-06-12 10:24:27,102 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 46.
dn4_1    | 2023-06-12 10:24:27,121 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 46.
dn4_1    | 2023-06-12 10:24:34,417 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 12/4988 blocks from 2 candidate containers.
dn4_1    | 2023-06-12 10:24:34,433 [BlockDeletingService#7] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/1/chunks/111677748019200001.block
dn4_1    | 2023-06-12 10:24:34,433 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201001.block
dn4_1    | 2023-06-12 10:24:34,440 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201002.block
dn4_1    | 2023-06-12 10:24:34,442 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019201003.block
dn4_1    | 2023-06-12 10:24:34,442 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200002.block
dn4_1    | 2023-06-12 10:24:34,443 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-53a3d7a4-8139-4d99-87ad-fbbf2ab68244/current/containerDir0/2/chunks/111677748019200003.block
