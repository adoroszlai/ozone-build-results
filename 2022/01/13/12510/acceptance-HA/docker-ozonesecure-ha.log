Attaching to ozonesecure-ha_kms_1, ozonesecure-ha_s3g_1, ozonesecure-ha_datanode2_1, ozonesecure-ha_kdc_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_om2_1, ozonesecure-ha_datanode1_1, ozonesecure-ha_om3_1, ozonesecure-ha_datanode3_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_om1_1, ozonesecure-ha_recon_1, ozonesecure-ha_scm3.org_1
datanode1_1  | Sleeping for 5 seconds
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2022-01-13 06:24:32,087 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 783c9b118d1a/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
datanode1_1  | STARTUP_MSG:   java = 11.0.10
datanode1_1  | ************************************************************/
datanode1_1  | 2022-01-13 06:24:32,172 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2022-01-13 06:24:33,838 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2022-01-13 06:24:34,724 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2022-01-13 06:24:35,716 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2022-01-13 06:24:35,717 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2022-01-13 06:24:36,857 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:783c9b118d1a ip:172.25.0.102
datanode1_1  | 2022-01-13 06:24:40,579 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2022-01-13 06:24:41,823 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2022-01-13 06:24:41,824 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2022-01-13 06:24:44,284 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2022-01-13 06:24:44,291 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2022-01-13 06:24:44,291 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2022-01-13 06:24:44,296 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2022-01-13 06:24:48,027 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2022-01-13 06:24:48,127 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:783c9b118d1a
datanode1_1  | 2022-01-13 06:24:48,127 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2022-01-13 06:24:48,187 [main] ERROR client.DNCertificateClient: Invalid domain 783c9b118d1a
datanode1_1  | 2022-01-13 06:24:48,191 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@783c9b118d1a
datanode1_1  | 2022-01-13 06:24:54,843 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode1_1  | 2022-01-13 06:24:54,937 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/977357248554.crt.
datanode1_1  | 2022-01-13 06:24:54,968 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-882210938510.crt.
datanode1_1  | 2022-01-13 06:24:54,997 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2022-01-13 06:24:55,011 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2022-01-13 06:24:55,152 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode1_1  | 2022-01-13 06:24:56,085 [main] INFO reflections.Reflections: Reflections took 633 ms to scan 2 urls, producing 85 keys and 173 values 
datanode1_1  | 2022-01-13 06:24:56,600 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2022-01-13 06:24:58,077 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2022-01-13 06:24:58,192 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode1_1  | 2022-01-13 06:24:58,220 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2022-01-13 06:24:58,221 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2022-01-13 06:24:58,393 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2022-01-13 06:24:58,558 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2022-01-13 06:24:58,572 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2022-01-13 06:24:58,573 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2022-01-13 06:24:58,573 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2022-01-13 06:24:58,598 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2022-01-13 06:24:58,757 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2022-01-13 06:24:58,779 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2022-01-13 06:25:09,425 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2022-01-13 06:25:10,227 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2022-01-13 06:25:11,126 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2022-01-13 06:25:11,126 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2022-01-13 06:25:11,178 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2022-01-13 06:25:11,179 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2022-01-13 06:25:11,180 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2022-01-13 06:25:11,180 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2022-01-13 06:25:11,181 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2022-01-13 06:25:25,180 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode1_1  | 2022-01-13 06:25:25,226 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:25:25,230 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2022-01-13 06:25:25,380 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2022-01-13 06:25:26,968 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode1_1  | 2022-01-13 06:25:29,288 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | Sleeping for 5 seconds
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2022-01-13 06:24:31,444 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = e367b1121f80/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
datanode2_1  | STARTUP_MSG:   java = 11.0.10
datanode2_1  | ************************************************************/
datanode2_1  | 2022-01-13 06:24:31,594 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2022-01-13 06:24:33,943 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2022-01-13 06:24:34,573 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2022-01-13 06:24:35,328 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2022-01-13 06:24:35,330 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2022-01-13 06:24:36,362 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e367b1121f80 ip:172.25.0.103
datanode2_1  | 2022-01-13 06:24:39,938 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2022-01-13 06:24:41,251 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2022-01-13 06:24:41,251 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2022-01-13 06:24:43,916 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2022-01-13 06:24:43,927 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2022-01-13 06:24:43,927 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2022-01-13 06:24:43,929 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2022-01-13 06:24:50,956 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2022-01-13 06:24:51,072 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:e367b1121f80
datanode2_1  | 2022-01-13 06:24:51,073 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2022-01-13 06:24:51,101 [main] ERROR client.DNCertificateClient: Invalid domain e367b1121f80
datanode2_1  | 2022-01-13 06:24:51,110 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@e367b1121f80
datanode2_1  | 2022-01-13 06:24:56,794 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode2_1  | 2022-01-13 06:24:56,920 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-882210938510.crt.
datanode2_1  | 2022-01-13 06:24:56,930 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/979457010743.crt.
datanode2_1  | 2022-01-13 06:24:56,955 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2022-01-13 06:24:56,986 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2022-01-13 06:24:57,059 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode2_1  | 2022-01-13 06:24:57,990 [main] INFO reflections.Reflections: Reflections took 694 ms to scan 2 urls, producing 85 keys and 173 values 
datanode2_1  | 2022-01-13 06:24:58,455 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode1_1  | 2022-01-13 06:25:29,288 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2022-01-13 06:25:29,288 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2022-01-13 06:25:29,573 [main] INFO util.log: Logging initialized @66534ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2022-01-13 06:25:30,893 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2022-01-13 06:25:31,040 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2022-01-13 06:25:31,057 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2022-01-13 06:25:31,057 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2022-01-13 06:25:31,057 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2022-01-13 06:25:31,093 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2022-01-13 06:25:31,533 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2022-01-13 06:25:31,540 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
datanode1_1  | 2022-01-13 06:25:31,787 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2022-01-13 06:25:31,787 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2022-01-13 06:25:31,849 [main] INFO server.session: node0 Scavenging every 660000ms
datanode1_1  | 2022-01-13 06:25:32,040 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2022-01-13 06:25:32,097 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bb7fc38{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2022-01-13 06:25:32,170 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@76a1cc1a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2022-01-13 06:25:33,550 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2022-01-13 06:25:33,845 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@35920655{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0-SNAPSHOT_jar-_-any-113276934861828427/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2022-01-13 06:25:33,979 [main] INFO server.AbstractConnector: Started ServerConnector@668ea404{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2022-01-13 06:25:33,984 [main] INFO server.Server: Started @70945ms
datanode1_1  | 2022-01-13 06:25:34,035 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2022-01-13 06:25:34,035 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2022-01-13 06:25:34,042 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2022-01-13 06:25:34,087 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode1_1  | 2022-01-13 06:25:34,352 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@d68671f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode1_1  | 2022-01-13 06:25:35,181 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2022-01-13 06:25:40,385 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2022-01-13 06:25:40,445 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode1_1  | 2022-01-13 06:25:42,195 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis ab6e22bd-d408-4246-98c0-61fe3297cb52
datanode1_1  | 2022-01-13 06:25:42,498 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: ab6e22bd-d408-4246-98c0-61fe3297cb52: start RPC server
datanode1_1  | 2022-01-13 06:25:42,531 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: ab6e22bd-d408-4246-98c0-61fe3297cb52: GrpcService started, listening on 9856
datanode1_1  | 2022-01-13 06:25:42,544 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: ab6e22bd-d408-4246-98c0-61fe3297cb52: GrpcService started, listening on 9857
datanode1_1  | 2022-01-13 06:25:42,554 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: ab6e22bd-d408-4246-98c0-61fe3297cb52: GrpcService started, listening on 9858
datanode1_1  | 2022-01-13 06:25:42,540 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode1_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:633)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:283)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:471)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: java.util.concurrent.TimeoutException
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode2_1  | 2022-01-13 06:24:59,737 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2022-01-13 06:24:59,792 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode2_1  | 2022-01-13 06:24:59,813 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2022-01-13 06:24:59,823 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2022-01-13 06:25:00,123 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2022-01-13 06:25:00,234 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2022-01-13 06:25:00,252 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2022-01-13 06:25:00,252 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2022-01-13 06:25:00,252 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2022-01-13 06:25:00,253 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2022-01-13 06:25:00,412 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2022-01-13 06:25:00,413 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2022-01-13 06:25:11,904 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2022-01-13 06:25:12,613 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2022-01-13 06:25:13,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2022-01-13 06:25:13,867 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2022-01-13 06:25:13,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2022-01-13 06:25:13,869 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2022-01-13 06:25:13,892 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:25:13,892 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2022-01-13 06:25:13,893 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2022-01-13 06:25:26,272 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode2_1  | 2022-01-13 06:25:26,328 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:25:26,333 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2022-01-13 06:25:26,498 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2022-01-13 06:25:28,384 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode2_1  | 2022-01-13 06:25:31,384 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2022-01-13 06:25:31,394 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2022-01-13 06:25:31,396 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2022-01-13 06:25:31,837 [main] INFO util.log: Logging initialized @69733ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2022-01-13 06:25:33,444 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2022-01-13 06:25:33,486 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2022-01-13 06:25:33,503 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2022-01-13 06:25:33,506 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2022-01-13 06:25:33,506 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2022-01-13 06:25:33,528 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2022-01-13 06:25:33,944 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2022-01-13 06:25:33,967 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
datanode2_1  | 2022-01-13 06:25:34,332 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2022-01-13 06:25:34,359 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2022-01-13 06:25:34,364 [main] INFO server.session: node0 Scavenging every 660000ms
datanode2_1  | 2022-01-13 06:25:34,581 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2022-01-13 06:25:34,602 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e3c4c12{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2022-01-13 06:25:34,688 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@21252bef{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2022-01-13 06:25:35,906 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2022-01-13 06:25:36,132 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@41c05747{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0-SNAPSHOT_jar-_-any-15475581790041087031/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2022-01-13 06:25:36,267 [main] INFO server.AbstractConnector: Started ServerConnector@34d644b5{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2022-01-13 06:25:36,280 [main] INFO server.Server: Started @74177ms
datanode2_1  | 2022-01-13 06:25:36,287 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2022-01-13 06:25:36,287 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2022-01-13 06:25:36,292 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2022-01-13 06:25:36,299 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode2_1  | 2022-01-13 06:25:36,541 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c5d73a6] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode2_1  | 2022-01-13 06:25:37,398 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2022-01-13 06:25:40,385 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2022-01-13 06:25:40,392 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode2_1  | 2022-01-13 06:25:41,840 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:25:42,144 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: b1a62a11-0048-42bf-a9c3-836e0373e878: start RPC server
datanode2_1  | 2022-01-13 06:25:42,172 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: b1a62a11-0048-42bf-a9c3-836e0373e878: GrpcService started, listening on 9856
datanode2_1  | 2022-01-13 06:25:42,190 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: b1a62a11-0048-42bf-a9c3-836e0373e878: GrpcService started, listening on 9857
datanode2_1  | 2022-01-13 06:25:42,207 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: b1a62a11-0048-42bf-a9c3-836e0373e878: GrpcService started, listening on 9858
datanode2_1  | 2022-01-13 06:25:42,317 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1a62a11-0048-42bf-a9c3-836e0373e878 is started using port 9858 for RATIS
datanode2_1  | 2022-01-13 06:25:42,327 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1a62a11-0048-42bf-a9c3-836e0373e878 is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2022-01-13 06:25:42,327 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b1a62a11-0048-42bf-a9c3-836e0373e878 is started using port 9856 for RATIS_SERVER
datanode2_1  | 2022-01-13 06:25:42,375 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$347/0x00000008405ce040@21f4e3cd] INFO util.JvmPauseMonitor: JvmPauseMonitor-b1a62a11-0048-42bf-a9c3-836e0373e878: Started
datanode2_1  | 2022-01-13 06:25:42,379 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2022-01-13 06:25:42,393 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2022-01-13 06:26:43,856 [Command processor thread] INFO server.RaftServer: b1a62a11-0048-42bf-a9c3-836e0373e878: addNew group-318A18735718:[b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] returns group-318A18735718:java.util.concurrent.CompletableFuture@54f92541[Not completed]
datanode2_1  | 2022-01-13 06:26:44,007 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878: new RaftServerImpl for group-318A18735718:[b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] with ContainerStateMachine:uninitialized
datanode2_1  | 2022-01-13 06:26:44,025 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2022-01-13 06:26:44,030 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2022-01-13 06:26:44,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2022-01-13 06:26:44,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:44,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2022-01-13 06:26:44,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2022-01-13 06:26:44,058 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2022-01-13 06:26:44,113 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: ConfigurationManager, init=-1: [b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2022-01-13 06:26:44,114 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2022-01-13 06:26:44,306 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2022-01-13 06:26:44,306 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2022-01-13 06:26:44,395 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9cb3b3c0-f3d5-4c7d-a7e4-318a18735718 does not exist. Creating ...
datanode2_1  | 2022-01-13 06:26:44,487 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9cb3b3c0-f3d5-4c7d-a7e4-318a18735718/in_use.lock acquired by nodename 7@e367b1121f80
datanode2_1  | 2022-01-13 06:26:44,552 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9cb3b3c0-f3d5-4c7d-a7e4-318a18735718 has been successfully formatted.
datanode2_1  | 2022-01-13 06:26:44,652 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-318A18735718: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2022-01-13 06:26:44,654 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:44,712 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2022-01-13 06:26:44,838 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2022-01-13 06:26:44,839 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:26:44,970 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:45,082 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2022-01-13 06:26:45,110 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2022-01-13 06:26:45,139 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9cb3b3c0-f3d5-4c7d-a7e4-318a18735718
datanode2_1  | 2022-01-13 06:26:45,180 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	... 1 more
datanode1_1  | 2022-01-13 06:25:42,734 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ab6e22bd-d408-4246-98c0-61fe3297cb52 is started using port 9858 for RATIS
datanode1_1  | 2022-01-13 06:25:42,734 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ab6e22bd-d408-4246-98c0-61fe3297cb52 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2022-01-13 06:25:42,734 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis ab6e22bd-d408-4246-98c0-61fe3297cb52 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2022-01-13 06:25:42,747 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$347/0x00000008405cec40@501e0313] INFO util.JvmPauseMonitor: JvmPauseMonitor-ab6e22bd-d408-4246-98c0-61fe3297cb52: Started
datanode1_1  | 2022-01-13 06:25:42,952 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2022-01-13 06:25:42,953 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2022-01-13 06:26:43,659 [Command processor thread] INFO server.RaftServer: ab6e22bd-d408-4246-98c0-61fe3297cb52: addNew group-448797766036:[ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1] returns group-448797766036:java.util.concurrent.CompletableFuture@3b0eb755[Not completed]
datanode1_1  | 2022-01-13 06:26:43,800 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52: new RaftServerImpl for group-448797766036:[ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1] with ContainerStateMachine:uninitialized
datanode1_1  | 2022-01-13 06:26:43,829 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2022-01-13 06:26:43,834 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2022-01-13 06:26:43,834 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2022-01-13 06:26:43,834 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:43,834 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2022-01-13 06:26:43,834 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2022-01-13 06:26:43,837 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2022-01-13 06:26:43,860 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: ConfigurationManager, init=-1: [ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2022-01-13 06:26:43,871 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2022-01-13 06:26:43,949 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2022-01-13 06:26:43,955 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2022-01-13 06:26:43,976 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2b9e81e9-4435-4567-af0a-448797766036 does not exist. Creating ...
datanode1_1  | 2022-01-13 06:26:44,028 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2b9e81e9-4435-4567-af0a-448797766036/in_use.lock acquired by nodename 6@783c9b118d1a
datanode1_1  | 2022-01-13 06:26:44,145 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2b9e81e9-4435-4567-af0a-448797766036 has been successfully formatted.
datanode1_1  | 2022-01-13 06:26:44,229 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-448797766036: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2022-01-13 06:26:44,239 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:44,342 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2022-01-13 06:26:44,511 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2022-01-13 06:26:44,532 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2022-01-13 06:26:44,764 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:44,877 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2022-01-13 06:26:44,878 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2022-01-13 06:26:44,957 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/2b9e81e9-4435-4567-af0a-448797766036
datanode1_1  | 2022-01-13 06:26:44,958 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2022-01-13 06:26:44,958 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2022-01-13 06:26:44,966 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:44,967 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2022-01-13 06:26:44,971 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2022-01-13 06:26:44,973 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2022-01-13 06:26:44,973 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2022-01-13 06:26:44,973 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2022-01-13 06:26:45,037 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:45,180 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:26:45,192 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:45,192 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2022-01-13 06:26:45,192 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2022-01-13 06:26:45,194 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2022-01-13 06:26:45,194 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2022-01-13 06:26:45,194 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2022-01-13 06:26:45,269 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:45,281 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:45,344 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:45,344 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:45,391 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:45,395 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2022-01-13 06:26:45,396 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2022-01-13 06:26:45,397 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2022-01-13 06:26:45,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2022-01-13 06:26:45,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2022-01-13 06:26:45,862 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: start as a follower, conf=-1: [b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:45,869 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2022-01-13 06:26:45,895 [pool-23-thread-1] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState
datanode2_1  | 2022-01-13 06:26:45,914 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-318A18735718,id=b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:26:45,989 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=9cb3b3c0-f3d5-4c7d-a7e4-318a18735718
datanode2_1  | 2022-01-13 06:26:46,006 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9cb3b3c0-f3d5-4c7d-a7e4-318a18735718.
datanode2_1  | 2022-01-13 06:26:46,020 [Command processor thread] INFO server.RaftServer: b1a62a11-0048-42bf-a9c3-836e0373e878: addNew group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] returns group-7A75614A6B11:java.util.concurrent.CompletableFuture@47cb9db8[Not completed]
datanode1_1  | 2022-01-13 06:26:45,038 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2022-01-13 06:26:45,064 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:45,064 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:45,089 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2022-01-13 06:26:45,091 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2022-01-13 06:26:45,092 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2022-01-13 06:26:45,092 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2022-01-13 06:26:45,099 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2022-01-13 06:26:45,100 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2022-01-13 06:26:45,498 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: start as a follower, conf=-1: [ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:45,500 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2022-01-13 06:26:45,531 [pool-23-thread-1] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState
datanode1_1  | 2022-01-13 06:26:45,552 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-448797766036,id=ab6e22bd-d408-4246-98c0-61fe3297cb52
datanode1_1  | 2022-01-13 06:26:45,642 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=2b9e81e9-4435-4567-af0a-448797766036
datanode1_1  | 2022-01-13 06:26:45,646 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=2b9e81e9-4435-4567-af0a-448797766036.
datanode1_1  | 2022-01-13 06:26:45,659 [Command processor thread] INFO server.RaftServer: ab6e22bd-d408-4246-98c0-61fe3297cb52: addNew group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] returns group-7A75614A6B11:java.util.concurrent.CompletableFuture@2ff8a7cc[Not completed]
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52: new RaftServerImpl for group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] with ContainerStateMachine:uninitialized
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2022-01-13 06:26:45,674 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2022-01-13 06:26:45,675 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2022-01-13 06:26:45,690 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2022-01-13 06:26:45,691 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2022-01-13 06:26:45,692 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 does not exist. Creating ...
datanode1_1  | 2022-01-13 06:26:45,707 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11/in_use.lock acquired by nodename 6@783c9b118d1a
datanode1_1  | 2022-01-13 06:26:45,710 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 has been successfully formatted.
datanode1_1  | 2022-01-13 06:26:45,752 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-7A75614A6B11: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2022-01-13 06:26:45,757 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:45,784 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2022-01-13 06:26:45,811 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2022-01-13 06:26:45,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2022-01-13 06:26:45,813 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:45,815 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | Sleeping for 5 seconds
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2022-01-13 06:24:31,501 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = a47071aed28b/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
datanode3_1  | STARTUP_MSG:   java = 11.0.10
datanode3_1  | ************************************************************/
datanode3_1  | 2022-01-13 06:24:31,626 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2022-01-13 06:24:33,892 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2022-01-13 06:24:34,620 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2022-01-13 06:24:35,555 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2022-01-13 06:24:35,555 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2022-01-13 06:24:36,968 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:a47071aed28b ip:172.25.0.104
datanode3_1  | 2022-01-13 06:24:41,081 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2022-01-13 06:24:42,329 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2022-01-13 06:24:42,351 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2022-01-13 06:24:44,826 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2022-01-13 06:24:44,837 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2022-01-13 06:24:44,841 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2022-01-13 06:24:44,842 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2022-01-13 06:24:49,286 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2022-01-13 06:24:49,371 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:a47071aed28b
datanode3_1  | 2022-01-13 06:24:49,385 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2022-01-13 06:24:49,405 [main] ERROR client.DNCertificateClient: Invalid domain a47071aed28b
datanode3_1  | 2022-01-13 06:24:49,410 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@a47071aed28b
datanode3_1  | 2022-01-13 06:24:55,141 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode3_1  | 2022-01-13 06:24:55,231 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-882210938510.crt.
datanode3_1  | 2022-01-13 06:24:55,250 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/978106138862.crt.
datanode3_1  | 2022-01-13 06:24:55,277 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2022-01-13 06:24:55,289 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2022-01-13 06:24:55,393 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878: new RaftServerImpl for group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] with ContainerStateMachine:uninitialized
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2022-01-13 06:26:46,042 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2022-01-13 06:26:46,057 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2022-01-13 06:26:46,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2022-01-13 06:26:46,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2022-01-13 06:26:46,057 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2022-01-13 06:26:46,057 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 does not exist. Creating ...
datanode2_1  | 2022-01-13 06:26:46,073 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11/in_use.lock acquired by nodename 7@e367b1121f80
datanode2_1  | 2022-01-13 06:26:46,084 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 has been successfully formatted.
datanode2_1  | 2022-01-13 06:26:46,084 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-7A75614A6B11: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2022-01-13 06:26:46,084 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:46,084 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2022-01-13 06:26:46,084 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:26:46,085 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:46,111 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2022-01-13 06:26:46,169 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2022-01-13 06:26:46,169 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2022-01-13 06:26:46,173 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2022-01-13 06:26:46,174 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2022-01-13 06:26:46,176 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:46,188 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:46,190 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:46,191 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:46,203 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:46,203 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2022-01-13 06:26:46,204 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2022-01-13 06:26:46,204 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2022-01-13 06:26:46,204 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2022-01-13 06:26:46,204 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2022-01-13 06:26:46,205 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:46,206 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2022-01-13 06:24:56,364 [main] INFO reflections.Reflections: Reflections took 675 ms to scan 2 urls, producing 85 keys and 173 values 
datanode3_1  | 2022-01-13 06:24:56,806 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
datanode3_1  | 2022-01-13 06:24:58,028 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2022-01-13 06:24:58,083 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode3_1  | 2022-01-13 06:24:58,147 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2022-01-13 06:24:58,149 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2022-01-13 06:24:58,379 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2022-01-13 06:24:58,581 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2022-01-13 06:24:58,583 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2022-01-13 06:24:58,603 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2022-01-13 06:24:58,603 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2022-01-13 06:24:58,610 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2022-01-13 06:24:58,750 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2022-01-13 06:24:58,769 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2022-01-13 06:25:08,912 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2022-01-13 06:25:09,627 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2022-01-13 06:25:11,081 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2022-01-13 06:25:11,090 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2022-01-13 06:25:11,094 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2022-01-13 06:25:11,119 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2022-01-13 06:25:11,120 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2022-01-13 06:25:11,166 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2022-01-13 06:25:11,167 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2022-01-13 06:25:23,581 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode3_1  | 2022-01-13 06:25:23,598 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:25:23,608 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2022-01-13 06:25:23,777 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2022-01-13 06:25:25,438 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode3_1  | 2022-01-13 06:25:29,116 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2022-01-13 06:25:29,117 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2022-01-13 06:25:29,118 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2022-01-13 06:25:29,493 [main] INFO util.log: Logging initialized @66827ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2022-01-13 06:25:31,060 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2022-01-13 06:25:31,147 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2022-01-13 06:25:31,210 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2022-01-13 06:25:31,210 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2022-01-13 06:25:31,210 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2022-01-13 06:25:31,234 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2022-01-13 06:25:31,630 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2022-01-13 06:25:31,684 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
datanode3_1  | 2022-01-13 06:25:32,046 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2022-01-13 06:25:32,047 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2022-01-13 06:25:32,056 [main] INFO server.session: node0 Scavenging every 600000ms
datanode3_1  | 2022-01-13 06:25:32,321 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2022-01-13 06:25:32,388 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5b2c9a69{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2022-01-13 06:25:32,411 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@951053f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2022-01-13 06:25:33,567 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2022-01-13 06:25:33,814 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4b59a1c1{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0-SNAPSHOT_jar-_-any-11302773718725232380/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2022-01-13 06:25:33,946 [main] INFO server.AbstractConnector: Started ServerConnector@299f9a81{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2022-01-13 06:25:33,968 [main] INFO server.Server: Started @71360ms
datanode3_1  | 2022-01-13 06:25:33,983 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2022-01-13 06:25:33,987 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2022-01-13 06:25:33,991 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2022-01-13 06:25:34,030 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode3_1  | 2022-01-13 06:25:34,452 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@217b1e19] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode1_1  | 2022-01-13 06:26:45,817 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:45,818 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:45,818 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2022-01-13 06:26:45,818 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2022-01-13 06:26:45,818 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2022-01-13 06:26:45,819 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2022-01-13 06:26:45,819 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2022-01-13 06:26:45,827 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2022-01-13 06:26:45,829 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:45,829 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2022-01-13 06:26:45,829 [pool-23-thread-1] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:26:45,856 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A75614A6B11,id=ab6e22bd-d408-4246-98c0-61fe3297cb52
datanode1_1  | 2022-01-13 06:26:45,871 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11
datanode1_1  | 2022-01-13 06:26:50,662 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState] INFO impl.FollowerState: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5131327545ns, electionTimeout:5073ms
datanode1_1  | 2022-01-13 06:26:50,662 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState
datanode1_1  | 2022-01-13 06:26:50,672 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2022-01-13 06:26:50,685 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2022-01-13 06:26:50,685 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-FollowerState] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1
datanode1_1  | 2022-01-13 06:26:50,786 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO impl.LeaderElection: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:50,800 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO impl.LeaderElection: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2022-01-13 06:26:50,801 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1
datanode1_1  | 2022-01-13 06:26:50,801 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2022-01-13 06:26:50,801 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-448797766036 with new leaderId: ab6e22bd-d408-4246-98c0-61fe3297cb52
datanode1_1  | 2022-01-13 06:26:50,801 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: change Leader from null to ab6e22bd-d408-4246-98c0-61fe3297cb52 at term 1 for becomeLeader, leader elected after 6563ms
datanode1_1  | 2022-01-13 06:26:50,999 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2022-01-13 06:26:51,002 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172953346ns, electionTimeout:5146ms
datanode1_1  | 2022-01-13 06:26:51,045 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:26:51,046 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2022-01-13 06:26:51,046 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2022-01-13 06:26:51,046 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2
datanode1_1  | 2022-01-13 06:26:51,160 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2022-01-13 06:26:51,163 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:51,189 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode1_1  | 2022-01-13 06:26:51,289 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2022-01-13 06:26:46,211 [pool-23-thread-1] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:26:46,215 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A75614A6B11,id=b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:26:46,231 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11
datanode2_1  | 2022-01-13 06:26:51,067 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState] INFO impl.FollowerState: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5171724567ns, electionTimeout:5151ms
datanode2_1  | 2022-01-13 06:26:51,067 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState
datanode2_1  | 2022-01-13 06:26:51,069 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2022-01-13 06:26:51,114 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2022-01-13 06:26:51,114 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1
datanode2_1  | 2022-01-13 06:26:51,175 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:51,177 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode2_1  | 2022-01-13 06:26:51,183 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1
datanode2_1  | 2022-01-13 06:26:51,187 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2022-01-13 06:26:51,187 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-318A18735718 with new leaderId: b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:26:51,187 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: change Leader from null to b1a62a11-0048-42bf-a9c3-836e0373e878 at term 1 for becomeLeader, leader elected after 6534ms
datanode2_1  | 2022-01-13 06:26:51,270 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058974205ns, electionTimeout:5053ms
datanode2_1  | 2022-01-13 06:26:51,282 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:26:51,283 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2022-01-13 06:26:51,283 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2022-01-13 06:26:51,283 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2
datanode2_1  | 2022-01-13 06:26:51,341 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2022-01-13 06:26:51,405 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:51,504 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:26:51,575 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2022-01-13 06:26:51,669 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2022-01-13 06:26:51,693 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2022-01-13 06:26:51,693 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2022-01-13 06:26:51,850 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:26:51,874 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2022-01-13 06:26:51,926 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderStateImpl
datanode2_1  | 2022-01-13 06:26:52,125 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2022-01-13 06:26:52,488 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-LeaderElection1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718: set configuration 0: [b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:53,232 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-7A75614A6B11, 1, (t:0, i:0))
datanode2_1  | 2022-01-13 06:26:53,257 [grpc-default-executor-0] INFO impl.VoteContext: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-CANDIDATE: reject ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: already has voted for b1a62a11-0048-42bf-a9c3-836e0373e878 at current term 1
datanode2_1  | 2022-01-13 06:26:53,373 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11 replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t1. Peer's state: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11:t1, leader=null, voted=b1a62a11-0048-42bf-a9c3-836e0373e878, raftlog=b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:53,518 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-318A18735718-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9cb3b3c0-f3d5-4c7d-a7e4-318a18735718/current/log_inprogress_0
datanode2_1  | 2022-01-13 06:26:53,928 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: receive requestVote(ELECTION, ab6e22bd-d408-4246-98c0-61fe3297cb52, group-7A75614A6B11, 1, (t:0, i:0))
datanode2_1  | 2022-01-13 06:26:53,951 [grpc-default-executor-0] INFO impl.VoteContext: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-CANDIDATE: reject ELECTION from ab6e22bd-d408-4246-98c0-61fe3297cb52: already has voted for b1a62a11-0048-42bf-a9c3-836e0373e878 at current term 1
datanode2_1  | 2022-01-13 06:26:53,952 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11 replies to ELECTION vote request: ab6e22bd-d408-4246-98c0-61fe3297cb52<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t1. Peer's state: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11:t1, leader=null, voted=b1a62a11-0048-42bf-a9c3-836e0373e878, raftlog=b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:26:54,442 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode2_1  | 2022-01-13 06:26:54,442 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection:   Response 0: b1a62a11-0048-42bf-a9c3-836e0373e878<-22377b14-877d-4b48-97e5-336a98a1cf76#0:FAIL-t1
datanode2_1  | 2022-01-13 06:26:54,442 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection:   Response 1: b1a62a11-0048-42bf-a9c3-836e0373e878<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1
datanode2_1  | 2022-01-13 06:26:54,444 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2 ELECTION round 0: result REJECTED
datanode2_1  | 2022-01-13 06:26:54,447 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode2_1  | 2022-01-13 06:26:54,472 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2
datanode2_1  | 2022-01-13 06:26:54,472 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:26:54,903 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11.
datanode2_1  | 2022-01-13 06:26:54,913 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878: new RaftServerImpl for group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] with ContainerStateMachine:uninitialized
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2022-01-13 06:26:54,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode2_1  | 2022-01-13 06:26:54,915 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e does not exist. Creating ...
datanode2_1  | 2022-01-13 06:26:54,918 [Command processor thread] INFO server.RaftServer: b1a62a11-0048-42bf-a9c3-836e0373e878: addNew group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] returns group-408A10C34F6E:java.util.concurrent.CompletableFuture@6971de26[Not completed]
datanode3_1  | 2022-01-13 06:25:35,349 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2022-01-13 06:25:40,420 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2022-01-13 06:25:40,453 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode3_1  | 2022-01-13 06:25:41,949 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 22377b14-877d-4b48-97e5-336a98a1cf76
datanode3_1  | 2022-01-13 06:25:42,228 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.RaftServer: 22377b14-877d-4b48-97e5-336a98a1cf76: start RPC server
datanode3_1  | 2022-01-13 06:25:42,242 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 22377b14-877d-4b48-97e5-336a98a1cf76: GrpcService started, listening on 9856
datanode3_1  | 2022-01-13 06:25:42,260 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 22377b14-877d-4b48-97e5-336a98a1cf76: GrpcService started, listening on 9857
datanode3_1  | 2022-01-13 06:25:42,261 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO server.GrpcService: 22377b14-877d-4b48-97e5-336a98a1cf76: GrpcService started, listening on 9858
datanode3_1  | 2022-01-13 06:25:42,393 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22377b14-877d-4b48-97e5-336a98a1cf76 is started using port 9858 for RATIS
datanode3_1  | 2022-01-13 06:25:42,394 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22377b14-877d-4b48-97e5-336a98a1cf76 is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2022-01-13 06:25:42,394 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22377b14-877d-4b48-97e5-336a98a1cf76 is started using port 9856 for RATIS_SERVER
datanode3_1  | 2022-01-13 06:25:42,394 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$347/0x00000008405ce040@2e66cda4] INFO util.JvmPauseMonitor: JvmPauseMonitor-22377b14-877d-4b48-97e5-336a98a1cf76: Started
datanode3_1  | 2022-01-13 06:25:42,462 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2022-01-13 06:25:42,463 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2022-01-13 06:26:12,650 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$347/0x00000008405ce040@2e66cda4] WARN util.JvmPauseMonitor: JvmPauseMonitor-22377b14-877d-4b48-97e5-336a98a1cf76: Detected pause in JVM or host machine (eg GC): pause of approximately 104303657ns.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=104ms
datanode3_1  | 2022-01-13 06:26:43,569 [Command processor thread] INFO server.RaftServer: 22377b14-877d-4b48-97e5-336a98a1cf76: addNew group-C03134748096:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1] returns group-C03134748096:java.util.concurrent.CompletableFuture@690832d5[Not completed]
datanode3_1  | 2022-01-13 06:26:43,656 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76: new RaftServerImpl for group-C03134748096:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1] with ContainerStateMachine:uninitialized
datanode3_1  | 2022-01-13 06:26:43,676 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2022-01-13 06:26:43,676 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2022-01-13 06:26:43,679 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2022-01-13 06:26:43,680 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:43,683 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2022-01-13 06:26:43,683 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2022-01-13 06:26:43,693 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2022-01-13 06:26:43,725 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2022-01-13 06:26:43,725 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2022-01-13 06:26:43,777 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2022-01-13 06:26:43,805 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2022-01-13 06:26:43,821 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7d03933b-289c-4ae9-b706-c03134748096 does not exist. Creating ...
datanode3_1  | 2022-01-13 06:26:43,891 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7d03933b-289c-4ae9-b706-c03134748096/in_use.lock acquired by nodename 8@a47071aed28b
datanode3_1  | 2022-01-13 06:26:43,964 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7d03933b-289c-4ae9-b706-c03134748096 has been successfully formatted.
datanode3_1  | 2022-01-13 06:26:44,056 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-C03134748096: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2022-01-13 06:26:44,065 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:44,129 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2022-01-13 06:26:44,258 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2022-01-13 06:26:44,258 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2022-01-13 06:26:44,532 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:44,654 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2022-01-13 06:26:44,663 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2022-01-13 06:26:44,718 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7d03933b-289c-4ae9-b706-c03134748096
datanode3_1  | 2022-01-13 06:26:44,718 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2022-01-13 06:26:44,718 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2022-01-13 06:26:44,743 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:44,744 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2022-01-13 06:26:44,745 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2022-01-13 06:26:44,758 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2022-01-13 06:26:44,758 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2022-01-13 06:26:44,759 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2022-01-13 06:26:44,846 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:44,857 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:44,904 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:44,921 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:44,949 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:44,959 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2022-01-13 06:26:44,968 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2022-01-13 06:26:44,968 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2022-01-13 06:26:44,976 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2022-01-13 06:26:44,982 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2022-01-13 06:26:45,316 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:45,321 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2022-01-13 06:26:45,334 [pool-23-thread-1] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState
datanode3_1  | 2022-01-13 06:26:45,381 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C03134748096,id=22377b14-877d-4b48-97e5-336a98a1cf76
datanode3_1  | 2022-01-13 06:26:45,519 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7d03933b-289c-4ae9-b706-c03134748096
datanode3_1  | 2022-01-13 06:26:45,522 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7d03933b-289c-4ae9-b706-c03134748096.
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): Loaded
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): setting up network...
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): set up 4 sockets
kdc_1        | Jan 13 06:22:47 kdc krb5kdc[6](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jan 13 06:22:51 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642054971, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:01 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642054981, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:02 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.114: ISSUE: authtime 1642054982, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, s3g/s3g@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:03 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.115: ISSUE: authtime 1642054983, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:25 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1642055005, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:31 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1642055011, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:36 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1642054983, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:23:37 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1642055005, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:23:40 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642054981, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:23:44 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055024, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:52 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055024, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:23:55 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1642055035, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:23:56 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055036, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:02 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1642055035, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:05 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055036, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:06 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1642055046, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:07 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1642055046, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kms_1        | Sleeping for 5 seconds
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
datanode2_1  | 2022-01-13 06:26:54,925 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e/in_use.lock acquired by nodename 7@e367b1121f80
datanode2_1  | 2022-01-13 06:26:54,929 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e has been successfully formatted.
datanode2_1  | 2022-01-13 06:26:54,930 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-408A10C34F6E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2022-01-13 06:26:54,939 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2022-01-13 06:26:54,939 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2022-01-13 06:26:54,939 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2022-01-13 06:26:54,963 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:26:54,963 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:54,964 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2022-01-13 06:26:54,965 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2022-01-13 06:26:54,965 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2022-01-13 06:26:54,965 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2022-01-13 06:26:54,965 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2022-01-13 06:26:55,002 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2022-01-13 06:26:55,002 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:55,003 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:55,003 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2022-01-13 06:26:55,006 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2022-01-13 06:26:55,011 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2022-01-13 06:26:55,011 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2022-01-13 06:26:55,011 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2022-01-13 06:26:55,011 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2022-01-13 06:26:55,012 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2022-01-13 06:26:55,018 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode2_1  | 2022-01-13 06:26:55,037 [pool-23-thread-1] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2022-01-13 06:26:55,037 [pool-23-thread-1] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FollowerState
datanode2_1  | 2022-01-13 06:26:55,048 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-408A10C34F6E,id=b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:26:55,057 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e
datanode2_1  | 2022-01-13 06:26:57,771 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e.
datanode2_1  | 2022-01-13 06:26:58,768 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-7A75614A6B11, 2, (t:0, i:0))
datanode2_1  | 2022-01-13 06:26:58,769 [grpc-default-executor-0] INFO impl.VoteContext: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FOLLOWER: reject ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: our priority 1 > candidate's priority 0
datanode2_1  | 2022-01-13 06:26:58,769 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:22377b14-877d-4b48-97e5-336a98a1cf76
datanode2_1  | 2022-01-13 06:26:58,769 [grpc-default-executor-0] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:26:58,769 [grpc-default-executor-0] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:26:58,774 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11 replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t2. Peer's state: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11:t2, leader=null, voted=null, raftlog=b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:51,289 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2022-01-13 06:26:51,290 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2022-01-13 06:26:51,446 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode1_1  | 2022-01-13 06:26:51,455 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode1_1  | 2022-01-13 06:26:51,490 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderStateImpl
datanode1_1  | 2022-01-13 06:26:51,968 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2022-01-13 06:26:52,098 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$347/0x00000008405cec40@501e0313] WARN util.JvmPauseMonitor: JvmPauseMonitor-ab6e22bd-d408-4246-98c0-61fe3297cb52: Detected pause in JVM or host machine (eg GC): pause of approximately 120495227ns. No GCs detected.
datanode1_1  | 2022-01-13 06:26:52,312 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-LeaderElection1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036: set configuration 0: [ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:53,202 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-7A75614A6B11, 1, (t:0, i:0))
datanode1_1  | 2022-01-13 06:26:53,245 [grpc-default-executor-0] INFO impl.VoteContext: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-CANDIDATE: reject ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: already has voted for ab6e22bd-d408-4246-98c0-61fe3297cb52 at current term 1
datanode1_1  | 2022-01-13 06:26:53,321 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11 replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1. Peer's state: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11:t1, leader=null, voted=ab6e22bd-d408-4246-98c0-61fe3297cb52, raftlog=ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:53,494 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-448797766036-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2b9e81e9-4435-4567-af0a-448797766036/current/log_inprogress_0
datanode1_1  | 2022-01-13 06:26:53,979 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode1_1  | 2022-01-13 06:26:53,979 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection:   Response 0: ab6e22bd-d408-4246-98c0-61fe3297cb52<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t1
datanode1_1  | 2022-01-13 06:26:53,982 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2 ELECTION round 0: result REJECTED
datanode1_1  | 2022-01-13 06:26:53,983 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode1_1  | 2022-01-13 06:26:53,984 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2
datanode1_1  | 2022-01-13 06:26:53,984 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:26:54,226 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: receive requestVote(ELECTION, b1a62a11-0048-42bf-a9c3-836e0373e878, group-7A75614A6B11, 1, (t:0, i:0))
datanode1_1  | 2022-01-13 06:26:54,227 [grpc-default-executor-0] INFO impl.VoteContext: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FOLLOWER: reject ELECTION from b1a62a11-0048-42bf-a9c3-836e0373e878: already has voted for ab6e22bd-d408-4246-98c0-61fe3297cb52 at current term 1
datanode1_1  | 2022-01-13 06:26:54,230 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11 replies to ELECTION vote request: b1a62a11-0048-42bf-a9c3-836e0373e878<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1. Peer's state: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11:t1, leader=null, voted=ab6e22bd-d408-4246-98c0-61fe3297cb52, raftlog=ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:54,891 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11.
datanode1_1  | 2022-01-13 06:26:54,907 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52: new RaftServerImpl for group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] with ContainerStateMachine:uninitialized
datanode1_1  | 2022-01-13 06:26:54,915 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2022-01-13 06:26:54,916 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
kdc_1        | Jan 13 06:24:10 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055050, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:16 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1642055056, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:18 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055050, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:22 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055062, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:24 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1642055056, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:40 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1642055080, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:41 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:41 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:43 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1642055083, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:45 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1642055085, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:45 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1642055085, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:24:49 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1642055083, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:49 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1642055085, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:50 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1642055085, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:52 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:53 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:24:54 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1642055080, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:25:00 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055062, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:25:09 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:25:36 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
om1_1        | Sleeping for 5 seconds
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2022-01-13 06:24:33,369 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
datanode3_1  | 2022-01-13 06:26:45,524 [Command processor thread] INFO server.RaftServer: 22377b14-877d-4b48-97e5-336a98a1cf76: addNew group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] returns group-7A75614A6B11:java.util.concurrent.CompletableFuture@7a6d59bd[Not completed]
datanode3_1  | 2022-01-13 06:26:45,573 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76: new RaftServerImpl for group-7A75614A6B11:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] with ContainerStateMachine:uninitialized
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2022-01-13 06:26:45,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2022-01-13 06:26:45,607 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2022-01-13 06:26:45,607 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2022-01-13 06:26:45,607 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2022-01-13 06:26:45,615 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2022-01-13 06:26:45,622 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2022-01-13 06:26:45,622 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 does not exist. Creating ...
datanode3_1  | 2022-01-13 06:26:45,631 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11/in_use.lock acquired by nodename 8@a47071aed28b
datanode3_1  | 2022-01-13 06:26:45,646 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11 has been successfully formatted.
datanode3_1  | 2022-01-13 06:26:45,661 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-7A75614A6B11: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2022-01-13 06:26:45,668 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:45,684 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2022-01-13 06:26:45,687 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2022-01-13 06:26:45,689 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2022-01-13 06:26:45,691 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:45,692 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/45dbb9df-9697-41d5-8750-7a75614a6b11
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2022-01-13 06:26:45,704 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2022-01-13 06:26:45,705 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2022-01-13 06:26:45,705 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2022-01-13 06:26:45,705 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:45,719 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:45,720 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:45,720 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2022-01-13 06:26:45,721 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2022-01-13 06:26:45,743 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2022-01-13 06:26:54,920 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2022-01-13 06:26:54,921 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2022-01-13 06:26:54,921 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2022-01-13 06:26:54,921 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode1_1  | 2022-01-13 06:26:54,921 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e does not exist. Creating ...
datanode1_1  | 2022-01-13 06:26:54,922 [Command processor thread] INFO server.RaftServer: ab6e22bd-d408-4246-98c0-61fe3297cb52: addNew group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] returns group-408A10C34F6E:java.util.concurrent.CompletableFuture@105660ba[Not completed]
datanode1_1  | 2022-01-13 06:26:54,941 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e/in_use.lock acquired by nodename 6@783c9b118d1a
datanode1_1  | 2022-01-13 06:26:54,948 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e has been successfully formatted.
datanode1_1  | 2022-01-13 06:26:54,977 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-408A10C34F6E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2022-01-13 06:26:54,979 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2022-01-13 06:26:54,979 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2022-01-13 06:26:54,980 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2022-01-13 06:26:54,980 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2022-01-13 06:26:54,980 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:54,984 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2022-01-13 06:26:54,988 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2022-01-13 06:26:55,008 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2022-01-13 06:26:55,008 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2022-01-13 06:26:55,008 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2022-01-13 06:26:55,009 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2022-01-13 06:26:55,052 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2022-01-13 06:26:55,053 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:55,053 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2022-01-13 06:26:55,064 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2022-01-13 06:26:55,066 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2022-01-13 06:26:55,066 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2022-01-13 06:26:55,067 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2022-01-13 06:26:55,068 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2022-01-13 06:26:55,068 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2022-01-13 06:26:55,069 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode1_1  | 2022-01-13 06:26:55,069 [pool-23-thread-1] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2022-01-13 06:26:55,069 [pool-23-thread-1] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FollowerState
datanode1_1  | 2022-01-13 06:26:55,080 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-408A10C34F6E,id=ab6e22bd-d408-4246-98c0-61fe3297cb52
datanode1_1  | 2022-01-13 06:26:55,096 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e
kdc_1        | Jan 13 06:25:37 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1642055081, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 13 06:25:39 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1642055080, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1        | Jan 13 06:25:45 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1642055145, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:25:47 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1642055147, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:25:48 kdc krb5kdc[6](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1642055148, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:25:51 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1642055145, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:25:51 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1642055147, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:25:52 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1642055148, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:25:56 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055109, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:26:03 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:26:15 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: ISSUE: authtime 1642054983, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jan 13 06:26:26 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055163, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:26:30 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om2@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 13 06:26:30 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om2@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 13 06:26:30 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om2@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 13 06:26:30 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.115: LOOKING_UP_SERVER: authtime 0, etypes {rep=UNSUPPORTED:(0)} recon/recon@EXAMPLE.COM for HTTP/om2@EXAMPLE.COM, Server not found in Kerberos database
kdc_1        | Jan 13 06:26:31 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055191, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jan 13 06:26:37 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055191, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jan 13 06:26:41 kdc krb5kdc[6](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1642055201, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode2_1  | 2022-01-13 06:26:58,774 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState was interrupted: {}
datanode2_1  | java.lang.InterruptedException: sleep interrupted
datanode2_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode2_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode2_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode2_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode2_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode2_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode2_1  | 2022-01-13 06:26:59,437 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-408A10C34F6E, 1, (t:0, i:0))
datanode2_1  | 2022-01-13 06:26:59,452 [grpc-default-executor-0] INFO impl.VoteContext: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FOLLOWER: accept ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: our priority 0 <= candidate's priority 0
datanode2_1  | 2022-01-13 06:26:59,452 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:22377b14-877d-4b48-97e5-336a98a1cf76
datanode2_1  | 2022-01-13 06:26:59,453 [grpc-default-executor-0] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FollowerState
datanode2_1  | 2022-01-13 06:26:59,453 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FollowerState] INFO impl.FollowerState: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FollowerState was interrupted: {}
datanode2_1  | java.lang.InterruptedException: sleep interrupted
datanode2_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode2_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode2_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode2_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode2_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode2_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode2_1  | 2022-01-13 06:26:59,453 [grpc-default-executor-0] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-FollowerState
datanode2_1  | 2022-01-13 06:26:59,464 [grpc-default-executor-0] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:OK-t1. Peer's state: b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E:t1, leader=null, voted=22377b14-877d-4b48-97e5-336a98a1cf76, raftlog=b1a62a11-0048-42bf-a9c3-836e0373e878@group-408A10C34F6E-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode2_1  | 2022-01-13 06:27:03,863 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5094022430ns, electionTimeout:5093ms
datanode2_1  | 2022-01-13 06:27:03,863 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState
datanode2_1  | 2022-01-13 06:27:03,863 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode2_1  | 2022-01-13 06:27:03,864 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2022-01-13 06:27:03,864 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: start b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3
datanode2_1  | 2022-01-13 06:27:03,873 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2022-01-13 06:27:03,943 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode2_1  | 2022-01-13 06:27:03,943 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection:   Response 0: b1a62a11-0048-42bf-a9c3-836e0373e878<-22377b14-877d-4b48-97e5-336a98a1cf76#0:OK-t3
datanode2_1  | 2022-01-13 06:27:03,943 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3 ELECTION round 0: result PASSED
datanode2_1  | 2022-01-13 06:27:03,943 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO impl.RoleInfo: b1a62a11-0048-42bf-a9c3-836e0373e878: shutdown b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3
datanode2_1  | 2022-01-13 06:27:03,943 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode2_1  | 2022-01-13 06:27:03,944 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7A75614A6B11 with new leaderId: b1a62a11-0048-42bf-a9c3-836e0373e878
datanode2_1  | 2022-01-13 06:27:03,944 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServer$Division: b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11: change Leader from null to b1a62a11-0048-42bf-a9c3-836e0373e878 at term 3 for becomeLeader, leader elected after 17859ms
datanode2_1  | 2022-01-13 06:27:03,944 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2022-01-13 06:27:03,944 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:27:03,944 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode2_1  | 2022-01-13 06:27:03,945 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om1_1        | STARTUP_MSG:   java = 11.0.10
om1_1        | ************************************************************/
om1_1        | 2022-01-13 06:24:33,444 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2022-01-13 06:24:43,658 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2022-01-13 06:24:44,315 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2022-01-13 06:24:44,316 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2022-01-13 06:24:44,319 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2022-01-13 06:24:46,556 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2022-01-13 06:24:46,556 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2022-01-13 06:24:46,632 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2022-01-13 06:24:50,408 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2022-01-13 06:24:54,885 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2022-01-13 06:24:54,885 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om1_1        | 2022-01-13 06:24:54,907 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2022-01-13 06:25:03,427 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2022-01-13 06:25:03,886 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2022-01-13 06:25:03,888 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2022-01-13 06:25:03,910 [main] ERROR client.OMCertificateClient: Invalid domain om1
om1_1        | 2022-01-13 06:25:03,929 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2022-01-13 06:25:03,930 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2022-01-13 06:25:03,935 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2022-01-13 06:25:03,935 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2022-01-13 06:25:03,968 [main] INFO om.OzoneManager: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:c6280d25-12c7-4451-bd27-cfac689faecd,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:om1
om1_1        | 2022-01-13 06:25:05,404 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2022-01-13 06:25:08,703 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e;layoutVersion=0
om1_1        | 2022-01-13 06:25:09,104 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2022-01-13 06:25:24,160 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om1_1        | STARTUP_MSG:   java = 11.0.10
om1_1        | ************************************************************/
om1_1        | 2022-01-13 06:25:24,269 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2022-01-13 06:25:38,998 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2022-01-13 06:25:39,730 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2022-01-13 06:25:39,730 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2022-01-13 06:25:39,731 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2022-01-13 06:25:39,819 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2022-01-13 06:25:40,718 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om1_1        | 2022-01-13 06:25:44,283 [main] INFO reflections.Reflections: Reflections took 2491 ms to scan 1 urls, producing 97 keys and 262 values [using 2 cores]
om1_1        | 2022-01-13 06:25:46,886 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2022-01-13 06:25:46,886 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2022-01-13 06:25:46,887 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2022-01-13 06:25:54,605 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om1_1        | 2022-01-13 06:25:55,702 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-882210938510.crt.
om1_1        | 2022-01-13 06:25:55,724 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2022-01-13 06:25:55,752 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/991667651165.crt.
om1_1        | 2022-01-13 06:25:56,011 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2022-01-13 06:25:57,096 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1        | 2022-01-13 06:25:57,098 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1        | 2022-01-13 06:25:58,152 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2022-01-13 06:25:58,161 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2022-01-13 06:25:59,051 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
om1_1        | 2022-01-13 06:25:59,413 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | Sleeping for 5 seconds
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2022-01-13 06:24:33,663 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om2_1        | STARTUP_MSG:   java = 11.0.10
om2_1        | ************************************************************/
om2_1        | 2022-01-13 06:24:33,761 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2022-01-13 06:24:43,054 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2022-01-13 06:24:44,057 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2022-01-13 06:24:44,113 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2022-01-13 06:24:44,115 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2022-01-13 06:24:46,246 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2022-01-13 06:24:46,258 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2022-01-13 06:24:46,349 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2022-01-13 06:24:51,891 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2022-01-13 06:24:55,571 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2022-01-13 06:24:55,571 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om2_1        | 2022-01-13 06:24:55,576 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2022-01-13 06:25:04,545 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2022-01-13 06:25:05,086 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2022-01-13 06:25:05,113 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2022-01-13 06:25:05,128 [main] ERROR client.OMCertificateClient: Invalid domain om2
om2_1        | 2022-01-13 06:25:05,130 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2022-01-13 06:25:05,134 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2022-01-13 06:25:05,139 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2022-01-13 06:25:05,143 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2022-01-13 06:25:05,194 [main] INFO om.OzoneManager: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:c6280d25-12c7-4451-bd27-cfac689faecd,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:om2
om2_1        | 2022-01-13 06:25:06,636 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2022-01-13 06:25:09,652 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e;layoutVersion=0
om2_1        | 2022-01-13 06:25:09,884 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2022-01-13 06:25:25,558 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om2_1        | STARTUP_MSG:   java = 11.0.10
om2_1        | ************************************************************/
om2_1        | 2022-01-13 06:25:25,700 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2022-01-13 06:25:40,197 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2022-01-13 06:25:41,319 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2022-01-13 06:25:41,320 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2022-01-13 06:25:41,323 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2022-01-13 06:25:41,494 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2022-01-13 06:25:42,453 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om2_1        | 2022-01-13 06:25:45,754 [main] INFO reflections.Reflections: Reflections took 2454 ms to scan 1 urls, producing 97 keys and 262 values [using 2 cores]
om2_1        | 2022-01-13 06:25:47,719 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2022-01-13 06:25:47,723 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2022-01-13 06:25:47,724 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2022-01-13 06:25:55,833 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om2_1        | 2022-01-13 06:25:56,470 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-882210938510.crt.
om2_1        | 2022-01-13 06:25:56,486 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/992544897544.crt.
om2_1        | 2022-01-13 06:25:56,490 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2022-01-13 06:25:56,622 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2022-01-13 06:25:57,748 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1        | 2022-01-13 06:25:57,754 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1        | 2022-01-13 06:25:59,105 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2022-01-13 06:25:59,105 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2022-01-13 06:25:59,797 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
om2_1        | 2022-01-13 06:26:00,131 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
kdc_1        | Jan 13 06:27:00 kdc krb5kdc[6](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1642055201, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode1_1  | 2022-01-13 06:26:57,390 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e.
datanode1_1  | 2022-01-13 06:26:58,731 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-7A75614A6B11, 2, (t:0, i:0))
datanode1_1  | 2022-01-13 06:26:58,731 [grpc-default-executor-0] INFO impl.VoteContext: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FOLLOWER: accept ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: our priority 0 <= candidate's priority 0
datanode1_1  | 2022-01-13 06:26:58,731 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:22377b14-877d-4b48-97e5-336a98a1cf76
datanode1_1  | 2022-01-13 06:26:58,732 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:26:58,732 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:26:58,736 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode1_1  | 2022-01-13 06:26:58,783 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11 replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:OK-t2. Peer's state: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11:t2, leader=null, voted=22377b14-877d-4b48-97e5-336a98a1cf76, raftlog=ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode1_1  | 2022-01-13 06:26:59,389 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E: receive requestVote(ELECTION, 22377b14-877d-4b48-97e5-336a98a1cf76, group-408A10C34F6E, 1, (t:0, i:0))
datanode1_1  | 2022-01-13 06:26:59,390 [grpc-default-executor-0] INFO impl.VoteContext: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FOLLOWER: reject ELECTION from 22377b14-877d-4b48-97e5-336a98a1cf76: our priority 1 > candidate's priority 0
datanode1_1  | 2022-01-13 06:26:59,390 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:22377b14-877d-4b48-97e5-336a98a1cf76
datanode1_1  | 2022-01-13 06:26:59,390 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FollowerState
datanode1_1  | 2022-01-13 06:26:59,390 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FollowerState
datanode1_1  | 2022-01-13 06:26:59,390 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FollowerState] INFO impl.FollowerState: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode1_1  | 2022-01-13 06:26:59,397 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E replies to ELECTION vote request: 22377b14-877d-4b48-97e5-336a98a1cf76<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1. Peer's state: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E:t1, leader=null, voted=null, raftlog=ab6e22bd-d408-4246-98c0-61fe3297cb52@group-408A10C34F6E-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode1_1  | 2022-01-13 06:27:03,906 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: receive requestVote(ELECTION, b1a62a11-0048-42bf-a9c3-836e0373e878, group-7A75614A6B11, 3, (t:0, i:0))
datanode1_1  | 2022-01-13 06:27:03,906 [grpc-default-executor-0] INFO impl.VoteContext: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FOLLOWER: accept ELECTION from b1a62a11-0048-42bf-a9c3-836e0373e878: our priority 0 <= candidate's priority 1
datanode1_1  | 2022-01-13 06:27:03,906 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:b1a62a11-0048-42bf-a9c3-836e0373e878
datanode1_1  | 2022-01-13 06:27:03,906 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: shutdown ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:27:03,906 [grpc-default-executor-0] INFO impl.RoleInfo: ab6e22bd-d408-4246-98c0-61fe3297cb52: start ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState
datanode1_1  | 2022-01-13 06:27:03,911 [ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode3_1  | 2022-01-13 06:26:45,743 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2022-01-13 06:26:45,744 [pool-23-thread-1] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:26:45,763 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A75614A6B11,id=22377b14-877d-4b48-97e5-336a98a1cf76
datanode3_1  | 2022-01-13 06:26:45,765 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11
datanode3_1  | 2022-01-13 06:26:50,409 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState] INFO impl.FollowerState: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5075261232ns, electionTimeout:5018ms
datanode3_1  | 2022-01-13 06:26:50,409 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState
datanode3_1  | 2022-01-13 06:26:50,410 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2022-01-13 06:26:50,415 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2022-01-13 06:26:50,415 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1
datanode3_1  | 2022-01-13 06:26:50,479 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:50,480 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2022-01-13 06:26:50,480 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1
datanode3_1  | 2022-01-13 06:26:50,500 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2022-01-13 06:26:50,500 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C03134748096 with new leaderId: 22377b14-877d-4b48-97e5-336a98a1cf76
datanode3_1  | 2022-01-13 06:26:50,733 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: change Leader from null to 22377b14-877d-4b48-97e5-336a98a1cf76 at term 1 for becomeLeader, leader elected after 6435ms
datanode3_1  | 2022-01-13 06:26:50,755 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2022-01-13 06:26:50,814 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5070254427ns, electionTimeout:5036ms
datanode3_1  | 2022-01-13 06:26:50,839 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:26:50,840 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2022-01-13 06:26:50,840 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2022-01-13 06:26:50,840 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2
datanode3_1  | 2022-01-13 06:26:50,892 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2022-01-13 06:26:50,905 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode3_1  | 2022-01-13 06:26:50,935 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:51,076 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2022-01-13 06:26:51,077 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2022-01-13 06:26:51,077 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2022-01-13 06:26:51,152 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2022-01-13 06:26:51,200 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode3_1  | 2022-01-13 06:26:51,238 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderStateImpl
datanode3_1  | 2022-01-13 06:26:51,534 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2022-01-13 06:26:51,899 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-LeaderElection1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096: set configuration 0: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:52,372 [22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-C03134748096-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7d03933b-289c-4ae9-b706-c03134748096/current/log_inprogress_0
datanode3_1  | 2022-01-13 06:26:53,521 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode3_1  | 2022-01-13 06:26:53,522 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection:   Response 0: 22377b14-877d-4b48-97e5-336a98a1cf76<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1
datanode3_1  | 2022-01-13 06:26:53,522 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection:   Response 1: 22377b14-877d-4b48-97e5-336a98a1cf76<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t1
datanode3_1  | 2022-01-13 06:26:53,522 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2 ELECTION round 0: result REJECTED
datanode3_1  | 2022-01-13 06:26:53,524 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode3_1  | 2022-01-13 06:26:53,524 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2
datanode3_1  | 2022-01-13 06:26:53,524 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection2] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:26:53,888 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11.
datanode3_1  | 2022-01-13 06:26:53,896 [Command processor thread] INFO server.RaftServer: 22377b14-877d-4b48-97e5-336a98a1cf76: addNew group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] returns group-408A10C34F6E:java.util.concurrent.CompletableFuture@4294beed[Not completed]
datanode3_1  | 2022-01-13 06:26:53,897 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76: new RaftServerImpl for group-408A10C34F6E:[22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0] with ContainerStateMachine:uninitialized
datanode3_1  | 2022-01-13 06:26:53,897 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2022-01-13 06:26:53,897 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2022-01-13 06:26:53,897 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E: ConfigurationManager, init=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2022-01-13 06:26:53,898 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2022-01-13 06:26:53,910 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2022-01-13 06:26:53,914 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode3_1  | 2022-01-13 06:26:53,914 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e does not exist. Creating ...
datanode3_1  | 2022-01-13 06:26:53,928 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e/in_use.lock acquired by nodename 8@a47071aed28b
datanode3_1  | 2022-01-13 06:26:54,006 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e has been successfully formatted.
datanode3_1  | 2022-01-13 06:26:54,039 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-408A10C34F6E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2022-01-13 06:26:54,043 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2022-01-13 06:26:54,045 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2022-01-13 06:26:54,045 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2022-01-13 06:26:54,045 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2022-01-13 06:26:54,046 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:54,046 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2022-01-13 06:26:54,046 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2022-01-13 06:26:54,046 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fe6afea2-6277-43ca-80b4-408a10c34f6e
datanode3_1  | 2022-01-13 06:26:54,046 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2022-01-13 06:26:54,101 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2022-01-13 06:26:54,101 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:54,102 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2022-01-13 06:26:54,102 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2022-01-13 06:27:03,945 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2022-01-13 06:27:03,945 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2022-01-13 06:27:03,945 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2022-01-13 06:27:03,945 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode2_1  | 2022-01-13 06:27:03,987 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2022-01-13 06:27:03,988 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:27:03,988 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2022-01-13 06:27:04,041 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode2_1  | 2022-01-13 06:27:04,042 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2022-01-13 06:27:04,054 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2022-01-13 06:27:04,079 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2022-01-13 06:27:04,080 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2022-01-13 06:27:04,080 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2022-01-13 06:27:04,084 [b1a62a11-0048-42bf-a9c3-836e0373e878@group-7A75614A6B11-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1        | Sleeping for 5 seconds
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2022-01-13 06:24:31,676 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om3_1        | STARTUP_MSG:   java = 11.0.10
om3_1        | ************************************************************/
om3_1        | 2022-01-13 06:24:31,747 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2022-01-13 06:24:41,971 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2022-01-13 06:24:42,993 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2022-01-13 06:24:43,005 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2022-01-13 06:24:43,011 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2022-01-13 06:24:44,541 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2022-01-13 06:24:44,544 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2022-01-13 06:24:44,622 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2022-01-13 06:24:49,983 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2022-01-13 06:24:53,885 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2022-01-13 06:24:53,885 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om3_1        | 2022-01-13 06:24:53,895 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2022-01-13 06:25:06,266 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2022-01-13 06:25:06,805 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2022-01-13 06:25:06,805 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2022-01-13 06:25:06,837 [main] ERROR client.OMCertificateClient: Invalid domain om3
om3_1        | 2022-01-13 06:25:06,852 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2022-01-13 06:25:06,878 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2022-01-13 06:25:06,878 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2022-01-13 06:25:06,885 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2022-01-13 06:25:06,891 [main] INFO om.OzoneManager: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:c6280d25-12c7-4451-bd27-cfac689faecd,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:om3
om3_1        | 2022-01-13 06:25:09,023 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om1_1        | 2022-01-13 06:25:59,416 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2022-01-13 06:25:59,522 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2022-01-13 06:26:00,347 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2022-01-13 06:26:00,410 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2022-01-13 06:26:00,606 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om1:9872, om3:9872, om2:9872
om1_1        | 2022-01-13 06:26:00,679 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2022-01-13 06:26:02,127 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2022-01-13 06:26:02,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om1_1        | 2022-01-13 06:26:02,683 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2022-01-13 06:26:02,684 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om1_1        | 2022-01-13 06:26:02,684 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2022-01-13 06:26:02,687 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2022-01-13 06:26:02,688 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2022-01-13 06:26:02,690 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2022-01-13 06:26:02,690 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2022-01-13 06:26:02,691 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2022-01-13 06:26:10,917 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2022-01-13 06:26:10,945 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2022-01-13 06:26:10,950 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2022-01-13 06:26:11,040 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2022-01-13 06:26:11,119 [main] INFO server.RaftServer: om1: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@bd8f424[Not completed]
om1_1        | 2022-01-13 06:26:11,123 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2022-01-13 06:26:11,277 [pool-24-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om1_1        | 2022-01-13 06:26:11,317 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2022-01-13 06:26:11,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2022-01-13 06:26:11,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2022-01-13 06:26:11,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2022-01-13 06:26:11,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2022-01-13 06:26:11,336 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2022-01-13 06:26:11,504 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2022-01-13 06:26:11,543 [pool-24-thread-1] INFO server.RaftServer$Division: om1@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om1_1        | 2022-01-13 06:26:11,543 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2022-01-13 06:26:11,565 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2022-01-13 06:26:11,570 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1        | 2022-01-13 06:26:11,572 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om1_1        | 2022-01-13 06:26:11,770 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om1
om1_1        | 2022-01-13 06:26:11,815 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2022-01-13 06:26:11,916 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2022-01-13 06:26:11,929 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om1_1        | 2022-01-13 06:26:11,946 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2022-01-13 06:26:11,977 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2022-01-13 06:26:12,081 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2022-01-13 06:26:12,081 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2022-01-13 06:26:12,279 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2022-01-13 06:26:12,355 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2022-01-13 06:26:12,390 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2022-01-13 06:26:12,475 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om1_1        | 2022-01-13 06:26:12,483 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2022-01-13 06:26:12,483 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2022-01-13 06:26:12,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2022-01-13 06:26:12,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2022-01-13 06:26:12,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2022-01-13 06:26:12,486 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1        | 2022-01-13 06:26:12,486 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2022-01-13 06:27:03,932 [grpc-default-executor-0] INFO server.RaftServer$Division: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11 replies to ELECTION vote request: b1a62a11-0048-42bf-a9c3-836e0373e878<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:OK-t3. Peer's state: ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11:t3, leader=null, voted=b1a62a11-0048-42bf-a9c3-836e0373e878, raftlog=ab6e22bd-d408-4246-98c0-61fe3297cb52@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
om2_1        | 2022-01-13 06:26:00,163 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2022-01-13 06:26:00,230 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2022-01-13 06:26:01,027 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2022-01-13 06:26:01,094 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2022-01-13 06:26:01,334 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om2:9872, om1:9872, om3:9872
om2_1        | 2022-01-13 06:26:01,493 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2022-01-13 06:26:02,900 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2022-01-13 06:26:03,263 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om2_1        | 2022-01-13 06:26:03,287 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2022-01-13 06:26:03,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om2_1        | 2022-01-13 06:26:03,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2022-01-13 06:26:03,303 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2022-01-13 06:26:03,304 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2022-01-13 06:26:03,317 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2022-01-13 06:26:03,323 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2022-01-13 06:26:03,324 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2022-01-13 06:26:09,119 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2022-01-13 06:26:09,122 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2022-01-13 06:26:09,126 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2022-01-13 06:26:09,170 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2022-01-13 06:26:09,177 [main] INFO server.RaftServer: om2: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@ab7ac41[Not completed]
om2_1        | 2022-01-13 06:26:09,177 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2022-01-13 06:26:09,376 [pool-24-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om2_1        | 2022-01-13 06:26:09,477 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2022-01-13 06:26:09,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2022-01-13 06:26:09,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2022-01-13 06:26:09,485 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2022-01-13 06:26:09,488 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2022-01-13 06:26:09,494 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2022-01-13 06:26:09,495 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2022-01-13 06:26:09,654 [pool-24-thread-1] INFO server.RaftServer$Division: om2@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om2_1        | 2022-01-13 06:26:09,655 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2022-01-13 06:26:09,693 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2022-01-13 06:26:09,704 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1        | 2022-01-13 06:26:09,731 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om2_1        | 2022-01-13 06:26:09,781 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2022-01-13 06:26:12,499 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2022-01-13 06:26:12,818 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2022-01-13 06:26:12,849 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2022-01-13 06:26:13,068 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2022-01-13 06:26:13,068 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2022-01-13 06:26:13,208 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2022-01-13 06:26:13,225 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2022-01-13 06:26:13,226 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2022-01-13 06:26:13,237 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2022-01-13 06:26:13,292 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2022-01-13 06:26:13,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2022-01-13 06:26:13,946 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2022-01-13 06:26:14,011 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2022-01-13 06:26:14,011 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2022-01-13 06:26:14,150 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2022-01-13 06:26:14,154 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2022-01-13 06:26:14,156 [Listener at om1/9862] INFO server.RaftServer$Division: om1@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2022-01-13 06:26:14,170 [Listener at om1/9862] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2022-01-13 06:26:14,172 [Listener at om1/9862] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2022-01-13 06:26:14,176 [Listener at om1/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om1
om1_1        | 2022-01-13 06:26:14,192 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1        | 2022-01-13 06:26:14,423 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2022-01-13 06:26:14,429 [Listener at om1/9862] INFO om.OzoneManager: Starting OM block token secret manager
om1_1        | 2022-01-13 06:26:14,431 [Listener at om1/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2022-01-13 06:26:14,436 [Listener at om1/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2022-01-13 06:26:14,437 [Listener at om1/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2022-01-13 06:26:14,450 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$406/0x00000008405d7040@4528a871] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2022-01-13 06:26:14,456 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2022-01-13 06:26:14,461 [Thread[Thread-17,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2022-01-13 06:26:14,714 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2022-01-13 06:26:14,716 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2022-01-13 06:26:14,717 [Listener at om1/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2022-01-13 06:26:14,841 [Listener at om1/9862] INFO util.log: Logging initialized @64151ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2022-01-13 06:26:15,286 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2022-01-13 06:26:15,310 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2022-01-13 06:26:15,324 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2022-01-13 06:26:15,324 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2022-01-13 06:26:15,327 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2022-01-13 06:26:15,342 [Listener at om1/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2022-01-13 06:26:15,523 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2022-01-13 06:26:15,534 [Listener at om1/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
om1_1        | 2022-01-13 06:26:15,691 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2022-01-13 06:26:15,699 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2022-01-13 06:26:15,701 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1        | 2022-01-13 06:26:15,813 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2022-01-13 06:26:15,833 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14abde3e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1        | 2022-01-13 06:26:15,837 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@200ef05b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2022-01-13 06:26:16,231 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2022-01-13 06:26:16,319 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@47e6004c{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0-SNAPSHOT_jar-_-any-979705365013795959/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/ozoneManager}
recon_1      | Sleeping for 5 seconds
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2022-01-13 06:22:55,931 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.18.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.18.RELEASE.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.18.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.18.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.18.RELEASE.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
recon_1      | STARTUP_MSG:   java = 11.0.10
recon_1      | ************************************************************/
recon_1      | 2022-01-13 06:22:55,966 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2022-01-13 06:22:59,404 [main] INFO reflections.Reflections: Reflections took 180 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1      | 2022-01-13 06:23:02,218 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2022-01-13 06:23:02,432 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2022-01-13 06:23:03,390 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file recon.keytab. Keytab auto renewal enabled : false
recon_1      | 2022-01-13 06:23:03,399 [main] INFO recon.ReconServer: Recon login successful.
recon_1      | 2022-01-13 06:23:04,534 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2022-01-13 06:23:09,153 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2022-01-13 06:23:10,380 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2022-01-13 06:23:10,437 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2022-01-13 06:23:10,452 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2022-01-13 06:23:13,479 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2022-01-13 06:23:13,479 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2022-01-13 06:23:13,480 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2022-01-13 06:23:13,527 [main] INFO util.log: Logging initialized @20150ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2022-01-13 06:23:13,867 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1      | 2022-01-13 06:23:13,893 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2022-01-13 06:23:13,894 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2022-01-13 06:23:13,895 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2022-01-13 06:23:13,895 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2022-01-13 06:23:13,903 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2022-01-13 06:23:14,251 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2022-01-13 06:23:15,071 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2022-01-13 06:23:15,108 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1      | 2022-01-13 06:23:15,136 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1      | 2022-01-13 06:23:15,229 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'id1'.
recon_1      | 2022-01-13 06:23:17,247 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:23:17,769 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:23:17,897 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:23:17,964 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2022-01-13 06:23:17,974 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2022-01-13 06:23:18,243 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:23:18,484 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1      | 2022-01-13 06:23:18,702 [main] INFO reflections.Reflections: Reflections took 212 ms to scan 3 urls, producing 103 keys and 217 values 
recon_1      | 2022-01-13 06:23:18,810 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2022-01-13 06:23:18,885 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2022-01-13 06:23:18,913 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2022-01-13 06:23:18,928 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1      | 2022-01-13 06:23:19,056 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2022-01-13 06:23:19,163 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2022-01-13 06:23:19,308 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1      | 2022-01-13 06:23:19,459 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2022-01-13 06:23:19,463 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1      | 2022-01-13 06:23:19,597 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2022-01-13 06:23:19,631 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2022-01-13 06:23:19,631 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2022-01-13 06:23:20,165 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2022-01-13 06:23:20,171 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
recon_1      | 2022-01-13 06:23:20,268 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2022-01-13 06:23:20,269 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2022-01-13 06:23:20,277 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1      | 2022-01-13 06:23:20,327 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2022-01-13 06:23:20,340 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4d0abb23{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1      | 2022-01-13 06:23:20,345 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2e19b30{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1      | 2022-01-13 06:23:21,106 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
recon_1      | 2022-01-13 06:23:21,129 [Listener at 0.0.0.0/9891] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/recon.keytab, for principal HTTP/recon@EXAMPLE.COM
om2_1        | 2022-01-13 06:26:09,898 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 8@om2
om2_1        | 2022-01-13 06:26:09,947 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2022-01-13 06:26:10,175 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om2_1        | 2022-01-13 06:26:10,178 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1        | 2022-01-13 06:26:10,251 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2022-01-13 06:26:10,360 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2022-01-13 06:26:10,368 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2022-01-13 06:26:10,544 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2022-01-13 06:26:10,670 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2022-01-13 06:26:10,675 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2022-01-13 06:26:10,752 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om2@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om2_1        | 2022-01-13 06:26:10,752 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2022-01-13 06:26:10,753 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2022-01-13 06:26:10,780 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2022-01-13 06:26:10,791 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2022-01-13 06:26:10,792 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1        | 2022-01-13 06:26:10,807 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2022-01-13 06:26:10,807 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2022-01-13 06:26:10,815 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2022-01-13 06:26:10,935 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2022-01-13 06:26:10,956 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2022-01-13 06:26:11,031 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2022-01-13 06:26:11,031 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2022-01-13 06:26:11,078 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2022-01-13 06:26:11,087 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2022-01-13 06:26:11,091 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2022-01-13 06:26:11,092 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2022-01-13 06:26:11,093 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2022-01-13 06:26:11,093 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2022-01-13 06:26:11,580 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2022-01-13 06:26:11,661 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2022-01-13 06:26:11,661 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2022-01-13 06:26:12,048 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2022-01-13 06:26:12,049 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2022-01-13 06:26:12,051 [Listener at om2/9862] INFO server.RaftServer$Division: om2@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2022-01-13 06:26:12,057 [Listener at om2/9862] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2022-01-13 06:26:12,062 [Listener at om2/9862] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2022-01-13 06:26:12,070 [Listener at om2/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om2
om2_1        | 2022-01-13 06:26:12,091 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1        | 2022-01-13 06:26:12,367 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2022-01-13 06:26:12,409 [Listener at om2/9862] INFO om.OzoneManager: Starting OM block token secret manager
om2_1        | 2022-01-13 06:26:12,413 [Listener at om2/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2022-01-13 06:26:12,414 [Listener at om2/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2022-01-13 06:26:12,414 [Listener at om2/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2022-01-13 06:26:12,507 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2022-01-13 06:26:12,542 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$406/0x00000008405d7040@4cbf1c30] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2022-01-13 06:26:12,618 [Thread[Thread-17,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2022-01-13 06:26:13,179 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$406/0x00000008405d7040@4cbf1c30] WARN util.JvmPauseMonitor: JvmPauseMonitor-om2: Detected pause in JVM or host machine (eg GC): pause of approximately 112845148ns. No GCs detected.
om2_1        | 2022-01-13 06:26:13,270 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2022-01-13 06:26:13,271 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2022-01-13 06:26:13,280 [Listener at om2/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2022-01-13 06:26:13,554 [Listener at om2/9862] INFO util.log: Logging initialized @61842ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2022-01-13 06:26:14,113 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2022-01-13 06:26:14,167 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2022-01-13 06:26:14,170 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2022-01-13 06:26:14,175 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2022-01-13 06:26:14,175 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2022-01-13 06:26:14,191 [Listener at om2/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2022-01-13 06:26:14,354 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2022-01-13 06:26:14,358 [Listener at om2/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
om2_1        | 2022-01-13 06:26:14,515 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1        | 2022-01-13 06:26:14,522 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1        | 2022-01-13 06:26:14,533 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1        | 2022-01-13 06:26:14,612 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2022-01-13 06:26:14,619 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4c33bf70{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1        | 2022-01-13 06:26:14,627 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2db55dec{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2022-01-13 06:26:15,088 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
datanode3_1  | 2022-01-13 06:26:54,102 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2022-01-13 06:26:54,140 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2022-01-13 06:26:54,145 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2022-01-13 06:26:54,146 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2022-01-13 06:26:54,180 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:54,183 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:54,184 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2022-01-13 06:26:54,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2022-01-13 06:26:54,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2022-01-13 06:26:54,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2022-01-13 06:26:54,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2022-01-13 06:26:54,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2022-01-13 06:26:54,201 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2022-01-13 06:26:54,208 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E: start as a follower, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode3_1  | 2022-01-13 06:26:54,208 [pool-23-thread-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2022-01-13 06:26:54,208 [pool-23-thread-1] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState
datanode3_1  | 2022-01-13 06:26:54,226 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-408A10C34F6E,id=22377b14-877d-4b48-97e5-336a98a1cf76
datanode3_1  | 2022-01-13 06:26:54,241 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e
datanode3_1  | 2022-01-13 06:26:54,329 [grpc-default-executor-0] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: receive requestVote(ELECTION, ab6e22bd-d408-4246-98c0-61fe3297cb52, group-7A75614A6B11, 1, (t:0, i:0))
datanode3_1  | 2022-01-13 06:26:54,372 [grpc-default-executor-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: receive requestVote(ELECTION, b1a62a11-0048-42bf-a9c3-836e0373e878, group-7A75614A6B11, 1, (t:0, i:0))
datanode3_1  | 2022-01-13 06:26:54,373 [grpc-default-executor-0] INFO impl.VoteContext: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FOLLOWER: reject ELECTION from ab6e22bd-d408-4246-98c0-61fe3297cb52: already has voted for 22377b14-877d-4b48-97e5-336a98a1cf76 at current term 1
datanode3_1  | 2022-01-13 06:26:54,389 [grpc-default-executor-0] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11 replies to ELECTION vote request: ab6e22bd-d408-4246-98c0-61fe3297cb52<-22377b14-877d-4b48-97e5-336a98a1cf76#0:FAIL-t1. Peer's state: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11:t1, leader=null, voted=22377b14-877d-4b48-97e5-336a98a1cf76, raftlog=22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:54,416 [grpc-default-executor-1] INFO impl.VoteContext: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FOLLOWER: reject ELECTION from b1a62a11-0048-42bf-a9c3-836e0373e878: already has voted for 22377b14-877d-4b48-97e5-336a98a1cf76 at current term 1
datanode3_1  | 2022-01-13 06:26:54,433 [grpc-default-executor-1] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11 replies to ELECTION vote request: b1a62a11-0048-42bf-a9c3-836e0373e878<-22377b14-877d-4b48-97e5-336a98a1cf76#0:FAIL-t1. Peer's state: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11:t1, leader=null, voted=22377b14-877d-4b48-97e5-336a98a1cf76, raftlog=22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:57,576 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e.
datanode3_1  | 2022-01-13 06:26:58,710 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5185468095ns, electionTimeout:5166ms
datanode3_1  | 2022-01-13 06:26:58,710 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:26:58,710 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode3_1  | 2022-01-13 06:26:58,710 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2022-01-13 06:26:58,710 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3
datanode3_1  | 2022-01-13 06:26:58,721 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode3_1  | 2022-01-13 06:26:58,790 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode3_1  | 2022-01-13 06:26:58,790 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection:   Response 0: 22377b14-877d-4b48-97e5-336a98a1cf76<-b1a62a11-0048-42bf-a9c3-836e0373e878#0:FAIL-t2
datanode3_1  | 2022-01-13 06:26:58,791 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3 ELECTION round 0: result REJECTED
datanode3_1  | 2022-01-13 06:26:58,791 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode3_1  | 2022-01-13 06:26:58,791 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3
datanode3_1  | 2022-01-13 06:26:58,791 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection3] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:26:59,358 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState] INFO impl.FollowerState: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5149961389ns, electionTimeout:5131ms
datanode3_1  | 2022-01-13 06:26:59,360 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState
datanode3_1  | 2022-01-13 06:26:59,360 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2022-01-13 06:26:59,364 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2022-01-13 06:26:59,364 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4
datanode3_1  | 2022-01-13 06:26:59,381 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0], old=null
datanode3_1  | 2022-01-13 06:26:59,420 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode3_1  | 2022-01-13 06:26:59,420 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.LeaderElection:   Response 0: 22377b14-877d-4b48-97e5-336a98a1cf76<-ab6e22bd-d408-4246-98c0-61fe3297cb52#0:FAIL-t1
datanode3_1  | 2022-01-13 06:26:59,421 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4 ELECTION round 0: result REJECTED
datanode3_1  | 2022-01-13 06:26:59,426 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode3_1  | 2022-01-13 06:26:59,426 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4
datanode3_1  | 2022-01-13 06:26:59,426 [22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-LeaderElection4] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-408A10C34F6E-FollowerState
datanode3_1  | 2022-01-13 06:27:03,865 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.FollowerState: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073371087ns, electionTimeout:5054ms
datanode3_1  | 2022-01-13 06:27:03,865 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:27:03,865 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode3_1  | 2022-01-13 06:27:03,865 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2022-01-13 06:27:03,865 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection5
datanode3_1  | 2022-01-13 06:27:03,889 [grpc-default-executor-0] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: receive requestVote(ELECTION, b1a62a11-0048-42bf-a9c3-836e0373e878, group-7A75614A6B11, 3, (t:0, i:0))
datanode3_1  | 2022-01-13 06:27:03,889 [grpc-default-executor-0] INFO impl.VoteContext: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-CANDIDATE: accept ELECTION from b1a62a11-0048-42bf-a9c3-836e0373e878: our priority 0 <= candidate's priority 1
datanode3_1  | 2022-01-13 06:27:03,889 [grpc-default-executor-0] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11: changes role from CANDIDATE to FOLLOWER at term 3 for candidate:b1a62a11-0048-42bf-a9c3-836e0373e878
datanode3_1  | 2022-01-13 06:27:03,889 [grpc-default-executor-0] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: shutdown 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection5
datanode3_1  | 2022-01-13 06:27:03,890 [grpc-default-executor-0] INFO impl.RoleInfo: 22377b14-877d-4b48-97e5-336a98a1cf76: start 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-FollowerState
datanode3_1  | 2022-01-13 06:27:03,893 [22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection5] INFO impl.LeaderElection: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-LeaderElection5: skip running since this is already CLOSING
om3_1        | 2022-01-13 06:25:11,411 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e;layoutVersion=0
om3_1        | 2022-01-13 06:25:11,730 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2022-01-13 06:25:27,670 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
om3_1        | STARTUP_MSG:   java = 11.0.10
om3_1        | ************************************************************/
om3_1        | 2022-01-13 06:25:27,886 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2022-01-13 06:25:42,209 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2022-01-13 06:25:43,307 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2022-01-13 06:25:43,308 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2022-01-13 06:25:43,328 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2022-01-13 06:25:43,532 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2022-01-13 06:25:44,414 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om3_1        | 2022-01-13 06:25:47,144 [main] INFO reflections.Reflections: Reflections took 1742 ms to scan 1 urls, producing 97 keys and 262 values [using 2 cores]
om3_1        | 2022-01-13 06:25:48,516 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2022-01-13 06:25:48,531 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2022-01-13 06:25:48,535 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2022-01-13 06:25:56,401 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om3_1        | 2022-01-13 06:25:57,383 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-882210938510.crt.
om3_1        | 2022-01-13 06:25:57,420 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/994773350302.crt.
om3_1        | 2022-01-13 06:25:57,448 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2022-01-13 06:25:57,782 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2022-01-13 06:25:58,810 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1        | 2022-01-13 06:25:58,817 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1        | 2022-01-13 06:25:59,939 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2022-01-13 06:25:59,940 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2022-01-13 06:26:00,660 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
om3_1        | 2022-01-13 06:26:01,092 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2022-01-13 06:26:01,092 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2022-01-13 06:26:01,224 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2022-01-13 06:26:02,473 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2022-01-13 06:26:02,547 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2022-01-13 06:26:02,746 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: id1 and peers: om3:9872, om1:9872, om2:9872
om3_1        | 2022-01-13 06:26:02,853 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2022-01-13 06:26:03,882 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2022-01-13 06:26:04,457 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om3_1        | 2022-01-13 06:26:04,459 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2022-01-13 06:26:04,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om3_1        | 2022-01-13 06:26:04,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2022-01-13 06:26:04,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2022-01-13 06:26:04,464 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2022-01-13 06:26:04,484 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2022-01-13 06:26:04,484 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2022-01-13 06:26:04,487 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2022-01-13 06:26:11,346 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2022-01-13 06:26:11,353 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2022-01-13 06:26:11,357 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2022-01-13 06:26:11,415 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2022-01-13 06:26:11,462 [main] INFO server.RaftServer: om3: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@bd8f424[Not completed]
om3_1        | 2022-01-13 06:26:11,463 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2022-01-13 06:26:11,621 [pool-24-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om3_1        | 2022-01-13 06:26:11,643 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2022-01-13 06:26:11,673 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2022-01-13 06:26:11,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2022-01-13 06:26:11,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2022-01-13 06:26:11,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2022-01-13 06:26:11,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1        | 2022-01-13 06:26:11,674 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2022-01-13 06:26:11,748 [pool-24-thread-1] INFO server.RaftServer$Division: om3@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om3_1        | 2022-01-13 06:26:11,755 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2022-01-13 06:26:11,909 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2022-01-13 06:26:11,911 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1        | 2022-01-13 06:26:11,922 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om3_1        | 2022-01-13 06:26:12,032 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 8@om3
om3_1        | 2022-01-13 06:26:12,091 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2022-01-13 06:26:12,181 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2022-01-13 06:26:12,285 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om3_1        | 2022-01-13 06:26:12,313 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1        | 2022-01-13 06:26:12,335 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2022-01-13 06:26:12,406 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2022-01-13 06:26:12,425 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2022-01-13 06:26:12,828 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2022-01-13 06:26:13,013 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2022-01-13 06:26:13,031 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2022-01-13 06:26:13,221 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om3@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om3_1        | 2022-01-13 06:26:13,228 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2022-01-13 06:26:13,229 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2022-01-13 06:26:13,230 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2022-01-13 06:26:13,230 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2022-01-13 06:26:13,230 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1        | 2022-01-13 06:26:13,289 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2022-01-13 06:26:13,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2022-01-13 06:26:16,406 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@1871068d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2022-01-13 06:26:16,407 [Listener at om1/9862] INFO server.Server: Started @65717ms
om1_1        | 2022-01-13 06:26:16,431 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2022-01-13 06:26:16,431 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2022-01-13 06:26:16,433 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2022-01-13 06:26:16,433 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2022-01-13 06:26:16,477 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1        | 2022-01-13 06:26:16,527 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om1_1        | 2022-01-13 06:26:16,601 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5b7b5484] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1        | 2022-01-13 06:26:19,337 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5166186054ns, electionTimeout:5153ms
om1_1        | 2022-01-13 06:26:19,339 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2022-01-13 06:26:19,342 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2022-01-13 06:26:19,345 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2022-01-13 06:26:19,345 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection1
om1_1        | 2022-01-13 06:26:19,359 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2022-01-13 06:26:23,557 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:37165
om1_1        | 2022-01-13 06:26:23,562 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2022-01-13 06:26:24,173 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om1_1        | 2022-01-13 06:26:24,182 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:FAIL-t1
om1_1        | 2022-01-13 06:26:24,182 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om1<-om2#0:FAIL-t1
om1_1        | 2022-01-13 06:26:24,183 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om1_1        | 2022-01-13 06:26:24,224 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om1_1        | 2022-01-13 06:26:24,225 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection1
om1_1        | 2022-01-13 06:26:24,225 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2022-01-13 06:26:25,057 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 1, (t:0, i:~))
om1_1        | 2022-01-13 06:26:25,059 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om1 at current term 1
om1_1        | 2022-01-13 06:26:25,068 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om3<-om1#0:FAIL-t1. Peer's state: om1@group-562213E44849:t1, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2022-01-13 06:26:27,557 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 2, (t:0, i:~))
om1_1        | 2022-01-13 06:26:27,560 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om1_1        | 2022-01-13 06:26:27,563 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:om2
om1_1        | 2022-01-13 06:26:27,563 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2022-01-13 06:26:27,567 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState was interrupted: {}
om1_1        | java.lang.InterruptedException: sleep interrupted
om1_1        | 	at java.base/java.lang.Thread.sleep(Native Method)
om1_1        | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
om1_1        | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
om1_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
om1_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
om1_1        | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
om1_1        | 2022-01-13 06:26:27,569 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2022-01-13 06:26:27,573 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om2<-om1#0:OK-t2. Peer's state: om1@group-562213E44849:t2, leader=null, voted=om2, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2022-01-13 06:26:27,927 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: change Leader from null to om2 at term 2 for appendEntries, leader elected after 15990ms
om1_1        | 2022-01-13 06:26:28,128 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om1_1        | 2022-01-13 06:26:28,194 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om1_1        | 2022-01-13 06:26:28,873 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om1_1        | 2022-01-13 06:26:31,664 [om1@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1        | [id: "om1"
om1_1        | address: "om1:9872"
s3g_1        | Sleeping for 5 seconds
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2022-01-13 06:23:02,854 [main] INFO security.UserGroupInformation: Login successful for user s3g/s3g@EXAMPLE.COM using keytab file s3g.keytab. Keytab auto renewal enabled : false
s3g_1        | 2022-01-13 06:23:02,883 [main] INFO s3.Gateway: S3Gateway login successful.
s3g_1        | 2022-01-13 06:23:03,224 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2022-01-13 06:23:03,225 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2022-01-13 06:23:03,225 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2022-01-13 06:23:03,396 [main] INFO util.log: Logging initialized @7327ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2022-01-13 06:23:03,979 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2022-01-13 06:23:04,036 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2022-01-13 06:23:04,038 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2022-01-13 06:23:04,038 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2022-01-13 06:23:04,039 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2022-01-13 06:23:04,041 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2022-01-13 06:23:04,476 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
recon_1      | 2022-01-13 06:23:24,557 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4f241437{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0-SNAPSHOT_jar-_-any-9889573258734139229/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0-SNAPSHOT.jar!/webapps/recon}
recon_1      | 2022-01-13 06:23:24,595 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@3a7d914c{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1      | 2022-01-13 06:23:24,595 [Listener at 0.0.0.0/9891] INFO server.Server: Started @31218ms
recon_1      | 2022-01-13 06:23:24,599 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1      | 2022-01-13 06:23:24,602 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1      | 2022-01-13 06:23:24,605 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1      | 2022-01-13 06:23:24,605 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1      | 2022-01-13 06:23:24,615 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1      | 2022-01-13 06:23:24,624 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1      | 2022-01-13 06:23:24,624 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1      | 2022-01-13 06:23:24,625 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:23:24,625 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1      | 2022-01-13 06:23:24,632 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1      | 2022-01-13 06:23:26,901 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:28,904 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:30,906 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm1.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:32,908 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:34,910 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:37,412 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c6280d25-12c7-4451-bd27-cfac689faecd is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1      | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:245)
recon_1      | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:165)
recon_1      | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:57462)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy44.submitRequest over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:39,414 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm2.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9860 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:41,416 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to scm3.org:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy44.submitRequest over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:43,601 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1      | 2022-01-13 06:23:43,602 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2022-01-13 06:23:43,602 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1      | 2022-01-13 06:23:43,605 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1      | 2022-01-13 06:23:43,606 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1      | 2022-01-13 06:23:43,713 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1      | 2022-01-13 06:23:43,714 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1      | 2022-01-13 06:23:43,730 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1      | 2022-01-13 06:23:43,731 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1      | 2022-01-13 06:23:43,748 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1      | 2022-01-13 06:23:43,750 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 19 milliseconds.
recon_1      | 2022-01-13 06:23:44,627 [pool-18-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1      | 2022-01-13 06:23:44,627 [pool-18-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1      | 2022-01-13 06:23:44,792 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:44,798 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:46,800 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 3 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:46,801 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 4 failover attempts. Trying to failover immediately.
datanode3_1  | 2022-01-13 06:27:03,914 [grpc-default-executor-0] INFO server.RaftServer$Division: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11 replies to ELECTION vote request: b1a62a11-0048-42bf-a9c3-836e0373e878<-22377b14-877d-4b48-97e5-336a98a1cf76#0:OK-t3. Peer's state: 22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11:t3, leader=null, voted=b1a62a11-0048-42bf-a9c3-836e0373e878, raftlog=22377b14-877d-4b48-97e5-336a98a1cf76@group-7A75614A6B11-SegmentedRaftLog:OPENED:c-1, conf=-1: [22377b14-877d-4b48-97e5-336a98a1cf76|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, ab6e22bd-d408-4246-98c0-61fe3297cb52|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0, b1a62a11-0048-42bf-a9c3-836e0373e878|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
om2_1        | 2022-01-13 06:26:15,138 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8dcc25{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0-SNAPSHOT_jar-_-any-3879277884219842347/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1        | 2022-01-13 06:26:15,191 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@128fe3ab{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1        | 2022-01-13 06:26:15,191 [Listener at om2/9862] INFO server.Server: Started @63478ms
om2_1        | 2022-01-13 06:26:15,215 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1        | 2022-01-13 06:26:15,222 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1        | 2022-01-13 06:26:15,228 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1        | 2022-01-13 06:26:15,229 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1        | 2022-01-13 06:26:15,230 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1        | 2022-01-13 06:26:15,732 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om2_1        | 2022-01-13 06:26:15,793 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@46f43f50] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1        | 2022-01-13 06:26:15,955 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:38727
om2_1        | 2022-01-13 06:26:15,987 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2022-01-13 06:26:17,116 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057426697ns, electionTimeout:5036ms
om2_1        | 2022-01-13 06:26:17,119 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2022-01-13 06:26:17,137 [om2@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2022-01-13 06:26:17,140 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1        | 2022-01-13 06:26:17,140 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderElection1
om2_1        | 2022-01-13 06:26:17,185 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2022-01-13 06:26:22,324 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.779845169s. [buffered_nanos=2942664920, waiting_for_connection]
om2_1        | 2022-01-13 06:26:22,324 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.750995944s. [buffered_nanos=2900808464, waiting_for_connection]
om2_1        | 2022-01-13 06:26:22,329 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION REJECTED received 0 response(s) and 2 exception(s):
om2_1        | 2022-01-13 06:26:22,329 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.779845169s. [buffered_nanos=2942664920, waiting_for_connection]
om2_1        | 2022-01-13 06:26:22,329 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.750995944s. [buffered_nanos=2900808464, waiting_for_connection]
om2_1        | 2022-01-13 06:26:22,329 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om2_1        | 2022-01-13 06:26:22,363 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om2_1        | 2022-01-13 06:26:22,363 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-LeaderElection1
om2_1        | 2022-01-13 06:26:22,363 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2022-01-13 06:26:23,358 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 1, (t:0, i:~))
om2_1        | 2022-01-13 06:26:23,368 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: reject ELECTION from om1: already has voted for om2 at current term 1
om2_1        | 2022-01-13 06:26:23,378 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om1<-om2#0:FAIL-t1. Peer's state: om2@group-562213E44849:t1, leader=null, voted=om2, raftlog=om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2022-01-13 06:26:24,972 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 1, (t:0, i:~))
om2_1        | 2022-01-13 06:26:24,972 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om2 at current term 1
om2_1        | 2022-01-13 06:26:24,973 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om3<-om2#0:FAIL-t1. Peer's state: om2@group-562213E44849:t1, leader=null, voted=om2, raftlog=om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2022-01-13 06:26:26,280 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:46049
om2_1        | 2022-01-13 06:26:26,314 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2022-01-13 06:26:27,449 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5085065482ns, electionTimeout:5012ms
om2_1        | 2022-01-13 06:26:27,449 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2022-01-13 06:26:27,449 [om2@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om2_1        | 2022-01-13 06:26:27,449 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:59Z
s3g_1        | STARTUP_MSG:   java = 11.0.10
s3g_1        | ************************************************************/
s3g_1        | 2022-01-13 06:23:04,518 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2022-01-13 06:23:04,653 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2022-01-13 06:23:04,717 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2022-01-13 06:23:04,719 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
s3g_1        | 2022-01-13 06:23:04,925 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2022-01-13 06:23:04,925 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2022-01-13 06:23:04,927 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1        | 2022-01-13 06:23:05,073 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2022-01-13 06:23:05,109 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bf22f18{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2022-01-13 06:23:05,111 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63648ee9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | WARNING: An illegal reflective access operation has occurred
s3g_1        | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1        | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1        | WARNING: All illegal access operations will be denied in a future release
s3g_1        | 2022-01-13 06:23:12,588 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | Jan 13, 2022 6:23:16 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1        | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1        | 
s3g_1        | 2022-01-13 06:23:16,077 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@41ece227{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0-SNAPSHOT_jar-_-any-10369789247174954978/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2022-01-13 06:23:16,123 [main] INFO server.AbstractConnector: Started ServerConnector@841e575{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2022-01-13 06:23:16,124 [main] INFO server.Server: Started @20055ms
s3g_1        | 2022-01-13 06:23:16,126 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
om3_1        | 2022-01-13 06:26:13,309 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2022-01-13 06:26:13,483 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2022-01-13 06:26:13,484 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2022-01-13 06:26:13,667 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2022-01-13 06:26:13,671 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2022-01-13 06:26:13,696 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2022-01-13 06:26:13,724 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2022-01-13 06:26:13,728 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2022-01-13 06:26:13,728 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2022-01-13 06:26:13,754 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2022-01-13 06:26:13,763 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2022-01-13 06:26:14,212 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2022-01-13 06:26:14,263 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2022-01-13 06:26:14,263 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2022-01-13 06:26:14,407 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2022-01-13 06:26:14,411 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2022-01-13 06:26:14,413 [Listener at om3/9862] INFO server.RaftServer$Division: om3@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2022-01-13 06:26:14,420 [Listener at om3/9862] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2022-01-13 06:26:14,426 [Listener at om3/9862] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2022-01-13 06:26:14,443 [Listener at om3/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om3
om3_1        | 2022-01-13 06:26:14,459 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1        | 2022-01-13 06:26:14,699 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2022-01-13 06:26:14,712 [Listener at om3/9862] INFO om.OzoneManager: Starting OM block token secret manager
om3_1        | 2022-01-13 06:26:14,713 [Listener at om3/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2022-01-13 06:26:14,716 [Listener at om3/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2022-01-13 06:26:14,716 [Listener at om3/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2022-01-13 06:26:14,723 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$406/0x00000008405d7040@4528a871] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2022-01-13 06:26:14,744 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2022-01-13 06:26:14,747 [Thread[Thread-17,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2022-01-13 06:26:14,930 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2022-01-13 06:26:14,930 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2022-01-13 06:26:14,931 [Listener at om3/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2022-01-13 06:26:15,012 [Listener at om3/9862] INFO util.log: Logging initialized @61701ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2022-01-13 06:26:15,396 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2022-01-13 06:26:15,421 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2022-01-13 06:26:15,425 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2022-01-13 06:26:15,426 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2022-01-13 06:26:15,428 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2022-01-13 06:26:15,433 [Listener at om3/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2022-01-13 06:26:15,532 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2022-01-13 06:26:15,533 [Listener at om3/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
om3_1        | 2022-01-13 06:26:15,692 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2022-01-13 06:26:15,695 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2022-01-13 06:26:15,697 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1        | 2022-01-13 06:26:15,817 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2022-01-13 06:26:15,822 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@14abde3e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1        | 2022-01-13 06:26:15,830 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@200ef05b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1        | 2022-01-13 06:26:16,344 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2022-01-13 06:26:16,418 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@47e6004c{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0-SNAPSHOT_jar-_-any-6312043842480335886/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | , id: "om3"
om1_1        | address: "om3:9872"
om1_1        | , id: "om2"
om1_1        | address: "om2:9872"
om1_1        | ]
om3_1        | 2022-01-13 06:26:16,483 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@1871068d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1        | 2022-01-13 06:26:16,485 [Listener at om3/9862] INFO server.Server: Started @63173ms
om3_1        | 2022-01-13 06:26:16,502 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1        | 2022-01-13 06:26:16,502 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1        | 2022-01-13 06:26:16,504 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1        | 2022-01-13 06:26:16,515 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1        | 2022-01-13 06:26:16,517 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1        | 2022-01-13 06:26:16,841 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om3_1        | 2022-01-13 06:26:16,881 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5b7b5484] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1        | 2022-01-13 06:26:17,777 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:37055
om3_1        | 2022-01-13 06:26:17,804 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om3_1        | 2022-01-13 06:26:19,584 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5159561261ns, electionTimeout:5138ms
om3_1        | 2022-01-13 06:26:19,589 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2022-01-13 06:26:19,599 [om3@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2022-01-13 06:26:19,640 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om3_1        | 2022-01-13 06:26:19,640 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderElection1
om3_1        | 2022-01-13 06:26:19,672 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2022-01-13 06:26:23,602 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 1, (t:0, i:~))
om3_1        | 2022-01-13 06:26:23,728 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-562213E44849-CANDIDATE: reject ELECTION from om1: already has voted for om3 at current term 1
om3_1        | 2022-01-13 06:26:23,770 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om1<-om3#0:FAIL-t1. Peer's state: om3@group-562213E44849:t1, leader=null, voted=om3, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2022-01-13 06:26:25,143 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1        | 2022-01-13 06:26:25,144 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t1
om3_1        | 2022-01-13 06:26:25,144 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t1
om3_1        | 2022-01-13 06:26:25,144 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om3_1        | 2022-01-13 06:26:25,146 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om3_1        | 2022-01-13 06:26:25,146 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-LeaderElection1
om3_1        | 2022-01-13 06:26:25,146 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2022-01-13 06:26:27,513 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 2, (t:0, i:~))
om3_1        | 2022-01-13 06:26:27,519 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-562213E44849-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om3_1        | 2022-01-13 06:26:27,519 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:om2
om3_1        | 2022-01-13 06:26:27,519 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2022-01-13 06:26:27,519 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2022-01-13 06:26:27,520 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState was interrupted: {}
om3_1        | java.lang.InterruptedException: sleep interrupted
om3_1        | 	at java.base/java.lang.Thread.sleep(Native Method)
om3_1        | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
om3_1        | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
om3_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
om3_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
om3_1        | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
om3_1        | 2022-01-13 06:26:27,544 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om2<-om3#0:OK-t2. Peer's state: om3@group-562213E44849:t2, leader=null, voted=om2, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2022-01-13 06:26:27,994 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: change Leader from null to om2 at term 2 for appendEntries, leader elected after 15680ms
om3_1        | 2022-01-13 06:26:28,115 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om3_1        | 2022-01-13 06:26:28,150 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om3_1        | 2022-01-13 06:26:28,807 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1        | 2022-01-13 06:26:31,541 [om3@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1        | [id: "om1"
om3_1        | address: "om1:9872"
om3_1        | , id: "om3"
om3_1        | address: "om3:9872"
om3_1        | , id: "om2"
om3_1        | address: "om2:9872"
om3_1        | ]
recon_1      | 2022-01-13 06:23:46,802 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:48,804 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 6 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:48,805 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 7 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:48,808 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:50,810 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 9 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:50,813 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 10 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:50,814 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:52,816 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 12 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:52,818 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 13 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:52,819 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:54,821 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 15 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:54,822 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 16 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:54,823 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:56,825 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 18 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:56,831 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 19 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:56,832 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 20 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:23:58,834 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 21 failover attempts. Trying to failover immediately.
om2_1        | 2022-01-13 06:26:27,449 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderElection2
om2_1        | 2022-01-13 06:26:27,462 [om2@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2022-01-13 06:26:27,573 [om2@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
om2_1        | 2022-01-13 06:26:27,573 [om2@group-562213E44849-LeaderElection2] INFO impl.LeaderElection:   Response 0: om2<-om3#0:OK-t2
om2_1        | 2022-01-13 06:26:27,573 [om2@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection2 ELECTION round 0: result PASSED
om2_1        | 2022-01-13 06:26:27,574 [om2@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-LeaderElection2
om2_1        | 2022-01-13 06:26:27,574 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
om2_1        | 2022-01-13 06:26:27,574 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om2@group-562213E44849: change Leader from null to om2 at term 2 for becomeLeader, leader elected after 17395ms
om2_1        | 2022-01-13 06:26:27,598 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1        | 2022-01-13 06:26:27,647 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1        | 2022-01-13 06:26:27,649 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1        | 2022-01-13 06:26:27,662 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om2_1        | 2022-01-13 06:26:27,662 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1        | 2022-01-13 06:26:27,664 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1        | 2022-01-13 06:26:27,673 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1        | 2022-01-13 06:26:27,676 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1        | 2022-01-13 06:26:27,688 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1        | 2022-01-13 06:26:27,689 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2022-01-13 06:26:27,689 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1        | 2022-01-13 06:26:27,697 [om2@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1        | 2022-01-13 06:26:27,697 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2022-01-13 06:26:27,697 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2022-01-13 06:26:27,704 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1        | 2022-01-13 06:26:27,704 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2022-01-13 06:26:27,704 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1        | 2022-01-13 06:26:27,704 [om2@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1        | 2022-01-13 06:26:27,705 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2022-01-13 06:26:27,705 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2022-01-13 06:26:27,709 [om2@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderStateImpl
om2_1        | 2022-01-13 06:26:27,740 [om2@group-562213E44849-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1        | 2022-01-13 06:26:27,808 [om2@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om2@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om2_1        | 2022-01-13 06:26:28,168 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om2_1        | 2022-01-13 06:26:28,866 [om2@group-562213E44849-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1        | [id: "om1"
om2_1        | address: "om1:9872"
om2_1        | , id: "om3"
om2_1        | address: "om3:9872"
om2_1        | , id: "om2"
om2_1        | address: "om2:9872"
om2_1        | ]
recon_1      | 2022-01-13 06:23:58,836 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 22 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:23:58,837 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 23 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:00,838 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 24 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:00,839 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 25 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:00,840 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 26 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:02,842 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 27 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:02,845 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 28 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:02,846 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 29 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:04,849 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 30 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:04,852 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 31 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:04,854 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 32 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:06,861 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 33 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:06,863 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 34 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:06,864 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 35 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:08,872 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 36 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:08,874 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 37 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:08,884 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 38 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:10,885 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 39 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:10,886 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 40 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:10,891 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 41 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:12,893 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 42 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:12,894 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 43 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:12,895 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 44 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:14,897 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 45 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:14,900 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 46 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:14,905 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 47 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:16,910 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 48 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:16,911 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 49 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:16,912 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 50 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:18,914 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 51 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:18,915 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 52 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:18,917 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 53 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:20,920 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 54 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:20,921 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 55 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:20,921 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 56 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:22,923 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 57 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:22,925 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 58 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:22,926 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 59 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:24,929 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 60 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:24,930 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 61 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:24,931 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 62 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:26,932 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 63 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:26,936 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 64 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:26,936 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 65 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:28,939 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 66 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:28,940 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 67 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:28,942 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 68 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:30,944 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 69 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:30,947 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 70 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:30,948 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 71 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:32,953 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 72 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:32,954 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 73 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:32,964 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 74 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:34,972 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 75 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:34,984 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 76 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:34,985 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 77 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:36,988 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 78 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:36,989 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 79 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:36,990 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 80 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:38,992 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 81 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:38,992 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 82 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:38,994 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 83 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:40,995 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 84 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:40,996 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 85 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:40,997 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 86 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:42,999 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 87 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:43,000 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 88 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:43,001 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 89 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:45,003 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 90 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:45,004 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 91 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:45,004 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 92 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:47,008 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 93 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:47,009 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 94 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:47,010 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 95 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:49,011 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 96 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:49,013 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 97 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:49,014 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 98 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:51,016 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 99 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:51,017 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 100 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:51,021 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 101 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:53,028 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 102 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:53,040 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 103 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:53,051 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 104 failover attempts. Trying to failover after sleeping for 2000ms.
scm1.org_1   | Sleeping for 5 seconds
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2022-01-13 06:23:03,929 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.10
scm1.org_1   | ************************************************************/
scm1.org_1   | 2022-01-13 06:23:04,019 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2022-01-13 06:23:04,857 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2022-01-13 06:23:05,200 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2022-01-13 06:23:05,222 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2022-01-13 06:23:05,654 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2022-01-13 06:23:05,654 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2022-01-13 06:23:05,734 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2022-01-13 06:23:10,280 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm1.org_1   | 2022-01-13 06:23:10,287 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2022-01-13 06:23:10,289 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2022-01-13 06:23:15,479 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2022-01-13 06:23:17,161 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2022-01-13 06:23:17,161 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2022-01-13 06:23:17,726 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2022-01-13 06:23:17,731 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2022-01-13 06:23:17,732 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:c6280d25-12c7-4451-bd27-cfac689faecd,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:scm-sub@scm1.org
scm1.org_1   | 2022-01-13 06:23:18,115 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2022-01-13 06:23:18,571 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2022-01-13 06:23:18,949 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm1.org_1   | 2022-01-13 06:23:18,960 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:18,963 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm1.org_1   | 2022-01-13 06:23:18,964 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:18,964 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:18,965 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2022-01-13 06:23:18,969 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:23:18,977 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2022-01-13 06:23:18,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2022-01-13 06:23:19,964 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2022-01-13 06:23:19,966 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2022-01-13 06:23:19,973 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2022-01-13 06:23:20,029 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2022-01-13 06:23:20,105 [main] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: addNew group-00FB17B6AB6E:[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|priority:0] returns group-00FB17B6AB6E:java.util.concurrent.CompletableFuture@12e5da86[Not completed]
scm1.org_1   | 2022-01-13 06:23:20,229 [pool-2-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd: new RaftServerImpl for group-00FB17B6AB6E:[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|priority:0] with SCMStateMachine:uninitialized
scm1.org_1   | 2022-01-13 06:23:20,245 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2022-01-13 06:23:20,245 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2022-01-13 06:23:20,245 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2022-01-13 06:23:20,246 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2022-01-13 06:23:20,246 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2022-01-13 06:23:20,246 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2022-01-13 06:23:20,252 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2022-01-13 06:23:20,271 [pool-2-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: ConfigurationManager, init=-1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|priority:0], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2022-01-13 06:23:20,283 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2022-01-13 06:23:20,298 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2022-01-13 06:23:20,301 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2022-01-13 06:23:20,303 [pool-2-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e does not exist. Creating ...
scm1.org_1   | 2022-01-13 06:23:20,370 [pool-2-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/in_use.lock acquired by nodename 85@scm1.org
scm1.org_1   | 2022-01-13 06:23:20,396 [pool-2-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e has been successfully formatted.
recon_1      | 2022-01-13 06:24:55,061 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 105 failover attempts. Trying to failover immediately.
scm1.org_1   | 2022-01-13 06:23:20,416 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2022-01-13 06:23:20,421 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2022-01-13 06:23:20,463 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2022-01-13 06:23:20,471 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:23:20,524 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2022-01-13 06:23:20,963 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:21,005 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2022-01-13 06:23:21,005 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2022-01-13 06:23:21,030 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: new c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm1.org_1   | 2022-01-13 06:23:21,039 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2022-01-13 06:23:21,040 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:21,041 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:21,041 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:21,048 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2022-01-13 06:23:21,049 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2022-01-13 06:23:21,050 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2022-01-13 06:23:21,059 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2022-01-13 06:23:21,106 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2022-01-13 06:23:21,107 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2022-01-13 06:23:21,138 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2022-01-13 06:23:21,139 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2022-01-13 06:23:21,175 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2022-01-13 06:23:21,176 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2022-01-13 06:23:21,177 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2022-01-13 06:23:21,177 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2022-01-13 06:23:21,196 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2022-01-13 06:23:21,197 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2022-01-13 06:23:21,349 [main] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: start as a follower, conf=-1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:21,353 [main] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2022-01-13 06:23:21,355 [main] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState
scm1.org_1   | 2022-01-13 06:23:21,403 [main] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-00FB17B6AB6E,id=c6280d25-12c7-4451-bd27-cfac689faecd
scm1.org_1   | 2022-01-13 06:23:21,452 [main] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: start RPC server
scm1.org_1   | 2022-01-13 06:23:21,642 [main] INFO server.GrpcService: c6280d25-12c7-4451-bd27-cfac689faecd: GrpcService started, listening on 9894
scm1.org_1   | 2022-01-13 06:23:21,688 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$338/0x000000084031f440@43a09ce2] INFO util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Started
scm1.org_1   | 2022-01-13 06:23:26,518 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.FollowerState: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5162972642ns, electionTimeout:5137ms
scm1.org_1   | 2022-01-13 06:23:26,519 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState
scm1.org_1   | 2022-01-13 06:23:26,520 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2022-01-13 06:23:26,523 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2022-01-13 06:23:26,523 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1
scm1.org_1   | 2022-01-13 06:23:26,531 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.LeaderElection: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:26,532 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.LeaderElection: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2022-01-13 06:23:26,532 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1
scm1.org_1   | 2022-01-13 06:23:26,533 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2022-01-13 06:23:26,533 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: change Leader from null to c6280d25-12c7-4451-bd27-cfac689faecd at term 1 for becomeLeader, leader elected after 6118ms
recon_1      | 2022-01-13 06:24:55,063 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 106 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:55,064 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 107 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:57,066 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 108 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:57,068 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 109 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:57,072 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 110 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:24:59,077 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 111 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:59,078 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 112 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:24:59,082 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 113 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:01,085 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 114 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:01,087 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 115 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:01,088 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 116 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:03,090 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 117 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:03,092 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 118 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:03,093 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 119 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:05,095 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 120 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:05,100 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 121 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:05,100 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 122 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:07,105 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 123 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:07,120 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 124 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:07,120 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 125 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:09,127 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 126 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:09,129 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 127 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:09,130 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 128 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:11,133 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 129 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:11,134 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 130 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:11,135 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 131 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:13,136 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 132 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:13,138 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 133 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:13,138 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 134 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:15,140 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 135 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:15,141 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 136 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:15,142 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 137 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:17,144 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 138 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:17,145 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 139 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:17,146 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 140 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:19,150 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 141 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:19,151 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 142 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:19,152 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 143 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:21,153 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 144 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:21,157 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 145 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:21,157 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 146 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:23,161 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 147 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:23,162 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 148 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:23,163 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 149 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:25,165 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 150 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:25,168 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 151 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:25,172 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 152 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:27,174 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 153 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:27,178 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 154 failover attempts. Trying to failover immediately.
scm2.org_1   | Sleeping for 5 seconds
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2022-01-13 06:23:25,088 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.10
scm2.org_1   | ************************************************************/
scm2.org_1   | 2022-01-13 06:23:25,116 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2022-01-13 06:23:25,226 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2022-01-13 06:23:25,226 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2022-01-13 06:23:25,297 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2022-01-13 06:23:25,297 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2022-01-13 06:23:25,307 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2022-01-13 06:23:25,535 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2022-01-13 06:23:25,536 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2022-01-13 06:23:27,808 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:29,810 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:31,812 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:33,813 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:35,845 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:38,075 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c6280d25-12c7-4451-bd27-cfac689faecd is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:245)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13937)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:40,076 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:42,078 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2022-01-13 06:23:44,243 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm2.org_1   | 2022-01-13 06:23:45,685 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm2.org_1   | 2022-01-13 06:23:45,687 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2022-01-13 06:23:45,691 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2022-01-13 06:23:47,348 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2022-01-13 06:23:47,418 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2022-01-13 06:23:47,421 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2022-01-13 06:23:47,433 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:949f8263-e6fc-4c45-bf68-7d22f1ef9f4f,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:scm-sub@scm2.org
scm2.org_1   | 2022-01-13 06:23:50,784 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2022-01-13 06:23:50,815 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e, SCMID 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f
scm2.org_1   | 2022-01-13 06:23:50,816 [main] INFO server.StorageContainerManager: Primary SCM Node ID c6280d25-12c7-4451-bd27-cfac689faecd
scm2.org_1   | 2022-01-13 06:23:50,871 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2022-01-13 06:23:53,853 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.10
scm2.org_1   | ************************************************************/
scm2.org_1   | 2022-01-13 06:23:53,866 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2022-01-13 06:23:53,971 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2022-01-13 06:23:53,971 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2022-01-13 06:23:54,030 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2022-01-13 06:23:54,031 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2022-01-13 06:23:54,086 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2022-01-13 06:25:27,179 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 155 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:29,185 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 156 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:29,186 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 157 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:29,189 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 158 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:31,191 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 159 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:31,192 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 160 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:31,200 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 161 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:33,202 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 162 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:33,203 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 163 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:33,204 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 164 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:35,205 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 165 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:35,206 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 166 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:35,207 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 167 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:37,185 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:46400
recon_1      | 2022-01-13 06:25:37,208 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 168 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:37,212 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 169 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:37,244 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 170 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:37,313 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:25:37,452 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50200
scm2.org_1   | 2022-01-13 06:23:54,120 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm2.org_1   | 2022-01-13 06:23:54,362 [main] INFO reflections.Reflections: Reflections took 127 ms to scan 3 urls, producing 103 keys and 217 values 
scm2.org_1   | 2022-01-13 06:23:54,964 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm2.org_1   | 2022-01-13 06:23:55,065 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2022-01-13 06:23:55,067 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/912940625625.crt.
scm2.org_1   | 2022-01-13 06:23:55,077 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2022-01-13 06:23:55,256 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2022-01-13 06:23:55,257 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2022-01-13 06:23:55,287 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2022-01-13 06:23:55,452 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2022-01-13 06:23:55,704 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2022-01-13 06:23:55,704 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2022-01-13 06:23:55,909 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2022-01-13 06:23:56,006 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:949f8263-e6fc-4c45-bf68-7d22f1ef9f4f
scm2.org_1   | 2022-01-13 06:23:56,200 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2022-01-13 06:23:56,361 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm2.org_1   | 2022-01-13 06:23:56,362 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2022-01-13 06:23:56,363 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm2.org_1   | 2022-01-13 06:23:56,372 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2022-01-13 06:23:56,372 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2022-01-13 06:23:56,373 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2022-01-13 06:23:56,375 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2022-01-13 06:23:56,376 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2022-01-13 06:23:56,378 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2022-01-13 06:23:57,433 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2022-01-13 06:23:57,445 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2022-01-13 06:23:57,445 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2022-01-13 06:23:57,459 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2022-01-13 06:23:57,469 [main] INFO server.RaftServer: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: addNew group-00FB17B6AB6E:[] returns group-00FB17B6AB6E:java.util.concurrent.CompletableFuture@1002d192[Not completed]
scm2.org_1   | 2022-01-13 06:23:57,501 [pool-14-thread-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: new RaftServerImpl for group-00FB17B6AB6E:[] with SCMStateMachine:uninitialized
scm2.org_1   | 2022-01-13 06:23:57,504 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2022-01-13 06:23:26,539 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2022-01-13 06:23:26,544 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:26,544 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2022-01-13 06:23:26,551 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2022-01-13 06:23:26,552 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2022-01-13 06:23:26,552 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2022-01-13 06:23:26,557 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:26,559 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2022-01-13 06:23:26,561 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl
scm1.org_1   | 2022-01-13 06:23:26,588 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2022-01-13 06:23:26,625 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 0: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:26,675 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_0
scm1.org_1   | 2022-01-13 06:23:27,654 [main] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: close
scm1.org_1   | 2022-01-13 06:23:27,654 [main] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: shutdown
scm1.org_1   | 2022-01-13 06:23:27,655 [main] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-00FB17B6AB6E,id=c6280d25-12c7-4451-bd27-cfac689faecd
scm1.org_1   | 2022-01-13 06:23:27,655 [main] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl
scm1.org_1   | 2022-01-13 06:23:27,660 [main] INFO impl.PendingRequests: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2022-01-13 06:23:27,663 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2022-01-13 06:23:27,664 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2022-01-13 06:23:27,665 [main] INFO impl.StateMachineUpdater: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2022-01-13 06:23:27,671 [main] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: closes. applyIndex: 0
scm1.org_1   | 2022-01-13 06:23:27,672 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1.org_1   | 2022-01-13 06:23:27,675 [main] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker close()
scm1.org_1   | 2022-01-13 06:23:27,677 [main] INFO server.GrpcService: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown server with port 9894 now
scm1.org_1   | 2022-01-13 06:23:27,682 [main] INFO server.GrpcService: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown server with port 9894 successfully
scm1.org_1   | 2022-01-13 06:23:27,682 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$338/0x000000084031f440@43a09ce2] INFO util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Stopped
scm1.org_1   | 2022-01-13 06:23:27,683 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2022-01-13 06:23:27,686 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e; layoutVersion=2; scmId=c6280d25-12c7-4451-bd27-cfac689faecd
scm1.org_1   | 2022-01-13 06:23:27,705 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2022-01-13 06:23:29,569 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.10
scm1.org_1   | ************************************************************/
scm1.org_1   | 2022-01-13 06:23:29,578 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2022-01-13 06:23:29,648 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2022-01-13 06:23:29,648 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2022-01-13 06:23:29,765 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2022-01-13 06:23:29,765 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2022-01-13 06:23:29,822 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2022-01-13 06:23:29,868 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm1.org_1   | 2022-01-13 06:23:30,118 [main] INFO reflections.Reflections: Reflections took 128 ms to scan 3 urls, producing 103 keys and 217 values 
scm1.org_1   | 2022-01-13 06:23:30,790 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm1.org_1   | 2022-01-13 06:23:30,920 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2022-01-13 06:23:30,929 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2022-01-13 06:23:30,933 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/882210938510.crt.
scm1.org_1   | 2022-01-13 06:23:31,079 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2022-01-13 06:23:31,079 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2022-01-13 06:23:31,111 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2022-01-13 06:23:31,294 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2022-01-13 06:23:31,629 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2022-01-13 06:23:31,629 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2022-01-13 06:23:31,761 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2022-01-13 06:23:31,783 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:c6280d25-12c7-4451-bd27-cfac689faecd
scm1.org_1   | 2022-01-13 06:23:31,868 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2022-01-13 06:23:31,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm1.org_1   | 2022-01-13 06:23:31,943 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:31,944 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm1.org_1   | 2022-01-13 06:23:31,944 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:31,944 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2022-01-13 06:23:31,945 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2022-01-13 06:23:31,947 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:23:31,948 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2022-01-13 06:23:31,949 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2022-01-13 06:23:32,572 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2022-01-13 06:23:32,574 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2022-01-13 06:23:32,574 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2022-01-13 06:23:32,601 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2022-01-13 06:23:32,604 [main] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: found a subdirectory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm1.org_1   | 2022-01-13 06:23:32,611 [main] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: addNew group-00FB17B6AB6E:[] returns group-00FB17B6AB6E:java.util.concurrent.CompletableFuture@36ad5f2a[Not completed]
scm1.org_1   | 2022-01-13 06:23:32,636 [pool-14-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd: new RaftServerImpl for group-00FB17B6AB6E:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2022-01-13 06:23:32,638 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2022-01-13 06:23:32,639 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2022-01-13 06:23:32,639 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2022-01-13 06:23:32,640 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2022-01-13 06:23:32,640 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2022-01-13 06:23:32,640 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2022-01-13 06:23:32,641 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2022-01-13 06:23:32,646 [pool-14-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2022-01-13 06:23:32,647 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2022-01-13 06:23:32,650 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2022-01-13 06:23:32,650 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1.org_1   | 2022-01-13 06:23:32,660 [pool-14-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/in_use.lock acquired by nodename 6@scm1.org
scm1.org_1   | 2022-01-13 06:23:32,668 [pool-14-thread-1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=c6280d25-12c7-4451-bd27-cfac689faecd} from /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/raft-meta
scm1.org_1   | 2022-01-13 06:23:32,706 [pool-14-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 0: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:32,707 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2022-01-13 06:23:32,708 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2022-01-13 06:23:32,717 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2022-01-13 06:23:32,717 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:23:32,731 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:32,742 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2022-01-13 06:23:32,742 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2022-01-13 06:23:32,748 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: new c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm2.org_1   | 2022-01-13 06:23:57,504 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2022-01-13 06:23:57,505 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2022-01-13 06:23:57,506 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2022-01-13 06:23:57,507 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2022-01-13 06:23:57,507 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2022-01-13 06:23:57,508 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2022-01-13 06:23:57,518 [pool-14-thread-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2022-01-13 06:23:57,518 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2022-01-13 06:23:57,529 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2022-01-13 06:23:57,530 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2.org_1   | 2022-01-13 06:23:57,532 [pool-14-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e does not exist. Creating ...
scm2.org_1   | 2022-01-13 06:23:57,577 [pool-14-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2022-01-13 06:23:57,615 [pool-14-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e has been successfully formatted.
scm2.org_1   | 2022-01-13 06:23:57,629 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2022-01-13 06:23:57,641 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2022-01-13 06:23:57,655 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2022-01-13 06:23:57,655 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2022-01-13 06:23:57,693 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2022-01-13 06:23:57,721 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2022-01-13 06:23:57,728 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2022-01-13 06:23:57,733 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: new 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm2.org_1   | 2022-01-13 06:23:57,739 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2022-01-13 06:23:57,740 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2022-01-13 06:23:57,741 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2.org_1   | 2022-01-13 06:23:57,742 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2.org_1   | 2022-01-13 06:23:57,742 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2022-01-13 06:23:57,744 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2022-01-13 06:23:57,744 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2022-01-13 06:23:57,745 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2022-01-13 06:23:57,763 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2022-01-13 06:23:57,771 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2022-01-13 06:23:57,778 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2022-01-13 06:23:57,783 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2022-01-13 06:23:57,788 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2022-01-13 06:23:57,795 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2022-01-13 06:23:57,796 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2022-01-13 06:23:57,797 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2022-01-13 06:23:57,798 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2022-01-13 06:23:57,799 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2022-01-13 06:23:57,879 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2022-01-13 06:23:57,880 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2022-01-13 06:23:57,880 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2022-01-13 06:23:58,413 [main] INFO ha.SequenceIdGenerator: upgrade localId to 109611004723200000
scm2.org_1   | 2022-01-13 06:23:58,424 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2022-01-13 06:23:58,428 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2022-01-13 06:23:58,436 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2022-01-13 06:23:58,565 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2022-01-13 06:23:58,612 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2.org_1   | 2022-01-13 06:23:58,639 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2022-01-13 06:23:58,761 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2022-01-13 06:23:58,786 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2022-01-13 06:23:58,786 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2022-01-13 06:23:58,886 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
recon_1      | 2022-01-13 06:25:37,546 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:25:39,246 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 171 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:39,264 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 172 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:39,265 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 173 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:39,565 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59892
recon_1      | 2022-01-13 06:25:39,646 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:25:41,266 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 174 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:41,267 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 175 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:41,267 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 176 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:41,847 [IPC Server handler 47 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22377b14-877d-4b48-97e5-336a98a1cf76
recon_1      | 2022-01-13 06:25:41,866 [IPC Server handler 47 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:25:41,978 [IPC Server handler 2 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1a62a11-0048-42bf-a9c3-836e0373e878
recon_1      | 2022-01-13 06:25:42,029 [IPC Server handler 2 on default port 9891] INFO node.SCMNodeManager: Registered Data node : b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:25:42,063 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ab6e22bd-d408-4246-98c0-61fe3297cb52
recon_1      | 2022-01-13 06:25:42,068 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:25:42,199 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 22377b14-877d-4b48-97e5-336a98a1cf76 to Node DB.
recon_1      | 2022-01-13 06:25:42,227 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node b1a62a11-0048-42bf-a9c3-836e0373e878 to Node DB.
recon_1      | 2022-01-13 06:25:42,251 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node ab6e22bd-d408-4246-98c0-61fe3297cb52 to Node DB.
recon_1      | 2022-01-13 06:25:42,776 [IPC Server handler 45 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:25:42,808 [IPC Server handler 25 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:25:43,110 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:25:43,268 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 177 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:43,270 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 178 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:43,291 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 179 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:45,293 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 180 failover attempts. Trying to failover immediately.
scm1.org_1   | 2022-01-13 06:23:32,749 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2022-01-13 06:23:32,749 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:32,750 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:32,751 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1.org_1   | 2022-01-13 06:23:32,752 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2022-01-13 06:23:32,753 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2022-01-13 06:23:32,753 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2022-01-13 06:23:32,754 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2022-01-13 06:23:32,766 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2022-01-13 06:23:32,767 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2022-01-13 06:23:32,801 [pool-14-thread-1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 0: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:32,802 [pool-14-thread-1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_0
scm1.org_1   | 2022-01-13 06:23:32,806 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:23:32,806 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2022-01-13 06:23:32,884 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2022-01-13 06:23:32,884 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2022-01-13 06:23:32,886 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2022-01-13 06:23:32,888 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2022-01-13 06:23:32,889 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2022-01-13 06:23:32,889 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2022-01-13 06:23:32,938 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2022-01-13 06:23:32,938 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2022-01-13 06:23:32,939 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2022-01-13 06:23:33,212 [main] INFO ha.SequenceIdGenerator: upgrade localId to 109611004723200000
scm1.org_1   | 2022-01-13 06:23:33,212 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2022-01-13 06:23:33,215 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2022-01-13 06:23:33,218 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2022-01-13 06:23:33,284 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2022-01-13 06:23:33,309 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1.org_1   | 2022-01-13 06:23:33,344 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1.org_1   | 2022-01-13 06:23:33,392 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2022-01-13 06:23:33,399 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2022-01-13 06:23:33,399 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2022-01-13 06:23:33,436 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2022-01-13 06:23:33,456 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2022-01-13 06:23:33,488 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2022-01-13 06:23:33,504 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2022-01-13 06:23:33,515 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 0 containers.
scm1.org_1   | 2022-01-13 06:23:33,522 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2022-01-13 06:23:33,527 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:23:33,529 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2022-01-13 06:23:33,619 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2022-01-13 06:23:33,629 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2022-01-13 06:23:33,631 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 882210938510 on primary SCM
scm1.org_1   | 2022-01-13 06:23:33,638 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2022-01-13 06:23:33,679 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2022-01-13 06:23:33,747 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2022-01-13 06:23:34,702 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2022-01-13 06:23:34,703 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2022-01-13 06:23:34,736 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2022-01-13 06:23:34,737 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2022-01-13 06:23:34,776 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2022-01-13 06:25:45,295 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 181 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:45,296 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 182 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:47,300 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 183 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:47,301 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 184 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:47,308 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 185 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:49,310 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 186 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:49,311 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 187 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:49,312 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 188 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:51,314 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 189 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:51,315 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 190 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:51,317 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 191 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:53,318 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 192 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:53,319 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 193 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:53,320 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 194 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:55,321 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 195 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:55,323 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 196 failover attempts. Trying to failover immediately.
scm3.org_1   | Sleeping for 5 seconds
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2022-01-13 06:24:05,823 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm2.org_1   | 2022-01-13 06:23:58,928 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2022-01-13 06:23:58,996 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2022-01-13 06:23:59,021 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2022-01-13 06:23:59,044 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 0 containers.
scm2.org_1   | 2022-01-13 06:23:59,064 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2022-01-13 06:23:59,080 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:23:59,083 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2022-01-13 06:23:59,147 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2022-01-13 06:23:59,262 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2022-01-13 06:23:59,322 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2022-01-13 06:24:00,829 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2022-01-13 06:24:00,840 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2022-01-13 06:24:00,913 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2022-01-13 06:24:00,939 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2022-01-13 06:24:01,016 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2022-01-13 06:24:01,031 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2022-01-13 06:24:01,302 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2.org_1   | 2022-01-13 06:24:01,322 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        false
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                                                Value
scm2.org_1   | Threshold                                          10
scm2.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2.org_1   | Max Size to Move per Iteration                     500GB
scm2.org_1   | Max Size Entering Target per Iteration             26GB
scm2.org_1   | Max Size Leaving Source per Iteration              26GB
scm2.org_1   | 
scm2.org_1   | 2022-01-13 06:24:01,322 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2.org_1   | 2022-01-13 06:24:01,322 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2022-01-13 06:24:01,326 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2022-01-13 06:24:01,333 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2022-01-13 06:24:01,335 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: start with initializing state, conf=-1: [], old=null
scm2.org_1   | 2022-01-13 06:24:01,339 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2.org_1   | 2022-01-13 06:24:01,340 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-00FB17B6AB6E,id=949f8263-e6fc-4c45-bf68-7d22f1ef9f4f
scm2.org_1   | 2022-01-13 06:24:01,355 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: start RPC server
scm2.org_1   | 2022-01-13 06:24:01,519 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: GrpcService started, listening on 9894
scm2.org_1   | 2022-01-13 06:24:01,533 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$419/0x0000000840537040@2a49753] INFO util.JvmPauseMonitor: JvmPauseMonitor-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Started
scm2.org_1   | 2022-01-13 06:24:01,540 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:01,540 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:01,540 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:04,135 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:04,152 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2022-01-13 06:24:04,153 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: change Leader from null to c6280d25-12c7-4451-bd27-cfac689faecd at term 2 for installSnapshot, leader elected after 6524ms
scm2.org_1   | 2022-01-13 06:24:04,155 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Received notification to install snapshot at index 4
scm2.org_1   | 2022-01-13 06:24:04,279 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: notifyInstallSnapshot: nextIndex is 0 but the leader's first available index is 4.
scm2.org_1   | 2022-01-13 06:24:04,280 [grpc-default-executor-0] INFO ha.SCMStateMachine: Received install snapshot notification from SCM leader: scm1.org:9894 with term index: (t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:04,284 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Downloading checkpoint from leader SCM scm1 and reloading state from the checkpoint.
scm2.org_1   | 2022-01-13 06:24:05,610 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
recon_1      | 2022-01-13 06:25:55,324 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 197 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:57,329 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 198 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:57,330 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 199 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:57,330 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 200 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:25:59,341 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 201 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:59,342 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 202 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:25:59,345 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 203 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:01,353 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 204 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:01,354 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 205 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:01,355 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 206 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:03,356 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 207 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:03,357 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 208 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:03,357 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 209 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:05,358 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 210 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:05,360 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 211 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:05,366 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 212 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:07,367 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 213 failover attempts. Trying to failover immediately.
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.10
scm3.org_1   | ************************************************************/
scm3.org_1   | 2022-01-13 06:24:05,874 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2022-01-13 06:24:06,142 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2022-01-13 06:24:06,144 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2022-01-13 06:24:06,323 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2022-01-13 06:24:06,324 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2022-01-13 06:24:06,335 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:06,904 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2022-01-13 06:24:06,904 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2022-01-13 06:24:07,376 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2022-01-13 06:24:07,867 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm3.org_1   | 2022-01-13 06:24:07,867 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2022-01-13 06:24:07,869 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2022-01-13 06:24:08,378 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2022-01-13 06:24:08,419 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2022-01-13 06:24:08,420 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2022-01-13 06:24:08,423 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:e3ebff3b-14c4-4117-a7dd-8ec94ff36cba,clusterId:CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e,subject:scm-sub@scm3.org
scm3.org_1   | 2022-01-13 06:24:09,348 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2022-01-13 06:24:09,398 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e, SCMID e3ebff3b-14c4-4117-a7dd-8ec94ff36cba
scm3.org_1   | 2022-01-13 06:24:09,403 [main] INFO server.StorageContainerManager: Primary SCM Node ID c6280d25-12c7-4451-bd27-cfac689faecd
scm3.org_1   | 2022-01-13 06:24:09,491 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2022-01-13 06:24:13,562 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.3.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.3.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/3eb7235ca879498c2d8ebcd5fd228d6e4cb16891 ; compiled by 'runner' on 2022-01-13T05:58Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.10
scm3.org_1   | ************************************************************/
scm3.org_1   | 2022-01-13 06:24:13,603 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2022-01-13 06:24:13,780 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2022-01-13 06:24:13,780 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2022-01-13 06:24:13,943 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2022-01-13 06:24:13,944 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2022-01-13 06:24:14,041 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:14,103 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm3.org_1   | 2022-01-13 06:24:14,698 [main] INFO reflections.Reflections: Reflections took 274 ms to scan 3 urls, producing 103 keys and 217 values 
scm3.org_1   | 2022-01-13 06:24:15,832 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm3.org_1   | 2022-01-13 06:24:16,090 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2022-01-13 06:24:16,103 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2022-01-13 06:24:16,106 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/932928948488.crt.
scm3.org_1   | 2022-01-13 06:24:16,357 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2022-01-13 06:24:16,357 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2022-01-13 06:24:16,441 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:16,829 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:17,277 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2022-01-13 06:24:17,282 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2022-01-13 06:24:17,567 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2022-01-13 06:24:17,668 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:e3ebff3b-14c4-4117-a7dd-8ec94ff36cba
scm3.org_1   | 2022-01-13 06:24:17,876 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2022-01-13 06:24:18,116 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm3.org_1   | 2022-01-13 06:24:18,118 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2022-01-13 06:24:18,118 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm3.org_1   | 2022-01-13 06:24:18,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2022-01-13 06:24:18,120 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2022-01-13 06:24:18,130 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2022-01-13 06:24:18,134 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:23:34,778 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2022-01-13 06:23:34,873 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1.org_1   | 2022-01-13 06:23:34,923 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        false
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                                                Value
scm1.org_1   | Threshold                                          10
scm1.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1.org_1   | Max Size to Move per Iteration                     500GB
scm1.org_1   | Max Size Entering Target per Iteration             26GB
scm1.org_1   | Max Size Leaving Source per Iteration              26GB
scm1.org_1   | 
scm3.org_1   | 2022-01-13 06:24:18,139 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2022-01-13 06:24:18,141 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2022-01-13 06:24:19,329 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2022-01-13 06:24:19,332 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2022-01-13 06:24:19,332 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2022-01-13 06:24:19,344 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2022-01-13 06:24:19,350 [main] INFO server.RaftServer: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: addNew group-00FB17B6AB6E:[] returns group-00FB17B6AB6E:java.util.concurrent.CompletableFuture@1002d192[Not completed]
scm3.org_1   | 2022-01-13 06:24:19,378 [pool-14-thread-1] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: new RaftServerImpl for group-00FB17B6AB6E:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2022-01-13 06:24:19,380 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2022-01-13 06:24:19,380 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2022-01-13 06:24:19,381 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2022-01-13 06:24:19,382 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2022-01-13 06:24:19,382 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2022-01-13 06:24:19,382 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2022-01-13 06:24:19,383 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2022-01-13 06:24:19,388 [pool-14-thread-1] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2022-01-13 06:24:19,388 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2022-01-13 06:24:19,395 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2022-01-13 06:24:19,396 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3.org_1   | 2022-01-13 06:24:19,398 [pool-14-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e does not exist. Creating ...
scm3.org_1   | 2022-01-13 06:24:19,410 [pool-14-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2022-01-13 06:24:19,425 [pool-14-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e has been successfully formatted.
scm3.org_1   | 2022-01-13 06:24:19,436 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2022-01-13 06:24:19,440 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2022-01-13 06:24:19,460 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2022-01-13 06:24:19,460 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2022-01-13 06:24:19,477 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2022-01-13 06:24:19,488 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2022-01-13 06:24:19,488 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2022-01-13 06:24:19,494 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: new e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm3.org_1   | 2022-01-13 06:24:19,495 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2022-01-13 06:24:19,495 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2022-01-13 06:24:19,496 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3.org_1   | 2022-01-13 06:24:19,497 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3.org_1   | 2022-01-13 06:24:19,498 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2022-01-13 06:24:19,499 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2022-01-13 06:24:19,500 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2022-01-13 06:24:19,500 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2022-01-13 06:24:19,508 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2022-01-13 06:24:19,508 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3.org_1   | 2022-01-13 06:24:19,514 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2022-01-13 06:24:19,514 [pool-14-thread-1] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2022-01-13 06:24:19,518 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2022-01-13 06:24:19,520 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1      | 2022-01-13 06:26:07,368 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 214 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:07,369 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 215 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:09,373 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 216 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:09,376 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om2:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 217 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:09,378 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om3:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 218 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:11,379 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From recon/172.25.0.115 to om1:9862 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 219 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:12,870 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:46502
recon_1      | 2022-01-13 06:26:12,879 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50298
recon_1      | 2022-01-13 06:26:12,904 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59988
recon_1      | 2022-01-13 06:26:12,949 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:12,960 [IPC Server handler 0 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:26:12,966 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:12,967 [IPC Server handler 9 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:26:13,056 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:13,080 [IPC Server handler 47 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net
recon_1      | 2022-01-13 06:26:17,623 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 220 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:21,287 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:05,618 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:05,621 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t0,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:05,648 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,823 [grpc-default-executor-0] INFO ha.InterSCMGrpcClient: Checkpoint is downloaded to /data/metadata/snapshot/scm.db-scm1-1642055044284.tar.gz
scm2.org_1   | 2022-01-13 06:24:05,882 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,883 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:05,883 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:05,884 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:05,891 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,915 [grpc-default-executor-1] INFO impl.RoleInfo: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: start 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-FollowerState
scm2.org_1   | 2022-01-13 06:24:05,919 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:05,922 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:05,929 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,934 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:05,935 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:05,935 [pool-16-thread-1] INFO ha.SCMSnapshotProvider: Successfully downloaded latest checkpoint from leader SCM: scm1 path /data/metadata/snapshot/scm.db-scm1-1642055044284
scm2.org_1   | 2022-01-13 06:24:05,939 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:05,940 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Downloaded checkpoint from Leader scm1 to the location /data/metadata/snapshot/scm.db-scm1-1642055044284
scm2.org_1   | 2022-01-13 06:24:05,954 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:05,954 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,954 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:05,983 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:05,983 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#2:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:05,984 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:05,986 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:05,990 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:05,990 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:05,996 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,005 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,005 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#3:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,008 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,011 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm3.org_1   | 2022-01-13 06:24:19,521 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2022-01-13 06:24:19,522 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2022-01-13 06:24:19,523 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2022-01-13 06:24:19,524 [pool-14-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2022-01-13 06:24:19,557 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2022-01-13 06:24:19,558 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2022-01-13 06:24:19,559 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2022-01-13 06:24:19,810 [main] INFO ha.SequenceIdGenerator: upgrade localId to 109611004723200000
scm3.org_1   | 2022-01-13 06:24:19,810 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2022-01-13 06:24:19,814 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2022-01-13 06:24:19,821 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2022-01-13 06:24:19,879 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2022-01-13 06:24:19,891 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3.org_1   | 2022-01-13 06:24:19,900 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2022-01-13 06:24:19,933 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2022-01-13 06:24:19,940 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2022-01-13 06:24:19,940 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2022-01-13 06:24:19,992 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2022-01-13 06:24:20,009 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2022-01-13 06:24:20,036 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2022-01-13 06:24:20,048 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2022-01-13 06:24:20,058 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 0 containers.
scm3.org_1   | 2022-01-13 06:24:20,065 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2022-01-13 06:24:20,069 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:24:20,071 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2022-01-13 06:24:20,101 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2022-01-13 06:24:20,162 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2022-01-13 06:24:20,230 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2022-01-13 06:24:21,074 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2022-01-13 06:24:21,076 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2022-01-13 06:24:21,096 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2022-01-13 06:24:21,099 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2022-01-13 06:24:21,122 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2022-01-13 06:24:21,122 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2022-01-13 06:24:21,197 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3.org_1   | 2022-01-13 06:24:21,214 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        false
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                                                Value
scm3.org_1   | Threshold                                          10
scm3.org_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3.org_1   | Max Size to Move per Iteration                     500GB
scm3.org_1   | Max Size Entering Target per Iteration             26GB
scm3.org_1   | Max Size Leaving Source per Iteration              26GB
scm3.org_1   | 
scm3.org_1   | 2022-01-13 06:24:21,215 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3.org_1   | 2022-01-13 06:24:21,215 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2022-01-13 06:24:21,219 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2022-01-13 06:24:21,221 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2022-01-13 06:24:21,222 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: start with initializing state, conf=-1: [], old=null
scm3.org_1   | 2022-01-13 06:24:21,223 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2022-01-13 06:24:21,225 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-00FB17B6AB6E,id=e3ebff3b-14c4-4117-a7dd-8ec94ff36cba
scm3.org_1   | 2022-01-13 06:24:21,228 [Listener at 0.0.0.0/9860] INFO server.RaftServer: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: start RPC server
scm3.org_1   | 2022-01-13 06:24:21,274 [Listener at 0.0.0.0/9860] INFO server.GrpcService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: GrpcService started, listening on 9894
scm3.org_1   | 2022-01-13 06:24:21,383 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,015 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,015 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,016 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,038 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,041 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#4:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,042 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,042 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,042 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,043 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,062 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,074 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,078 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#5:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,085 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,085 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,086 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,107 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,107 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#6:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,109 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,114 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,117 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,123 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,124 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,134 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 221 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:26,144 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 222 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:26,317 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 223 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:26,324 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om3 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm3.org_1   | 2022-01-13 06:24:21,383 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm3.org_1   | 2022-01-13 06:24:21,383 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm3.org_1   | 2022-01-13 06:24:21,408 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$419/0x0000000840537040@455cb6d9] INFO util.JvmPauseMonitor: JvmPauseMonitor-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Started
scm3.org_1   | 2022-01-13 06:24:28,296 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:28,330 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2022-01-13 06:24:28,348 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: change Leader from null to c6280d25-12c7-4451-bd27-cfac689faecd at term 2 for installSnapshot, leader elected after 8894ms
scm3.org_1   | 2022-01-13 06:24:28,364 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Received notification to install snapshot at index 10
scm3.org_1   | 2022-01-13 06:24:28,521 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: notifyInstallSnapshot: nextIndex is 0 but the leader's first available index is 10.
scm3.org_1   | 2022-01-13 06:24:28,544 [grpc-default-executor-0] INFO ha.SCMStateMachine: Received install snapshot notification from SCM leader: scm1.org:9894 with term index: (t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:28,594 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Downloading checkpoint from leader SCM scm1 and reloading state from the checkpoint.
scm3.org_1   | 2022-01-13 06:24:30,897 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:30,985 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,090 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t0,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,222 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,329 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,332 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:31,334 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,336 [grpc-default-executor-0] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,356 [grpc-default-executor-2] INFO impl.RoleInfo: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: start e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-FollowerState
scm3.org_1   | 2022-01-13 06:24:31,362 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:31,384 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,435 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,450 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:31,473 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm1.org_1   | 2022-01-13 06:23:34,923 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2022-01-13 06:23:34,924 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2022-01-13 06:23:34,927 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2022-01-13 06:23:34,928 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2022-01-13 06:23:34,929 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: start as a follower, conf=0: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:34,931 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2022-01-13 06:23:34,936 [Listener at 0.0.0.0/9860] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState
scm1.org_1   | 2022-01-13 06:23:34,944 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-00FB17B6AB6E,id=c6280d25-12c7-4451-bd27-cfac689faecd
scm1.org_1   | 2022-01-13 06:23:34,959 [Listener at 0.0.0.0/9860] INFO server.RaftServer: c6280d25-12c7-4451-bd27-cfac689faecd: start RPC server
scm1.org_1   | 2022-01-13 06:23:35,020 [Listener at 0.0.0.0/9860] INFO server.GrpcService: c6280d25-12c7-4451-bd27-cfac689faecd: GrpcService started, listening on 9894
scm1.org_1   | 2022-01-13 06:23:35,027 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2022-01-13 06:23:35,028 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2022-01-13 06:23:35,029 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$438/0x000000084054f040@ce5a2ea] INFO util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Started
scm1.org_1   | 2022-01-13 06:23:35,031 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm1.org_1   | 2022-01-13 06:23:35,035 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm1.org_1   | 2022-01-13 06:23:35,156 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2022-01-13 06:23:35,171 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2022-01-13 06:23:35,171 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2022-01-13 06:23:35,524 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2022-01-13 06:23:35,527 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2022-01-13 06:23:35,536 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2022-01-13 06:23:35,561 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2022-01-13 06:23:35,568 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2022-01-13 06:23:35,569 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2022-01-13 06:23:35,569 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2022-01-13 06:23:35,593 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2022-01-13 06:23:35,596 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2022-01-13 06:23:35,597 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2022-01-13 06:23:35,597 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2022-01-13 06:23:35,676 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4ac2b4c6] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1.org_1   | 2022-01-13 06:23:35,685 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2022-01-13 06:23:35,685 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2022-01-13 06:23:35,686 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2022-01-13 06:23:35,713 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7460ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2022-01-13 06:23:35,813 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2022-01-13 06:23:35,821 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2022-01-13 06:23:35,822 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2022-01-13 06:23:35,823 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2022-01-13 06:23:35,823 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2022-01-13 06:23:35,826 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2022-01-13 06:23:35,864 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2022-01-13 06:23:35,865 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
scm1.org_1   | 2022-01-13 06:23:35,897 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2022-01-13 06:23:35,897 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2022-01-13 06:23:35,898 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm1.org_1   | 2022-01-13 06:23:35,915 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2022-01-13 06:23:35,923 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@219bad15{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2022-01-13 06:23:35,924 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1cd93693{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2022-01-13 06:23:36,009 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2022-01-13 06:23:36,019 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@747216b6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0-SNAPSHOT_jar-_-any-10129192068907515178/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2022-01-13 06:23:36,030 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@68d1f4e2{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2022-01-13 06:23:36,030 [Listener at 0.0.0.0/9860] INFO server.Server: Started @7777ms
scm1.org_1   | 2022-01-13 06:23:36,032 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2022-01-13 06:23:36,032 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2022-01-13 06:23:36,034 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2022-01-13 06:23:37,108 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:35985
scm1.org_1   | 2022-01-13 06:23:37,132 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:23:37,968 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:45932
scm1.org_1   | 2022-01-13 06:23:37,988 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:23:40,035 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.FollowerState: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099060445ns, electionTimeout:5086ms
scm1.org_1   | 2022-01-13 06:23:40,036 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState
scm1.org_1   | 2022-01-13 06:23:40,037 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2022-01-13 06:23:40,040 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2022-01-13 06:23:40,040 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-FollowerState] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1
scm1.org_1   | 2022-01-13 06:23:40,061 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.LeaderElection: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:40,061 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.LeaderElection: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2022-01-13 06:23:40,062 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: shutdown c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1
scm1.org_1   | 2022-01-13 06:23:40,065 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2022-01-13 06:23:40,065 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2022-01-13 06:23:40,065 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2022-01-13 06:23:40,069 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: change Leader from null to c6280d25-12c7-4451-bd27-cfac689faecd at term 2 for becomeLeader, leader elected after 7358ms
scm1.org_1   | 2022-01-13 06:23:40,076 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2022-01-13 06:23:40,082 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:40,083 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2022-01-13 06:23:40,089 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2022-01-13 06:23:40,089 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2022-01-13 06:23:40,090 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2022-01-13 06:23:40,097 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2022-01-13 06:23:40,102 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1.org_1   | 2022-01-13 06:23:40,104 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO impl.RoleInfo: c6280d25-12c7-4451-bd27-cfac689faecd: start c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl
scm1.org_1   | 2022-01-13 06:23:40,112 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om3,nodeAddress=om3:9862 after 224 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1      | 2022-01-13 06:26:28,341 [pool-18-thread-1] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om1 is not the leader. Could not determine the leader node.
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:211)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createLeaderErrorException(OzoneManagerProtocolServerSideTranslatorPB.java:198)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:191)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:150)
recon_1      | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
recon_1      | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:124)
recon_1      | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
recon_1      | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
recon_1      | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
recon_1      | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1      | , while invoking $Proxy43.submitRequest over nodeId=om1,nodeAddress=om1:9862 after 225 failover attempts. Trying to failover immediately.
recon_1      | 2022-01-13 06:26:30,341 [pool-18-thread-1] ERROR impl.OzoneManagerServiceProviderImpl: Unable to update Recon's metadata with new OM DB. 
recon_1      | java.lang.reflect.UndeclaredThrowableException
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
recon_1      | 	at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:536)
recon_1      | 	at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:517)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getOzoneManagerDBSnapshot(OzoneManagerServiceProviderImpl.java:297)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.updateReconOmDBWithNewSnapshot(OzoneManagerServiceProviderImpl.java:329)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:427)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$start$0(OzoneManagerServiceProviderImpl.java:233)
recon_1      | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1      | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
recon_1      | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
recon_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1      | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1      | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1      | Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://om2:9874/dbCheckpoint
recon_1      | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1      | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1      | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
recon_1      | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.wrapExceptionWithMessage(KerberosAuthenticator.java:232)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:219)
recon_1      | 	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:350)
recon_1      | 	at org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:186)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconUtils.makeHttpCall(ReconUtils.java:237)
recon_1      | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$getOzoneManagerDBSnapshot$1(OzoneManagerServiceProviderImpl.java:298)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1      | 	... 12 more
recon_1      | Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:360)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:204)
recon_1      | 	... 19 more
recon_1      | Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)
recon_1      | 	at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:773)
recon_1      | 	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:266)
recon_1      | 	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:196)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:336)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:310)
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:31,479 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,480 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,483 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,530 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm1.org_1   | 2022-01-13 06:23:40,121 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_0 to /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_0-0
scm1.org_1   | 2022-01-13 06:23:40,132 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderElection1] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:23:40,142 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_1
scm1.org_1   | 2022-01-13 06:23:40,149 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2022-01-13 06:23:40,156 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2022-01-13 06:23:40,157 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:23:40,158 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2022-01-13 06:23:40,159 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2022-01-13 06:23:40,160 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2022-01-13 06:23:40,167 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2022-01-13 06:23:40,168 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2022-01-13 06:23:40,794 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:41948
scm1.org_1   | 2022-01-13 06:23:40,815 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:23:47,969 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:47904
scm1.org_1   | 2022-01-13 06:23:47,991 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:23:48,367 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f
scm1.org_1   | 2022-01-13 06:23:50,547 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:23:50,547 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2022-01-13 06:23:50,547 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2022-01-13 06:23:50,571 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 1746733.826us
scm1.org_1   | 2022-01-13 06:23:52,900 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42148
scm1.org_1   | 2022-01-13 06:23:52,933 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:24:02,679 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:46334
scm1.org_1   | 2022-01-13 06:24:02,792 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:02,795 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: c6280d25-12c7-4451-bd27-cfac689faecd: Submitting SetConfiguration request to Ratis server with new SCM peers list: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:02,813 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: receive setConfiguration SetConfigurationRequest:client-5A24FD849911->c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E, cid=0, seq=0, RW, null, peers:[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:02,813 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-5A24FD849911->c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E, cid=0, seq=0, RW, null, peers:[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:02,839 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2022-01-13 06:24:02,847 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:24:02,849 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2022-01-13 06:24:02,885 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2022-01-13 06:24:02,911 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2022-01-13 06:24:02,911 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2022-01-13 06:24:02,964 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:02,995 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,135 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#7:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,136 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,138 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,143 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,144 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,153 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,172 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,172 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#8:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,187 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,188 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   | 2022-01-13 06:24:31,533 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:31,634 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:31,639 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#2:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:31,677 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,683 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:31,683 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,687 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,710 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,767 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,768 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:31,772 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,775 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,783 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:31,788 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#3:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:31,789 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,941 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:31,942 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#4:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:31,955 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:31,959 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:31,969 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:31,972 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:31,976 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,084 [grpc-default-executor-2] INFO ha.InterSCMGrpcClient: Checkpoint is downloaded to /data/metadata/snapshot/scm.db-scm1-1642055068594.tar.gz
scm3.org_1   | 2022-01-13 06:24:32,109 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,115 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#5:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,160 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,171 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,188 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,188 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,190 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,232 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,233 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#9:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,246 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,251 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,252 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,252 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,255 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,269 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Installing checkpoint with SCMTransactionInfo 2#4
scm2.org_1   | 2022-01-13 06:24:06,282 [pool-16-thread-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: StateMachine successfully installed snapshot index 4. Reloading the StateMachine.
scm2.org_1   | 2022-01-13 06:24:06,282 [pool-16-thread-1] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally -1 -> 4
scm2.org_1   | 2022-01-13 06:24:06,282 [pool-16-thread-1] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 4
scm2.org_1   | 2022-01-13 06:24:06,288 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,284 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,288 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#10:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm2.org_1   | 2022-01-13 06:24:06,289 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,289 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,290 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,292 [pool-16-thread-1] INFO raftlog.RaftLog: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 4
scm2.org_1   | 2022-01-13 06:24:06,301 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,320 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,327 [grpc-default-executor-0] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#11:FAIL-t2,INCONSISTENCY,nextIndex=5,followerCommit=4
scm2.org_1   | 2022-01-13 06:24:06,333 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,333 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,333 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,334 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm2.org_1   | 2022-01-13 06:24:06,337 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,354 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: Failed appendEntries as snapshot (4) installation is in progress
scm2.org_1   | 2022-01-13 06:24:06,359 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#12:FAIL-t2,INCONSISTENCY,nextIndex=5,followerCommit=4
scm2.org_1   | 2022-01-13 06:24:06,370 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,373 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: InstallSnapshot notification result: SNAPSHOT_INSTALLED, at index: 4
scm2.org_1   | 2022-01-13 06:24:06,374 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set new configuration index: 1
scm2.org_1   | configurationEntry {
scm2.org_1   |   peers {
scm2.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm2.org_1   |     address: "scm1.org:9894"
scm2.org_1   |   }
scm2.org_1   | }
scm2.org_1   |  from snapshot
scm2.org_1   | 2022-01-13 06:24:06,374 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 1: [c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:06,375 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,SNAPSHOT_INSTALLED,snapshotIndex=4
scm2.org_1   | 2022-01-13 06:24:06,388 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm2.org_1   | 2022-01-13 06:24:06,476 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Installing checkpoint with SCMTransactionInfo 2#4
scm2.org_1   | 2022-01-13 06:24:06,494 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Replaced DB with checkpoint, term: 2, index: 4
scm2.org_1   | 2022-01-13 06:24:06,495 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2022-01-13 06:24:06,501 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2022-01-13 06:24:06,591 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SequenceIdGenerator: reinitialize SequenceIdGenerator.
scm2.org_1   | 2022-01-13 06:24:06,616 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2.org_1   | 2022-01-13 06:24:06,617 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Reloaded SCM state with Term: 2 and Index: 4
scm2.org_1   | 2022-01-13 06:24:06,618 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater: snapshotIndex: setUnconditionally -1 -> 4
scm2.org_1   | 2022-01-13 06:24:06,618 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater: appliedIndex: setUnconditionally -1 -> 4
scm2.org_1   | 2022-01-13 06:24:09,048 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: Starting segment from index:5
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1      | 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:310)
recon_1      | 	... 20 more
scm1.org_1   | 2022-01-13 06:24:05,285 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42340
scm1.org_1   | 2022-01-13 06:24:05,381 [grpc-default-executor-1] INFO ha.SCMDBCheckpointProvider: Received request to obtain SCM DB checkpoint snapshot
scm1.org_1   | 2022-01-13 06:24:05,432 [grpc-default-executor-1] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/scm.db_checkpoint_1642055045386 in 46 milliseconds
scm1.org_1   | 2022-01-13 06:24:05,520 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:24:05,692 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received the first reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t0,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:05,736 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:05,780 [grpc-default-executor-1] INFO ha.SCMGrpcOutputStream: Sent 7521 bytes for cluster CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm1.org_1   | 2022-01-13 06:24:05,808 [grpc-default-executor-1] INFO ha.SCMDBCheckpointProvider: Time taken to write the checkpoint to response output stream: 369 milliseconds
scm1.org_1   | 2022-01-13 06:24:05,813 [grpc-default-executor-1] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/scm.db_checkpoint_1642055045386
scm1.org_1   | 2022-01-13 06:24:05,833 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,833 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,911 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:05,912 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:05,914 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,914 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,966 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:05,966 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:05,967 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:05,968 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,968 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:05,969 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:05,987 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:05,999 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:05,999 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,000 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
recon_1      | Caused by: KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:226)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:237)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCredsSingle(CredentialsUtil.java:482)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:340)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:314)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:169)
recon_1      | 	at java.security.jgss/sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:490)
recon_1      | 	at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:697)
recon_1      | 	... 27 more
recon_1      | Caused by: KrbException: Identifier doesn't match expected value (906)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.TGSRep.init(TGSRep.java:65)
recon_1      | 	at java.security.jgss/sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60)
recon_1      | 	at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55)
recon_1      | 	... 35 more
recon_1      | 2022-01-13 06:26:42,663 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:50372
recon_1      | 2022-01-13 06:26:42,819 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:46588
recon_1      | 2022-01-13 06:26:42,927 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:43,005 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:43,007 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60072
recon_1      | 2022-01-13 06:26:43,043 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1      | 2022-01-13 06:26:43,993 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=7d03933b-289c-4ae9-b706-c03134748096. Trying to get from SCM.
recon_1      | 2022-01-13 06:26:44,451 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]] to Recon pipeline metadata.
recon_1      | 2022-01-13 06:26:44,632 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]].
recon_1      | 2022-01-13 06:26:44,692 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 183525.881us
recon_1      | 2022-01-13 06:26:44,701 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=7d03933b-289c-4ae9-b706-c03134748096 reported by 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:44,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]] moved to OPEN state
recon_1      | 2022-01-13 06:26:44,720 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 14562.016us
recon_1      | 2022-01-13 06:26:44,721 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=2b9e81e9-4435-4567-af0a-448797766036. Trying to get from SCM.
recon_1      | 2022-01-13 06:26:44,754 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:ab6e22bd-d408-4246-98c0-61fe3297cb52, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]] to Recon pipeline metadata.
recon_1      | 2022-01-13 06:26:44,756 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:ab6e22bd-d408-4246-98c0-61fe3297cb52, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]].
scm1.org_1   | 2022-01-13 06:24:06,002 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,008 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,033 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,034 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,035 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,035 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,068 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,069 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,069 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,070 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,071 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,091 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,101 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,101 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,102 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,103 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,117 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,128 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,129 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,130 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,130 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,151 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,160 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,160 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,162 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,162 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,181 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,196 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,196 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,219 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,220 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,252 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,256 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,263 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,279 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,279 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,300 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:06,304 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,304 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,305 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,311 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,337 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 0 -> 5
scm1.org_1   | 2022-01-13 06:24:06,350 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:06,350 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:06,361 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: followerNextIndex = 5 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,362 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0-t2,notify:(t:2, i:4)
scm1.org_1   | 2022-01-13 06:24:06,363 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateUnconditionally 5 -> 5
scm1.org_1   | 2022-01-13 06:24:06,389 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f#0:FAIL-t2,SNAPSHOT_INSTALLED,snapshotIndex=4
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f-InstallSnapshotResponseHandler: Follower installed snapshot at index 4
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: snapshotIndex: setUnconditionally 0 -> 4
scm2.org_1   | 2022-01-13 06:24:09,582 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_5
scm2.org_1   | 2022-01-13 06:24:09,652 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 7: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
recon_1      | 2022-01-13 06:26:44,761 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 6706.516us
recon_1      | 2022-01-13 06:26:44,762 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9cb3b3c0-f3d5-4c7d-a7e4-318a18735718. Trying to get from SCM.
recon_1      | 2022-01-13 06:26:44,803 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]] to Recon pipeline metadata.
recon_1      | 2022-01-13 06:26:44,804 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]].
recon_1      | 2022-01-13 06:26:44,804 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 833.79us
recon_1      | 2022-01-13 06:26:44,804 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=9cb3b3c0-f3d5-4c7d-a7e4-318a18735718 reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:44,805 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]] moved to OPEN state
recon_1      | 2022-01-13 06:26:44,805 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 527.994us
recon_1      | 2022-01-13 06:26:45,665 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11. Trying to get from SCM.
recon_1      | 2022-01-13 06:26:45,694 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]] to Recon pipeline metadata.
recon_1      | 2022-01-13 06:26:45,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]].
recon_1      | 2022-01-13 06:26:45,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 7841.701us
recon_1      | 2022-01-13 06:26:45,702 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:45,763 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:46,114 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:50,817 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:50,873 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:51,255 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:54,038 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e. Trying to get from SCM.
recon_1      | 2022-01-13 06:26:54,054 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: fe6afea2-6277-43ca-80b4-408a10c34f6e, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.773Z[UTC]] to Recon pipeline metadata.
recon_1      | 2022-01-13 06:26:54,070 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fe6afea2-6277-43ca-80b4-408a10c34f6e, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.773Z[UTC]].
recon_1      | 2022-01-13 06:26:54,071 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 16261.597us
recon_1      | 2022-01-13 06:26:54,071 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e reported by 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:54,071 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:54,991 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:54,992 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2022-01-13 06:24:09,694 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:09,898 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-00FB17B6AB6E:[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm2.org_1   | 2022-01-13 06:24:09,912 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2022-01-13 06:24:09,918 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm2.org_1   | 2022-01-13 06:24:09,931 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm2.org_1   | 2022-01-13 06:24:09,998 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:24:10,000 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2022-01-13 06:24:10,003 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2022-01-13 06:24:10,005 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2022-01-13 06:24:10,217 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2022-01-13 06:24:10,299 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2022-01-13 06:24:10,299 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2022-01-13 06:24:11,006 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2022-01-13 06:24:11,109 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2022-01-13 06:24:11,284 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2022-01-13 06:24:11,303 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2022-01-13 06:24:11,303 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2022-01-13 06:24:11,387 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2022-01-13 06:24:11,388 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2022-01-13 06:24:11,392 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2022-01-13 06:24:11,392 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2022-01-13 06:24:11,588 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2022-01-13 06:24:11,590 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2022-01-13 06:24:11,590 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2022-01-13 06:24:11,591 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm2.org_1   | 2022-01-13 06:24:11,761 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:11,761 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:11,761 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2022-01-13 06:24:12,261 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7ca46aaf] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2.org_1   | 2022-01-13 06:24:12,303 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2022-01-13 06:24:12,305 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2022-01-13 06:24:12,307 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2022-01-13 06:24:12,442 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @21115ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2022-01-13 06:24:12,701 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2022-01-13 06:24:12,745 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2022-01-13 06:24:12,747 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2022-01-13 06:24:12,749 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2022-01-13 06:24:12,750 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2022-01-13 06:24:12,756 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2022-01-13 06:24:12,851 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2022-01-13 06:24:12,852 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
scm2.org_1   | 2022-01-13 06:24:13,025 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2022-01-13 06:24:13,025 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2022-01-13 06:24:13,027 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2022-01-13 06:24:13,100 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2022-01-13 06:24:13,111 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2f48adcc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2022-01-13 06:24:13,112 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@43b9ef41{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,175 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,179 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,181 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,253 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,266 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#6:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,287 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,289 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,314 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,317 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,369 [pool-16-thread-1] INFO ha.SCMSnapshotProvider: Successfully downloaded latest checkpoint from leader SCM: scm1 path /data/metadata/snapshot/scm.db-scm1-1642055068594
scm3.org_1   | 2022-01-13 06:24:32,370 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Downloaded checkpoint from Leader scm1 to the location /data/metadata/snapshot/scm.db-scm1-1642055068594
scm3.org_1   | 2022-01-13 06:24:32,396 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,457 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,465 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#7:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,472 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,473 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,473 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,473 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,474 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,514 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,518 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#8:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,521 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,525 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: matchIndex: setUnconditionally 0 -> 4
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: setUnconditionally 5 -> 5
scm2.org_1   | 2022-01-13 06:24:13,344 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2022-01-13 06:24:13,378 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6f0969db{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0-SNAPSHOT_jar-_-any-14901818680141476883/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2022-01-13 06:24:13,413 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@4a1b710{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2022-01-13 06:24:13,413 [Listener at 0.0.0.0/9860] INFO server.Server: Started @22086ms
scm2.org_1   | 2022-01-13 06:24:13,430 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2022-01-13 06:24:13,431 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2022-01-13 06:24:13,433 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2022-01-13 06:24:35,916 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 11: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm2.org_1   | 2022-01-13 06:24:35,989 [grpc-default-executor-1] INFO server.RaftServer$Division: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E: set configuration 13: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2022-01-13 06:24:53,809 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:24:53,821 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2022-01-13 06:24:53,822 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2022-01-13 06:24:54,188 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:24:55,797 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:25:07,813 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:25:08,768 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:25:10,915 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:25:37,077 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59738
scm2.org_1   | 2022-01-13 06:25:37,219 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:59462
scm2.org_1   | 2022-01-13 06:25:37,412 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:25:38,028 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:25:39,612 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:34988
scm2.org_1   | 2022-01-13 06:25:39,652 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:25:43,156 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$419/0x0000000840537040@2a49753] WARN util.JvmPauseMonitor: JvmPauseMonitor-949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: Detected pause in JVM or host machine (eg GC): pause of approximately 187635816ns.
scm2.org_1   | GC pool 'ParNew' had collection(s): count=1 time=221ms
scm2.org_1   | 2022-01-13 06:25:43,348 [IPC Server handler 12 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1a62a11-0048-42bf-a9c3-836e0373e878
scm2.org_1   | 2022-01-13 06:25:43,388 [IPC Server handler 28 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22377b14-877d-4b48-97e5-336a98a1cf76
scm2.org_1   | 2022-01-13 06:25:43,466 [IPC Server handler 28 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,527 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
recon_1      | 2022-01-13 06:26:55,060 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e reported by ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:26:55,060 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:27:04,007 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:27:04,007 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 reported by b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1      | 2022-01-13 06:27:04,007 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]] moved to OPEN state
recon_1      | 2022-01-13 06:27:04,008 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@64504a65, cost 718.891us
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO leader.FollowerInfo: Follower c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f acknowledged installing snapshot
scm1.org_1   | 2022-01-13 06:24:06,390 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->949f8263-e6fc-4c45-bf68-7d22f1ef9f4f: nextIndex: updateToMax old=5, new=5, updated? false
scm1.org_1   | 2022-01-13 06:24:07,306 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:40112
scm1.org_1   | 2022-01-13 06:24:07,320 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:08,701 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38136
scm1.org_1   | 2022-01-13 06:24:08,707 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:08,707 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba
scm1.org_1   | 2022-01-13 06:24:08,979 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:24:09,087 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 352711.655us
scm1.org_1   | 2022-01-13 06:24:09,612 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 7: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2022-01-13 06:24:09,675 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:24:09,729 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 949f8263-e6fc-4c45-bf68-7d22f1ef9f4f.
scm1.org_1   | 2022-01-13 06:24:11,992 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:48288
scm1.org_1   | 2022-01-13 06:24:12,011 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:18,519 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42540
scm1.org_1   | 2022-01-13 06:24:18,592 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:24:24,644 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:40328
scm1.org_1   | 2022-01-13 06:24:24,831 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:24,853 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: c6280d25-12c7-4451-bd27-cfac689faecd: Submitting SetConfiguration request to Ratis server with new SCM peers list: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:24,853 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: receive setConfiguration SetConfigurationRequest:client-5A24FD849911->c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E, cid=1, seq=0, RW, null, peers:[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:24,853 [IPC Server handler 2 on default port 9863] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-5A24FD849911->c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E, cid=1, seq=0, RW, null, peers:[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2022-01-13 06:24:24,854 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2022-01-13 06:24:24,854 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2022-01-13 06:24:24,854 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1.org_1   | 2022-01-13 06:24:24,878 [IPC Server handler 2 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2022-01-13 06:24:24,878 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2022-01-13 06:24:24,878 [IPC Server handler 2 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2022-01-13 06:24:24,944 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:24,967 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:25,537 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$438/0x000000084054f040@ce5a2ea] WARN util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Detected pause in JVM or host machine (eg GC): pause of approximately 265943734ns.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=212ms
scm1.org_1   | 2022-01-13 06:24:31,231 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received the first reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t0,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:31,231 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:31,248 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,248 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,359 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:31,359 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:31,388 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,389 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,509 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:31,509 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:31,510 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,510 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,550 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:31,595 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:31,678 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:31,722 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:31,722 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:31,739 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,739 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,761 [grpc-default-executor-1] INFO ha.SCMDBCheckpointProvider: Received request to obtain SCM DB checkpoint snapshot
scm1.org_1   | 2022-01-13 06:24:31,817 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:31,826 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:31,858 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:31,865 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,880 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:31,900 [grpc-default-executor-1] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/scm.db_checkpoint_1642055071776 in 123 milliseconds
scm1.org_1   | 2022-01-13 06:24:31,990 [grpc-default-executor-1] INFO ha.SCMGrpcOutputStream: Sent 8679 bytes for cluster CID-055d8419-3c3e-47d9-a087-00fb17b6ab6e
scm1.org_1   | 2022-01-13 06:24:32,013 [grpc-default-executor-1] INFO ha.SCMDBCheckpointProvider: Time taken to write the checkpoint to response output stream: 111 milliseconds
scm1.org_1   | 2022-01-13 06:24:32,037 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,037 [grpc-default-executor-1] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/scm.db_checkpoint_1642055071776
scm1.org_1   | 2022-01-13 06:24:32,048 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,048 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,052 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,055 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,209 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,221 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,221 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,230 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,239 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,406 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,407 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,407 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,408 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,408 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,479 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,481 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,481 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,483 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,491 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,551 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,563 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,563 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,564 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,567 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,612 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,620 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,620 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,634 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,653 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,724 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,729 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,738 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,741 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,749 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,805 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,807 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,807 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,810 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,819 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,871 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:32,883 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,887 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,891 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,894 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,944 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:32,947 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:32,947 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm3.org_1   | 2022-01-13 06:24:32,529 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,532 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,593 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,593 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#9:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,603 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,605 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,605 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,607 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,622 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,665 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,666 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#10:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,676 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,688 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,691 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,691 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,703 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,756 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,759 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#11:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,771 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,774 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,783 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,783 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,788 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,825 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,847 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#12:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,854 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,862 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm2.org_1   | 2022-01-13 06:25:43,538 [IPC Server handler 12 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2022-01-13 06:25:43,528 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2022-01-13 06:25:43,614 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2022-01-13 06:25:43,651 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2022-01-13 06:25:43,698 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2022-01-13 06:25:44,796 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]].
scm2.org_1   | 2022-01-13 06:25:44,796 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:25:45,005 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]].
scm2.org_1   | 2022-01-13 06:25:45,006 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:13,127 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:59562
scm2.org_1   | 2022-01-13 06:26:13,212 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59834
scm2.org_1   | 2022-01-13 06:26:13,228 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:35086
scm2.org_1   | 2022-01-13 06:26:13,262 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:13,289 [IPC Server handler 21 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ab6e22bd-d408-4246-98c0-61fe3297cb52
scm2.org_1   | 2022-01-13 06:26:13,292 [IPC Server handler 21 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2022-01-13 06:26:13,293 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2022-01-13 06:26:13,299 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2022-01-13 06:26:13,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2022-01-13 06:26:13,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2022-01-13 06:26:13,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2.org_1   | 2022-01-13 06:26:13,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2022-01-13 06:26:13,334 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:13,372 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:13,369 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2022-01-13 06:26:13,578 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]].
scm2.org_1   | 2022-01-13 06:26:13,591 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:13,755 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]].
scm2.org_1   | 2022-01-13 06:26:13,756 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:24:32,948 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:32,948 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:33,032 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 0
scm1.org_1   | 2022-01-13 06:24:33,063 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm1.org_1   | 2022-01-13 06:24:33,069 [grpc-default-executor-2] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1.org_1   | 2022-01-13 06:24:33,070 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:33,075 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-GrpcLogAppender: send c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm1.org_1   | 2022-01-13 06:24:33,252 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: received a reply c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,SNAPSHOT_INSTALLED,snapshotIndex=10
scm1.org_1   | 2022-01-13 06:24:33,256 [grpc-default-executor-1] INFO server.GrpcLogAppender: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba-InstallSnapshotResponseHandler: Follower installed snapshot at index 10
scm1.org_1   | 2022-01-13 06:24:33,259 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: snapshotIndex: setUnconditionally 0 -> 10
scm1.org_1   | 2022-01-13 06:24:33,259 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: matchIndex: setUnconditionally 0 -> 10
scm1.org_1   | 2022-01-13 06:24:33,263 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: setUnconditionally 11 -> 11
scm1.org_1   | 2022-01-13 06:24:33,264 [grpc-default-executor-1] INFO leader.FollowerInfo: Follower c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba acknowledged installing snapshot
scm1.org_1   | 2022-01-13 06:24:33,256 [grpc-default-executor-2] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateUnconditionally 0 -> 11
scm1.org_1   | 2022-01-13 06:24:33,268 [grpc-default-executor-1] INFO leader.FollowerInfo: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: nextIndex: updateToMax old=11, new=11, updated? false
scm1.org_1   | 2022-01-13 06:24:35,913 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 11: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2022-01-13 06:24:35,953 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-LeaderStateImpl] INFO server.RaftServer$Division: c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E: set configuration 13: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2022-01-13 06:24:36,015 [IPC Server handler 2 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba.
scm1.org_1   | 2022-01-13 06:24:42,763 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38378
scm1.org_1   | 2022-01-13 06:24:42,833 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:49,232 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:41588
scm1.org_1   | 2022-01-13 06:24:49,683 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:49,949 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:38972
scm1.org_1   | 2022-01-13 06:24:50,095 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:51,152 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:43244
scm1.org_1   | 2022-01-13 06:24:51,251 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:24:52,745 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49880
scm1.org_1   | 2022-01-13 06:24:52,943 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:52,944 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 783c9b118d1a, UUID: ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:24:53,369 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:49828
scm1.org_1   | 2022-01-13 06:24:53,484 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:53,493 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn a47071aed28b, UUID: 22377b14-877d-4b48-97e5-336a98a1cf76
scm1.org_1   | 2022-01-13 06:24:53,799 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:24:53,852 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 475078.891us
scm1.org_1   | 2022-01-13 06:24:54,155 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:24:54,186 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 132965.569us
scm1.org_1   | 2022-01-13 06:24:55,123 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:46074
scm1.org_1   | 2022-01-13 06:24:55,200 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:24:55,205 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn e367b1121f80, UUID: b1a62a11-0048-42bf-a9c3-836e0373e878
scm1.org_1   | 2022-01-13 06:24:55,762 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:24:55,799 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 170305.129us
scm1.org_1   | 2022-01-13 06:25:00,770 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42692
scm1.org_1   | 2022-01-13 06:25:00,940 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:25:06,590 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:49860
scm1.org_1   | 2022-01-13 06:25:06,701 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm2.org_1   | 2022-01-13 06:26:13,876 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fe6afea2-6277-43ca-80b4-408a10c34f6e, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.773Z[UTC]].
scm2.org_1   | 2022-01-13 06:26:13,879 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:42,686 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59920
scm2.org_1   | 2022-01-13 06:26:42,758 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:42,966 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:59648
scm2.org_1   | 2022-01-13 06:26:43,014 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:43,047 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:35172
scm2.org_1   | 2022-01-13 06:26:43,090 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2022-01-13 06:26:44,062 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]] moved to OPEN state
scm2.org_1   | 2022-01-13 06:26:44,512 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:44,745 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:45,014 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2022-01-13 06:26:45,076 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@6f0302d3, cost 992946.854us
scm2.org_1   | 2022-01-13 06:26:45,103 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:45,103 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:45,680 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:45,743 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:46,113 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:50,820 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:50,900 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:51,216 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:54,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:54,956 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:26:55,059 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:27:04,021 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]] moved to OPEN state
scm2.org_1   | 2022-01-13 06:27:04,044 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@6f0302d3, cost 367.496us
scm2.org_1   | 2022-01-13 06:27:04,045 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2022-01-13 06:27:04,148 [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,871 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,875 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,878 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,912 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,912 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,918 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#13:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,919 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:32,923 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:32,923 [grpc-default-executor-2] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:32,926 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,976 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:32,991 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#14:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1
scm3.org_1   | 2022-01-13 06:24:32,996 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:32,992 [pool-16-thread-1] INFO ha.SCMHAManagerImpl: Installing checkpoint with SCMTransactionInfo 2#10
scm3.org_1   | 2022-01-13 06:24:33,025 [pool-16-thread-1] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: StateMachine successfully installed snapshot index 10. Reloading the StateMachine.
scm3.org_1   | 2022-01-13 06:24:33,026 [pool-16-thread-1] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: flushIndex: setUnconditionally -1 -> 10
scm3.org_1   | 2022-01-13 06:24:33,026 [pool-16-thread-1] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 10
scm3.org_1   | 2022-01-13 06:24:33,033 [pool-16-thread-1] INFO raftlog.RaftLog: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 10
scm3.org_1   | 2022-01-13 06:24:33,023 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:33,047 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:33,055 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,IN_PROGRESS
scm3.org_1   | 2022-01-13 06:24:33,060 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:33,109 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: Failed appendEntries as snapshot (10) installation is in progress
scm3.org_1   | 2022-01-13 06:24:33,125 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: inconsistency entries. Reply:c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#15:FAIL-t2,INCONSISTENCY,nextIndex=11,followerCommit=10
scm3.org_1   | 2022-01-13 06:24:33,153 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: receive installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:33,157 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: InstallSnapshot notification result: SNAPSHOT_INSTALLED, at index: 10
scm3.org_1   | 2022-01-13 06:24:33,171 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set new configuration index: 9
scm3.org_1   | configurationEntry {
scm3.org_1   |   peers {
scm3.org_1   |     id: "949f8263-e6fc-4c45-bf68-7d22f1ef9f4f"
scm3.org_1   |     address: "scm2.org:9894"
scm3.org_1   |   }
scm3.org_1   |   peers {
scm3.org_1   |     id: "c6280d25-12c7-4451-bd27-cfac689faecd"
scm3.org_1   |     address: "scm1.org:9894"
scm3.org_1   |   }
scm3.org_1   | }
scm3.org_1   |  from snapshot
scm3.org_1   | 2022-01-13 06:24:33,176 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 9: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:33,189 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: reply installSnapshot: c6280d25-12c7-4451-bd27-cfac689faecd<-e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0:FAIL-t2,SNAPSHOT_INSTALLED,snapshotIndex=10
scm3.org_1   | 2022-01-13 06:24:33,227 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba: Completed INSTALL_SNAPSHOT, lastRequest: c6280d25-12c7-4451-bd27-cfac689faecd->e3ebff3b-14c4-4117-a7dd-8ec94ff36cba#0-t2,notify:(t:2, i:10)
scm3.org_1   | 2022-01-13 06:24:33,648 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Installing checkpoint with SCMTransactionInfo 2#10
scm3.org_1   | 2022-01-13 06:24:33,764 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Replaced DB with checkpoint, term: 2, index: 10
scm3.org_1   | 2022-01-13 06:24:33,787 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:33,800 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2022-01-13 06:24:33,971 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SequenceIdGenerator: reinitialize SequenceIdGenerator.
scm3.org_1   | 2022-01-13 06:24:33,988 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3.org_1   | 2022-01-13 06:24:34,017 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO ha.SCMHAManagerImpl: Reloaded SCM state with Term: 2 and Index: 10
scm3.org_1   | 2022-01-13 06:24:34,021 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater: snapshotIndex: setUnconditionally -1 -> 10
scm3.org_1   | 2022-01-13 06:24:34,031 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO impl.StateMachineUpdater: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater: appliedIndex: setUnconditionally -1 -> 10
scm3.org_1   | 2022-01-13 06:24:35,927 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 11: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm3.org_1   | 2022-01-13 06:24:35,983 [grpc-default-executor-3] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: Starting segment from index:11
scm3.org_1   | 2022-01-13 06:24:36,277 [grpc-default-executor-3] INFO server.RaftServer$Division: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E: set configuration 13: [949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2022-01-13 06:24:36,450 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-00FB17B6AB6E:[949f8263-e6fc-4c45-bf68-7d22f1ef9f4f|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, e3ebff3b-14c4-4117-a7dd-8ec94ff36cba|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, c6280d25-12c7-4451-bd27-cfac689faecd|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm3.org_1   | 2022-01-13 06:24:36,469 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2022-01-13 06:24:36,483 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm3.org_1   | 2022-01-13 06:24:36,505 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm3.org_1   | 2022-01-13 06:24:37,275 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/055d8419-3c3e-47d9-a087-00fb17b6ab6e/current/log_inprogress_11
scm3.org_1   | 2022-01-13 06:24:37,439 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:24:37,539 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2022-01-13 06:24:37,567 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2022-01-13 06:24:37,620 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2022-01-13 06:24:37,834 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2022-01-13 06:24:38,007 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2022-01-13 06:24:38,007 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2022-01-13 06:24:39,892 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2022-01-13 06:24:39,902 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2022-01-13 06:24:40,197 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2022-01-13 06:24:40,363 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2022-01-13 06:24:40,382 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2022-01-13 06:24:40,954 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2022-01-13 06:24:40,964 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2022-01-13 06:25:06,860 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49916
scm1.org_1   | 2022-01-13 06:25:07,004 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:25:07,159 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:53946
scm1.org_1   | 2022-01-13 06:25:07,219 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:25:07,242 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: c49ebdfc-bef8-4937-a052-899d29d0d692
scm1.org_1   | 2022-01-13 06:25:07,792 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:25:07,979 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 331418.468us
scm1.org_1   | 2022-01-13 06:25:08,220 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:39076
scm1.org_1   | 2022-01-13 06:25:08,300 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:25:08,300 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: d10aa740-12c5-427d-bd41-eb0601b191f8
scm1.org_1   | 2022-01-13 06:25:08,738 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:25:08,800 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 259338.902us
scm1.org_1   | 2022-01-13 06:25:10,443 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:52076
scm1.org_1   | 2022-01-13 06:25:10,501 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:25:10,502 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: 1e60f0c9-d5b0-4de5-a33c-6f892761bbc6
scm1.org_1   | 2022-01-13 06:25:10,800 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:46106
scm1.org_1   | 2022-01-13 06:25:10,831 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:25:10,835 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 160749.613us
scm1.org_1   | 2022-01-13 06:25:10,921 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:25:37,210 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:46938
scm1.org_1   | 2022-01-13 06:25:37,302 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:52748
scm1.org_1   | 2022-01-13 06:25:37,378 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:25:37,386 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:25:38,984 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$438/0x000000084054f040@ce5a2ea] WARN util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Detected pause in JVM or host machine (eg GC): pause of approximately 151873056ns.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=183ms
scm1.org_1   | 2022-01-13 06:25:39,170 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58722
scm1.org_1   | 2022-01-13 06:25:39,595 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:25:40,139 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$438/0x000000084054f040@ce5a2ea] WARN util.JvmPauseMonitor: JvmPauseMonitor-c6280d25-12c7-4451-bd27-cfac689faecd: Detected pause in JVM or host machine (eg GC): pause of approximately 151208779ns. No GCs detected.
scm1.org_1   | 2022-01-13 06:25:43,254 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1a62a11-0048-42bf-a9c3-836e0373e878
scm1.org_1   | 2022-01-13 06:25:43,316 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2022-01-13 06:25:43,320 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22377b14-877d-4b48-97e5-336a98a1cf76
scm1.org_1   | 2022-01-13 06:25:43,375 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2022-01-13 06:25:43,398 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2022-01-13 06:25:43,398 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2022-01-13 06:25:43,644 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2022-01-13 06:25:43,776 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2022-01-13 06:25:43,875 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9cb3b3c0-f3d5-4c7d-a7e4-318a18735718 to datanode:b1a62a11-0048-42bf-a9c3-836e0373e878
scm1.org_1   | 2022-01-13 06:25:44,530 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]].
scm1.org_1   | 2022-01-13 06:25:44,530 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:25:44,627 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 377071.666us
scm1.org_1   | 2022-01-13 06:25:44,694 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7d03933b-289c-4ae9-b706-c03134748096 to datanode:22377b14-877d-4b48-97e5-336a98a1cf76
scm1.org_1   | 2022-01-13 06:25:44,794 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]].
scm1.org_1   | 2022-01-13 06:25:44,808 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:25:44,817 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 112197.472us
scm1.org_1   | 2022-01-13 06:25:51,174 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:39120
scm1.org_1   | 2022-01-13 06:25:51,259 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:25:52,051 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:43392
scm1.org_1   | 2022-01-13 06:25:52,171 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:25:52,627 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:41752
scm1.org_1   | 2022-01-13 06:25:52,723 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2022-01-13 06:25:56,668 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42826
scm1.org_1   | 2022-01-13 06:25:56,787 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:26:01,370 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:54078
scm1.org_1   | 2022-01-13 06:26:01,398 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:26:02,304 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:39208
scm1.org_1   | 2022-01-13 06:26:02,318 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:26:03,514 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:52208
scm1.org_1   | 2022-01-13 06:26:03,530 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2022-01-13 06:26:13,102 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:47042
scm1.org_1   | 2022-01-13 06:26:13,184 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:52838
scm1.org_1   | 2022-01-13 06:26:13,213 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58822
scm1.org_1   | 2022-01-13 06:26:13,262 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:13,292 [IPC Server handler 31 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:26:13,292 [IPC Server handler 31 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2022-01-13 06:26:13,295 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2022-01-13 06:26:13,301 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2b9e81e9-4435-4567-af0a-448797766036 to datanode:ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:26:13,355 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2022-01-13 06:26:13,355 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2022-01-13 06:26:13,356 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2022-01-13 06:26:13,408 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2022-01-13 06:26:13,408 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2022-01-13 06:26:13,408 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2022-01-13 06:26:13,410 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:13,410 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:13,410 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]].
scm1.org_1   | 2022-01-13 06:26:13,410 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:13,411 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 96289.036us
scm1.org_1   | 2022-01-13 06:26:13,623 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 to datanode:b1a62a11-0048-42bf-a9c3-836e0373e878
scm1.org_1   | 2022-01-13 06:26:13,627 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 to datanode:ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:26:13,634 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 to datanode:22377b14-877d-4b48-97e5-336a98a1cf76
scm3.org_1   | 2022-01-13 06:24:40,973 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2022-01-13 06:24:40,987 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2022-01-13 06:24:41,327 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2022-01-13 06:24:41,360 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2022-01-13 06:24:41,360 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2022-01-13 06:24:41,374 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2022-01-13 06:24:41,899 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm3.org_1   | 2022-01-13 06:24:41,928 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm3.org_1   | 2022-01-13 06:24:41,931 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm3.org_1   | 2022-01-13 06:24:43,277 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@398f6622] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3.org_1   | 2022-01-13 06:24:43,472 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2022-01-13 06:24:43,487 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2022-01-13 06:24:43,489 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2022-01-13 06:24:43,734 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @33496ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2022-01-13 06:24:44,782 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2022-01-13 06:24:44,828 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2022-01-13 06:24:44,836 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2022-01-13 06:24:44,847 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2022-01-13 06:24:44,851 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2022-01-13 06:24:44,854 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2022-01-13 06:24:45,260 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2022-01-13 06:24:45,269 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.10+9-LTS
scm3.org_1   | 2022-01-13 06:24:45,622 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2022-01-13 06:24:45,635 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2022-01-13 06:24:45,642 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm3.org_1   | 2022-01-13 06:24:45,819 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2022-01-13 06:24:45,830 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7306f65f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2022-01-13 06:24:45,853 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1088b120{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2022-01-13 06:24:46,819 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2022-01-13 06:24:46,905 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@54a81a18{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0-SNAPSHOT_jar-_-any-18303281881519803895/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2022-01-13 06:24:47,024 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@6abd88fe{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2022-01-13 06:24:47,026 [Listener at 0.0.0.0/9860] INFO server.Server: Started @36788ms
scm3.org_1   | 2022-01-13 06:24:47,048 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2022-01-13 06:24:47,048 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2022-01-13 06:24:47,060 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2022-01-13 06:24:54,375 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:24:54,379 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2022-01-13 06:24:54,379 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2022-01-13 06:24:54,514 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:24:55,816 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:07,864 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:08,865 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:10,840 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:37,460 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49830
scm1.org_1   | 2022-01-13 06:26:13,711 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]].
scm1.org_1   | 2022-01-13 06:26:13,714 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:13,719 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 84586.69us
scm1.org_1   | 2022-01-13 06:26:13,773 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e to datanode:22377b14-877d-4b48-97e5-336a98a1cf76
scm1.org_1   | 2022-01-13 06:26:13,777 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e to datanode:b1a62a11-0048-42bf-a9c3-836e0373e878
scm1.org_1   | 2022-01-13 06:26:13,778 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e to datanode:ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:26:13,807 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fe6afea2-6277-43ca-80b4-408a10c34f6e, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.773Z[UTC]].
scm1.org_1   | 2022-01-13 06:26:13,808 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:13,808 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 29043.018us
scm1.org_1   | 2022-01-13 06:26:13,843 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=fe6afea2-6277-43ca-80b4-408a10c34f6e contains same datanodes as previous pipelines: PipelineID=45dbb9df-9697-41d5-8750-7a75614a6b11 nodeIds: 22377b14-877d-4b48-97e5-336a98a1cf76, b1a62a11-0048-42bf-a9c3-836e0373e878, ab6e22bd-d408-4246-98c0-61fe3297cb52
scm1.org_1   | 2022-01-13 06:26:26,650 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42914
scm1.org_1   | 2022-01-13 06:26:26,685 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:26:37,664 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42936
scm1.org_1   | 2022-01-13 06:26:37,710 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:26:42,721 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:52922
scm1.org_1   | 2022-01-13 06:26:42,985 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:43,138 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:47136
scm1.org_1   | 2022-01-13 06:26:43,154 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58908
scm1.org_1   | 2022-01-13 06:26:43,172 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:43,208 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:44,153 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]] moved to OPEN state
scm1.org_1   | 2022-01-13 06:26:44,331 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.115:46449
scm1.org_1   | 2022-01-13 06:26:44,352 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:26:44,419 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:37,507 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:25:37,606 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:53004
scm3.org_1   | 2022-01-13 06:25:37,683 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:25:39,536 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51426
scm3.org_1   | 2022-01-13 06:25:39,594 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:25:42,951 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b1a62a11-0048-42bf-a9c3-836e0373e878
scm3.org_1   | 2022-01-13 06:25:42,999 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22377b14-877d-4b48-97e5-336a98a1cf76
scm3.org_1   | 2022-01-13 06:25:43,120 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 978106138862, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2022-01-13 06:25:43,190 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 979457010743, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2022-01-13 06:25:43,204 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2022-01-13 06:25:43,263 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2022-01-13 06:25:43,257 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2022-01-13 06:25:43,406 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2022-01-13 06:25:45,002 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]].
scm3.org_1   | 2022-01-13 06:25:45,007 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:25:45,037 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]].
scm3.org_1   | 2022-01-13 06:25:45,044 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:12,723 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49938
scm3.org_1   | 2022-01-13 06:26:13,024 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:26:13,042 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:53106
scm3.org_1   | 2022-01-13 06:26:13,056 [IPC Server handler 7 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/ab6e22bd-d408-4246-98c0-61fe3297cb52
scm3.org_1   | 2022-01-13 06:26:13,080 [IPC Server handler 7 on default port 9861] INFO node.SCMNodeManager: Registered Data node : ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 977357248554, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2022-01-13 06:26:13,088 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2022-01-13 06:26:13,066 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51532
scm3.org_1   | 2022-01-13 06:26:13,148 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:26:13,228 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2022-01-13 06:26:13,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2022-01-13 06:26:13,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2022-01-13 06:26:13,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3.org_1   | 2022-01-13 06:26:13,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2022-01-13 06:26:13,239 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2022-01-13 06:26:13,313 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2022-01-13 06:26:44,468 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 225225.255us
scm1.org_1   | 2022-01-13 06:26:44,546 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:ab6e22bd-d408-4246-98c0-61fe3297cb52, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]] moved to OPEN state
scm1.org_1   | 2022-01-13 06:26:44,564 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:44,689 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:44,707 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 160294.175us
scm1.org_1   | 2022-01-13 06:26:44,708 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 9cb3b3c0-f3d5-4c7d-a7e4-318a18735718, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:25:43.661Z[UTC]] moved to OPEN state
scm1.org_1   | 2022-01-13 06:26:44,777 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:44,872 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:44,899 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 190864.888us
scm1.org_1   | 2022-01-13 06:26:44,899 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:45,670 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:45,761 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:46,137 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:50,543 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:50,848 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:51,265 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:13,470 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 2b9e81e9-4435-4567-af0a-448797766036, Nodes: ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.301Z[UTC]].
scm3.org_1   | 2022-01-13 06:26:13,470 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:13,723 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]].
scm3.org_1   | 2022-01-13 06:26:13,725 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:13,833 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: fe6afea2-6277-43ca-80b4-408a10c34f6e, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-01-13T06:26:13.773Z[UTC]].
scm3.org_1   | 2022-01-13 06:26:13,841 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:42,861 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:53182
scm3.org_1   | 2022-01-13 06:26:42,927 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50024
scm3.org_1   | 2022-01-13 06:26:42,954 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:26:43,012 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:26:43,097 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51612
scm3.org_1   | 2022-01-13 06:26:43,107 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2022-01-13 06:26:44,096 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7d03933b-289c-4ae9-b706-c03134748096, Nodes: 22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:22377b14-877d-4b48-97e5-336a98a1cf76, CreationTimestamp2022-01-13T06:25:44.694Z[UTC]] moved to OPEN state
scm3.org_1   | 2022-01-13 06:26:44,424 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:44,774 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:44,944 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2022-01-13 06:26:44,964 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@39ed00b9, cost 859477.739us
scm3.org_1   | 2022-01-13 06:26:44,997 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:44,997 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:45,679 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:45,736 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:46,123 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:50,520 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:50,874 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:51,209 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:54,088 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:54,949 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:26:55,055 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:27:03,992 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]] moved to OPEN state
scm3.org_1   | 2022-01-13 06:27:03,993 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@39ed00b9, cost 412.495us
scm3.org_1   | 2022-01-13 06:27:03,993 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2022-01-13 06:27:04,142 [e3ebff3b-14c4-4117-a7dd-8ec94ff36cba@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:26:54,092 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:54,976 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:26:55,063 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:27:00,293 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43044
scm1.org_1   | 2022-01-13 06:27:00,337 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2022-01-13 06:27:03,985 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 45dbb9df-9697-41d5-8750-7a75614a6b11, Nodes: b1a62a11-0048-42bf-a9c3-836e0373e878{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ab6e22bd-d408-4246-98c0-61fe3297cb52{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22377b14-877d-4b48-97e5-336a98a1cf76{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b1a62a11-0048-42bf-a9c3-836e0373e878, CreationTimestamp2022-01-13T06:26:13.623Z[UTC]] moved to OPEN state
scm1.org_1   | 2022-01-13 06:27:04,074 [c6280d25-12c7-4451-bd27-cfac689faecd@group-00FB17B6AB6E-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2022-01-13 06:27:04,074 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@3dc210f1, cost 88672.206us
scm1.org_1   | 2022-01-13 06:27:04,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2022-01-13 06:27:04,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2022-01-13 06:27:04,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2022-01-13 06:27:04,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2022-01-13 06:27:04,102 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2022-01-13 06:27:04,122 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2022-01-13 06:27:04,122 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
