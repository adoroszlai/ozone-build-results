rm: cannot remove '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/*': No such file or directory
Executing test in compatibility
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network compatibility_default
Network compatibility_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "compatibility_default" with the default driver
Pulling datanode (apache/ozone-runner:20220228-1)...
20220228-1: Pulling from apache/ozone-runner
Digest: sha256:4e7fac095177756b946899f202c1c5971ebc31e76cb6153695df453a84583487
Status: Downloaded newer image for apache/ozone-runner:20220228-1
Creating compatibility_recon_1 ... 
Creating compatibility_scm_1   ... 
Creating compatibility_s3g_1   ... 
Creating compatibility_om_1    ... 
Creating compatibility_datanode_1 ... 
Creating compatibility_datanode_1 ... done
Creating compatibility_om_1       ... done
Creating compatibility_scm_1      ... done
Creating compatibility_recon_1    ... done
Creating compatibility_s3g_1      ... done
WARNING: HADOOP_OPTS has been deprecated by OZONE_OPTS.
SECONDS: 31
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 458b703e5658/172.18.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 458b703e5658/172.18.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 458b703e5658/172.18.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 458b703e5658/172.18.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:683d5321-ec7b-4d31-b0fb-e405d6c1cd69 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:683d5321-ec7b-4d31-b0fb-e405d6c1cd69 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.2:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=1) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=0) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
WARNING: HADOOP_OPTS has been deprecated by OZONE_OPTS.
SECONDS: 39
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Dn :: Test datanode compatibility                                             
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Dn :: Test datanode compatibility                                     | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode.xml
==============================================================================
Om :: Test om compatibility                                                   
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Om :: Test om compatibility                                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-1.xml
==============================================================================
Recon :: Test recon compatibility                                             
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Recon :: Test recon compatibility                                     | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-2.xml
==============================================================================
Scm :: Test scm compatibility                                                 
==============================================================================
Picks up command line options                                         | PASS |
------------------------------------------------------------------------------
Scm :: Test scm compatibility                                         | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/compatibility/result/robot-compatibility-compatibility-dn-datanode-3.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping compatibility_s3g_1      ... 
Stopping compatibility_datanode_1 ... 
Stopping compatibility_om_1       ... 
Stopping compatibility_recon_1    ... 
Stopping compatibility_scm_1      ... 
Stopping compatibility_s3g_1      ... done
Stopping compatibility_om_1       ... done
Stopping compatibility_scm_1      ... done
Stopping compatibility_recon_1    ... done
Stopping compatibility_datanode_1 ... done
Removing compatibility_s3g_1      ... 
Removing compatibility_datanode_1 ... 
Removing compatibility_om_1       ... 
Removing compatibility_recon_1    ... 
Removing compatibility_scm_1      ... 
Removing compatibility_s3g_1      ... done
Removing compatibility_om_1       ... done
Removing compatibility_recon_1    ... done
Removing compatibility_datanode_1 ... done
Removing compatibility_scm_1      ... done
Removing network compatibility_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/compatibility/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/compatibility/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/compatibility.xml
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-1.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-2.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode-3.xml'
removed 'compatibility/result/robot-compatibility-compatibility-dn-datanode.xml'
removed 'compatibility/result/log.html'
removed 'compatibility/result/report.html'
renamed 'compatibility/result/dn-audit-f7c40ec9e607.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/compatibility/dn-audit-f7c40ec9e607.log'
renamed 'compatibility/result/docker-compatibility-compatibility-dn-datanode.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/compatibility/docker-compatibility-compatibility-dn-datanode.log'
renamed 'compatibility/result/om-audit-eb1356a0af43.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/compatibility/om-audit-eb1356a0af43.log'
renamed 'compatibility/result/scm-audit-458b703e5658.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/compatibility/scm-audit-458b703e5658.log'
Executing test in ozone-csi
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozone-csi_default
Network ozone-csi_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozone-csi_default" with the default driver
Creating ozone-csi_om_1 ... 
Creating ozone-csi_scm_1 ... 
Creating ozone-csi_csi_1 ... 
Creating ozone-csi_datanode_1 ... 
Creating ozone-csi_datanode_2 ... 
Creating ozone-csi_datanode_3 ... 
Creating ozone-csi_csi_1      ... done
Creating ozone-csi_datanode_1 ... done
Creating ozone-csi_om_1       ... done
Creating ozone-csi_datanode_2 ... done
Creating ozone-csi_datanode_3 ... done
Creating ozone-csi_scm_1      ... done
SECONDS: 41
com.google.protobuf.ServiceException: java.net.ConnectException: Call From b27c1f28afde/172.19.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From b27c1f28afde/172.19.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From b27c1f28afde/172.19.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From b27c1f28afde/172.19.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8ea8132f-8324-400e-8434-4ba6d10cde1d is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:8ea8132f-8324-400e-8434-4ba6d10cde1d is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.3:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=1) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=0) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 56
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Csi :: Smoketest Ozone CSI service                                            
==============================================================================
Check if CSI server is started                                        | PASS |
------------------------------------------------------------------------------
Test CSI identity service                                             | PASS |
------------------------------------------------------------------------------
Csi :: Smoketest Ozone CSI service                                    | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-csi/result/robot-ozone-csi-ozone-csi-csi-csi.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozone-csi_datanode_3 ... 
Stopping ozone-csi_csi_1      ... 
Stopping ozone-csi_datanode_1 ... 
Stopping ozone-csi_datanode_2 ... 
Stopping ozone-csi_om_1       ... 
Stopping ozone-csi_scm_1      ... 
Stopping ozone-csi_csi_1      ... done
Stopping ozone-csi_om_1       ... done
Stopping ozone-csi_scm_1      ... done
Stopping ozone-csi_datanode_1 ... done
Stopping ozone-csi_datanode_3 ... done
Stopping ozone-csi_datanode_2 ... done
Removing ozone-csi_datanode_3 ... 
Removing ozone-csi_csi_1      ... 
Removing ozone-csi_datanode_1 ... 
Removing ozone-csi_datanode_2 ... 
Removing ozone-csi_om_1       ... 
Removing ozone-csi_scm_1      ... 
Removing ozone-csi_scm_1      ... done
Removing ozone-csi_datanode_3 ... done
Removing ozone-csi_csi_1      ... done
Removing ozone-csi_datanode_1 ... done
Removing ozone-csi_datanode_2 ... done
Removing ozone-csi_om_1       ... done
Removing network ozone-csi_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-csi/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-csi/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi.xml
removed 'ozone-csi/result/robot-ozone-csi-ozone-csi-csi-csi.xml'
removed 'ozone-csi/result/log.html'
removed 'ozone-csi/result/report.html'
renamed 'ozone-csi/result/dn-audit-0d5e3c75f86d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-0d5e3c75f86d.log'
renamed 'ozone-csi/result/dn-audit-85e0242e75ab.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-85e0242e75ab.log'
renamed 'ozone-csi/result/dn-audit-c9f504a0fa61.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/dn-audit-c9f504a0fa61.log'
renamed 'ozone-csi/result/docker-ozone-csi-ozone-csi-csi-csi.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/docker-ozone-csi-ozone-csi-csi-csi.log'
renamed 'ozone-csi/result/om-audit-aae65341cc2c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/om-audit-aae65341cc2c.log'
renamed 'ozone-csi/result/scm-audit-b27c1f28afde.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-csi/scm-audit-b27c1f28afde.log'
Executing test in ozone-om-prepare
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/scm': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/om2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/om3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data/om1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/data': Operation not permitted
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_dn1_1 ... done
Creating ozone-om-prepare_dn2_1 ... done
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_om3_1 ... done
SECONDS: 41
com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbcd22fd520/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbcd22fd520/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From adbcd22fd520/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 58
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om2 : LEADER (om2)
==============================================================================
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.           
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Checks if the expected data is present in OM                          | PASS |
------------------------------------------------------------------------------
Test write operation fails                                            | PASS |
------------------------------------------------------------------------------
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.xml
==============================================================================
Om-Cancel-Prepare :: Smoke test for ozone manager cancel prepare              
==============================================================================
Cancel Ozone Manager Prepare                                          | PASS |
------------------------------------------------------------------------------
Test write operations                                                 | PASS |
------------------------------------------------------------------------------
Om-Cancel-Prepare :: Smoke test for ozone manager cancel prepare      | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-1.xml
==============================================================================
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.           
==============================================================================
Prepare Ozone Manager                                                 | PASS |
------------------------------------------------------------------------------
Checks if the expected data is present in OM                          | PASS |
------------------------------------------------------------------------------
Test write operation fails                                            | PASS |
------------------------------------------------------------------------------
Om-Prepare :: Smoke test to test preparing OMs in an OM HA cluster.   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-2.xml
==============================================================================
Om-Prepared :: Smoke test to test that OMs are prepared in an OM HA cluster.  
==============================================================================
Test create volume fails                                              | PASS |
------------------------------------------------------------------------------
Test list volumes succeeds                                            | PASS |
------------------------------------------------------------------------------
Om-Prepared :: Smoke test to test that OMs are prepared in an OM H... | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-3.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_om2_1 ... done
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_dn2_1 ... done
Removing network ozone-om-prepare_net
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_dn2_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_om3_1 ... done
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_dn1_1 ... done
SECONDS: 37
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=2) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=2) >= threshold (=2)
SECONDS: 50
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
==============================================================================
Om-Prepared :: Smoke test to test that OMs are prepared in an OM HA cluster.  
==============================================================================
Test create volume fails                                              | PASS |
------------------------------------------------------------------------------
Test list volumes succeeds                                            | PASS |
------------------------------------------------------------------------------
Om-Prepared :: Smoke test to test that OMs are prepared in an OM H... | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-4.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_om2_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_dn2_1 ... done
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing network ozone-om-prepare_net
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozone-om-prepare_net
Network ozone-om-prepare_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozone-om-prepare_net" with driver "bridge"
Creating ozone-om-prepare_dn2_1 ... 
Creating ozone-om-prepare_dn3_1 ... 
Creating ozone-om-prepare_om1_1 ... 
Creating ozone-om-prepare_om3_1 ... 
Creating ozone-om-prepare_scm_1 ... 
Creating ozone-om-prepare_om2_1 ... 
Creating ozone-om-prepare_dn1_1 ... 
Creating ozone-om-prepare_om2_1 ... done
Creating ozone-om-prepare_dn2_1 ... done
Creating ozone-om-prepare_scm_1 ... done
Creating ozone-om-prepare_dn1_1 ... done
Creating ozone-om-prepare_dn3_1 ... done
Creating ozone-om-prepare_om1_1 ... done
Creating ozone-om-prepare_om3_1 ... done
SECONDS: 37
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:cb1478b1-3087-4d0f-abdf-3bc6ed121f54 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 53
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om3 : LEADER (om3)
==============================================================================
Loaddata :: Smoketest ozone cluster startup                                   
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Loaddata :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-5.xml
==============================================================================
Readdata :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Readdata :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-6.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozone-om-prepare_scm_1 ... 
Stopping ozone-om-prepare_dn1_1 ... 
Stopping ozone-om-prepare_om2_1 ... 
Stopping ozone-om-prepare_om3_1 ... 
Stopping ozone-om-prepare_dn3_1 ... 
Stopping ozone-om-prepare_om1_1 ... 
Stopping ozone-om-prepare_dn2_1 ... 
Stopping ozone-om-prepare_om1_1 ... done
Stopping ozone-om-prepare_om3_1 ... done
Stopping ozone-om-prepare_om2_1 ... done
Stopping ozone-om-prepare_scm_1 ... done
Stopping ozone-om-prepare_dn2_1 ... done
Stopping ozone-om-prepare_dn3_1 ... done
Stopping ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_scm_1 ... 
Removing ozone-om-prepare_dn1_1 ... 
Removing ozone-om-prepare_om2_1 ... 
Removing ozone-om-prepare_om3_1 ... 
Removing ozone-om-prepare_dn3_1 ... 
Removing ozone-om-prepare_om1_1 ... 
Removing ozone-om-prepare_dn2_1 ... 
Removing ozone-om-prepare_om3_1 ... done
Removing ozone-om-prepare_dn2_1 ... done
Removing ozone-om-prepare_scm_1 ... done
Removing ozone-om-prepare_dn3_1 ... done
Removing ozone-om-prepare_om1_1 ... done
Removing ozone-om-prepare_dn1_1 ... done
Removing ozone-om-prepare_om2_1 ... done
Removing network ozone-om-prepare_net
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-om-prepare/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare.xml
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-1.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-2.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-3.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-4.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-5.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm-6.xml'
removed 'ozone-om-prepare/result/robot-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.xml'
removed 'ozone-om-prepare/result/log.html'
removed 'ozone-om-prepare/result/report.html'
renamed 'ozone-om-prepare/result/dn-audit-430d1fa57e76.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-430d1fa57e76.log'
renamed 'ozone-om-prepare/result/dn-audit-58436c79510d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-58436c79510d.log'
renamed 'ozone-om-prepare/result/dn-audit-74204d6b9d2e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-74204d6b9d2e.log'
renamed 'ozone-om-prepare/result/dn-audit-87489e9a8164.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-87489e9a8164.log'
renamed 'ozone-om-prepare/result/dn-audit-8b27bd61c163.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-8b27bd61c163.log'
renamed 'ozone-om-prepare/result/dn-audit-903871900dec.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-903871900dec.log'
renamed 'ozone-om-prepare/result/dn-audit-97b1da5e711d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-97b1da5e711d.log'
renamed 'ozone-om-prepare/result/dn-audit-aa83f80a6b89.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-aa83f80a6b89.log'
renamed 'ozone-om-prepare/result/dn-audit-b4b1149385bf.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/dn-audit-b4b1149385bf.log'
renamed 'ozone-om-prepare/result/docker-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/docker-ozone-om-prepare-ozone-om-prepare-om-prepare-scm.log'
renamed 'ozone-om-prepare/result/om-audit-040708869ac3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-040708869ac3.log'
renamed 'ozone-om-prepare/result/om-audit-0783e1971735.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-0783e1971735.log'
renamed 'ozone-om-prepare/result/om-audit-4a9619ae23de.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-4a9619ae23de.log'
renamed 'ozone-om-prepare/result/om-audit-4fcef6bb9d8f.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-4fcef6bb9d8f.log'
renamed 'ozone-om-prepare/result/om-audit-89761c928311.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-89761c928311.log'
renamed 'ozone-om-prepare/result/om-audit-adf5ef59e448.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-adf5ef59e448.log'
renamed 'ozone-om-prepare/result/om-audit-b1d2d3814485.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-b1d2d3814485.log'
renamed 'ozone-om-prepare/result/om-audit-c83ebef00dac.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-c83ebef00dac.log'
renamed 'ozone-om-prepare/result/om-audit-fc3ba0984eae.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/om-audit-fc3ba0984eae.log'
renamed 'ozone-om-prepare/result/scm-audit-60c0c0f5e4b2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-60c0c0f5e4b2.log'
renamed 'ozone-om-prepare/result/scm-audit-65529349c08a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-65529349c08a.log'
renamed 'ozone-om-prepare/result/scm-audit-adbcd22fd520.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-om-prepare/scm-audit-adbcd22fd520.log'
Executing test in ozone-topology
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozone-topology_net
Network ozone-topology_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozone-topology_net" with driver "bridge"
Creating ozone-topology_datanode_3_1 ... 
Creating ozone-topology_datanode_2_1 ... 
Creating ozone-topology_datanode_4_1 ... 
Creating ozone-topology_scm_1        ... 
Creating ozone-topology_datanode_6_1 ... 
Creating ozone-topology_om_1         ... 
Creating ozone-topology_datanode_5_1 ... 
Creating ozone-topology_datanode_1_1 ... 
Creating ozone-topology_datanode_4_1 ... done
Creating ozone-topology_datanode_2_1 ... done
Creating ozone-topology_datanode_6_1 ... done
Creating ozone-topology_datanode_1_1 ... done
Creating ozone-topology_datanode_3_1 ... done
Creating ozone-topology_datanode_5_1 ... done
Creating ozone-topology_scm_1        ... done
Creating ozone-topology_om_1         ... done
SECONDS: 64
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07dd5dda1bc7/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07dd5dda1bc7/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 07dd5dda1bc7/10.5.0.71 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c4178317-9bdc-4082-982d-1fff3871416f is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:c4178317-9bdc-4082-982d-1fff3871416f is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.5.0.71:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=4) >= required datanodes (=4) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 81
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Start freon testing                                                   | FAIL |
Test timeout 5 minutes exceeded.
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | FAIL |
2 tests, 1 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_1_1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_2_1_HddsDatanodeService.stack
jstack 11 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_3_1_HddsDatanodeService.stack
jstack 6 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_4_1_HddsDatanodeService.stack
jstack 8 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_5_1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_datanode_6_1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_om_1_OzoneManagerStarter.stack
jstack 754 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_scm_1_Freon.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozone-topology/result/ozone-topology_scm_1_StorageContainerManagerStarter.stack
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozone-topology_datanode_1_1 ... 
Stopping ozone-topology_scm_1        ... 
Stopping ozone-topology_datanode_6_1 ... 
Stopping ozone-topology_datanode_5_1 ... 
Stopping ozone-topology_om_1         ... 
Stopping ozone-topology_datanode_2_1 ... 
Stopping ozone-topology_datanode_4_1 ... 
Stopping ozone-topology_datanode_3_1 ... 
Stopping ozone-topology_om_1         ... done
Stopping ozone-topology_scm_1        ... done
Stopping ozone-topology_datanode_6_1 ... done
Stopping ozone-topology_datanode_5_1 ... done
Stopping ozone-topology_datanode_3_1 ... done
Stopping ozone-topology_datanode_2_1 ... done
Stopping ozone-topology_datanode_4_1 ... done
Stopping ozone-topology_datanode_1_1 ... done
Removing ozone-topology_datanode_1_1 ... 
Removing ozone-topology_scm_1        ... 
Removing ozone-topology_datanode_6_1 ... 
Removing ozone-topology_datanode_5_1 ... 
Removing ozone-topology_om_1         ... 
Removing ozone-topology_datanode_2_1 ... 
Removing ozone-topology_datanode_4_1 ... 
Removing ozone-topology_datanode_3_1 ... 
Removing ozone-topology_datanode_5_1 ... done
Removing ozone-topology_datanode_6_1 ... done
Removing ozone-topology_datanode_1_1 ... done
Removing ozone-topology_datanode_3_1 ... done
Removing ozone-topology_scm_1        ... done
Removing ozone-topology_om_1         ... done
Removing ozone-topology_datanode_4_1 ... done
Removing ozone-topology_datanode_2_1 ... done
Removing network ozone-topology_net
ERROR: Test execution of ozone-topology is FAILED!!!!
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology.xml
removed 'ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml'
renamed 'ozone-topology/result/dn-audit-640c7fca21fb.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-640c7fca21fb.log'
renamed 'ozone-topology/result/dn-audit-6c42bd8d1818.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-6c42bd8d1818.log'
renamed 'ozone-topology/result/dn-audit-778c02440557.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-778c02440557.log'
renamed 'ozone-topology/result/dn-audit-cdace630d646.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-cdace630d646.log'
renamed 'ozone-topology/result/dn-audit-ea9961234dc1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-ea9961234dc1.log'
renamed 'ozone-topology/result/dn-audit-fb8065d5712e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/dn-audit-fb8065d5712e.log'
renamed 'ozone-topology/result/docker-ozone-topology-ozone-topology-basic-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/docker-ozone-topology-ozone-topology-basic-scm.log'
renamed 'ozone-topology/result/om-audit-31108a23ca22.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/om-audit-31108a23ca22.log'
renamed 'ozone-topology/result/ozone-topology_datanode_1_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_1_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_datanode_2_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_2_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_datanode_3_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_3_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_datanode_4_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_4_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_datanode_5_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_5_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_datanode_6_1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_datanode_6_1_HddsDatanodeService.stack'
renamed 'ozone-topology/result/ozone-topology_om_1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_om_1_OzoneManagerStarter.stack'
renamed 'ozone-topology/result/ozone-topology_scm_1_Freon.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_scm_1_Freon.stack'
renamed 'ozone-topology/result/ozone-topology_scm_1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/ozone-topology_scm_1_StorageContainerManagerStarter.stack'
renamed 'ozone-topology/result/scm-audit-07dd5dda1bc7.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozone-topology/scm-audit-07dd5dda1bc7.log'
Executing test in ozones3-haproxy
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozones3-haproxy_default
Network ozones3-haproxy_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozones3-haproxy_default" with the default driver
Pulling s3g (haproxy:latest)...
latest: Pulling from library/haproxy
Digest: sha256:a0f702c3ce2466affdd77c685bfcafdb45618c17d65d690587324081c9443a07
Status: Downloaded newer image for haproxy:latest
Creating ozones3-haproxy_s3g_1 ... 
Creating ozones3-haproxy_scm_1 ... 
Creating ozones3-haproxy_s3g3_1 ... 
Creating ozones3-haproxy_om_1   ... 
Creating ozones3-haproxy_datanode_1 ... 
Creating ozones3-haproxy_datanode_2 ... 
Creating ozones3-haproxy_datanode_3 ... 
Creating ozones3-haproxy_s3g1_1     ... 
Creating ozones3-haproxy_s3g2_1     ... 
Creating ozones3-haproxy_datanode_1 ... done
Creating ozones3-haproxy_s3g1_1     ... done
Creating ozones3-haproxy_s3g_1      ... done
Creating ozones3-haproxy_scm_1      ... done
Creating ozones3-haproxy_om_1       ... done
Creating ozones3-haproxy_datanode_2 ... done
Creating ozones3-haproxy_s3g3_1     ... done
Creating ozones3-haproxy_datanode_3 ... done
Creating ozones3-haproxy_s3g2_1     ... done
SECONDS: 46
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 35bb68cb21c6/172.20.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 35bb68cb21c6/172.20.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.3:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 35bb68cb21c6/172.20.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.3:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:a0f70935-c012-43c3-96f9-da6d5b077137 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.3:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:a0f70935-c012-43c3-96f9-da6d5b077137 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.20.0.3:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 58
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 64
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Basic :: Smoketest ozone cluster startup                                      
==============================================================================
Check webui static resources                                          | PASS |
------------------------------------------------------------------------------
Start freon testing                                                   | PASS |
------------------------------------------------------------------------------
Basic :: Smoketest ozone cluster startup                              | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozones3-haproxy/result/robot-ozones3-haproxy-ozones3-haproxy-basic-scm.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozones3-haproxy_datanode_1 ... 
Stopping ozones3-haproxy_s3g2_1     ... 
Stopping ozones3-haproxy_datanode_2 ... 
Stopping ozones3-haproxy_s3g1_1     ... 
Stopping ozones3-haproxy_datanode_3 ... 
Stopping ozones3-haproxy_om_1       ... 
Stopping ozones3-haproxy_s3g3_1     ... 
Stopping ozones3-haproxy_s3g_1      ... 
Stopping ozones3-haproxy_scm_1      ... 
Stopping ozones3-haproxy_s3g_1      ... done
Stopping ozones3-haproxy_s3g3_1     ... done
Stopping ozones3-haproxy_s3g1_1     ... done
Stopping ozones3-haproxy_s3g2_1     ... done
Stopping ozones3-haproxy_om_1       ... done
Stopping ozones3-haproxy_scm_1      ... done
Stopping ozones3-haproxy_datanode_2 ... done
Stopping ozones3-haproxy_datanode_1 ... done
Stopping ozones3-haproxy_datanode_3 ... done
Removing ozones3-haproxy_datanode_1 ... 
Removing ozones3-haproxy_s3g2_1     ... 
Removing ozones3-haproxy_datanode_2 ... 
Removing ozones3-haproxy_s3g1_1     ... 
Removing ozones3-haproxy_datanode_3 ... 
Removing ozones3-haproxy_om_1       ... 
Removing ozones3-haproxy_s3g3_1     ... 
Removing ozones3-haproxy_s3g_1      ... 
Removing ozones3-haproxy_scm_1      ... 
Removing ozones3-haproxy_s3g_1      ... done
Removing ozones3-haproxy_om_1       ... done
Removing ozones3-haproxy_datanode_1 ... done
Removing ozones3-haproxy_s3g1_1     ... done
Removing ozones3-haproxy_datanode_2 ... done
Removing ozones3-haproxy_s3g2_1     ... done
Removing ozones3-haproxy_scm_1      ... done
Removing ozones3-haproxy_s3g3_1     ... done
Removing ozones3-haproxy_datanode_3 ... done
Removing network ozones3-haproxy_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozones3-haproxy/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozones3-haproxy/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy.xml
removed 'ozones3-haproxy/result/robot-ozones3-haproxy-ozones3-haproxy-basic-scm.xml'
removed 'ozones3-haproxy/result/log.html'
removed 'ozones3-haproxy/result/report.html'
renamed 'ozones3-haproxy/result/dn-audit-84efdc62aca6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/dn-audit-84efdc62aca6.log'
renamed 'ozones3-haproxy/result/dn-audit-e6e5461cd59c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/dn-audit-e6e5461cd59c.log'
renamed 'ozones3-haproxy/result/dn-audit-fd0c70ee695f.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/dn-audit-fd0c70ee695f.log'
renamed 'ozones3-haproxy/result/docker-ozones3-haproxy-ozones3-haproxy-basic-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/docker-ozones3-haproxy-ozones3-haproxy-basic-scm.log'
renamed 'ozones3-haproxy/result/om-audit-232c0901e687.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/om-audit-232c0901e687.log'
renamed 'ozones3-haproxy/result/scm-audit-35bb68cb21c6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozones3-haproxy/scm-audit-35bb68cb21c6.log'
Executing test in ozonescripts
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ozonescripts_default
Network ozonescripts_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ozonescripts_default" with the default driver
Building datanode
Sending build context to Docker daemon  30.21kB
Step 1/17 : ARG OZONE_RUNNER_IMAGE
Step 2/17 : ARG OZONE_RUNNER_VERSION
Step 3/17 : FROM ${OZONE_RUNNER_IMAGE}:${OZONE_RUNNER_VERSION}
 ---> 4fb679209166
Step 4/17 : RUN sudo yum install -y openssh-clients openssh-server
 ---> Running in 060a93b91864
Loaded plugins: fastestmirror, ovl
Determining fastest mirrors
 * base: centos.mirror.lstn.net
 * epel: pubmirror1.math.uh.edu
 * extras: mirror.cs.uwp.edu
 * updates: la.mirrors.clouvider.net
Resolving Dependencies
--> Running transaction check
---> Package openssh-clients.x86_64 0:7.4p1-22.el7_9 will be installed
--> Processing Dependency: openssh = 7.4p1-22.el7_9 for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: fipscheck-lib(x86-64) >= 1.3.0 for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: libfipscheck.so.1()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
--> Processing Dependency: libedit.so.0()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
---> Package openssh-server.x86_64 0:7.4p1-22.el7_9 will be installed
--> Processing Dependency: libwrap.so.0()(64bit) for package: openssh-server-7.4p1-22.el7_9.x86_64
--> Running transaction check
---> Package fipscheck-lib.x86_64 0:1.4.1-6.el7 will be installed
--> Processing Dependency: /usr/bin/fipscheck for package: fipscheck-lib-1.4.1-6.el7.x86_64
---> Package libedit.x86_64 0:3.0-12.20121213cvs.el7 will be installed
---> Package openssh.x86_64 0:7.4p1-22.el7_9 will be installed
---> Package tcp_wrappers-libs.x86_64 0:7.6-77.el7 will be installed
--> Running transaction check
---> Package fipscheck.x86_64 0:1.4.1-6.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package               Arch       Version                     Repository   Size
================================================================================
Installing:
 openssh-clients       x86_64     7.4p1-22.el7_9              updates     655 k
 openssh-server        x86_64     7.4p1-22.el7_9              updates     459 k
Installing for dependencies:
 fipscheck             x86_64     1.4.1-6.el7                 base         21 k
 fipscheck-lib         x86_64     1.4.1-6.el7                 base         11 k
 libedit               x86_64     3.0-12.20121213cvs.el7      base         92 k
 openssh               x86_64     7.4p1-22.el7_9              updates     510 k
 tcp_wrappers-libs     x86_64     7.6-77.el7                  base         66 k

Transaction Summary
================================================================================
Install  2 Packages (+5 Dependent packages)

Total download size: 1.8 M
Installed size: 5.8 M
Downloading packages:
--------------------------------------------------------------------------------
Total                                              2.7 MB/s | 1.8 MB  00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : fipscheck-1.4.1-6.el7.x86_64                                 1/7 
  Installing : fipscheck-lib-1.4.1-6.el7.x86_64                             2/7 
  Installing : openssh-7.4p1-22.el7_9.x86_64                                3/7 
  Installing : tcp_wrappers-libs-7.6-77.el7.x86_64                          4/7 
  Installing : libedit-3.0-12.20121213cvs.el7.x86_64                        5/7 
  Installing : openssh-clients-7.4p1-22.el7_9.x86_64                        6/7 
  Installing : openssh-server-7.4p1-22.el7_9.x86_64                         7/7 
  Verifying  : fipscheck-lib-1.4.1-6.el7.x86_64                             1/7 
  Verifying  : openssh-server-7.4p1-22.el7_9.x86_64                         2/7 
  Verifying  : fipscheck-1.4.1-6.el7.x86_64                                 3/7 
  Verifying  : libedit-3.0-12.20121213cvs.el7.x86_64                        4/7 
  Verifying  : openssh-clients-7.4p1-22.el7_9.x86_64                        5/7 
  Verifying  : tcp_wrappers-libs-7.6-77.el7.x86_64                          6/7 
  Verifying  : openssh-7.4p1-22.el7_9.x86_64                                7/7 

Installed:
  openssh-clients.x86_64 0:7.4p1-22.el7_9                                       
  openssh-server.x86_64 0:7.4p1-22.el7_9                                        

Dependency Installed:
  fipscheck.x86_64 0:1.4.1-6.el7            fipscheck-lib.x86_64 0:1.4.1-6.el7  
  libedit.x86_64 0:3.0-12.20121213cvs.el7   openssh.x86_64 0:7.4p1-22.el7_9     
  tcp_wrappers-libs.x86_64 0:7.6-77.el7    

Complete!
Removing intermediate container 060a93b91864
 ---> a74a88f013be
Step 5/17 : RUN sudo ssh-keygen -A
 ---> Running in 40f40a53c98b
ssh-keygen: generating new host keys: RSA1 RSA DSA ECDSA ED25519 
Removing intermediate container 40f40a53c98b
 ---> a5b01898605e
Step 6/17 : RUN sudo mkdir -p /run/sshd
 ---> Running in 5238d51ca26a
Removing intermediate container 5238d51ca26a
 ---> fd8205171803
Step 7/17 : RUN sudo sed -i "s/.*UsePrivilegeSeparation.*/UsePrivilegeSeparation no/g" /etc/ssh/sshd_config
 ---> Running in d699fa0d00b9
Removing intermediate container d699fa0d00b9
 ---> 0178239f9c5e
Step 8/17 : RUN sudo sed -i "s/.*PermitUserEnvironment.*/PermitUserEnvironment yes/g" /etc/ssh/sshd_config
 ---> Running in 6e635380c2bb
Removing intermediate container 6e635380c2bb
 ---> 005b9c597ae3
Step 9/17 : RUN sudo sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd
 ---> Running in 0ad79e32c40b
Removing intermediate container 0ad79e32c40b
 ---> a777ec1b5db1
Step 10/17 : RUN sudo usermod -d /opt hadoop
 ---> Running in adfa544dd640
Removing intermediate container adfa544dd640
 ---> 6b7d1af5da44
Step 11/17 : ADD .ssh /opt/.ssh
 ---> 38e38c9ef30e
Step 12/17 : RUN sudo chown -R hadoop /opt/.ssh
 ---> Running in 40d0f4579a7b
Removing intermediate container 40d0f4579a7b
 ---> 823d8fb7cf3d
Step 13/17 : RUN sudo chown hadoop /opt
 ---> Running in b63cbdce03f6
Removing intermediate container b63cbdce03f6
 ---> 8e284612525f
Step 14/17 : RUN sudo chmod 600 /opt/.ssh/*
 ---> Running in c23a3dbb15c2
Removing intermediate container c23a3dbb15c2
 ---> dd5df18147c6
Step 15/17 : RUN sudo chmod 700 /opt/.ssh
 ---> Running in 71aaf271d787
Removing intermediate container 71aaf271d787
 ---> fb377284cc7e
Step 16/17 : RUN sudo sh -c 'echo "export JAVA_HOME=/usr/lib/jvm/jre/" >> /etc/profile'
 ---> Running in e38a8eec487f
Removing intermediate container e38a8eec487f
 ---> 33858833c8b3
Step 17/17 : CMD ["sudo","/usr/sbin/sshd","-D"]
 ---> Running in c15f322fd283
Removing intermediate container c15f322fd283
 ---> 7355970e2e05
Successfully built 7355970e2e05
Successfully tagged ozone-runner-scripts:20220228-1
Image for service datanode was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
Creating ozonescripts_datanode_1 ... 
Creating ozonescripts_scm_1      ... 
Creating ozonescripts_om_1       ... 
Creating ozonescripts_scm_1      ... done
Creating ozonescripts_om_1       ... done
Creating ozonescripts_datanode_1 ... done
Port 22 is available on scm
Port 22 is available on om
Port 22 is available on datanode
No OM HA service, no need to wait
+ docker-compose ps
+ grep datanode
+ awk '{print $1}'
+ xargs -n1 docker inspect --format '{{ .Config.Hostname }}'
+ docker-compose ps
+ grep ozonescripts
+ awk '{print $1}'
+ xargs -I CONTAINER -n1 docker exec CONTAINER cp /opt/hadoop/etc/hadoop/workers /etc/hadoop/workers
+ docker-compose exec -T scm /opt/hadoop/bin/ozone scm --init
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2022-03-12 01:20:53,477 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = c810c44637f3/172.21.0.3
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.3.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/7cfd846027256198afd6522ce5ccb233ee16279b ; compiled by 'runner' on 2022-03-12T00:50Z
STARTUP_MSG:   java = 11.0.14.1
************************************************************/
2022-03-12 01:20:53,491 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2022-03-12 01:20:53,539 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2022-03-12 01:20:53,559 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
2022-03-12 01:20:53,559 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
2022-03-12 01:20:53,676 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2022-03-12 01:20:53,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
2022-03-12 01:20:53,792 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2022-03-12 01:20:53,793 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
2022-03-12 01:20:53,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2022-03-12 01:20:53,794 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
2022-03-12 01:20:53,796 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
2022-03-12 01:20:53,799 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-03-12 01:20:53,800 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2022-03-12 01:20:53,801 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
2022-03-12 01:20:53,981 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2022-03-12 01:20:53,984 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2022-03-12 01:20:53,985 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-03-12 01:20:53,996 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2022-03-12 01:20:54,002 [main] INFO server.RaftServer: ed65c6cf-a845-4339-9c5e-d1ed83e00561: addNew group-B0ACDC5D2A6A:[ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|priority:0] returns group-B0ACDC5D2A6A:java.util.concurrent.CompletableFuture@4d098f9b[Not completed]
2022-03-12 01:20:54,027 [pool-2-thread-1] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561: new RaftServerImpl for group-B0ACDC5D2A6A:[ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|priority:0] with SCMStateMachine:uninitialized
2022-03-12 01:20:54,029 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
2022-03-12 01:20:54,030 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2022-03-12 01:20:54,032 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2022-03-12 01:20:54,032 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
2022-03-12 01:20:54,033 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-03-12 01:20:54,035 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2022-03-12 01:20:54,037 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2022-03-12 01:20:54,041 [pool-2-thread-1] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: ConfigurationManager, init=-1: [ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|priority:0], old=null, confs=<EMPTY_MAP>
2022-03-12 01:20:54,042 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
2022-03-12 01:20:54,051 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2022-03-12 01:20:54,053 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2022-03-12 01:20:54,056 [pool-2-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a does not exist. Creating ...
2022-03-12 01:20:54,085 [pool-2-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a/in_use.lock acquired by nodename 45@c810c44637f3
2022-03-12 01:20:54,093 [pool-2-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a has been successfully formatted.
2022-03-12 01:20:54,097 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
2022-03-12 01:20:54,099 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2022-03-12 01:20:54,117 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2022-03-12 01:20:54,117 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-03-12 01:20:54,126 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2022-03-12 01:20:54,215 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2022-03-12 01:20:54,223 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2022-03-12 01:20:54,224 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2022-03-12 01:20:54,228 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: new ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a
2022-03-12 01:20:54,229 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2022-03-12 01:20:54,230 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2022-03-12 01:20:54,231 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
2022-03-12 01:20:54,232 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
2022-03-12 01:20:54,233 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2022-03-12 01:20:54,236 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2022-03-12 01:20:54,236 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2022-03-12 01:20:54,237 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2022-03-12 01:20:54,247 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
2022-03-12 01:20:54,248 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2022-03-12 01:20:54,255 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2022-03-12 01:20:54,255 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2022-03-12 01:20:54,259 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2022-03-12 01:20:54,260 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
2022-03-12 01:20:54,261 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2022-03-12 01:20:54,262 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
2022-03-12 01:20:54,263 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
2022-03-12 01:20:54,265 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
2022-03-12 01:20:54,292 [main] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: start as a follower, conf=-1: [ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|priority:0], old=null
2022-03-12 01:20:54,296 [main] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2022-03-12 01:20:54,298 [main] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: start ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState
2022-03-12 01:20:54,307 [main] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B0ACDC5D2A6A,id=ed65c6cf-a845-4339-9c5e-d1ed83e00561
2022-03-12 01:20:54,317 [main] INFO server.RaftServer: ed65c6cf-a845-4339-9c5e-d1ed83e00561: start RPC server
2022-03-12 01:20:54,365 [main] INFO server.GrpcService: ed65c6cf-a845-4339-9c5e-d1ed83e00561: GrpcService started, listening on 9894
2022-03-12 01:20:54,368 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$300/0x0000000840254c40@2b62442c] INFO util.JvmPauseMonitor: JvmPauseMonitor-ed65c6cf-a845-4339-9c5e-d1ed83e00561: Started
2022-03-12 01:20:59,486 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState] INFO impl.FollowerState: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5188177245ns, electionTimeout:5183ms
2022-03-12 01:20:59,487 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: shutdown ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState
2022-03-12 01:20:59,487 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2022-03-12 01:20:59,490 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
2022-03-12 01:20:59,490 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-FollowerState] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: start ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1
2022-03-12 01:20:59,498 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO impl.LeaderElection: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|priority:0], old=null
2022-03-12 01:20:59,498 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO impl.LeaderElection: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2022-03-12 01:20:59,499 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: shutdown ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1
2022-03-12 01:20:59,499 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2022-03-12 01:20:59,499 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: change Leader from null to ed65c6cf-a845-4339-9c5e-d1ed83e00561 at term 1 for becomeLeader, leader elected after 5402ms
2022-03-12 01:20:59,506 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2022-03-12 01:20:59,511 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2022-03-12 01:20:59,512 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2022-03-12 01:20:59,518 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2022-03-12 01:20:59,518 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2022-03-12 01:20:59,519 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2022-03-12 01:20:59,524 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2022-03-12 01:20:59,526 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2022-03-12 01:20:59,528 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: start ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderStateImpl
2022-03-12 01:20:59,551 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker: Starting segment from index:0
2022-03-12 01:20:59,597 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderElection1] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: set configuration 0: [ed65c6cf-a845-4339-9c5e-d1ed83e00561|rpc:c810c44637f3:9894|admin:|client:|dataStream:|priority:0], old=null
2022-03-12 01:20:59,648 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a/current/log_inprogress_0
2022-03-12 01:21:00,370 [main] INFO server.RaftServer: ed65c6cf-a845-4339-9c5e-d1ed83e00561: close
2022-03-12 01:21:00,372 [main] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: shutdown
2022-03-12 01:21:00,372 [main] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-B0ACDC5D2A6A,id=ed65c6cf-a845-4339-9c5e-d1ed83e00561
2022-03-12 01:21:00,372 [main] INFO impl.RoleInfo: ed65c6cf-a845-4339-9c5e-d1ed83e00561: shutdown ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-LeaderStateImpl
2022-03-12 01:21:00,378 [main] INFO impl.PendingRequests: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-PendingRequests: sendNotLeaderResponses
2022-03-12 01:21:00,381 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-StateMachineUpdater] INFO impl.StateMachineUpdater: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-StateMachineUpdater: Took a snapshot at index 0
2022-03-12 01:21:00,381 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-StateMachineUpdater] INFO impl.StateMachineUpdater: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-03-12 01:21:00,382 [main] INFO impl.StateMachineUpdater: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-StateMachineUpdater: set stopIndex = 0
2022-03-12 01:21:00,387 [main] INFO server.RaftServer$Division: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A: closes. applyIndex: 0
2022-03-12 01:21:00,389 [ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-03-12 01:21:00,391 [main] INFO segmented.SegmentedRaftLogWorker: ed65c6cf-a845-4339-9c5e-d1ed83e00561@group-B0ACDC5D2A6A-SegmentedRaftLogWorker close()
2022-03-12 01:21:00,393 [main] INFO server.GrpcService: ed65c6cf-a845-4339-9c5e-d1ed83e00561: shutdown server with port 9894 now
2022-03-12 01:21:00,404 [main] INFO server.GrpcService: ed65c6cf-a845-4339-9c5e-d1ed83e00561: shutdown server with port 9894 successfully
2022-03-12 01:21:00,404 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$300/0x0000000840254c40@2b62442c] INFO util.JvmPauseMonitor: JvmPauseMonitor-ed65c6cf-a845-4339-9c5e-d1ed83e00561: Stopped
2022-03-12 01:21:00,404 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2022-03-12 01:21:00,409 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a; layoutVersion=2; scmId=ed65c6cf-a845-4339-9c5e-d1ed83e00561
2022-03-12 01:21:00,417 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at c810c44637f3/172.21.0.3
************************************************************/
+ docker-compose exec -T scm /opt/hadoop/sbin/start-ozone.sh
Starting datanodes
85f0fff5b3b5: Warning: Permanently added '85f0fff5b3b5,172.21.0.2' (ECDSA) to the list of known hosts.
85f0fff5b3b5: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
85f0fff5b3b5: WARNING: /opt/hadoop/logs does not exist. Creating.
Starting Ozone Manager nodes [om]
om: Warning: Permanently added 'om,172.21.0.4' (ECDSA) to the list of known hosts.
om: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
Starting storage container manager nodes [scm]
scm: Warning: Permanently added 'scm,172.21.0.3' (ECDSA) to the list of known hosts.
scm: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
+ sleep 10
+ docker-compose exec -T om /opt/hadoop/bin/ozone om --init
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2022-03-12 01:21:27,312 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting OzoneManager
STARTUP_MSG:   host = e787da407c02/172.21.0.4
STARTUP_MSG:   args = [--init]
STARTUP_MSG:   version = 1.3.0-SNAPSHOT
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.25.3.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0-SNAPSHOT.jar
STARTUP_MSG:   build = https://github.com/apache/ozone/7cfd846027256198afd6522ce5ccb233ee16279b ; compiled by 'runner' on 2022-03-12T00:50Z
STARTUP_MSG:   java = 11.0.14.1
************************************************************/
2022-03-12 01:21:27,320 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
2022-03-12 01:21:28,370 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2022-03-12 01:21:28,404 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.4:9862
2022-03-12 01:21:28,405 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2022-03-12 01:21:28,405 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
2022-03-12 01:21:28,411 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-5476cfe6-3e58-437f-8dd8-b0acdc5d2a6a;layoutVersion=0
2022-03-12 01:21:28,804 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down OzoneManager at e787da407c02/172.21.0.4
************************************************************/
+ docker-compose exec -T scm /opt/hadoop/sbin/start-ozone.sh
Starting datanodes
85f0fff5b3b5: Warning: Permanently added '85f0fff5b3b5,172.21.0.2' (ECDSA) to the list of known hosts.
85f0fff5b3b5: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
85f0fff5b3b5: datanode is running as process 75.  Stop it first.
Starting Ozone Manager nodes [om]
om: Warning: Permanently added 'om,172.21.0.4' (ECDSA) to the list of known hosts.
om: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
Starting storage container manager nodes [scm]
scm: Warning: Permanently added 'scm,172.21.0.3' (ECDSA) to the list of known hosts.
scm: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm: scm is running as process 514.  Stop it first.
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /usr/local/bin/dumb-init -- entrypoint.sh sudo /usr/sbin/sshd -D
    7 ?        Ss     0:00 sudo /usr/sbin/sshd -D
   13 ?        S      0:00 /usr/sbin/sshd -D
   75 ?        Sl     0:09 /usr/lib/jvm/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/etc/hadoop/dn-audit-log4j2.properties -Dorg.apache.ratis.thirdparty.io.netty.leakDetection.level=disabled -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -Dhadoop.log.dir=/opt/hadoop/logs -Dhadoop.log.file=ozone-hadoop-datanode-85f0fff5b3b5.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.ozone.HddsDatanodeService
  249 ?        Rs     0:00 ps xa
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /usr/local/bin/dumb-init -- entrypoint.sh sudo /usr/sbin/sshd -D
    8 ?        Ss     0:00 sudo /usr/sbin/sshd -D
   14 ?        S      0:00 /usr/sbin/sshd -D
  262 ?        Sl     0:07 /usr/lib/jvm/jre/bin/java -Dproc_om -Djava.net.preferIPv4Stack=true -Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dorg.apache.ratis.thirdparty.io.netty.leakDetection.level=disabled -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false -Dlog4j.configurationFile=/etc/hadoop/om-audit-log4j2.properties -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -Dhadoop.log.dir=/opt/hadoop/logs -Dhadoop.log.file=ozone-hadoop-om-e787da407c02.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.ozone.om.OzoneManagerStarter
  336 ?        Rs     0:00 ps xa
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /usr/local/bin/dumb-init -- entrypoint.sh sudo /usr/sbin/sshd -D
    7 ?        Ss     0:00 sudo /usr/sbin/sshd -D
   13 ?        S      0:00 /usr/sbin/sshd -D
  514 ?        Sl     0:11 /usr/lib/jvm/jre/bin/java -Dproc_scm -Djava.net.preferIPv4Stack=true -Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dorg.apache.ratis.thirdparty.io.netty.leakDetection.level=disabled -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false -Dlog4j.configurationFile=/etc/hadoop/scm-audit-log4j2.properties -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -Dhadoop.log.dir=/opt/hadoop/logs -Dhadoop.log.file=ozone-hadoop-scm-c810c44637f3.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter
 1299 ?        Rs     0:00 ps xa
==============================================================================
Pipeline :: Test ozone admin pipeline command                                 
==============================================================================
Create pipeline                                                       | PASS |
------------------------------------------------------------------------------
List pipelines                                                        | PASS |
------------------------------------------------------------------------------
List pipelines with explicit host                                     | PASS |
------------------------------------------------------------------------------
Deactivate pipeline                                                   | PASS |
------------------------------------------------------------------------------
Activate pipeline                                                     | PASS |
------------------------------------------------------------------------------
Close pipeline                                                        | PASS |
------------------------------------------------------------------------------
Incomplete command                                                    | PASS |
------------------------------------------------------------------------------
Pipeline :: Test ozone admin pipeline command                         | PASS |
7 tests, 7 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonescripts/result/robot-ozonescripts-ozonescripts-pipeline-scm.xml
Stopping datanodes
85f0fff5b3b5: Warning: Permanently added '85f0fff5b3b5,172.21.0.2' (ECDSA) to the list of known hosts.
85f0fff5b3b5: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
Stopping Ozone Manager nodes [om]
om: Warning: Permanently added 'om,172.21.0.4' (ECDSA) to the list of known hosts.
om: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
Stopping storage container manager nodes [scm]
scm: Warning: Permanently added 'scm,172.21.0.3' (ECDSA) to the list of known hosts.
scm: No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ozonescripts_om_1       ... 
Stopping ozonescripts_scm_1      ... 
Stopping ozonescripts_datanode_1 ... 
Stopping ozonescripts_scm_1      ... done
Stopping ozonescripts_datanode_1 ... done
Stopping ozonescripts_om_1       ... done
Removing ozonescripts_om_1       ... 
Removing ozonescripts_scm_1      ... 
Removing ozonescripts_datanode_1 ... 
Removing ozonescripts_scm_1      ... done
Removing ozonescripts_datanode_1 ... done
Removing ozonescripts_om_1       ... done
Removing network ozonescripts_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozonescripts/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/ozonescripts/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozonescripts.xml
removed 'ozonescripts/result/robot-ozonescripts-ozonescripts-pipeline-scm.xml'
removed 'ozonescripts/result/log.html'
removed 'ozonescripts/result/report.html'
renamed 'ozonescripts/result/docker-ozonescripts-ozonescripts-pipeline-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozonescripts/docker-ozonescripts-ozonescripts-pipeline-scm.log'
renamed 'ozonescripts/result/om-audit-e787da407c02.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/ozonescripts/om-audit-e787da407c02.log'
Executing test in restart
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/scm': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/om': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/s3g': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/recon': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/data': Operation not permitted
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network restart_net
Network restart_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "restart_net" with driver "bridge"
Creating restart_s3g_1 ... 
Creating restart_dn1_1 ... 
Creating restart_om_1  ... 
Creating restart_dn2_1 ... 
Creating restart_recon_1 ... 
Creating restart_scm_1   ... 
Creating restart_dn3_1   ... 
Creating restart_recon_1 ... done
Creating restart_dn1_1   ... done
Creating restart_s3g_1   ... done
Creating restart_dn3_1   ... done
Creating restart_scm_1   ... done
Creating restart_om_1    ... done
Creating restart_dn2_1   ... done
SECONDS: 46
com.google.protobuf.ServiceException: java.net.ConnectException: Call From f91115bb625e/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f91115bb625e/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f91115bb625e/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From f91115bb625e/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6d707cce-52c3-457e-9851-f426b979fe05 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6d707cce-52c3-457e-9851-f426b979fe05 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 59
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 66
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Generate :: Test freon data generation commands                               
==============================================================================
Ozone Client Key Generator                                            | PASS |
------------------------------------------------------------------------------
OM Key Generator                                                      | PASS |
------------------------------------------------------------------------------
OM Bucket Generator                                                   | PASS |
------------------------------------------------------------------------------
DN Chunk Generator                                                    | PASS |
------------------------------------------------------------------------------
Generate :: Test freon data generation commands                       | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/restart/result/robot-restart-restart-generate-scm.xml
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | PASS |
------------------------------------------------------------------------------
DN Chunk Validator                                                    | PASS |
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/restart/result/robot-restart-restart-generate-scm-1.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping restart_dn3_1   ... 
Stopping restart_scm_1   ... 
Stopping restart_dn2_1   ... 
Stopping restart_dn1_1   ... 
Stopping restart_recon_1 ... 
Stopping restart_om_1    ... 
Stopping restart_s3g_1   ... 
Stopping restart_s3g_1   ... done
Stopping restart_recon_1 ... done
Stopping restart_om_1    ... done
Stopping restart_scm_1   ... done
Stopping restart_dn1_1   ... done
Stopping restart_dn3_1   ... done
Stopping restart_dn2_1   ... done
Removing restart_dn3_1   ... 
Removing restart_scm_1   ... 
Removing restart_dn2_1   ... 
Removing restart_dn1_1   ... 
Removing restart_recon_1 ... 
Removing restart_om_1    ... 
Removing restart_s3g_1   ... 
Removing restart_om_1    ... done
Removing restart_dn2_1   ... done
Removing restart_dn1_1   ... done
Removing restart_s3g_1   ... done
Removing restart_dn3_1   ... done
Removing restart_scm_1   ... done
Removing restart_recon_1 ... done
Removing network restart_net
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network restart_net
Network restart_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "restart_net" with driver "bridge"
Creating restart_om_1 ... 
Creating restart_dn1_1 ... 
Creating restart_dn3_1 ... 
Creating restart_scm_1 ... 
Creating restart_s3g_1 ... 
Creating restart_dn2_1 ... 
Creating restart_recon_1 ... 
Creating restart_s3g_1   ... done
Creating restart_scm_1   ... done
Creating restart_om_1    ... done
Creating restart_dn2_1   ... done
Creating restart_dn3_1   ... done
Creating restart_dn1_1   ... done
Creating restart_recon_1 ... done
SECONDS: 40
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7b8f107e2556/10.9.0.17 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6d707cce-52c3-457e-9851-f426b979fe05 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6d707cce-52c3-457e-9851-f426b979fe05 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/10.9.0.17:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=2) >= threshold (=2)
SECONDS: 48
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | PASS |
------------------------------------------------------------------------------
DN Chunk Validator                                                    | PASS |
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/restart/result/robot-restart-restart-generate-scm-2.xml
==============================================================================
Generate :: Test freon data generation commands                               
==============================================================================
Ozone Client Key Generator                                            | PASS |
------------------------------------------------------------------------------
OM Key Generator                                                      | PASS |
------------------------------------------------------------------------------
OM Bucket Generator                                                   | PASS |
------------------------------------------------------------------------------
DN Chunk Generator                                                    | PASS |
------------------------------------------------------------------------------
Generate :: Test freon data generation commands                       | PASS |
4 tests, 4 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/restart/result/robot-restart-restart-generate-scm-3.xml
==============================================================================
Validate :: Test freon data validation commands                               
==============================================================================
Ozone Client Key Validator                                            | PASS |
------------------------------------------------------------------------------
DN Chunk Validator                                                    | PASS |
------------------------------------------------------------------------------
Validate :: Test freon data validation commands                       | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/restart/result/robot-restart-restart-generate-scm-4.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping restart_scm_1   ... 
Stopping restart_s3g_1   ... 
Stopping restart_dn2_1   ... 
Stopping restart_recon_1 ... 
Stopping restart_om_1    ... 
Stopping restart_dn3_1   ... 
Stopping restart_dn1_1   ... 
Stopping restart_s3g_1   ... done
Stopping restart_recon_1 ... done
Stopping restart_om_1    ... done
Stopping restart_scm_1   ... done
Stopping restart_dn1_1   ... done
Stopping restart_dn3_1   ... done
Stopping restart_dn2_1   ... done
Removing restart_scm_1   ... 
Removing restart_s3g_1   ... 
Removing restart_dn2_1   ... 
Removing restart_recon_1 ... 
Removing restart_om_1    ... 
Removing restart_dn3_1   ... 
Removing restart_dn1_1   ... 
Removing restart_dn3_1   ... done
Removing restart_dn1_1   ... done
Removing restart_s3g_1   ... done
Removing restart_dn2_1   ... done
Removing restart_scm_1   ... done
Removing restart_om_1    ... done
Removing restart_recon_1 ... done
Removing network restart_net
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/restart/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart.xml
removed 'restart/result/robot-restart-restart-generate-scm-1.xml'
removed 'restart/result/robot-restart-restart-generate-scm-2.xml'
removed 'restart/result/robot-restart-restart-generate-scm-3.xml'
removed 'restart/result/robot-restart-restart-generate-scm-4.xml'
removed 'restart/result/robot-restart-restart-generate-scm.xml'
removed 'restart/result/log.html'
removed 'restart/result/report.html'
renamed 'restart/result/dn-audit-141318fc86d9.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-141318fc86d9.log'
renamed 'restart/result/dn-audit-514bc257790a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-514bc257790a.log'
renamed 'restart/result/dn-audit-53056f4adef5.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-53056f4adef5.log'
renamed 'restart/result/dn-audit-73a9a31a4e10.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-73a9a31a4e10.log'
renamed 'restart/result/dn-audit-c1316631db8e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-c1316631db8e.log'
renamed 'restart/result/dn-audit-e416af497c4a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/dn-audit-e416af497c4a.log'
renamed 'restart/result/docker-restart-restart-generate-scm.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/docker-restart-restart-generate-scm.log'
renamed 'restart/result/om-audit-2ede8b9f4c67.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/om-audit-2ede8b9f4c67.log'
renamed 'restart/result/om-audit-d6bdf213c5e6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/om-audit-d6bdf213c5e6.log'
renamed 'restart/result/scm-audit-7b8f107e2556.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/scm-audit-7b8f107e2556.log'
renamed 'restart/result/scm-audit-f91115bb625e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/restart/scm-audit-f91115bb625e.log'
Executing test in upgrade
Executing test in /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade
--- RUNNING NON-ROLLING UPGRADE TEST FROM 1.1.0 TO 1.2.0 ---
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/scm': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/om2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/s3g': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/om3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/dn1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/dn3': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/recon': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/dn2': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data/om1': Operation not permitted
chown: changing ownership of '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/data': Operation not permitted
--- SETTING UP OLD VERSION 1.1.0 ---
--- RUNNING WITH OLD VERSION 1.1.0 ---
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ha_net
Network ha_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ha_net" with driver "bridge"
Pulling om1 (apache/ozone:1.1.0)...
1.1.0: Pulling from apache/ozone
Digest: sha256:6dd13dca7a08ad072b027d30187363bc3552c63947325e871cb51199fbaef172
Status: Downloaded newer image for apache/ozone:1.1.0
Creating ha_om2_1 ... 
Creating ha_recon_1 ... 
Creating ha_scm_1   ... 
Creating ha_dn1_1   ... 
Creating ha_om3_1   ... 
Creating ha_om1_1   ... 
Creating ha_dn2_1   ... 
Creating ha_dn3_1   ... 
Creating ha_s3g_1   ... 
Creating ha_om2_1   ... done
Creating ha_dn1_1   ... done
Creating ha_om3_1   ... done
Creating ha_scm_1   ... done
Creating ha_dn3_1   ... done
Creating ha_om1_1   ... done
Creating ha_recon_1 ... done
Creating ha_s3g_1   ... done
Creating ha_dn2_1   ... done
SECONDS: 42
Retrying connect to server: scm/10.9.0.14:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/10.9.0.14:9860. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/10.9.0.14:9860. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/10.9.0.14:9860. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/10.9.0.14:9860. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=2) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 58
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 66
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 72
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om2 : LEADER (om2)
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0-1.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ha_s3g_1   ... 
Stopping ha_dn3_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_dn2_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_scm_1   ... 
Stopping ha_dn1_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_om2_1   ... 
Stopping ha_dn3_1   ... done
Stopping ha_om3_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_scm_1   ... done
Stopping ha_s3g_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_om2_1   ... done
Stopping ha_dn2_1   ... done
Removing ha_s3g_1   ... 
Removing ha_dn3_1   ... 
Removing ha_om1_1   ... 
Removing ha_dn2_1   ... 
Removing ha_om3_1   ... 
Removing ha_scm_1   ... 
Removing ha_dn1_1   ... 
Removing ha_recon_1 ... 
Removing ha_om2_1   ... 
Removing ha_om3_1   ... done
Removing ha_s3g_1   ... done
Removing ha_om1_1   ... done
Removing ha_dn3_1   ... done
Removing ha_recon_1 ... done
Removing ha_scm_1   ... done
Removing ha_dn1_1   ... done
Removing ha_dn2_1   ... done
Removing ha_om2_1   ... done
Removing network ha_net
--- RUNNING WITH NEW VERSION 1.2.0 PRE-FINALIZED ---
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ha_net
Network ha_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ha_net" with driver "bridge"
Creating ha_om3_1 ... 
Creating ha_om1_1 ... 
Creating ha_dn2_1 ... 
Creating ha_dn3_1 ... 
Creating ha_recon_1 ... 
Creating ha_scm_1   ... 
Creating ha_s3g_1   ... 
Creating ha_om2_1   ... 
Creating ha_dn1_1   ... 
Creating ha_dn2_1   ... done
Creating ha_recon_1 ... done
Creating ha_om3_1   ... done
Creating ha_scm_1   ... done
Creating ha_dn3_1   ... done
Creating ha_om2_1   ... done
Creating ha_om1_1   ... done
Creating ha_dn1_1   ... done
Creating ha_s3g_1   ... done
SECONDS: 44
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=1) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=2) >= threshold (=2)
SECONDS: 56
SCM is out of safe mode.
Safe mode is off
Could not determine or connect to OM Leader.
Waiting for OM leader for service omservice
SECONDS: 9
Found OM leader for service omservice: om3 : LEADER (om3)
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-1.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-2.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-3.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-4.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-5.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-6.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-7.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-8.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-pre-finalized-9.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ha_dn1_1   ... 
Stopping ha_s3g_1   ... 
Stopping ha_om2_1   ... 
Stopping ha_dn3_1   ... 
Stopping ha_scm_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_dn2_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_s3g_1   ... done
Stopping ha_om3_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_om2_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_scm_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_dn2_1   ... done
Removing ha_dn1_1   ... 
Removing ha_s3g_1   ... 
Removing ha_om2_1   ... 
Removing ha_dn3_1   ... 
Removing ha_scm_1   ... 
Removing ha_recon_1 ... 
Removing ha_dn2_1   ... 
Removing ha_om1_1   ... 
Removing ha_om3_1   ... 
Removing ha_scm_1   ... done
Removing ha_recon_1 ... done
Removing ha_s3g_1   ... done
Removing ha_om3_1   ... done
Removing ha_om1_1   ... done
Removing ha_dn2_1   ... done
Removing ha_dn3_1   ... done
Removing ha_om2_1   ... done
Removing ha_dn1_1   ... done
Removing network ha_net
--- RUNNING WITH OLD VERSION 1.1.0 AFTER DOWNGRADE ---
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ha_net
Network ha_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ha_net" with driver "bridge"
Creating ha_recon_1 ... 
Creating ha_dn1_1   ... 
Creating ha_om2_1   ... 
Creating ha_s3g_1   ... 
Creating ha_dn3_1   ... 
Creating ha_om1_1   ... 
Creating ha_om3_1   ... 
Creating ha_dn2_1   ... 
Creating ha_scm_1   ... 
Creating ha_recon_1 ... done
Creating ha_s3g_1   ... done
Creating ha_scm_1   ... done
Creating ha_om1_1   ... done
Creating ha_dn3_1   ... done
Creating ha_om2_1   ... done
Creating ha_om3_1   ... done
Creating ha_dn1_1   ... done
Creating ha_dn2_1   ... done
SECONDS: 42
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 57
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0-downgraded.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0-downgraded-1.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0-downgraded-2.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.1.0-downgraded-3.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ha_dn2_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_scm_1   ... 
Stopping ha_dn3_1   ... 
Stopping ha_om1_1   ... 
Stopping ha_om2_1   ... 
Stopping ha_s3g_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_dn1_1   ... 
Stopping ha_om3_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_dn2_1   ... done
Stopping ha_s3g_1   ... done
Stopping ha_scm_1   ... done
Stopping ha_om2_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn1_1   ... done
Stopping ha_recon_1 ... done
Removing ha_dn2_1   ... 
Removing ha_om3_1   ... 
Removing ha_scm_1   ... 
Removing ha_dn3_1   ... 
Removing ha_om1_1   ... 
Removing ha_om2_1   ... 
Removing ha_s3g_1   ... 
Removing ha_recon_1 ... 
Removing ha_dn1_1   ... 
Removing ha_s3g_1   ... done
Removing ha_scm_1   ... done
Removing ha_recon_1 ... done
Removing ha_dn1_1   ... done
Removing ha_om1_1   ... done
Removing ha_dn3_1   ... done
Removing ha_om3_1   ... done
Removing ha_om2_1   ... done
Removing ha_dn2_1   ... done
Removing network ha_net
--- RUNNING WITH NEW VERSION 1.2.0 FINALIZED ---
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network ha_net
Network ha_net not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "ha_net" with driver "bridge"
Creating ha_om1_1 ... 
Creating ha_recon_1 ... 
Creating ha_om3_1   ... 
Creating ha_scm_1   ... 
Creating ha_s3g_1   ... 
Creating ha_om2_1   ... 
Creating ha_dn3_1   ... 
Creating ha_dn1_1   ... 
Creating ha_dn2_1   ... 
Creating ha_om1_1   ... done
Creating ha_recon_1 ... done
Creating ha_dn1_1   ... done
Creating ha_om3_1   ... done
Creating ha_s3g_1   ... done
Creating ha_dn2_1   ... done
Creating ha_scm_1   ... done
Creating ha_dn3_1   ... done
Creating ha_om2_1   ... done
SECONDS: 50
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:false, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=2)
SECONDS: 62
SCM is out of safe mode.
Safe mode is off
Found OM leader for service omservice: om1 : LEADER (om1)
==============================================================================
Finalize :: Finalize Upgrade of OMs and SCM                                   
==============================================================================
Finalize SCM                                                          | PASS |
------------------------------------------------------------------------------
Finalize OMs                                                          | PASS |
------------------------------------------------------------------------------
Finalize :: Finalize Upgrade of OMs and SCM                           | PASS |
2 tests, 2 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-1.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-2.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-3.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-4.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-5.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-6.xml
==============================================================================
Check-Mlv :: Check Metadata layout version present in a version file.         
==============================================================================
Check MLV                                                             | PASS |
------------------------------------------------------------------------------
Check-Mlv :: Check Metadata layout version present in a version file. | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-7.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-8.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-9.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-10.xml
==============================================================================
Generate :: Generate data                                                     
==============================================================================
Create a volume, bucket and key                                       | PASS |
------------------------------------------------------------------------------
Generate :: Generate data                                             | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-11.xml
==============================================================================
Validate :: Smoketest ozone cluster startup                                   
==============================================================================
Read data from previously created key                                 | PASS |
------------------------------------------------------------------------------
Validate :: Smoketest ozone cluster startup                           | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/upgrade/result/robot-1.2.0-finalized-12.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping ha_dn3_1   ... 
Stopping ha_om2_1   ... 
Stopping ha_s3g_1   ... 
Stopping ha_dn2_1   ... 
Stopping ha_dn1_1   ... 
Stopping ha_scm_1   ... 
Stopping ha_om3_1   ... 
Stopping ha_recon_1 ... 
Stopping ha_om1_1   ... 
Stopping ha_s3g_1   ... done
Stopping ha_om3_1   ... done
Stopping ha_om1_1   ... done
Stopping ha_om2_1   ... done
Stopping ha_scm_1   ... done
Stopping ha_recon_1 ... done
Stopping ha_dn2_1   ... done
Stopping ha_dn3_1   ... done
Stopping ha_dn1_1   ... done
Removing ha_dn3_1   ... 
Removing ha_om2_1   ... 
Removing ha_s3g_1   ... 
Removing ha_dn2_1   ... 
Removing ha_dn1_1   ... 
Removing ha_scm_1   ... 
Removing ha_om3_1   ... 
Removing ha_recon_1 ... 
Removing ha_om1_1   ... 
Removing ha_om3_1   ... done
Removing ha_s3g_1   ... done
Removing ha_scm_1   ... done
Removing ha_recon_1 ... done
Removing ha_om1_1   ... done
Removing ha_om2_1   ... done
Removing ha_dn1_1   ... done
Removing ha_dn2_1   ... done
Removing ha_dn3_1   ... done
Removing network ha_net
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/1.1.0-1.2.0.xml
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0-downgraded-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0-downgraded-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0-downgraded-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0-downgraded.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.1.0.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-10.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-11.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-12.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-4.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-5.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-6.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-7.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-8.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized-9.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-finalized.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-1.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-2.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-3.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-4.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-5.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-6.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-7.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-8.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized-9.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/robot-1.2.0-pre-finalized.xml'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/log.html'
removed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/report.html'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/docker-1.1.0-downgraded.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/1.1.0-1.2.0/docker-1.1.0-downgraded.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/docker-1.1.0.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/1.1.0-1.2.0/docker-1.1.0.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/docker-1.2.0-finalized.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/1.1.0-1.2.0/docker-1.2.0-finalized.log'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/upgrades/non-rolling-upgrade/1.1.0-1.2.0/result/docker-1.2.0-pre-finalized.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/1.1.0-1.2.0/docker-1.2.0-pre-finalized.log'
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/upgrade/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/upgrade.xml
removed 'upgrade/result/1.1.0-1.2.0.xml'
removed 'upgrade/result/log.html'
removed 'upgrade/result/report.html'
renamed 'upgrade/result/1.1.0-1.2.0' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/upgrade/1.1.0-1.2.0'
Executing test in xcompat
Starting cluster with COMPOSE_FILE=new-cluster.yaml:clients.yaml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network xcompat_default
Network xcompat_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "xcompat_default" with the default driver
Pulling old_client_1_0_0 (apache/ozone:1.0.0)...
1.0.0: Pulling from apache/ozone
Digest: sha256:8d797cb22fc643a302e9bf34e81131ad5cab7256ae99813d3d8fff01263f26a9
Status: Downloaded newer image for apache/ozone:1.0.0
Pulling old_client_1_2_1 (apache/ozone:1.2.1)...
1.2.1: Pulling from apache/ozone
Digest: sha256:1a5f4e28568aaa4aaea6ed560d45b167e0dd8274660e81d746379c2aa6594b7b
Status: Downloaded newer image for apache/ozone:1.2.1
Creating xcompat_datanode_1 ... 
Creating xcompat_datanode_2 ... 
Creating xcompat_datanode_3 ... 
Creating xcompat_recon_1    ... 
Creating xcompat_new_client_1 ... 
Creating xcompat_om_1         ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_datanode_1         ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_scm_1              ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_om_1               ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_recon_1            ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_s3g_1              ... done
SECONDS: 52
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32013a709ec3/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32013a709ec3/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32013a709ec3/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: java.net.ConnectException: Call From 32013a709ec3/172.22.0.9 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6c19565f-2fb9-4177-9d85-74265376fe74 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6c19565f-2fb9-4177-9d85-74265376fe74 is not the leader. Could not determine the leader node. at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109) at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246) at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169) at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:60718) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966) , while invoking $Proxy20.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.22.0.9:9860 after 6 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 67
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:true, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=1) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 75
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2022-03-12 01:40:15,308 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2022-03-12 01:40:15,528 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-03-12 01:40:15,529 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2022-03-12 01:40:15,734 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup
2022-03-12 01:40:15,813 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:16,814 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:17,482 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2022-03-12 01:40:17,667 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with hadoop as owner and Versioning false and Storage Type set to DISK and Encryption set to false 
2022-03-12 01:40:17,815 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:18,815 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:18,959 [pool-2-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2022-03-12 01:40:19,282 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2022-03-12 01:40:19,816 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:20,816 [Thread-5] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:40:21,817 [Thread-5] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2022-03-12 01:40:21,921 [shutdown-hook-0] INFO metrics: type=TIMER, name=key-create, count=1, min=3638.764044, max=3638.764044, mean=3638.764044, stddev=0.0, median=3638.764044, p75=3638.764044, p95=3638.764044, p98=3638.764044, p99=3638.764044, p999=3638.764044, mean_rate=0.24068390275128113, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2022-03-12 01:40:21,923 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Total execution time (sec): 6
2022-03-12 01:40:21,927 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Failures: 0
2022-03-12 01:40:21,927 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-1.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-write :: Write Compatibility       | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-2.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-read-1.0.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-3.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.0.0-read-1.3.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-4.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.0.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-5.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-6.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-7.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.1.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-8.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-9.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-10.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-11.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.2.1-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-12.xml
==============================================================================
xcompat-cluster-1.3.0-client-1.3.0-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.3.0-client-1.3.0-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-13.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_new_client_1       ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_recon_1            ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_2         ... done
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_om_1               ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_recon_1            ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_datanode_2         ... done
Removing xcompat_recon_1            ... done
Removing xcompat_om_1               ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_scm_1              ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_datanode_1         ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_datanode_3         ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network xcompat_default
Network xcompat_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "xcompat_default" with the default driver
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_scm_1              ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_recon_1            ... 
Creating xcompat_om_1               ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_recon_1            ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_om_1               ... done
Creating xcompat_scm_1              ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_datanode_1         ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_datanode_3         ... done
SECONDS: 29
Retrying connect to server: scm/172.23.0.4:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.23.0.4:9860. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 34
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 39
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 43
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 48
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 53
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 57
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 62
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 66
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 71
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 75
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 80
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 85
SCM is in safe mode. validated:false, DataNodeSafeModeRule, registeredDns 0 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 91
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registeredDns 3 >= requiredDns 3 validated:false, HealthyPipelineSafeModeRule, currentHealthyPipelineCount 0 >= healthyPipelineThresholdCount 1 validated:true, ContainerSafeModeRule, currentContainerThreshold 1.0 >= safeModeCutoff 0.99 validated:true, AtleastOneDatanodeReportedRule, currentReportedPipelineCount 0 >= thresholdCount 0
SECONDS: 96
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2022-03-12 01:46:33 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
2022-03-12 01:46:33 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
2022-03-12 01:46:33 INFO  MetricsSystemImpl:191 - ozone-freon metrics system started
2022-03-12 01:46:34 INFO  BaseFreonGenerator:247 - Executing test with prefix warmup
2022-03-12 01:46:34 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2022-03-12 01:46:34 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-03-12 01:46:35 INFO  RpcClient:314 - Creating Volume: vol1, with hadoop as owner.
2022-03-12 01:46:35 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2022-03-12 01:46:35 INFO  RpcClient:459 - Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2022-03-12 01:46:35 WARN  MetricsSystemImpl:151 - ozone-freon metrics system already initialized!
2022-03-12 01:46:36 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2022-03-12 01:46:37 INFO  ProgressBar:163 - Progress: 0.00 % (0 out of 1)
2022-03-12 01:46:38 INFO  ProgressBar:163 - Progress: 100.00 % (1 out of 1)
2022-03-12 01:46:38 INFO  metrics:107 - type=TIMER, name=key-create, count=1, min=2514.738781, max=2514.738781, mean=2514.738781, stddev=0.0, median=2514.738781, p75=2514.738781, p95=2514.738781, p98=2514.738781, p99=2514.738781, p999=2514.738781, mean_rate=0.31940680453070747, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2022-03-12 01:46:38 INFO  BaseFreonGenerator:75 - Total execution time (sec): 4
2022-03-12 01:46:38 INFO  BaseFreonGenerator:75 - Failures: 0
2022-03-12 01:46:38 INFO  BaseFreonGenerator:75 - Successful executions: 1
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-14.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-15.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-write :: Write Compatibility       | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-16.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-read-1.0.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-17.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.0.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.0.0-read-1.3.0 :: Read Compatibility   | PASS |
3 critical tests, 3 passed, 0 failed
3 tests total, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-18.xml
==============================================================================
xcompat-cluster-1.0.0-client-1.3.0-read-1.0.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.0.0-client-1.3.0-read-1.0.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-19.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping xcompat_new_client_1       ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_new_client_1       ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_recon_1            ... done
Removing xcompat_new_client_1       ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_om_1               ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_recon_1            ... 
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_s3g_1              ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_datanode_1         ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_recon_1            ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_om_1               ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_scm_1              ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network xcompat_default
Network xcompat_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "xcompat_default" with the default driver
Creating xcompat_scm_1 ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_om_1               ... 
Creating xcompat_recon_1            ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_om_1               ... done
Creating xcompat_scm_1              ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_recon_1            ... done
Creating xcompat_s3g_1              ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_datanode_1         ... done
SECONDS: 35
Retrying connect to server: scm/172.24.0.2:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.24.0.2:9860. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.24.0.2:9860. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.24.0.2:9860. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) Retrying connect to server: scm/172.24.0.2:9860. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 44
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 49
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 54
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 59
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 64
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 69
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 74
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 79
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 84
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 89
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 94
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 99
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 104
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 109
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 114
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 119
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 123
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 128
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 133
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 138
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 143
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 148
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 153
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 158
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 163
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 169
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 174
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2022-03-12 01:51:34,174 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2022-03-12 01:51:34,328 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-03-12 01:51:34,329 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2022-03-12 01:51:34,511 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup
2022-03-12 01:51:34,528 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:51:35,179 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2022-03-12 01:51:35,416 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2022-03-12 01:51:35,533 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:51:36,361 [pool-1-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2022-03-12 01:51:36,534 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:51:36,612 [pool-1-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2022-03-12 01:51:36,613 [pool-1-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C65EF30D08F7->580793ff-0576-4e3d-a157-631102368da0
2022-03-12 01:51:36,616 [pool-1-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
2022-03-12 01:51:37,535 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:51:38,535 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:51:39,536 [Thread-3] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2022-03-12 01:51:39,570 [Thread-2] INFO metrics: type=TIMER, name=key-create, count=1, min=3250.087635, max=3250.087635, mean=3250.087635, stddev=0.0, median=3250.087635, p75=3250.087635, p95=3250.087635, p98=3250.087635, p99=3250.087635, p999=3250.087635, mean_rate=0.24346200817578728, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2022-03-12 01:51:39,571 [Thread-2] INFO freon.BaseFreonGenerator: Total execution time (sec): 5
2022-03-12 01:51:39,571 [Thread-2] INFO freon.BaseFreonGenerator: Failures: 0
2022-03-12 01:51:39,571 [Thread-2] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-20.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-21.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-22.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-23.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.1.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.1.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-24.xml
==============================================================================
xcompat-cluster-1.1.0-client-1.3.0-read-1.1.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.1.0-client-1.3.0-read-1.1.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-25.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_recon_1            ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_new_client_1       ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_3         ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_recon_1            ... done
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_recon_1            ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_om_1               ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_scm_1              ... 
Removing xcompat_new_client_1       ... done
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_recon_1            ... done
Removing xcompat_scm_1              ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_om_1               ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_datanode_1         ... done
Removing network xcompat_default
Starting cluster with COMPOSE_FILE=old-cluster.yaml:clients.yaml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Removing network xcompat_default
Network xcompat_default not found.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Creating network "xcompat_default" with the default driver
Creating xcompat_scm_1 ... 
Creating xcompat_om_1  ... 
Creating xcompat_old_client_1_2_1_1 ... 
Creating xcompat_datanode_1         ... 
Creating xcompat_datanode_2         ... 
Creating xcompat_datanode_3         ... 
Creating xcompat_new_client_1       ... 
Creating xcompat_old_client_1_0_0_1 ... 
Creating xcompat_old_client_1_1_0_1 ... 
Creating xcompat_s3g_1              ... 
Creating xcompat_recon_1            ... 
Creating xcompat_scm_1              ... done
Creating xcompat_old_client_1_2_1_1 ... done
Creating xcompat_datanode_3         ... done
Creating xcompat_recon_1            ... done
Creating xcompat_old_client_1_0_0_1 ... done
Creating xcompat_om_1               ... done
Creating xcompat_datanode_1         ... done
Creating xcompat_new_client_1       ... done
Creating xcompat_old_client_1_1_0_1 ... done
Creating xcompat_datanode_2         ... done
Creating xcompat_s3g_1              ... done
SECONDS: 40
com.google.protobuf.ServiceException: java.net.ConnectException: Call From 5babf24ba233/172.25.0.3 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.25.0.3:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. SCM is in safe mode. validated:false, DataNodeSafeModeRule, registered datanodes (=0) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 52
SCM is in safe mode. validated:true, DataNodeSafeModeRule, registered datanodes (=3) >= required datanodes (=3) validated:false, HealthyPipelineSafeModeRule, healthy Ratis/THREE pipelines (=0) >= healthyPipelineThresholdCount (=1) validated:true, ContainerSafeModeRule, % of containers with at least one reported replica (=1.00) >= safeModeCutoff (=0.99) validated:true, AtleastOneDatanodeReportedRule, reported Ratis/THREE pipelines with at least one datanode (=0) >= threshold (=0)
SECONDS: 58
SCM is out of safe mode.
Safe mode is off
No OM HA service, no need to wait
2022-03-12 01:54:42,946 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2022-03-12 01:54:43,127 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-03-12 01:54:43,127 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started
2022-03-12 01:54:43,342 [main] INFO freon.BaseFreonGenerator: Executing test with prefix warmup
2022-03-12 01:54:43,425 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:54:44,435 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:54:44,654 [main] INFO rpc.RpcClient: Creating Volume: vol1, with hadoop as owner and space quota set to -1 bytes, counts quota set to -1
2022-03-12 01:54:44,776 [main] INFO rpc.RpcClient: Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false 
2022-03-12 01:54:45,437 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:54:45,632 [pool-2-thread-1] WARN impl.MetricsSystemImpl: ozone-freon metrics system already initialized!
2022-03-12 01:54:45,872 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2022-03-12 01:54:46,437 [Thread-3] INFO freon.ProgressBar: Progress: 0.00 % (0 out of 1)
2022-03-12 01:54:47,439 [Thread-3] INFO freon.ProgressBar: Progress: 100.00 % (1 out of 1)
2022-03-12 01:54:47,863 [shutdown-hook-0] INFO metrics: type=TIMER, name=key-create, count=1, min=2513.78824, max=2513.78824, mean=2513.78824, stddev=0.0, median=2513.78824, p75=2513.78824, p95=2513.78824, p98=2513.78824, p99=2513.78824, p999=2513.78824, mean_rate=0.32830418801030603, m1=0.0, m5=0.0, m15=0.0, rate_unit=events/second, duration_unit=milliseconds
2022-03-12 01:54:47,864 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Total execution time (sec): 4
2022-03-12 01:54:47,865 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Failures: 0
2022-03-12 01:54:47,865 [shutdown-hook-0] INFO freon.BaseFreonGenerator: Successful executions: 1
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-26.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-27.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-write :: Write Compatibility               
==============================================================================
Key Can Be Written                                                    | PASS |
------------------------------------------------------------------------------
Dir Can Be Created                                                    | PASS |
------------------------------------------------------------------------------
File Can Be Put                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-write :: Write Compatibility       | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-28.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-29.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.2.1-read-1.3.0 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.2.1-read-1.3.0 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-30.xml
==============================================================================
xcompat-cluster-1.2.1-client-1.3.0-read-1.2.1 :: Read Compatibility           
==============================================================================
Key Can Be Read                                                       | PASS |
------------------------------------------------------------------------------
Dir Can Be Listed                                                     | PASS |
------------------------------------------------------------------------------
File Can Be Get                                                       | PASS |
------------------------------------------------------------------------------
xcompat-cluster-1.2.1-client-1.3.0-read-1.2.1 :: Read Compatibility   | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/xcompat/result/robot-xcompat-xcompat-write-new_client-31.xml
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
--no-ansi option is deprecated and will be removed in future versions. Use `--ansi never` instead.
Stopping xcompat_recon_1            ... 
Stopping xcompat_s3g_1              ... 
Stopping xcompat_old_client_1_1_0_1 ... 
Stopping xcompat_new_client_1       ... 
Stopping xcompat_datanode_2         ... 
Stopping xcompat_datanode_1         ... 
Stopping xcompat_old_client_1_0_0_1 ... 
Stopping xcompat_datanode_3         ... 
Stopping xcompat_old_client_1_2_1_1 ... 
Stopping xcompat_om_1               ... 
Stopping xcompat_scm_1              ... 
Stopping xcompat_old_client_1_1_0_1 ... done
Stopping xcompat_old_client_1_2_1_1 ... done
Stopping xcompat_old_client_1_0_0_1 ... done
Stopping xcompat_new_client_1       ... done
Stopping xcompat_s3g_1              ... done
Stopping xcompat_om_1               ... done
Stopping xcompat_recon_1            ... done
Stopping xcompat_scm_1              ... done
Stopping xcompat_datanode_1         ... done
Stopping xcompat_datanode_2         ... done
Stopping xcompat_datanode_3         ... done
Removing xcompat_recon_1            ... 
Removing xcompat_s3g_1              ... 
Removing xcompat_old_client_1_1_0_1 ... 
Removing xcompat_new_client_1       ... 
Removing xcompat_datanode_2         ... 
Removing xcompat_datanode_1         ... 
Removing xcompat_old_client_1_0_0_1 ... 
Removing xcompat_datanode_3         ... 
Removing xcompat_old_client_1_2_1_1 ... 
Removing xcompat_om_1               ... 
Removing xcompat_scm_1              ... 
Removing xcompat_old_client_1_1_0_1 ... done
Removing xcompat_datanode_2         ... done
Removing xcompat_recon_1            ... done
Removing xcompat_datanode_1         ... done
Removing xcompat_om_1               ... done
Removing xcompat_old_client_1_2_1_1 ... done
Removing xcompat_datanode_3         ... done
Removing xcompat_s3g_1              ... done
Removing xcompat_old_client_1_0_0_1 ... done
Removing xcompat_new_client_1       ... done
Removing xcompat_scm_1              ... done
Removing network xcompat_default
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/xcompat/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/xcompat/result/report.html
Output:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat.xml
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-1.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-10.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-11.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-12.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-13.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-14.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-15.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-16.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-17.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-18.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-19.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-2.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-20.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-21.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-22.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-23.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-24.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-25.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-26.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-27.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-28.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-29.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-3.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-30.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-31.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-4.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-5.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-6.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-7.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-8.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client-9.xml'
removed 'xcompat/result/robot-xcompat-xcompat-write-new_client.xml'
removed 'xcompat/result/log.html'
removed 'xcompat/result/report.html'
renamed 'xcompat/result/dn-audit-01e93ee18ae5.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-01e93ee18ae5.log'
renamed 'xcompat/result/dn-audit-0ab39aea8df6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-0ab39aea8df6.log'
renamed 'xcompat/result/dn-audit-24f318099011.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-24f318099011.log'
renamed 'xcompat/result/dn-audit-2a7eec5b4046.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-2a7eec5b4046.log'
renamed 'xcompat/result/dn-audit-36d98574f291.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-36d98574f291.log'
renamed 'xcompat/result/dn-audit-47a4e0659f5c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-47a4e0659f5c.log'
renamed 'xcompat/result/dn-audit-59862b8ec349.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-59862b8ec349.log'
renamed 'xcompat/result/dn-audit-8513f26141ee.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-8513f26141ee.log'
renamed 'xcompat/result/dn-audit-aae0ef4f4b2a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-aae0ef4f4b2a.log'
renamed 'xcompat/result/dn-audit-c6b4ebfc750c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-c6b4ebfc750c.log'
renamed 'xcompat/result/dn-audit-d1380fdf6818.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-d1380fdf6818.log'
renamed 'xcompat/result/dn-audit-eebd51c0fc85.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/dn-audit-eebd51c0fc85.log'
renamed 'xcompat/result/docker-xcompat-xcompat-write-new_client.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/docker-xcompat-xcompat-write-new_client.log'
renamed 'xcompat/result/om-audit-1e2ed828d37d.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/om-audit-1e2ed828d37d.log'
renamed 'xcompat/result/om-audit-7f3ebb455515.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/om-audit-7f3ebb455515.log'
renamed 'xcompat/result/om-audit-d691ed734eb5.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/om-audit-d691ed734eb5.log'
renamed 'xcompat/result/om-audit-e532abbe92f7.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/om-audit-e532abbe92f7.log'
renamed 'xcompat/result/scm-audit-32013a709ec3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/scm-audit-32013a709ec3.log'
renamed 'xcompat/result/scm-audit-5babf24ba233.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/scm-audit-5babf24ba233.log'
renamed 'xcompat/result/scm-audit-92dd1a22d1aa.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/scm-audit-92dd1a22d1aa.log'
renamed 'xcompat/result/scm-audit-949f9f1d8b8b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/xcompat/scm-audit-949f9f1d8b8b.log'
Log:     /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/log.html
Report:  /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-1.3.0-SNAPSHOT/compose/result/report.html
