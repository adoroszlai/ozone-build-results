<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="444.418" tests="8" errors="2" skipped="0" failures="0">
  <properties>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/classes:/home/runner/.m2/repository/org/apache/ozone/ozone-common/1.3.0-SNAPSHOT/ozone-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/grpc/grpc-netty/1.48.1/grpc-netty-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-core/1.48.1/grpc-core-1.48.1.jar:/home/runner/.m2/repository/com/google/android/annotations/4.1.1.4/annotations-4.1.1.4.jar:/home/runner/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.21/animal-sniffer-annotations-1.21.jar:/home/runner/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/home/runner/.m2/repository/io/perfmark/perfmark-api/0.25.0/perfmark-api-0.25.0.jar:/home/runner/.m2/repository/io/netty/netty-codec-http2/4.1.79.Final/netty-codec-http2-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-common/4.1.79.Final/netty-common-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-buffer/4.1.79.Final/netty-buffer-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-http/4.1.79.Final/netty-codec-http-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler-proxy/4.1.79.Final/netty-handler-proxy-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-socks/4.1.79.Final/netty-codec-socks-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-classes/2.0.53.Final/netty-tcnative-classes-2.0.53.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/home/runner/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.3.0-SNAPSHOT/hdds-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-client/1.3.0-SNAPSHOT/hdds-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-client/1.3.0-SNAPSHOT/ozone-interface-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-test-utils/1.3.0-SNAPSHOT/hdds-test-utils-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/home/runner/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/home/runner/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/runner/.m2/repository/org/checkerframework/checker-qual/3.12.0/checker-qual-3.12.0.jar:/home/runner/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/home/runner/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/home/runner/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/runner/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/runner/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.3.0-SNAPSHOT/hdds-server-scm-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-server/1.3.0-SNAPSHOT/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.1/hadoop-hdfs-3.3.1.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.3.0-SNAPSHOT/hdds-server-framework-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-server/1.3.0-SNAPSHOT/hdds-interface-server-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.1/log4j-core-2.17.1.jar:/home/runner/.m2/repository/com/lmax/disruptor/3.4.2/disruptor-3.4.2.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.43.v20210629/jetty-servlet-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-security/9.4.43.v20210629/jetty-security-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/rocksdb/rocksdbjni/6.29.5/rocksdbjni-6.29.5.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_dropwizard/0.7.0/simpleclient_dropwizard-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient/0.7.0/simpleclient-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_common/0.7.0/simpleclient_common-0.7.0.jar:/home/runner/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.4/metrics-core-3.2.4.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.3.1/hadoop-hdfs-client-3.3.1.jar:/home/runner/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.67/bcprov-jdk15on-1.67.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.3.0-SNAPSHOT/ozone-manager-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/aspectj/aspectjrt/1.9.7/aspectjrt-1.9.7.jar:/home/runner/.m2/repository/org/aspectj/aspectjweaver/1.9.7/aspectjweaver-1.9.7.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-client/1.3.0-SNAPSHOT/hdds-interface-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-storage/1.3.0-SNAPSHOT/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-intg/2.3.0/ranger-intg-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-common/2.3.0/ranger-plugins-common-2.3.0.jar:/home/runner/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-cred/2.3.0/ranger-plugins-cred-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-audit/2.3.0/ranger-plugins-audit-2.3.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-client/9.4.31.v20200723/jetty-client-9.4.31.v20200723.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore-nio/4.4.6/httpcore-nio-4.4.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpasyncclient/4.1.3/httpasyncclient-4.1.3.jar:/home/runner/.m2/repository/com/carrotsearch/hppc/0.8.0/hppc-0.8.0.jar:/home/runner/.m2/repository/org/apache/orc/orc-core/1.5.8/orc-core-1.5.8.jar:/home/runner/.m2/repository/net/java/dev/jna/jna/5.2.0/jna-5.2.0.jar:/home/runner/.m2/repository/net/java/dev/jna/jna-platform/5.2.0/jna-platform-5.2.0.jar:/home/runner/.m2/repository/com/kstruct/gethostname4j/0.0.2/gethostname4j-0.0.2.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugin-classloader/2.3.0/ranger-plugin-classloader-2.3.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-minikdc/3.3.1/hadoop-minikdc-3.3.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/runner/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-s3gateway/1.3.0-SNAPSHOT/ozone-s3gateway-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/runner/.m2/repository/org/jboss/weld/servlet/weld-servlet/2.4.7.Final/weld-servlet-2.4.7.Final.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.33/jersey-container-servlet-core-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-common/2.33/jersey-common-2.33.jar:/home/runner/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/cdi/jersey-cdi1x/2.33/jersey-cdi1x-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.33/jersey-hk2-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.33/jersey-media-jaxb-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0/hk2-api-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0/hk2-utils-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0/aopalliance-repackaged-2.5.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.13.2/jackson-dataformat-xml-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.2/jackson-core-2.13.2.jar:/home/runner/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/runner/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/runner/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.13.2/jackson-module-jaxb-annotations-2.13.2.jar:/home/runner/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.3/jakarta.xml.bind-api-2.3.3.jar:/home/runner/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/runner/.m2/repository/javax/enterprise/cdi-api/1.2/cdi-api-1.2.jar:/home/runner/.m2/repository/javax/el/javax.el-api/3.0.0/javax.el-api-3.0.0.jar:/home/runner/.m2/repository/javax/interceptor/javax.interceptor-api/1.2/javax.interceptor-api-1.2.jar:/home/runner/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/runner/.m2/repository/javax/xml/bind/jaxb-api/2.3.0/jaxb-api-2.3.0.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.0.1/jaxb-runtime-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-core/2.3.0.1/jaxb-core-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/txw2/2.3.0.1/txw2-2.3.0.1.jar:/home/runner/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.5/istack-commons-runtime-3.0.5.jar:/home/runner/.m2/repository/org/jvnet/staxex/stax-ex/1.7.8/stax-ex-1.7.8.jar:/home/runner/.m2/repository/com/sun/xml/fastinfoset/FastInfoset/1.2.13/FastInfoset-1.2.13.jar:/home/runner/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf/1.48.1/grpc-protobuf-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-api/1.48.1/grpc-api-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-context/1.48.1/grpc-context-1.48.1.jar:/home/runner/.m2/repository/com/google/api/grpc/proto-google-common-protos/2.9.0/proto-google-common-protos-2.9.0.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf-lite/1.48.1/grpc-protobuf-lite-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-stub/1.48.1/grpc-stub-1.48.1.jar:/home/runner/.m2/repository/io/netty/netty-transport/4.1.79.Final/netty-transport-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver/4.1.79.Final/netty-resolver-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-csi/1.3.0-SNAPSHOT/ozone-csi-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java-util/3.19.2/protobuf-java-util-3.19.2.jar:/home/runner/.m2/repository/com/google/code/gson/gson/2.9.0/gson-2.9.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-config/1.3.0-SNAPSHOT/hdds-config-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.79.Final/netty-transport-native-epoll-4.1.79.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.79.Final/netty-transport-classes-epoll-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.79.Final/netty-transport-native-unix-common-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-recon/1.3.0-SNAPSHOT/ozone-recon-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-reconcodegen/1.3.0-SNAPSHOT/ozone-reconcodegen-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-multibindings/4.0/guice-multibindings-4.0.jar:/home/runner/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/runner/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-assistedinject/4.0/guice-assistedinject-4.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.33/jersey-container-servlet-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/guice-bridge/2.5.0/guice-bridge-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-server/2.33/jersey-server-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-client/2.33/jersey-client-2.33.jar:/home/runner/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/home/runner/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-json-jackson/2.33/jersey-media-json-jackson-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/jersey-entity-filtering/2.33/jersey-entity-filtering-2.33.jar:/home/runner/.m2/repository/org/jooq/jooq/3.11.10/jooq-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-meta/3.11.10/jooq-meta-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-codegen/3.11.10/jooq-codegen-3.11.10.jar:/home/runner/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/runner/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/home/runner/.m2/repository/org/xerial/sqlite-jdbc/3.25.2/sqlite-jdbc-3.25.2.jar:/home/runner/.m2/repository/org/springframework/spring-jdbc/5.2.20.RELEASE/spring-jdbc-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-beans/5.2.20.RELEASE/spring-beans-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-core/5.2.20.RELEASE/spring-core-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-jcl/5.2.20.RELEASE/spring-jcl-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-tx/5.2.20.RELEASE/spring-tx-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-client/1.3.0-SNAPSHOT/ozone-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-erasurecode/1.3.0-SNAPSHOT/hdds-erasurecode-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem/1.3.0-SNAPSHOT/ozone-filesystem-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem-common/1.3.0-SNAPSHOT/ozone-filesystem-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-tools/1.3.0-SNAPSHOT/ozone-tools-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-tools/2.3.0/ratis-tools-2.3.0.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-core/1.12.261/aws-java-sdk-core-1.12.261.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/runner/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.13.2/jackson-dataformat-cbor-2.13.2.jar:/home/runner/.m2/repository/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.12.261/aws-java-sdk-s3-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.12.261/aws-java-sdk-kms-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/jmespath-java/1.12.261/jmespath-java-1.12.261.jar:/home/runner/.m2/repository/org/kohsuke/metainf-services/metainf-services/1.8/metainf-services-1.8.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-tools/1.3.0-SNAPSHOT/hdds-tools-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.3.0-SNAPSHOT/ozone-manager-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.3.0-SNAPSHOT/hdds-common-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-client/1.3.0-SNAPSHOT/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/info/picocli/picocli/4.6.1/picocli-4.6.1.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.2/jackson-annotations-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.13.2/jackson-datatype-jsr310-2.13.2.jar:/home/runner/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server/2.3.0/ratis-server-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-client/2.3.0/ratis-client-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server-api/2.3.0/ratis-server-api-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-metrics/2.3.0/ratis-metrics-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-netty/2.3.0/ratis-netty-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-grpc/2.3.0/ratis-grpc-2.3.0.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.1/log4j-api-2.17.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-pool2/2.6.0/commons-pool2-2.6.0.jar:/home/runner/.m2/repository/org/bouncycastle/bcpkix-jdk15on/1.67/bcpkix-jdk15on-1.67.jar:/home/runner/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/runner/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/runner/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/runner/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-client/1.6.0/jaeger-client-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-thrift/1.6.0/jaeger-thrift-1.6.0.jar:/home/runner/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/runner/.m2/repository/com/squareup/okhttp3/okhttp/4.9.0/okhttp-4.9.0.jar:/home/runner/.m2/repository/com/squareup/okio/okio/2.8.0/okio-2.8.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-core/1.6.0/jaeger-core-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-tracerresolver/1.6.0/jaeger-tracerresolver-1.6.0.jar:/home/runner/.m2/repository/io/opentracing/contrib/opentracing-tracerresolver/0.1.8/opentracing-tracerresolver-0.1.8.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib/1.6.21/kotlin-stdlib-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib-common/1.6.21/kotlin-stdlib-common-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/annotations/13.0/annotations-13.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar:/home/runner/.m2/repository/org/yaml/snakeyaml/1.26/snakeyaml-1.26.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-admin/1.3.0-SNAPSHOT/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/junit/junit/4.13.1/junit-4.13.1.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/home/runner/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/home/runner/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-migrationsupport/5.8.2/junit-jupiter-migrationsupport-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/home/runner/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/home/runner/.m2/repository/org/mockito/mockito-core/2.28.2/mockito-core-2.28.2.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy/1.9.10/byte-buddy-1.9.10.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.10/byte-buddy-agent-1.9.10.jar:/home/runner/.m2/repository/org/objenesis/objenesis/1.0/objenesis-1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.1/hadoop-kms-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-auth/3.3.1/hadoop-auth-3.3.1.jar:/home/runner/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/home/runner/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/runner/.m2/repository/net/minidev/json-smart/2.4.7/json-smart-2.4.7.jar:/home/runner/.m2/repository/net/minidev/accessors-smart/2.4.7/accessors-smart-2.4.7.jar:/home/runner/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar:/home/runner/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/runner/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/runner/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-server/9.4.43.v20210629/jetty-server-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-http/9.4.43.v20210629/jetty-http-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-io/9.4.43.v20210629/jetty-io-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.43.v20210629/jetty-webapp-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.43.v20210629/jetty-xml-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.1/hadoop-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/runner/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/runner/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/runner/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/home/runner/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/home/runner/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/runner/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/runner/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/runner/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar:/home/runner/.m2/repository/org/slf4j/jul-to-slf4j/1.7.30/jul-to-slf4j-1.7.30.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util/9.4.43.v20210629/jetty-util-9.4.43.v20210629.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.2.2/jackson-databind-2.13.2.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.1/hadoop-kms-3.3.1-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.3.0-SNAPSHOT/hdds-server-scm-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/home/runner/.m2/repository/io/netty/netty-codec/4.1.79.Final/netty-codec-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler/4.1.79.Final/netty-handler-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-test/1.3.0-SNAPSHOT/hdds-hadoop-dependency-test-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.1/hadoop-common-3.3.1-tests.jar:/home/runner/.m2/repository/org/assertj/assertj-core/3.12.2/assertj-core-3.12.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.1/hadoop-hdfs-3.3.1-tests.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.43.v20210629/jetty-util-ajax-9.4.43.v20210629.jar:/home/runner/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/runner/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/home/runner/.m2/repository/io/netty/netty-all/4.1.79.Final/netty-all-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-dns/4.1.79.Final/netty-codec-dns-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-haproxy/4.1.79.Final/netty-codec-haproxy-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-memcache/4.1.79.Final/netty-codec-memcache-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-mqtt/4.1.79.Final/netty-codec-mqtt-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-redis/4.1.79.Final/netty-codec-redis-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-smtp/4.1.79.Final/netty-codec-smtp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-stomp/4.1.79.Final/netty-codec-stomp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-xml/4.1.79.Final/netty-codec-xml-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns/4.1.79.Final/netty-resolver-dns-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-rxtx/4.1.79.Final/netty-transport-rxtx-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-sctp/4.1.79.Final/netty-transport-sctp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-udt/4.1.79.Final/netty-transport-udt-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-kqueue/4.1.79.Final/netty-transport-classes-kqueue-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-classes-macos/4.1.79.Final/netty-resolver-dns-classes-macos-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.79.Final/netty-transport-native-epoll-4.1.79.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.79.Final/netty-transport-native-kqueue-4.1.79.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.79.Final/netty-transport-native-kqueue-4.1.79.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.79.Final/netty-resolver-dns-native-macos-4.1.79.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.79.Final/netty-resolver-dns-native-macos-4.1.79.Final-osx-aarch_64.jar:/home/runner/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/runner/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.1/hadoop-distcp-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.3.1/hadoop-mapreduce-client-jobclient-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.3.1/hadoop-mapreduce-client-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.3.1/hadoop-yarn-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.3.1/hadoop-yarn-api-3.3.1.jar:/home/runner/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.13.2/jackson-jaxrs-json-provider-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.13.2/jackson-jaxrs-base-2.13.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.3.1/hadoop-yarn-client-3.3.1.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/jline/jline/3.9.0/jline-3.9.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.1/hadoop-mapreduce-client-core-3.3.1.jar:/home/runner/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/runner/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-annotations/3.3.1/hadoop-annotations-3.3.1.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/../lib/tools.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.1/hadoop-distcp-3.3.1-tests.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-common/2.3.0/ratis-common-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-thirdparty-misc/1.0.0/ratis-thirdparty-misc-1.0.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-proto/2.3.0/ratis-proto-2.3.0.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-annotation-processing/1.3.0-SNAPSHOT/ozone-annotation-processing-1.3.0-SNAPSHOT.jar:"/>
    <property name="java.vm.vendor" value="Temurin"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="test.build.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir"/>
    <property name="test.cache.data" value=""/>
    <property name="java.vendor.url" value="https://adoptium.net/"/>
    <property name="user.timezone" value="Etc/UTC"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="os.name" value="Linux"/>
    <property name="test.build.data" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire/surefirebooter5376812183418539045.jar /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire 2022-08-19T21-42-47_994-jvmRun1 surefire2505687791794821642tmp surefire_13906568484347893588tmp"/>
    <property name="surefire.test.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/classes:/home/runner/.m2/repository/org/apache/ozone/ozone-common/1.3.0-SNAPSHOT/ozone-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/grpc/grpc-netty/1.48.1/grpc-netty-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-core/1.48.1/grpc-core-1.48.1.jar:/home/runner/.m2/repository/com/google/android/annotations/4.1.1.4/annotations-4.1.1.4.jar:/home/runner/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.21/animal-sniffer-annotations-1.21.jar:/home/runner/.m2/repository/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar:/home/runner/.m2/repository/io/perfmark/perfmark-api/0.25.0/perfmark-api-0.25.0.jar:/home/runner/.m2/repository/io/netty/netty-codec-http2/4.1.79.Final/netty-codec-http2-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-common/4.1.79.Final/netty-common-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-buffer/4.1.79.Final/netty-buffer-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-http/4.1.79.Final/netty-codec-http-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler-proxy/4.1.79.Final/netty-handler-proxy-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-socks/4.1.79.Final/netty-codec-socks-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-classes/2.0.53.Final/netty-tcnative-classes-2.0.53.Final.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-tcnative-boringssl-static/2.0.53.Final/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/home/runner/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.3.0-SNAPSHOT/hdds-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-client/1.3.0-SNAPSHOT/hdds-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-client/1.3.0-SNAPSHOT/ozone-interface-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-test-utils/1.3.0-SNAPSHOT/hdds-test-utils-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/home/runner/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/home/runner/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/runner/.m2/repository/org/checkerframework/checker-qual/3.12.0/checker-qual-3.12.0.jar:/home/runner/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/home/runner/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/home/runner/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/runner/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/runner/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.3.0-SNAPSHOT/hdds-server-scm-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-server/1.3.0-SNAPSHOT/hdds-hadoop-dependency-server-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.1/hadoop-hdfs-3.3.1.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-framework/1.3.0-SNAPSHOT/hdds-server-framework-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-server/1.3.0-SNAPSHOT/hdds-interface-server-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.1/log4j-core-2.17.1.jar:/home/runner/.m2/repository/com/lmax/disruptor/3.4.2/disruptor-3.4.2.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-servlet/9.4.43.v20210629/jetty-servlet-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-security/9.4.43.v20210629/jetty-security-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/rocksdb/rocksdbjni/6.29.5/rocksdbjni-6.29.5.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_dropwizard/0.7.0/simpleclient_dropwizard-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient/0.7.0/simpleclient-0.7.0.jar:/home/runner/.m2/repository/io/prometheus/simpleclient_common/0.7.0/simpleclient_common-0.7.0.jar:/home/runner/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.4/metrics-core-3.2.4.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.3.1/hadoop-hdfs-client-3.3.1.jar:/home/runner/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.67/bcprov-jdk15on-1.67.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.3.0-SNAPSHOT/ozone-manager-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/aspectj/aspectjrt/1.9.7/aspectjrt-1.9.7.jar:/home/runner/.m2/repository/org/aspectj/aspectjweaver/1.9.7/aspectjweaver-1.9.7.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-client/1.3.0-SNAPSHOT/hdds-interface-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-interface-storage/1.3.0-SNAPSHOT/ozone-interface-storage-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-intg/2.3.0/ranger-intg-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-common/2.3.0/ranger-plugins-common-2.3.0.jar:/home/runner/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-cred/2.3.0/ranger-plugins-cred-2.3.0.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugins-audit/2.3.0/ranger-plugins-audit-2.3.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-client/9.4.31.v20200723/jetty-client-9.4.31.v20200723.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore-nio/4.4.6/httpcore-nio-4.4.6.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpasyncclient/4.1.3/httpasyncclient-4.1.3.jar:/home/runner/.m2/repository/com/carrotsearch/hppc/0.8.0/hppc-0.8.0.jar:/home/runner/.m2/repository/org/apache/orc/orc-core/1.5.8/orc-core-1.5.8.jar:/home/runner/.m2/repository/net/java/dev/jna/jna/5.2.0/jna-5.2.0.jar:/home/runner/.m2/repository/net/java/dev/jna/jna-platform/5.2.0/jna-platform-5.2.0.jar:/home/runner/.m2/repository/com/kstruct/gethostname4j/0.0.2/gethostname4j-0.0.2.jar:/home/runner/.m2/repository/org/apache/ranger/ranger-plugin-classloader/2.3.0/ranger-plugin-classloader-2.3.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-minikdc/3.3.1/hadoop-minikdc-3.3.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/runner/.m2/repository/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-s3gateway/1.3.0-SNAPSHOT/ozone-s3gateway-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/runner/.m2/repository/org/jboss/weld/servlet/weld-servlet/2.4.7.Final/weld-servlet-2.4.7.Final.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.33/jersey-container-servlet-core-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-common/2.33/jersey-common-2.33.jar:/home/runner/.m2/repository/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/cdi/jersey-cdi1x/2.33/jersey-cdi1x-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/inject/jersey-hk2/2.33/jersey-hk2-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.33/jersey-media-jaxb-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0/hk2-api-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0/hk2-utils-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0/aopalliance-repackaged-2.5.0.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.13.2/jackson-dataformat-xml-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.13.2/jackson-core-2.13.2.jar:/home/runner/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/runner/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/runner/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.13.2/jackson-module-jaxb-annotations-2.13.2.jar:/home/runner/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.3/jakarta.xml.bind-api-2.3.3.jar:/home/runner/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/runner/.m2/repository/javax/enterprise/cdi-api/1.2/cdi-api-1.2.jar:/home/runner/.m2/repository/javax/el/javax.el-api/3.0.0/javax.el-api-3.0.0.jar:/home/runner/.m2/repository/javax/interceptor/javax.interceptor-api/1.2/javax.interceptor-api-1.2.jar:/home/runner/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/runner/.m2/repository/javax/xml/bind/jaxb-api/2.3.0/jaxb-api-2.3.0.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-runtime/2.3.0.1/jaxb-runtime-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/jaxb-core/2.3.0.1/jaxb-core-2.3.0.1.jar:/home/runner/.m2/repository/org/glassfish/jaxb/txw2/2.3.0.1/txw2-2.3.0.1.jar:/home/runner/.m2/repository/com/sun/istack/istack-commons-runtime/3.0.5/istack-commons-runtime-3.0.5.jar:/home/runner/.m2/repository/org/jvnet/staxex/stax-ex/1.7.8/stax-ex-1.7.8.jar:/home/runner/.m2/repository/com/sun/xml/fastinfoset/FastInfoset/1.2.13/FastInfoset-1.2.13.jar:/home/runner/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf/1.48.1/grpc-protobuf-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-api/1.48.1/grpc-api-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-context/1.48.1/grpc-context-1.48.1.jar:/home/runner/.m2/repository/com/google/api/grpc/proto-google-common-protos/2.9.0/proto-google-common-protos-2.9.0.jar:/home/runner/.m2/repository/io/grpc/grpc-protobuf-lite/1.48.1/grpc-protobuf-lite-1.48.1.jar:/home/runner/.m2/repository/io/grpc/grpc-stub/1.48.1/grpc-stub-1.48.1.jar:/home/runner/.m2/repository/io/netty/netty-transport/4.1.79.Final/netty-transport-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver/4.1.79.Final/netty-resolver-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-csi/1.3.0-SNAPSHOT/ozone-csi-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/protobuf/protobuf-java-util/3.19.2/protobuf-java-util-3.19.2.jar:/home/runner/.m2/repository/com/google/code/gson/gson/2.9.0/gson-2.9.0.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-config/1.3.0-SNAPSHOT/hdds-config-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.79.Final/netty-transport-native-epoll-4.1.79.Final-linux-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.79.Final/netty-transport-classes-epoll-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.79.Final/netty-transport-native-unix-common-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-recon/1.3.0-SNAPSHOT/ozone-recon-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-reconcodegen/1.3.0-SNAPSHOT/ozone-reconcodegen-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-multibindings/4.0/guice-multibindings-4.0.jar:/home/runner/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/runner/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-assistedinject/4.0/guice-assistedinject-4.0.jar:/home/runner/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.33/jersey-container-servlet-2.33.jar:/home/runner/.m2/repository/org/glassfish/hk2/guice-bridge/2.5.0/guice-bridge-2.5.0.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-server/2.33/jersey-server-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/core/jersey-client/2.33/jersey-client-2.33.jar:/home/runner/.m2/repository/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/home/runner/.m2/repository/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/home/runner/.m2/repository/org/glassfish/jersey/media/jersey-media-json-jackson/2.33/jersey-media-json-jackson-2.33.jar:/home/runner/.m2/repository/org/glassfish/jersey/ext/jersey-entity-filtering/2.33/jersey-entity-filtering-2.33.jar:/home/runner/.m2/repository/org/jooq/jooq/3.11.10/jooq-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-meta/3.11.10/jooq-meta-3.11.10.jar:/home/runner/.m2/repository/org/jooq/jooq-codegen/3.11.10/jooq-codegen-3.11.10.jar:/home/runner/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/home/runner/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar:/home/runner/.m2/repository/org/xerial/sqlite-jdbc/3.25.2/sqlite-jdbc-3.25.2.jar:/home/runner/.m2/repository/org/springframework/spring-jdbc/5.2.20.RELEASE/spring-jdbc-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-beans/5.2.20.RELEASE/spring-beans-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-core/5.2.20.RELEASE/spring-core-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-jcl/5.2.20.RELEASE/spring-jcl-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/springframework/spring-tx/5.2.20.RELEASE/spring-tx-5.2.20.RELEASE.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-client/1.3.0-SNAPSHOT/ozone-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-erasurecode/1.3.0-SNAPSHOT/hdds-erasurecode-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem/1.3.0-SNAPSHOT/ozone-filesystem-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-filesystem-common/1.3.0-SNAPSHOT/ozone-filesystem-common-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-tools/1.3.0-SNAPSHOT/ozone-tools-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-tools/2.3.0/ratis-tools-2.3.0.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-core/1.12.261/aws-java-sdk-core-1.12.261.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/runner/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/runner/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.13.2/jackson-dataformat-cbor-2.13.2.jar:/home/runner/.m2/repository/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.12.261/aws-java-sdk-s3-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.12.261/aws-java-sdk-kms-1.12.261.jar:/home/runner/.m2/repository/com/amazonaws/jmespath-java/1.12.261/jmespath-java-1.12.261.jar:/home/runner/.m2/repository/org/kohsuke/metainf-services/metainf-services/1.8/metainf-services-1.8.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-tools/1.3.0-SNAPSHOT/hdds-tools-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-manager/1.3.0-SNAPSHOT/ozone-manager-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-common/1.3.0-SNAPSHOT/hdds-common-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-client/1.3.0-SNAPSHOT/hdds-hadoop-dependency-client-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/info/picocli/picocli/4.6.1/picocli-4.6.1.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.13.2/jackson-annotations-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.13.2/jackson-datatype-jsr310-2.13.2.jar:/home/runner/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server/2.3.0/ratis-server-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-client/2.3.0/ratis-client-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-server-api/2.3.0/ratis-server-api-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-metrics/2.3.0/ratis-metrics-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-netty/2.3.0/ratis-netty-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-grpc/2.3.0/ratis-grpc-2.3.0.jar:/home/runner/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.1/log4j-api-2.17.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-pool2/2.6.0/commons-pool2-2.6.0.jar:/home/runner/.m2/repository/org/bouncycastle/bcpkix-jdk15on/1.67/bcpkix-jdk15on-1.67.jar:/home/runner/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/runner/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/runner/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/runner/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-client/1.6.0/jaeger-client-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-thrift/1.6.0/jaeger-thrift-1.6.0.jar:/home/runner/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/runner/.m2/repository/com/squareup/okhttp3/okhttp/4.9.0/okhttp-4.9.0.jar:/home/runner/.m2/repository/com/squareup/okio/okio/2.8.0/okio-2.8.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-core/1.6.0/jaeger-core-1.6.0.jar:/home/runner/.m2/repository/io/jaegertracing/jaeger-tracerresolver/1.6.0/jaeger-tracerresolver-1.6.0.jar:/home/runner/.m2/repository/io/opentracing/contrib/opentracing-tracerresolver/0.1.8/opentracing-tracerresolver-0.1.8.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib/1.6.21/kotlin-stdlib-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/kotlin/kotlin-stdlib-common/1.6.21/kotlin-stdlib-common-1.6.21.jar:/home/runner/.m2/repository/org/jetbrains/annotations/13.0/annotations-13.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar:/home/runner/.m2/repository/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar:/home/runner/.m2/repository/org/yaml/snakeyaml/1.26/snakeyaml-1.26.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-interface-admin/1.3.0-SNAPSHOT/hdds-interface-admin-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/junit/junit/4.13.1/junit-4.13.1.jar:/home/runner/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/home/runner/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/home/runner/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-migrationsupport/5.8.2/junit-jupiter-migrationsupport-5.8.2.jar:/home/runner/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/home/runner/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/home/runner/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/home/runner/.m2/repository/org/mockito/mockito-core/2.28.2/mockito-core-2.28.2.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy/1.9.10/byte-buddy-1.9.10.jar:/home/runner/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.10/byte-buddy-agent-1.9.10.jar:/home/runner/.m2/repository/org/objenesis/objenesis/1.0/objenesis-1.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.1/hadoop-kms-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-auth/3.3.1/hadoop-auth-3.3.1.jar:/home/runner/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/home/runner/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/runner/.m2/repository/net/minidev/json-smart/2.4.7/json-smart-2.4.7.jar:/home/runner/.m2/repository/net/minidev/accessors-smart/2.4.7/accessors-smart-2.4.7.jar:/home/runner/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar:/home/runner/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar:/home/runner/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/runner/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/runner/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/runner/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-server/9.4.43.v20210629/jetty-server-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-http/9.4.43.v20210629/jetty-http-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-io/9.4.43.v20210629/jetty-io-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-webapp/9.4.43.v20210629/jetty-webapp-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-xml/9.4.43.v20210629/jetty-xml-9.4.43.v20210629.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.1/hadoop-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/runner/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/runner/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/runner/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/runner/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/runner/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/runner/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar:/home/runner/.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar:/home/runner/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/runner/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/runner/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/runner/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/runner/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/runner/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/runner/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar:/home/runner/.m2/repository/org/slf4j/jul-to-slf4j/1.7.30/jul-to-slf4j-1.7.30.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util/9.4.43.v20210629/jetty-util-9.4.43.v20210629.jar:/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.13.2.2/jackson-databind-2.13.2.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-kms/3.3.1/hadoop-kms-3.3.1-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-server-scm/1.3.0-SNAPSHOT/hdds-server-scm-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT-tests.jar:/home/runner/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/home/runner/.m2/repository/io/netty/netty-codec/4.1.79.Final/netty-codec-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-handler/4.1.79.Final/netty-handler-4.1.79.Final.jar:/home/runner/.m2/repository/org/apache/ozone/hdds-hadoop-dependency-test/1.3.0-SNAPSHOT/hdds-hadoop-dependency-test-1.3.0-SNAPSHOT.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-common/3.3.1/hadoop-common-3.3.1-tests.jar:/home/runner/.m2/repository/org/assertj/assertj-core/3.12.2/assertj-core-3.12.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.3.1/hadoop-hdfs-3.3.1-tests.jar:/home/runner/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.4.43.v20210629/jetty-util-ajax-9.4.43.v20210629.jar:/home/runner/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/runner/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/home/runner/.m2/repository/io/netty/netty-all/4.1.79.Final/netty-all-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-dns/4.1.79.Final/netty-codec-dns-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-haproxy/4.1.79.Final/netty-codec-haproxy-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-memcache/4.1.79.Final/netty-codec-memcache-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-mqtt/4.1.79.Final/netty-codec-mqtt-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-redis/4.1.79.Final/netty-codec-redis-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-smtp/4.1.79.Final/netty-codec-smtp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-stomp/4.1.79.Final/netty-codec-stomp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-codec-xml/4.1.79.Final/netty-codec-xml-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns/4.1.79.Final/netty-resolver-dns-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-rxtx/4.1.79.Final/netty-transport-rxtx-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-sctp/4.1.79.Final/netty-transport-sctp-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-udt/4.1.79.Final/netty-transport-udt-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-classes-kqueue/4.1.79.Final/netty-transport-classes-kqueue-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-classes-macos/4.1.79.Final/netty-resolver-dns-classes-macos-4.1.79.Final.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-epoll/4.1.79.Final/netty-transport-native-epoll-4.1.79.Final-linux-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.79.Final/netty-transport-native-kqueue-4.1.79.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.79.Final/netty-transport-native-kqueue-4.1.79.Final-osx-aarch_64.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.79.Final/netty-resolver-dns-native-macos-4.1.79.Final-osx-x86_64.jar:/home/runner/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.79.Final/netty-resolver-dns-native-macos-4.1.79.Final-osx-aarch_64.jar:/home/runner/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/runner/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.1/hadoop-distcp-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.3.1/hadoop-mapreduce-client-jobclient-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.3.1/hadoop-mapreduce-client-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.3.1/hadoop-yarn-common-3.3.1.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.3.1/hadoop-yarn-api-3.3.1.jar:/home/runner/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.13.2/jackson-jaxrs-json-provider-2.13.2.jar:/home/runner/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.13.2/jackson-jaxrs-base-2.13.2.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.3.1/hadoop-yarn-client-3.3.1.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar:/home/runner/.m2/repository/org/jline/jline/3.9.0/jline-3.9.0.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.1/hadoop-mapreduce-client-core-3.3.1.jar:/home/runner/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/runner/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/runner/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-annotations/3.3.1/hadoop-annotations-3.3.1.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/../lib/tools.jar:/home/runner/.m2/repository/org/apache/hadoop/hadoop-distcp/3.3.1/hadoop-distcp-3.3.1-tests.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-common/2.3.0/ratis-common-2.3.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-thirdparty-misc/1.0.0/ratis-thirdparty-misc-1.0.0.jar:/home/runner/.m2/repository/org/apache/ratis/ratis-proto/2.3.0/ratis-proto-2.3.0.jar:/home/runner/.m2/repository/org/apache/ozone/ozone-annotation-processing/1.3.0-SNAPSHOT/ozone-annotation-processing-1.3.0-SNAPSHOT.jar:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/runner"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre"/>
    <property name="java.security.krb5.conf" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/krb5.conf"/>
    <property name="basedir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test"/>
    <property name="file.separator" value="/"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="skip.installnpx" value="true"/>
    <property name="surefire.fork.timeout" value="3600"/>
    <property name="surefire.real.class.path" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/surefire/surefirebooter5376812183418539045.jar:/home/runner/.m2/repository/org/jacoco/org.jacoco.agent/0.8.5/org.jacoco.agent-0.8.5-runtime.jar"/>
    <property name="hadoop.log.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/temurin-8-jdk-amd64/jre/classes"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="skip.npx" value="true"/>
    <property name="java.runtime.version" value="1.8.0_332-b09"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="user.name" value="runner"/>
    <property name="path.separator" value=":"/>
    <property name="java.security.egd" value="file:///dev/urandom"/>
    <property name="os.version" value="5.4.0-1086-azure"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="test.build.webapps" value=""/>
    <property name="localRepository" value="/home/runner/.m2/repository"/>
    <property name="jetty.git.hash" value="526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8"/>
    <property name="java.vendor.url.bug" value="https://github.com/adoptium/adoptium-support/issues"/>
    <property name="java.io.tmpdir" value="/tmp"/>
    <property name="require.test.libhadoop" value=""/>
    <property name="surefire.rerunFailingTestsCount" value="5"/>
    <property name="java.version" value="1.8.0_332"/>
    <property name="user.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test"/>
    <property name="os.arch" value="amd64"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="test.build.classes" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="hadoop.tmp.dir" value="/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/tmp"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vendor" value="Temurin"/>
    <property name="java.vm.version" value="25.332-b09"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="java.class.version" value="52.0"/>
  </properties>
  <testcase name="testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="55.896"/>
  <testcase name="testContainerIsReplicatedWhenAllNodesGotoMaintenance" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="21.687"/>
  <testcase name="testMaintenanceEndsAutomaticallyAtTimeout" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="45.785"/>
  <testcase name="testSingleNodeWithOpenPipelineCanGotoMaintenance" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="49.004"/>
  <testcase name="testSCMHandlesRestartForMaintenanceNode" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="32.86"/>
  <testcase name="testStoppedDecommissionedNodeTakesSCMStateOnRestart" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="38.607"/>
  <testcase name="testEnteringMaintenanceNodeCompletesAfterSCMRestart" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="100.017">
    <error type="java.io.IOException"><![CDATA[java.io.IOException: Failed to obtain available cluster in time
	at org.apache.hadoop.ozone.MiniOzoneClusterProvider.provide(MiniOzoneClusterProvider.java:162)
	at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.setUp(TestDecommissionAndMaintenance.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:126)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeEachMethod(TimeoutExtension.java:76)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:506)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeBeforeEachMethodAdapter$21(ClassBasedTestDescriptor.java:491)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeEachMethods$3(TestMethodTestDescriptor.java:171)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeMethodsOrCallbacksUntilExceptionOccurs$6(TestMethodTestDescriptor.java:199)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeMethodsOrCallbacksUntilExceptionOccurs(TestMethodTestDescriptor.java:199)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeEachMethods(TestMethodTestDescriptor.java:168)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
]]></error>
    <system-out><![CDATA[2022-08-19 21:51:29,155 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:29,174 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:29,198 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:29,363 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: addNew group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0] returns group-7C0AE6DE117A:java.util.concurrent.CompletableFuture@90432af[Not completed]
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(190)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: new RaftServerImpl for group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0] with ContainerStateMachine:uninitialized
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-08-19 21:51:29,364 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2022-08-19 21:51:29,365 [pool-2033-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(107)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: ConfigurationManager, init=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null, confs=<EMPTY_MAP>
2022-08-19 21:51:29,365 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis] (custom)
2022-08-19 21:51:29,365 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2022-08-19 21:51:29,365 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2022-08-19 21:51:29,366 [pool-2033-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(135)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a does not exist. Creating ...
2022-08-19 21:51:29,368 [pool-2033-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(230)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a/in_use.lock acquired by nodename 5947@fv-az316-71
2022-08-19 21:51:29,372 [pool-2033-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(89)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a has been successfully formatted.
2022-08-19 21:51:29,372 [pool-2033-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(236)) - group-7C0AE6DE117A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2022-08-19 21:51:29,380 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedContainer(1252)) - Container #4 is over replicated. Expected replica count is 3, but found 4.
2022-08-19 21:51:29,380 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1587)) - Sending delete container command for container #4 to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:29,381 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedContainer(1252)) - Container #5 is over replicated. Expected replica count is 3, but found 4.
2022-08-19 21:51:29,381 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1587)) - Sending delete container command for container #5 to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:29,381 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleOverReplicatedContainer(1252)) - Container #6 is over replicated. Expected replica count is 3, but found 4.
2022-08-19 21:51:29,381 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendDeleteCommand(1587)) - Sending delete container command for container #6 to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:29,381 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2022-08-19 21:51:29,388 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(187)) - new cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2022-08-19 21:51:29,390 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2022-08-19 21:51:29,393 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2022-08-19 21:51:29,394 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2022-08-19 21:51:29,394 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2022-08-19 21:51:29,394 [pool-2033-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,394 [pool-2033-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2022-08-19 21:51:29,400 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2022-08-19 21:51:29,403 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2022-08-19 21:51:29,403 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2022-08-19 21:51:29,403 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2022-08-19 21:51:29,404 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2022-08-19 21:51:29,404 [pool-2033-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2022-08-19 21:51:29,404 [pool-2033-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(310)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: start as a follower, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null
2022-08-19 21:51:29,404 [pool-2033-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2022-08-19 21:51:29,404 [pool-2033-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: start cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:29,412 [pool-2033-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:29,415 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(765)) - Created group PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:29,447 [grpc-default-executor-5] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - a39fe283-ca9e-4823-bf8b-692faa51e960: addNew group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0] returns group-7C0AE6DE117A:java.util.concurrent.CompletableFuture@461ec65c[Not completed]
2022-08-19 21:51:29,448 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(190)) - a39fe283-ca9e-4823-bf8b-692faa51e960: new RaftServerImpl for group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0] with ContainerStateMachine:uninitialized
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2022-08-19 21:51:29,449 [pool-2901-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(107)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: ConfigurationManager, init=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0], old=null, confs=<EMPTY_MAP>
2022-08-19 21:51:29,450 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis] (custom)
2022-08-19 21:51:29,450 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2022-08-19 21:51:29,450 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2022-08-19 21:51:29,450 [pool-2901-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(135)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a does not exist. Creating ...
2022-08-19 21:51:29,455 [pool-2901-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(230)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a/in_use.lock acquired by nodename 5947@fv-az316-71
2022-08-19 21:51:29,461 [pool-2901-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(89)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a has been successfully formatted.
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(236)) - group-7C0AE6DE117A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2022-08-19 21:51:29,462 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,463 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(187)) - new a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2022-08-19 21:51:29,464 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2022-08-19 21:51:29,468 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2022-08-19 21:51:29,468 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2022-08-19 21:51:29,469 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2022-08-19 21:51:29,475 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2022-08-19 21:51:29,476 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2022-08-19 21:51:29,479 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(310)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: start as a follower, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0], old=null
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2022-08-19 21:51:29,480 [pool-2901-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - a39fe283-ca9e-4823-bf8b-692faa51e960: start a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:29,483 [pool-2901-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:29,548 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: addNew group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0] returns group-7C0AE6DE117A:java.util.concurrent.CompletableFuture@1f13b109[Not completed]
2022-08-19 21:51:29,549 [pool-2010-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(190)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: new RaftServerImpl for group-7C0AE6DE117A:[cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0] with ContainerStateMachine:uninitialized
2022-08-19 21:51:29,549 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2022-08-19 21:51:29,549 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2022-08-19 21:51:29,549 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2022-08-19 21:51:29,549 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(107)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: ConfigurationManager, init=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null, confs=<EMPTY_MAP>
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis] (custom)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2022-08-19 21:51:29,550 [pool-2010-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(135)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a does not exist. Creating ...
2022-08-19 21:51:29,554 [pool-2010-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(230)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a/in_use.lock acquired by nodename 5947@fv-az316-71
2022-08-19 21:51:29,566 [pool-2010-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(89)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a has been successfully formatted.
2022-08-19 21:51:29,587 [pool-2010-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(236)) - group-7C0AE6DE117A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2022-08-19 21:51:29,588 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(187)) - new bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2022-08-19 21:51:29,590 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2022-08-19 21:51:29,603 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2022-08-19 21:51:29,604 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2022-08-19 21:51:29,604 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2022-08-19 21:51:29,604 [pool-2010-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,604 [pool-2010-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:29,620 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2022-08-19 21:51:29,620 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2022-08-19 21:51:29,621 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2022-08-19 21:51:29,621 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2022-08-19 21:51:29,621 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2022-08-19 21:51:29,621 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2022-08-19 21:51:29,624 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(310)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: start as a follower, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: start bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:29,625 [pool-2010-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:29,629 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(765)) - Created group PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:29,652 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a.
2022-08-19 21:51:29,874 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS THREE PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a.
2022-08-19 21:51:30,149 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:51:30,169 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@2cb595bd{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:51:30,175 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@42a0208e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:51:30,175 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:30,175 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@5d91922f{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:51:30,176 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@43b42e58{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:30,179 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(537)) - Stopping the StorageContainerManager
2022-08-19 21:51:30,179 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1552)) - Container Balancer is not running.
2022-08-19 21:51:30,179 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1559)) - Stopping Replication Manager Service.
2022-08-19 21:51:30,179 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(239)) - Stopping Replication Monitor Thread.
2022-08-19 21:51:30,185 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1566)) - Stopping the Datanode Admin Monitor.
2022-08-19 21:51:30,185 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1573)) - Stopping Lease Manager of the command watchers
2022-08-19 21:51:30,185 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1580)) - Stopping datanode service RPC server
2022-08-19 21:51:30,185 [Mini-Cluster-Provider-Reap] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(414)) - Stopping the RPC server for DataNodes
2022-08-19 21:51:30,187 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 42355
2022-08-19 21:51:30,195 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:51:30,195 [IPC Server listener on 42355] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 42355
2022-08-19 21:51:30,233 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(870)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2022-08-19 21:51:30,233 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1588)) - Stopping block service RPC server
2022-08-19 21:51:30,233 [Mini-Cluster-Provider-Reap] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(161)) - Stopping the RPC server for Block Protocol
2022-08-19 21:51:30,237 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:run(543)) - Replication Monitor Thread is stopped
2022-08-19 21:51:30,245 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 36019
2022-08-19 21:51:30,269 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1595)) - Stopping the StorageContainerLocationProtocol RPC server
2022-08-19 21:51:30,269 [Mini-Cluster-Provider-Reap] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(179)) - Stopping the RPC server for Client Protocol
2022-08-19 21:51:30,271 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 46865
2022-08-19 21:51:30,273 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1602)) - Stopping Storage Container Manager HTTP server.
2022-08-19 21:51:30,283 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@14e83c9d{scm,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2022-08-19 21:51:30,309 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@50e0b472{HTTP/1.1, (http/1.1)}{0.0.0.0:36231}
2022-08-19 21:51:30,309 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:30,309 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@54ef9698{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2022-08-19 21:51:30,313 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:51:30,314 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@66e218d8{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:30,317 [IPC Server listener on 46865] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 46865
2022-08-19 21:51:30,319 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1608)) - Stopping SCM LayoutVersionManager Service.
2022-08-19 21:51:30,319 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1616)) - Stopping Block Manager Service.
2022-08-19 21:51:30,319 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2022-08-19 21:51:30,319 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2022-08-19 21:51:30,319 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1643)) - Stopping SCM Event Queue.
2022-08-19 21:51:30,321 [IPC Server listener on 36019] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 36019
2022-08-19 21:51:30,322 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:51:30,327 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1654)) - Stopping SCM HA services.
2022-08-19 21:51:30,327 [Mini-Cluster-Provider-Reap] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(149)) - Stopping RatisPipelineUtilsThread.
2022-08-19 21:51:30,330 [RatisPipelineUtilsThread - 0] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:run(180)) - RatisPipelineUtilsThread is interrupted.
2022-08-19 21:51:30,330 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(131)) - Stopping BackgroundPipelineScrubber Service.
2022-08-19 21:51:30,330 [BackgroundPipelineScrubberThread] WARN  BackgroundPipelineScrubber (BackgroundSCMService.java:run(115)) - BackgroundPipelineScrubber is interrupted, exit
2022-08-19 21:51:30,330 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1664)) - Stopping SCM MetadataStore.
2022-08-19 21:51:30,334 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping HddsDatanode metrics system...
2022-08-19 21:51:30,371 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2022-08-19 21:51:30,381 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:30,381 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - HddsDatanode metrics system stopped.
2022-08-19 21:51:30,897 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:30,905 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:addNew(96)) - a39fe283-ca9e-4823-bf8b-692faa51e960: addNew group-0CF05E7018BA:[a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1] returns group-0CF05E7018BA:java.util.concurrent.CompletableFuture@6f060ee1[Not completed]
2022-08-19 21:51:30,909 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:<init>(190)) - a39fe283-ca9e-4823-bf8b-692faa51e960: new RaftServerImpl for group-0CF05E7018BA:[a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1] with ContainerStateMachine:uninitialized
2022-08-19 21:51:30,909 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.min = 5s (custom)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.timeout.max = 5200ms (custom)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.sleep.time = 25ms (default)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.rpc.slowness.timeout = 300s (custom)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.sleep.deviation.threshold = 300ms (default)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServer$Division (ServerState.java:<init>(107)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: ConfigurationManager, init=-1: [a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1], old=null, confs=<EMPTY_MAP>
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.dir = [/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis] (custom)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.corruption.policy = EXCEPTION (default)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.storage.free-space.min = 0MB (=0) (default)
2022-08-19 21:51:30,910 [pool-2901-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:analyzeStorage(135)) - The storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba does not exist. Creating ...
2022-08-19 21:51:30,924 [pool-2901-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectoryImpl.java:tryLock(230)) - Lock on /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba/in_use.lock acquired by nodename 5947@fv-az316-71
2022-08-19 21:51:30,926 [pool-2901-thread-1] INFO  storage.RaftStorage (RaftStorageImpl.java:format(89)) - Storage directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba has been successfully formatted.
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(236)) - group-0CF05E7018BA: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.notification.no-leader.timeout = 300s (custom)
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.use.memory = false (default)
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.gap = 1000000 (custom)
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2022-08-19 21:51:30,927 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2022-08-19 21:51:30,928 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:30,929 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.num.max = 2 (custom)
2022-08-19 21:51:30,929 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2022-08-19 21:51:30,929 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(187)) - new a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba
2022-08-19 21:51:30,929 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.byte-limit = 4294967296 (custom)
2022-08-19 21:51:30,929 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.queue.element-limit = 1024 (custom)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.segment.size.max = 1048576 (custom)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.preallocated.size = 16384 (custom)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.force.sync.num = 128 (default)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync = true (default)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2022-08-19 21:51:30,930 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2022-08-19 21:51:30,933 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:openPipeline(367)) - Pipeline Pipeline[ Id: c852c910-65d7-459f-8576-0cf05e7018ba, Nodes: a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: DECOMMISSIONED, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a39fe283-ca9e-4823-bf8b-692faa51e960, CreationTimestamp2022-08-19T21:51:28.047Z[Etc/UTC]] moved to OPEN state
2022-08-19 21:51:30,942 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.write.buffer.size = 1048576 (custom)
2022-08-19 21:51:30,942 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.unsafe-flush.enabled = false (default)
2022-08-19 21:51:30,942 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(135)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.snapshot.retention.file.num = 5 (custom)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.purge.upto.snapshot.index = false (default)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.expirytime = 600000ms (custom)
2022-08-19 21:51:30,943 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.retrycache.statistics.expirytime = 100s (default)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.cached = true (default)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.server.size = 0 (default)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.cached = true (default)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.threadpool.client.size = 0 (default)
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:start(310)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: start as a follower, conf=-1: [a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1], old=null
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: changes role from      null to FOLLOWER at term 0 for startAsFollower
2022-08-19 21:51:30,947 [pool-2901-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - a39fe283-ca9e-4823-bf8b-692faa51e960: start a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState
2022-08-19 21:51:30,951 [pool-2901-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0CF05E7018BA,id=a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:30,952 [Command processor thread] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:addGroup(765)) - Created group PipelineID=c852c910-65d7-459f-8576-0cf05e7018ba
2022-08-19 21:51:30,952 [Command processor thread] INFO  commandhandler.CreatePipelineCommandHandler (CreatePipelineCommandHandler.java:handle(113)) - Created Pipeline RATIS ONE PipelineID=c852c910-65d7-459f-8576-0cf05e7018ba.
2022-08-19 21:51:30,959 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:30,961 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(448)) - Shutting down the Mini Ozone Cluster
2022-08-19 21:51:30,961 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(463)) - Stopping the Mini Ozone Cluster
2022-08-19 21:51:30,961 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(545)) - Stopping the OzoneManager
2022-08-19 21:51:30,961 [Mini-Cluster-Provider-Reap] INFO  om.OzoneManager (OzoneManager.java:stop(2022)) - om1[localhost:0]: Stopping Ozone Manager
2022-08-19 21:51:30,961 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 40081
2022-08-19 21:51:30,969 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 0
2022-08-19 21:51:30,989 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:51:30,989 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - om1: close
2022-08-19 21:51:30,989 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - om1@group-C5BA1605619E: shutdown
2022-08-19 21:51:30,990 [Mini-Cluster-Provider-Reap] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id="om1"
2022-08-19 21:51:30,990 [Mini-Cluster-Provider-Reap] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - om1: shutdown om1@group-C5BA1605619E-LeaderStateImpl
2022-08-19 21:51:30,990 [Mini-Cluster-Provider-Reap] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - om1@group-C5BA1605619E-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:31,000 [Mini-Cluster-Provider-Reap] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - om1@group-C5BA1605619E-StateMachineUpdater: set stopIndex = 84
2022-08-19 21:51:31,005 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:takeSnapshot(441)) - Current Snapshot Index (t:1, i:84)
2022-08-19 21:51:31,005 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - om1@group-C5BA1605619E-StateMachineUpdater: Took a snapshot at index 84
2022-08-19 21:51:31,005 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - om1@group-C5BA1605619E-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 84
2022-08-19 21:51:31,005 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(495)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2022-08-19 21:51:31,005 [om1@group-C5BA1605619E-StateMachineUpdater] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(466)) - Stopping OMDoubleBuffer flush thread
2022-08-19 21:51:31,006 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:flushTransactions(385)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit.
2022-08-19 21:51:31,011 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (ServerState.java:close(429)) - om1@group-C5BA1605619E: closes. applyIndex: 84
2022-08-19 21:51:31,011 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:31,012 [Mini-Cluster-Provider-Reap] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - om1@group-C5BA1605619E-SegmentedRaftLogWorker close()
2022-08-19 21:51:31,017 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - om1: shutdown server with port 34587 now
2022-08-19 21:51:31,033 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - om1: shutdown server with port 34587 successfully
2022-08-19 21:51:31,033 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@b0d63f0] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-om1: Stopped
2022-08-19 21:51:31,043 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerStateMachine (OzoneManagerStateMachine.java:close(495)) - StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
2022-08-19 21:51:31,043 [Mini-Cluster-Provider-Reap] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(478)) - OMDoubleBuffer flush thread is not running.
2022-08-19 21:51:31,043 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service KeyDeletingService
2022-08-19 21:51:31,044 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service DirectoryDeletingService
2022-08-19 21:51:31,044 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service OpenKeyCleanupService
2022-08-19 21:51:31,045 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@19343372{ozoneManager,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager}
2022-08-19 21:51:31,046 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@3cc1481a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:51:31,046 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:31,046 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@7210ed3a{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2022-08-19 21:51:31,046 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@589ac77b{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:31,067 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(522)) - Stopping the HddsDatanodes
2022-08-19 21:51:31,382 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:32,382 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:33,168 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7, PipelineID=d2beb680-6a79-42ce-b624-a051a4acd906]
2022-08-19 21:51:33,169 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #1 closed for pipeline=PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7
2022-08-19 21:51:33,169 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(71)) - Close container Event triggered for container : #1
2022-08-19 21:51:33,169 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #2 closed for pipeline=PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7
2022-08-19 21:51:33,169 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closeContainersForPipeline(421)) - Container #3 closed for pipeline=PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7
2022-08-19 21:51:33,170 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 11529a58-8de9-4065-9b5d-878c94c2fef7, Nodes: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:9be08fa4-db91-4a24-879a-a3d3cdb2f03f, CreationTimestamp2022-08-19T21:49:42.090Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:33,170 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: d2beb680-6a79-42ce-b624-a051a4acd906, Nodes: 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:3b149e26-2900-4c91-bac5-6c1144b851a0, CreationTimestamp2022-08-19T21:49:40.968Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:33,170 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(71)) - Close container Event triggered for container : #2
2022-08-19 21:51:33,170 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO  container.CloseContainerEventHandler (CloseContainerEventHandler.java:onMessage(71)) - Close container Event triggered for container : #3
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:33,383 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:33,636 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:33,658 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:33,669 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=dfb2c1ca-2485-4997-b293-f1b3b9fcce53, PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a]
2022-08-19 21:51:33,669 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: dfb2c1ca-2485-4997-b293-f1b3b9fcce53, Nodes: bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:bdbc92fb-0793-4258-a46b-ef02ae2b7ef2, CreationTimestamp2022-08-19T21:49:42.818Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:33,670 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 2d807aad-7a73-41c9-9883-7c0ae6de117a, Nodes: a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: DECOMMISSIONED, persistedOpStateExpiryEpochSec: 0}bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2022-08-19T21:51:28.048Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,384 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:34,386 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 3 milliseconds for processing 6 containers.
2022-08-19 21:51:34,453 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5049555304ns, electionTimeout:5040ms
2022-08-19 21:51:34,454 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:34,454 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2022-08-19 21:51:34,454 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = false (custom)
2022-08-19 21:51:34,454 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: start cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96
2022-08-19 21:51:34,462 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(310)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96 ELECTION round 0: submit vote requests at term 1 for -1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null
2022-08-19 21:51:34,477 [grpc-default-executor-0] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1152)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: receive requestVote(ELECTION, cb1bd840-ee39-43a2-b605-64ef3e0805c8, group-7C0AE6DE117A, 1, (t:0, i:0))
2022-08-19 21:51:34,477 [grpc-default-executor-0] INFO  impl.VoteContext (VoteContext.java:log(48)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FOLLOWER: reject ELECTION from cb1bd840-ee39-43a2-b605-64ef3e0805c8: our priority 1 > candidate's priority 0
2022-08-19 21:51:34,477 [grpc-default-executor-0] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:34,477 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:34,486 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - a39fe283-ca9e-4823-bf8b-692faa51e960: start a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:34,486 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState was interrupted
2022-08-19 21:51:34,505 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1152)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: receive requestVote(ELECTION, cb1bd840-ee39-43a2-b605-64ef3e0805c8, group-7C0AE6DE117A, 1, (t:0, i:0))
2022-08-19 21:51:34,505 [grpc-default-executor-4] INFO  impl.VoteContext (VoteContext.java:log(48)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FOLLOWER: accept ELECTION from cb1bd840-ee39-43a2-b605-64ef3e0805c8: our priority 0 <= candidate's priority 0
2022-08-19 21:51:34,505 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:34,505 [grpc-default-executor-4] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: shutdown bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:34,505 [grpc-default-executor-4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: start bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:34,507 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState was interrupted
2022-08-19 21:51:34,518 [grpc-default-executor-4] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1184)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A replies to ELECTION vote request: cb1bd840-ee39-43a2-b605-64ef3e0805c8<-bdbc92fb-0793-4258-a46b-ef02ae2b7ef2#0:OK-t1. Peer's state: bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A:t1, leader=null, voted=cb1bd840-ee39-43a2-b605-64ef3e0805c8, raftlog=bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLog:OPENED:c-1, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null
2022-08-19 21:51:34,566 [grpc-default-executor-0] INFO  server.RaftServer$Division (RaftServerImpl.java:requestVote(1184)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A replies to ELECTION vote request: cb1bd840-ee39-43a2-b605-64ef3e0805c8<-a39fe283-ca9e-4823-bf8b-692faa51e960#0:FAIL-t1. Peer's state: a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A:t1, leader=null, voted=null, raftlog=a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLog:OPENED:c-1, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0], old=null
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(90)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96: ELECTION REJECTED received 2 response(s) and 0 exception(s):
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 0: cb1bd840-ee39-43a2-b605-64ef3e0805c8<-a39fe283-ca9e-4823-bf8b-692faa51e960#0:FAIL-t1
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(94)) -   Response 1: cb1bd840-ee39-43a2-b605-64ef3e0805c8<-bdbc92fb-0793-4258-a46b-ef02ae2b7ef2#0:OK-t1
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96 ELECTION round 0: result REJECTED
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96
2022-08-19 21:51:34,569 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-LeaderElection96] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: start cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:35,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,386 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #1 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #2 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:35,387 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:36,026 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,026 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,027 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,028 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,031 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 1 is closed with bcsId 37.
2022-08-19 21:51:36,031 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 1 is closed with bcsId 37.
2022-08-19 21:51:36,034 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #1 to CLOSED state, datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
2022-08-19 21:51:36,076 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:36,076 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:36,079 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,079 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 1 is synced with bcsId 37.
2022-08-19 21:51:36,081 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: close
2022-08-19 21:51:36,081 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53: shutdown
2022-08-19 21:51:36,081 [Mini-Cluster-Provider-Reap] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-F1B3B9FCCE53,id=bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:36,081 [Mini-Cluster-Provider-Reap] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: shutdown bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-LeaderStateImpl
2022-08-19 21:51:36,081 [Mini-Cluster-Provider-Reap] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:36,083 [Mini-Cluster-Provider-Reap] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:36,085 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-F1B3B9FCCE53: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/dfb2c1ca-2485-4997-b293-f1b3b9fcce53/sm/snapshot.1_0
2022-08-19 21:51:36,085 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: close
2022-08-19 21:51:36,085 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7: shutdown
2022-08-19 21:51:36,085 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-878C94C2FEF7,id=3b149e26-2900-4c91-bac5-6c1144b851a0
2022-08-19 21:51:36,085 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: shutdown 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-FollowerState
2022-08-19 21:51:36,085 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater: set stopIndex = 50
2022-08-19 21:51:36,085 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-F1B3B9FCCE53: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-3/data/ratis/dfb2c1ca-2485-4997-b293-f1b3b9fcce53/sm/snapshot.1_0 took: 1 ms
2022-08-19 21:51:36,086 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:36,086 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-878C94C2FEF7: Taking a snapshot at:(t:1, i:47) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_47
2022-08-19 21:51:36,086 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:36,086 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-4] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 1 is closed with bcsId 37.
2022-08-19 21:51:36,086 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (ServerState.java:close(429)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53: closes. applyIndex: 0
2022-08-19 21:51:36,087 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:36,087 [Mini-Cluster-Provider-Reap] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-F1B3B9FCCE53-SegmentedRaftLogWorker close()
2022-08-19 21:51:36,087 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: shutdown
2022-08-19 21:51:36,087 [Mini-Cluster-Provider-Reap] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:36,087 [Mini-Cluster-Provider-Reap] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: shutdown bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:36,088 [Mini-Cluster-Provider-Reap] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-StateMachineUpdater: set stopIndex = -1
2022-08-19 21:51:36,088 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-FollowerState was interrupted
2022-08-19 21:51:36,089 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-878C94C2FEF7: Finished taking a snapshot at:(t:1, i:47) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_47 took: 3 ms
2022-08-19 21:51:36,089 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater: Took a snapshot at index 47
2022-08-19 21:51:36,089 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 47
2022-08-19 21:51:36,089 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-878C94C2FEF7: Taking a snapshot at:(t:1, i:47) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_47
2022-08-19 21:51:36,092 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (ServerState.java:close(429)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A: closes. applyIndex: -1
2022-08-19 21:51:36,092 [bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:36,093 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-878C94C2FEF7: Finished taking a snapshot at:(t:1, i:47) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_47 took: 4 ms
2022-08-19 21:51:36,093 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater: Took a snapshot at index 47
2022-08-19 21:51:36,093 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-StateMachineUpdater: snapshotIndex: updateIncreasingly 47 -> 47
2022-08-19 21:51:36,094 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7: closes. applyIndex: 47
2022-08-19 21:51:36,094 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:36,095 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-SegmentedRaftLogWorker close()
2022-08-19 21:51:36,095 [Mini-Cluster-Provider-Reap] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2@group-7C0AE6DE117A-SegmentedRaftLogWorker close()
2022-08-19 21:51:36,108 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: shutdown server with port 38117 now
2022-08-19 21:51:36,123 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(143)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5175508332ns, electionTimeout:5154ms
2022-08-19 21:51:36,123 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState
2022-08-19 21:51:36,123 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2022-08-19 21:51:36,123 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.leaderelection.pre-vote = false (custom)
2022-08-19 21:51:36,123 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-FollowerState] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - a39fe283-ca9e-4823-bf8b-692faa51e960: start a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97
2022-08-19 21:51:36,133 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,134 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,137 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-878C94C2FEF7-FollowerState was interrupted
2022-08-19 21:51:36,140 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906: shutdown
2022-08-19 21:51:36,141 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-A051A4ACD906,id=3b149e26-2900-4c91-bac5-6c1144b851a0
2022-08-19 21:51:36,141 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: shutdown 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-LeaderStateImpl
2022-08-19 21:51:36,141 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:36,142 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:36,142 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-A051A4ACD906: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/d2beb680-6a79-42ce-b624-a051a4acd906/sm/snapshot.1_0
2022-08-19 21:51:36,144 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: shutdown server with port 38117 successfully
2022-08-19 21:51:36,145 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@75c114fa] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-bdbc92fb-0793-4258-a46b-ef02ae2b7ef2: Stopped
2022-08-19 21:51:36,153 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,153 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,157 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(310)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97 ELECTION round 0: submit vote requests at term 1 for -1: [a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1], old=null
2022-08-19 21:51:36,157 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(312)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97 ELECTION round 0: result PASSED (term=1)
2022-08-19 21:51:36,158 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 2 is closed with bcsId 41.
2022-08-19 21:51:36,159 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(130)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97
2022-08-19 21:51:36,159 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServer$Division (RaftServerImpl.java:setRole(299)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2022-08-19 21:51:36,159 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:handleLeaderChangedNotification(863)) - Leader change notification received for group: group-0CF05E7018BA with new leaderId: a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:36,160 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServer$Division (ServerState.java:setLeader(287)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: change Leader from null to a39fe283-ca9e-4823-bf8b-692faa51e960 at term 1 for becomeLeader, leader elected after 5232ms
2022-08-19 21:51:36,161 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.staging.catchup.gap = 1000 (default)
2022-08-19 21:51:36,161 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2022-08-19 21:51:36,161 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout = 180s (custom)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.timeout.denomination = 1s (default)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.watch.element-limit = 65536 (default)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.element-limit = 1024 (custom)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(46)) - raft.server.write.follower.gap.ratio.max = -1.0 (default)
2022-08-19 21:51:36,162 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(139)) - a39fe283-ca9e-4823-bf8b-692faa51e960: start a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderStateImpl
2022-08-19 21:51:36,163 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(425)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker: Starting segment from index:0
2022-08-19 21:51:36,164 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(629)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker: created new log segment /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba/current/log_inprogress_0
2022-08-19 21:51:36,153 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-A051A4ACD906: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-1/data/ratis/d2beb680-6a79-42ce-b624-a051a4acd906/sm/snapshot.1_0 took: 11 ms
2022-08-19 21:51:36,213 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:36,213 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:36,214 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906: closes. applyIndex: 0
2022-08-19 21:51:36,213 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:36,188 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 2 is closed with bcsId 41.
2022-08-19 21:51:36,186 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderElection97] INFO  server.RaftServer$Division (ServerState.java:setRaftConf(393)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: set configuration 0: [a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1], old=null
2022-08-19 21:51:36,215 [3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:36,216 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 3b149e26-2900-4c91-bac5-6c1144b851a0@group-A051A4ACD906-SegmentedRaftLogWorker close()
2022-08-19 21:51:36,216 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: shutdown server with port 32961 now
2022-08-19 21:51:36,219 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 close command to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f
2022-08-19 21:51:36,219 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 close command to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f
2022-08-19 21:51:36,219 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 close command to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0
2022-08-19 21:51:36,219 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 11529a58-8de9-4065-9b5d-878c94c2fef7, Nodes: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:9be08fa4-db91-4a24-879a-a3d3cdb2f03f, CreationTimestamp2022-08-19T21:49:42.090Z[Etc/UTC]] removed.
2022-08-19 21:51:36,219 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=d2beb680-6a79-42ce-b624-a051a4acd906 close command to datanode 3b149e26-2900-4c91-bac5-6c1144b851a0
2022-08-19 21:51:36,220 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: d2beb680-6a79-42ce-b624-a051a4acd906, Nodes: 3b149e26-2900-4c91-bac5-6c1144b851a0{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=42431, RATIS=32961, RATIS_ADMIN=32961, RATIS_SERVER=32961, STANDALONE=46243], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:3b149e26-2900-4c91-bac5-6c1144b851a0, CreationTimestamp2022-08-19T21:49:40.968Z[Etc/UTC]] removed.
2022-08-19 21:51:36,220 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/3b149e26-2900-4c91-bac5-6c1144b851a0
2022-08-19 21:51:36,220 [grpc-default-executor-0] WARN  server.GrpcServerProtocolService (LogUtils.java:warn(122)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: installSnapshot onError, lastRequest: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f->3b149e26-2900-4c91-bac5-6c1144b851a0#605-t1,previous=(t:1, i:50),leaderCommit=50,initializing? true,entries: size=1, first=(t:1, i:51), METADATAENTRY(c:50): org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2022-08-19 21:51:36,222 [FixedThreadPoolWithAffinityExecutor-1-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #2 to CLOSED state, datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
2022-08-19 21:51:36,226 [grpc-default-executor-0] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
2022-08-19 21:51:36,226 [grpc-default-executor-0] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0: nextIndex: updateUnconditionally 53 -> 52
2022-08-19 21:51:36,226 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - 3b149e26-2900-4c91-bac5-6c1144b851a0: shutdown server with port 32961 successfully
2022-08-19 21:51:36,227 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@4d04e970] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-3b149e26-2900-4c91-bac5-6c1144b851a0: Stopped
2022-08-19 21:51:36,232 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,272 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 2 is synced with bcsId 41.
2022-08-19 21:51:36,235 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2022-08-19 21:51:36,273 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0: nextIndex: updateUnconditionally 53 -> 52
2022-08-19 21:51:36,279 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2022-08-19 21:51:36,279 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0: nextIndex: updateUnconditionally 53 -> 52
2022-08-19 21:51:36,290 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-5] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 2 is closed with bcsId 41.
2022-08-19 21:51:36,298 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2022-08-19 21:51:36,299 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0: nextIndex: updateUnconditionally 53 -> 52
2022-08-19 21:51:36,302 [grpc-default-executor-5] WARN  server.GrpcLogAppender (LogUtils.java:warn(122)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
2022-08-19 21:51:36,302 [grpc-default-executor-5] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0: nextIndex: updateUnconditionally 53 -> 52
2022-08-19 21:51:36,357 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 3 is synced with bcsId 45.
2022-08-19 21:51:36,358 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 3 is synced with bcsId 45.
2022-08-19 21:51:36,365 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 3 is closed with bcsId 45.
2022-08-19 21:51:36,368 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: remove    LEADER 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7:t1, leader=9be08fa4-db91-4a24-879a-a3d3cdb2f03f, voted=9be08fa4-db91-4a24-879a-a3d3cdb2f03f, raftlog=9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-SegmentedRaftLog:OPENED:c53, conf=0: [9be08fa4-db91-4a24-879a-a3d3cdb2f03f|rpc:10.1.0.12:43831|dataStream:|priority:1, 71c734fa-d1a0-4259-a661-d2e5ce85131f|rpc:10.1.0.12:43961|dataStream:|priority:0, 3b149e26-2900-4c91-bac5-6c1144b851a0|rpc:10.1.0.12:32961|dataStream:|priority:0], old=null RUNNING
2022-08-19 21:51:36,368 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7: shutdown
2022-08-19 21:51:36,368 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-878C94C2FEF7,id=9be08fa4-db91-4a24-879a-a3d3cdb2f03f
2022-08-19 21:51:36,368 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: shutdown 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-LeaderStateImpl
2022-08-19 21:51:36,369 [Command processor thread] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:36,369 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->71c734fa-d1a0-4259-a661-d2e5ce85131f-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(171)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->71c734fa-d1a0-4259-a661-d2e5ce85131f-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2022-08-19 21:51:36,369 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-GrpcLogAppender-LogAppenderDaemon] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(171)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->3b149e26-2900-4c91-bac5-6c1144b851a0-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2022-08-19 21:51:36,380 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater: set stopIndex = 53
2022-08-19 21:51:36,380 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-878C94C2FEF7: Taking a snapshot at:(t:1, i:51) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_51
2022-08-19 21:51:36,382 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-878C94C2FEF7: Finished taking a snapshot at:(t:1, i:51) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_51 took: 2 ms
2022-08-19 21:51:36,382 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater: Took a snapshot at index 51
2022-08-19 21:51:36,382 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 51
2022-08-19 21:51:36,382 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-878C94C2FEF7: Taking a snapshot at:(t:1, i:51) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_51
2022-08-19 21:51:36,383 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-878C94C2FEF7: Finished taking a snapshot at:(t:1, i:51) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_51 took: 1 ms
2022-08-19 21:51:36,383 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater: Took a snapshot at index 51
2022-08-19 21:51:36,383 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-StateMachineUpdater: snapshotIndex: updateIncreasingly 51 -> 51
2022-08-19 21:51:36,384 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7: closes. applyIndex: 51
2022-08-19 21:51:36,384 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:36,384 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7-SegmentedRaftLogWorker close()
2022-08-19 21:51:36,389 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #1 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:36,389 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #1 to datanode a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:36,389 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #2 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:36,389 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #2 to datanode a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendCloseCommand(1512)) - Sending close container command for container #3 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}.
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #5 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #5 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #6 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #6 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:36,390 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:36,390 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(404)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7
2022-08-19 21:51:36,390 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 command on datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f.
2022-08-19 21:51:36,472 [grpc-default-executor-0] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(140)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: Completed APPEND_ENTRIES, lastRequest: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f->71c734fa-d1a0-4259-a661-d2e5ce85131f#632-t1,previous=(t:1, i:52),leaderCommit=52,initializing? true,entries: size=1, first=(t:1, i:53), METADATAENTRY(c:52)
2022-08-19 21:51:36,474 [grpc-default-executor-4] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(339)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->71c734fa-d1a0-4259-a661-d2e5ce85131f-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2022-08-19 21:51:36,474 [grpc-default-executor-4] INFO  leader.FollowerInfo (FollowerInfoImpl.java:lambda$new$0(48)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-878C94C2FEF7->71c734fa-d1a0-4259-a661-d2e5ce85131f: nextIndex: updateUnconditionally 54 -> 53
2022-08-19 21:51:36,476 [FixedThreadPoolWithAffinityExecutor-8-0] INFO  container.IncrementalContainerReportHandler (AbstractContainerReportHandler.java:updateContainerState(292)) - Moving container #3 to CLOSED state, datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
2022-08-19 21:51:36,477 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 is not found
2022-08-19 21:51:36,482 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 3 is synced with bcsId 45.
2022-08-19 21:51:36,482 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:flushAndSyncDB(436)) - Container 3 is synced with bcsId 45.
2022-08-19 21:51:36,485 [ContainerOp-11529a58-8de9-4065-9b5d-878c94c2fef7-6] INFO  keyvalue.KeyValueContainer (KeyValueContainer.java:close(351)) - Container 3 is closed with bcsId 45.
2022-08-19 21:51:36,615 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:36,615 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=dfb2c1ca-2485-4997-b293-f1b3b9fcce53 close command to datanode bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: dfb2c1ca-2485-4997-b293-f1b3b9fcce53, Nodes: bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:bdbc92fb-0793-4258-a46b-ef02ae2b7ef2, CreationTimestamp2022-08-19T21:49:42.818Z[Etc/UTC]] removed.
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a close command to datanode a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a close command to datanode bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a close command to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 2d807aad-7a73-41c9-9883-7c0ae6de117a, Nodes: a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: DECOMMISSIONED, persistedOpStateExpiryEpochSec: 0}bdbc92fb-0793-4258-a46b-ef02ae2b7ef2{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=37085, RATIS=38117, RATIS_ADMIN=38117, RATIS_SERVER=38117, STANDALONE=45225], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:, CreationTimestamp2022-08-19T21:51:28.048Z[Etc/UTC]] removed.
2022-08-19 21:51:36,616 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/bdbc92fb-0793-4258-a46b-ef02ae2b7ef2
2022-08-19 21:51:37,164 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a is not found
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #3 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #3 to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #4 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #4 to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #5 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #5 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1195)) - Container #6 is under replicated. Expected replica count is 3, but found 2.
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.LegacyReplicationManager (LegacyReplicationManager.java:sendReplicateCommand(1559)) - Sending replicate container command for container #6 to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} from datanodes [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,391 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:37,427 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a is not found
2022-08-19 21:51:37,487 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 is not found
2022-08-19 21:51:37,773 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: remove  FOLLOWER 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7:t1, leader=9be08fa4-db91-4a24-879a-a3d3cdb2f03f, voted=9be08fa4-db91-4a24-879a-a3d3cdb2f03f, raftlog=71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-SegmentedRaftLog:OPENED:c52, conf=0: [9be08fa4-db91-4a24-879a-a3d3cdb2f03f|rpc:10.1.0.12:43831|dataStream:|priority:1, 71c734fa-d1a0-4259-a661-d2e5ce85131f|rpc:10.1.0.12:43961|dataStream:|priority:0, 3b149e26-2900-4c91-bac5-6c1144b851a0|rpc:10.1.0.12:32961|dataStream:|priority:0], old=null RUNNING
2022-08-19 21:51:37,773 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7: shutdown
2022-08-19 21:51:37,773 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-878C94C2FEF7,id=71c734fa-d1a0-4259-a661-d2e5ce85131f
2022-08-19 21:51:37,773 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: shutdown 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-FollowerState
2022-08-19 21:51:37,773 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater: set stopIndex = 52
2022-08-19 21:51:37,774 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-878C94C2FEF7: Taking a snapshot at:(t:1, i:52) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_52
2022-08-19 21:51:37,774 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-FollowerState was interrupted
2022-08-19 21:51:37,776 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-878C94C2FEF7: Finished taking a snapshot at:(t:1, i:52) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7/sm/snapshot.1_52 took: 1 ms
2022-08-19 21:51:37,776 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater: Took a snapshot at index 52
2022-08-19 21:51:37,776 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 52
2022-08-19 21:51:37,777 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7: closes. applyIndex: 52
2022-08-19 21:51:37,777 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:37,777 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7-SegmentedRaftLogWorker close()
2022-08-19 21:51:37,779 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(404)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-878C94C2FEF7: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/ratis/11529a58-8de9-4065-9b5d-878c94c2fef7
2022-08-19 21:51:37,779 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=11529a58-8de9-4065-9b5d-878c94c2fef7 command on datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f.
2022-08-19 21:51:37,779 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 5 from [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,789 [grpc-default-executor-4] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (5) to other datanode
2022-08-19 21:51:37,792 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 6 from [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,796 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 4 from [9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:37,838 [grpc-default-executor-5] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (6) to other datanode
2022-08-19 21:51:37,842 [grpc-default-executor-0] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (4) to other datanode
2022-08-19 21:51:37,879 [grpc-default-executor-4] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 27531 bytes for container 5
2022-08-19 21:51:37,881 [grpc-default-executor-2] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 5 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/replication/work/container-5.tar.gz
2022-08-19 21:51:37,887 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 5 is downloaded with size 27531, starting to import.
2022-08-19 21:51:37,966 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 5 is replicated successfully
2022-08-19 21:51:37,966 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 5 is replicated.
2022-08-19 21:51:37,968 [grpc-default-executor-5] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 31089 bytes for container 6
2022-08-19 21:51:37,972 [grpc-default-executor-4] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 6 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/replication/work/container-6.tar.gz
2022-08-19 21:51:37,977 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 6 is downloaded with size 31089, starting to import.
2022-08-19 21:51:38,033 [grpc-default-executor-0] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 32993 bytes for container 4
2022-08-19 21:51:38,038 [grpc-default-executor-4] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 4 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/replication/work/container-4.tar.gz
2022-08-19 21:51:38,049 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 4 is downloaded with size 32993, starting to import.
2022-08-19 21:51:38,065 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 6 is replicated successfully
2022-08-19 21:51:38,065 [ContainerReplicationThread-1] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 6 is replicated.
2022-08-19 21:51:38,097 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 4 is replicated successfully
2022-08-19 21:51:38,097 [ContainerReplicationThread-2] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 4 is replicated.
2022-08-19 21:51:38,160 [EventQueue-PipelineReportForPipelineReportHandler] INFO  pipeline.PipelineReportHandler (PipelineReportHandler.java:processPipelineReport(115)) - Reported pipeline PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a is not found
2022-08-19 21:51:38,167 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 1 from [71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:38,175 [grpc-default-executor-4] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (1) to other datanode
2022-08-19 21:51:38,189 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - a39fe283-ca9e-4823-bf8b-692faa51e960: remove  FOLLOWER a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A:t1, leader=null, voted=null, raftlog=a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLog:OPENED:c-1, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|dataStream:|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|dataStream:|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|dataStream:|priority:0], old=null RUNNING
2022-08-19 21:51:38,189 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: shutdown
2022-08-19 21:51:38,189 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:38,189 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:38,189 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-StateMachineUpdater: set stopIndex = -1
2022-08-19 21:51:38,189 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-FollowerState was interrupted
2022-08-19 21:51:38,190 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(429)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: closes. applyIndex: -1
2022-08-19 21:51:38,190 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 2 from [9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:38,207 [grpc-default-executor-4] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 18755 bytes for container 1
2022-08-19 21:51:38,208 [grpc-default-executor-0] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 1 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/replication/work/container-1.tar.gz
2022-08-19 21:51:38,212 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 1 is downloaded with size 18755, starting to import.
2022-08-19 21:51:38,265 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:38,267 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A-SegmentedRaftLogWorker close()
2022-08-19 21:51:38,273 [grpc-default-executor-4] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (2) to other datanode
2022-08-19 21:51:38,313 [grpc-default-executor-4] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 20864 bytes for container 2
2022-08-19 21:51:38,313 [grpc-default-executor-0] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 2 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/replication/work/container-2.tar.gz
2022-08-19 21:51:38,317 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(404)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-7C0AE6DE117A: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:38,318 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a command on datanode a39fe283-ca9e-4823-bf8b-692faa51e960.
2022-08-19 21:51:38,320 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:38,357 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:38,357 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:38,382 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 2 is downloaded with size 20864, starting to import.
2022-08-19 21:51:38,387 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:38,393 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:38,408 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 1 is replicated successfully
2022-08-19 21:51:38,408 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 1 is replicated.
2022-08-19 21:51:38,425 [Command processor thread] INFO  server.RaftServer (RaftServerProxy.java:remove(107)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: remove  FOLLOWER cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A:t1, leader=null, voted=cb1bd840-ee39-43a2-b605-64ef3e0805c8, raftlog=cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLog:OPENED:c-1, conf=-1: [cb1bd840-ee39-43a2-b605-64ef3e0805c8|rpc:10.1.0.12:46071|priority:0, a39fe283-ca9e-4823-bf8b-692faa51e960|rpc:10.1.0.12:36057|priority:1, bdbc92fb-0793-4258-a46b-ef02ae2b7ef2|rpc:10.1.0.12:38117|priority:0], old=null RUNNING
2022-08-19 21:51:38,425 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: shutdown
2022-08-19 21:51:38,425 [Command processor thread] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-7C0AE6DE117A,id=cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:38,425 [Command processor thread] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(110)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState
2022-08-19 21:51:38,425 [Command processor thread] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-StateMachineUpdater: set stopIndex = -1
2022-08-19 21:51:38,425 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState] INFO  impl.FollowerState (FollowerState.java:run(152)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-FollowerState was interrupted
2022-08-19 21:51:38,426 [Command processor thread] INFO  server.RaftServer$Division (ServerState.java:close(429)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: closes. applyIndex: -1
2022-08-19 21:51:38,426 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:38,426 [Command processor thread] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A-SegmentedRaftLogWorker close()
2022-08-19 21:51:38,436 [Command processor thread] INFO  server.RaftServer$Division (RaftServerImpl.java:groupRemove(404)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-7C0AE6DE117A: Succeed to remove RaftStorageDirectory Storage Directory /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/2d807aad-7a73-41c9-9883-7c0ae6de117a
2022-08-19 21:51:38,436 [Command processor thread] INFO  commandhandler.ClosePipelineCommandHandler (ClosePipelineCommandHandler.java:handle(78)) - Close Pipeline PipelineID=2d807aad-7a73-41c9-9883-7c0ae6de117a command on datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8.
2022-08-19 21:51:38,447 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 2 is replicated successfully
2022-08-19 21:51:38,447 [ContainerReplicationThread-1] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 2 is replicated.
2022-08-19 21:51:38,448 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 3 from [71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:38,464 [grpc-default-executor-0] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (3) to other datanode
2022-08-19 21:51:38,559 [grpc-default-executor-0] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 26796 bytes for container 3
2022-08-19 21:51:38,562 [grpc-default-executor-4] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 3 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/replication/work/container-3.tar.gz
2022-08-19 21:51:38,576 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 3 is downloaded with size 26796, starting to import.
2022-08-19 21:51:38,631 [ContainerReplicationThread-0] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 3 is replicated successfully
2022-08-19 21:51:38,632 [ContainerReplicationThread-0] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 3 is replicated.
2022-08-19 21:51:39,395 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:40,391 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 5 from [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:40,395 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:40,401 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(105)) - Starting replication of container 6 from [a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}]
2022-08-19 21:51:40,412 [grpc-default-executor-0] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (5) to other datanode
2022-08-19 21:51:40,415 [grpc-default-executor-4] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (6) to other datanode
2022-08-19 21:51:40,567 [grpc-default-executor-0] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 33678 bytes for container 5
2022-08-19 21:51:40,598 [grpc-default-executor-4] INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 38956 bytes for container 6
2022-08-19 21:51:40,601 [grpc-default-executor-5] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 5 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/replication/work/container-5.tar.gz
2022-08-19 21:51:40,604 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 5 is downloaded with size 33678, starting to import.
2022-08-19 21:51:40,640 [grpc-default-executor-0] INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(197)) - Container 6 is downloaded to /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/replication/work/container-6.tar.gz
2022-08-19 21:51:40,649 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(117)) - Container 6 is downloaded with size 38956, starting to import.
2022-08-19 21:51:40,756 [ContainerReplicationThread-1] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 5 is replicated successfully
2022-08-19 21:51:40,756 [ContainerReplicationThread-1] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 5 is replicated.
2022-08-19 21:51:40,790 [ContainerReplicationThread-2] INFO  replication.DownloadAndImportReplicator (DownloadAndImportReplicator.java:replicate(122)) - Container 6 is replicated successfully
2022-08-19 21:51:40,791 [ContainerReplicationThread-2] INFO  replication.ReplicationSupervisor (ReplicationSupervisor.java:run(176)) - Container 6 is replicated.
2022-08-19 21:51:41,369 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:51:41,381 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@12ddcf4c{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:51:41,384 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@18586601{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:51:41,385 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:41,385 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@335b45d3{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:51:41,385 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@2d56f034{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:41,396 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:41,406 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:51:41,422 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@61a57deb{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:51:41,423 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@7facc050{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:51:41,423 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:41,424 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@6121ded6{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:51:41,424 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@6c50ca6{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:42,396 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:43,061 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:43,068 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:43,397 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:43,548 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=c852c910-65d7-459f-8576-0cf05e7018ba]
2022-08-19 21:51:43,548 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: c852c910-65d7-459f-8576-0cf05e7018ba, Nodes: a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: DECOMMISSIONED, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a39fe283-ca9e-4823-bf8b-692faa51e960, CreationTimestamp2022-08-19T21:51:28.047Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:44,150 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=9740b06a-2e4b-4a59-813e-844ce54afb87]
2022-08-19 21:51:44,150 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 9740b06a-2e4b-4a59-813e-844ce54afb87, Nodes: 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:71c734fa-d1a0-4259-a661-d2e5ce85131f, CreationTimestamp2022-08-19T21:49:42.085Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:44,398 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:45,398 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:46,389 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:46,392 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: close
2022-08-19 21:51:46,392 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87: shutdown
2022-08-19 21:51:46,392 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-844CE54AFB87,id=71c734fa-d1a0-4259-a661-d2e5ce85131f
2022-08-19 21:51:46,392 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: shutdown 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-LeaderStateImpl
2022-08-19 21:51:46,392 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:46,395 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:46,395 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-844CE54AFB87: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/ratis/9740b06a-2e4b-4a59-813e-844ce54afb87/sm/snapshot.1_0
2022-08-19 21:51:46,399 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:46,405 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-844CE54AFB87: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-2/data/ratis/9740b06a-2e4b-4a59-813e-844ce54afb87/sm/snapshot.1_0 took: 10 ms
2022-08-19 21:51:46,405 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:46,406 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:46,414 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87: closes. applyIndex: 0
2022-08-19 21:51:46,414 [71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:46,416 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f@group-844CE54AFB87-SegmentedRaftLogWorker close()
2022-08-19 21:51:46,416 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: shutdown server with port 43961 now
2022-08-19 21:51:46,419 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - 71c734fa-d1a0-4259-a661-d2e5ce85131f: shutdown server with port 43961 successfully
2022-08-19 21:51:46,419 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@f472b5a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-71c734fa-d1a0-4259-a661-d2e5ce85131f: Stopped
2022-08-19 21:51:46,429 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:46,430 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - a39fe283-ca9e-4823-bf8b-692faa51e960: close
2022-08-19 21:51:46,430 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: shutdown
2022-08-19 21:51:46,430 [Mini-Cluster-Provider-Reap] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0CF05E7018BA,id=a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:46,430 [Mini-Cluster-Provider-Reap] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-LeaderStateImpl
2022-08-19 21:51:46,430 [Mini-Cluster-Provider-Reap] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:46,432 [Mini-Cluster-Provider-Reap] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:46,432 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-0CF05E7018BA: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba/sm/snapshot.1_0
2022-08-19 21:51:46,433 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-0CF05E7018BA: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-5/data/ratis/c852c910-65d7-459f-8576-0cf05e7018ba/sm/snapshot.1_0 took: 1 ms
2022-08-19 21:51:46,434 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:46,434 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:46,438 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (ServerState.java:close(429)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA: closes. applyIndex: 0
2022-08-19 21:51:46,438 [a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:46,439 [Mini-Cluster-Provider-Reap] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - a39fe283-ca9e-4823-bf8b-692faa51e960@group-0CF05E7018BA-SegmentedRaftLogWorker close()
2022-08-19 21:51:46,463 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:46,463 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=c852c910-65d7-459f-8576-0cf05e7018ba close command to datanode a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:46,463 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: c852c910-65d7-459f-8576-0cf05e7018ba, Nodes: a39fe283-ca9e-4823-bf8b-692faa51e960{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=46823, RATIS=36057, RATIS_ADMIN=36057, RATIS_SERVER=36057, STANDALONE=38131], networkLocation: /default-rack, certSerialId: null, persistedOpState: DECOMMISSIONED, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:a39fe283-ca9e-4823-bf8b-692faa51e960, CreationTimestamp2022-08-19T21:51:28.047Z[Etc/UTC]] removed.
2022-08-19 21:51:46,464 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/a39fe283-ca9e-4823-bf8b-692faa51e960
2022-08-19 21:51:46,465 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown server with port 36057 now
2022-08-19 21:51:46,474 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - a39fe283-ca9e-4823-bf8b-692faa51e960: shutdown server with port 36057 successfully
2022-08-19 21:51:46,474 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@18630571] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-a39fe283-ca9e-4823-bf8b-692faa51e960: Stopped
2022-08-19 21:51:47,031 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:47,035 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:47,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:47,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=9740b06a-2e4b-4a59-813e-844ce54afb87 close command to datanode 71c734fa-d1a0-4259-a661-d2e5ce85131f
2022-08-19 21:51:47,165 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 9740b06a-2e4b-4a59-813e-844ce54afb87, Nodes: 71c734fa-d1a0-4259-a661-d2e5ce85131f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=44371, RATIS=43961, RATIS_ADMIN=43961, RATIS_SERVER=43961, STANDALONE=38613], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:71c734fa-d1a0-4259-a661-d2e5ce85131f, CreationTimestamp2022-08-19T21:49:42.085Z[Etc/UTC]] removed.
2022-08-19 21:51:47,166 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/71c734fa-d1a0-4259-a661-d2e5ce85131f
2022-08-19 21:51:47,399 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:47,400 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,400 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:47,400 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,400 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:47,400 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,400 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:47,400 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,401 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:47,401 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,401 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:47,401 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:47,401 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:48,360 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:48,368 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:48,403 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:48,404 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,404 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:48,404 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,404 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:48,404 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,405 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:48,405 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,405 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:48,405 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,405 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:48,405 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:48,405 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:48,440 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:48,441 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:48,514 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:48,515 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:49,406 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:49,406 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,406 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:49,406 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,406 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:49,406 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,407 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:49,407 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,407 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:49,407 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,407 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:49,407 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:49,407 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:50,408 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:50,408 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,408 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:50,408 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,408 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:50,408 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,408 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:50,408 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,409 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:50,409 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,409 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:50,409 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:50,409 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:51,409 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:51,409 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,410 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:51,410 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,410 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:51,410 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,410 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:51,410 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,410 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:51,410 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,411 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:51,411 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:51,411 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:51,453 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:51:51,457 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:51,461 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:51,465 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@be8a43c{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:51:51,469 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@12e36ef9{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:51:51,469 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:51,469 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@6e557a9f{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:51:51,470 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@5b2de4f{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:51,559 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:51:51,637 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@32121140{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:51:51,638 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@1b881f1f{HTTP/1.1, (http/1.1)}{0.0.0.0:38793}
2022-08-19 21:51:51,638 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:51:51,685 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@32c8d67{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:51:51,688 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@28100232{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:51:52,411 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:52,411 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,412 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:52,412 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,412 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:52,413 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,413 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:52,413 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,413 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:52,413 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,413 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:52,413 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:52,413 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 2 milliseconds for processing 6 containers.
2022-08-19 21:51:53,414 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:53,414 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 1.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,414 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:53,414 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 2.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,415 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:53,415 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,415 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:53,415 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 4.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,415 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:53,415 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 5.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,415 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(192)) - Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
2022-08-19 21:51:53,415 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 6.
org.apache.hadoop.hdds.scm.exceptions.SCMException: Not enough healthy nodes to allocate container. 2  datanodes required. Found 1
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:193)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:53,416 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:53,889 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=90198c15-ad72-4d0f-a0a6-eb383b93fedc]
2022-08-19 21:51:53,890 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: 90198c15-ad72-4d0f-a0a6-eb383b93fedc, Nodes: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:9be08fa4-db91-4a24-879a-a3d3cdb2f03f, CreationTimestamp2022-08-19T21:49:39.421Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:54,416 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:54,416 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:54,416 [ReplicationMonitor] ERROR scm.SCMCommonPlacementPolicy (SCMCommonPlacementPolicy.java:chooseDatanodesInternal(183)) - No healthy node found to allocate container.
2022-08-19 21:51:54,416 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1228)) - Exception while replicating container 3.
org.apache.hadoop.hdds.scm.exceptions.SCMException: No healthy node found to allocate container.
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodesInternal(SCMCommonPlacementPolicy.java:184)
	at org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodesInternal(SCMContainerPlacementRandom.java:78)
	at org.apache.hadoop.hdds.scm.SCMCommonPlacementPolicy.chooseDatanodes(SCMCommonPlacementPolicy.java:148)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.handleUnderReplicatedContainer(LegacyReplicationManager.java:1192)
	at org.apache.hadoop.hdds.scm.container.replication.LegacyReplicationManager.processContainer(LegacyReplicationManager.java:539)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.processAll(ReplicationManager.java:274)
	at org.apache.hadoop.hdds.scm.container.replication.ReplicationManager.run(ReplicationManager.java:538)
	at java.lang.Thread.run(Thread.java:750)
2022-08-19 21:51:54,416 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:54,416 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:54,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:54,417 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:54,692 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(59)) - Datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=e5c7db09-5531-42eb-b37d-ed3980b9c224]
2022-08-19 21:51:54,692 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineManagerImpl (PipelineManagerImpl.java:closePipeline(442)) - Pipeline Pipeline[ Id: e5c7db09-5531-42eb-b37d-ed3980b9c224, Nodes: cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:cb1bd840-ee39-43a2-b605-64ef3e0805c8, CreationTimestamp2022-08-19T21:49:44.524Z[Etc/UTC]] moved to CLOSED state
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:55,417 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:56,418 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:51:56,480 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:56,481 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: close
2022-08-19 21:51:56,481 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC: shutdown
2022-08-19 21:51:56,481 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EB383B93FEDC,id=9be08fa4-db91-4a24-879a-a3d3cdb2f03f
2022-08-19 21:51:56,481 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: shutdown 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-LeaderStateImpl
2022-08-19 21:51:56,481 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:56,484 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-EB383B93FEDC: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/90198c15-ad72-4d0f-a0a6-eb383b93fedc/sm/snapshot.1_0
2022-08-19 21:51:56,485 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:56,490 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-EB383B93FEDC: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-0/data/ratis/90198c15-ad72-4d0f-a0a6-eb383b93fedc/sm/snapshot.1_0 took: 6 ms
2022-08-19 21:51:56,491 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:56,491 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:56,493 [ForkJoinPool.commonPool-worker-0] INFO  server.RaftServer$Division (ServerState.java:close(429)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC: closes. applyIndex: 0
2022-08-19 21:51:56,493 [9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:56,493 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f@group-EB383B93FEDC-SegmentedRaftLogWorker close()
2022-08-19 21:51:56,494 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: shutdown server with port 43831 now
2022-08-19 21:51:56,510 [grpc-default-executor-4] WARN  server.GrpcClientProtocolService (LogUtils.java:warn(122)) - 1-UnorderedRequestStreamObserver1: onError: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
2022-08-19 21:51:56,576 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - 9be08fa4-db91-4a24-879a-a3d3cdb2f03f: shutdown server with port 43831 successfully
2022-08-19 21:51:56,577 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@5d8944a6] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-9be08fa4-db91-4a24-879a-a3d3cdb2f03f: Stopped
2022-08-19 21:51:56,699 [Mini-Cluster-Provider-Reap] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(399)) - Attempting to stop container services.
2022-08-19 21:51:56,701 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer (RaftServerProxy.java:lambda$close$7(398)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: close
2022-08-19 21:51:56,701 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (RaftServerImpl.java:lambda$close$4(434)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224: shutdown
2022-08-19 21:51:56,703 [Mini-Cluster-Provider-Reap] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-ED3980B9C224,id=cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:56,704 [Mini-Cluster-Provider-Reap] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(93)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-LeaderStateImpl
2022-08-19 21:51:56,704 [Mini-Cluster-Provider-Reap] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(282)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-PendingRequests: sendNotLeaderResponses
2022-08-19 21:51:56,706 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(304)) - group-ED3980B9C224: Taking a snapshot at:(t:1, i:0) file /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/e5c7db09-5531-42eb-b37d-ed3980b9c224/sm/snapshot.1_0
2022-08-19 21:51:56,706 [Mini-Cluster-Provider-Reap] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(153)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater: set stopIndex = 0
2022-08-19 21:51:56,708 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:takeSnapshot(315)) - group-ED3980B9C224: Finished taking a snapshot at:(t:1, i:0) file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ab145674-15b2-43d3-adec-1c515833e2e7/datanode-4/data/ratis/e5c7db09-5531-42eb-b37d-ed3980b9c224/sm/snapshot.1_0 took: 1 ms
2022-08-19 21:51:56,708 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:takeSnapshot(287)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater: Took a snapshot at index 0
2022-08-19 21:51:56,708 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:lambda$new$0(92)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
2022-08-19 21:51:56,708 [Mini-Cluster-Provider-Reap] INFO  server.RaftServer$Division (ServerState.java:close(429)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224: closes. applyIndex: 0
2022-08-19 21:51:56,709 [cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(336)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2022-08-19 21:51:56,709 [Mini-Cluster-Provider-Reap] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(246)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8@group-ED3980B9C224-SegmentedRaftLogWorker close()
2022-08-19 21:51:56,709 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(269)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown server with port 46071 now
2022-08-19 21:51:56,769 [Mini-Cluster-Provider-Reap] INFO  server.GrpcService (GrpcService.java:closeImpl(278)) - cb1bd840-ee39-43a2-b605-64ef3e0805c8: shutdown server with port 46071 successfully
2022-08-19 21:51:56,769 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$715/942413177@1a062666] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(109)) - JvmPauseMonitor-cb1bd840-ee39-43a2-b605-64ef3e0805c8: Stopped
2022-08-19 21:51:56,896 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:56,897 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=90198c15-ad72-4d0f-a0a6-eb383b93fedc close command to datanode 9be08fa4-db91-4a24-879a-a3d3cdb2f03f
2022-08-19 21:51:56,897 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: 90198c15-ad72-4d0f-a0a6-eb383b93fedc, Nodes: 9be08fa4-db91-4a24-879a-a3d3cdb2f03f{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=35255, RATIS=43831, RATIS_ADMIN=43831, RATIS_SERVER=43831, STANDALONE=33517], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:9be08fa4-db91-4a24-879a-a3d3cdb2f03f, CreationTimestamp2022-08-19T21:49:39.421Z[Etc/UTC]] removed.
2022-08-19 21:51:56,897 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/9be08fa4-db91-4a24-879a-a3d3cdb2f03f
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:57,419 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:57,467 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:57,477 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:57,701 [EventQueue-DeadNodeForDeadNodeHandler] INFO  node.DeadNodeHandler (DeadNodeHandler.java:onMessage(81)) - A dead datanode is detected. cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
2022-08-19 21:51:57,701 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$1(244)) - Send pipeline:PipelineID=e5c7db09-5531-42eb-b37d-ed3980b9c224 close command to datanode cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:57,701 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManagerImpl (PipelineStateManagerImpl.java:removePipeline(245)) - Pipeline Pipeline[ Id: e5c7db09-5531-42eb-b37d-ed3980b9c224, Nodes: cb1bd840-ee39-43a2-b605-64ef3e0805c8{ip: 10.1.0.12, host: fv-az316-71.recyyijfajmutojoqv5pud5plf.dx.internal.cloudapp.net, ports: [REPLICATION=36021, RATIS=46071, RATIS_ADMIN=46071, RATIS_SERVER=46071, STANDALONE=38119], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:cb1bd840-ee39-43a2-b605-64ef3e0805c8, CreationTimestamp2022-08-19T21:49:44.524Z[Etc/UTC]] removed.
2022-08-19 21:51:57,702 [EventQueue-DeadNodeForDeadNodeHandler] INFO  net.NetworkTopologyImpl (NetworkTopologyImpl.java:remove(190)) - Removed a node: /default-rack/cb1bd840-ee39-43a2-b605-64ef3e0805c8
2022-08-19 21:51:58,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:58,419 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:58,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:58,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:58,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:58,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:58,420 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:51:58,605 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:58,610 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:58,825 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service BlockDeletingService
2022-08-19 21:51:58,829 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service StaleRecoveringContainerScrubbingService
2022-08-19 21:51:59,155 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:51:59,175 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:51:59,420 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:51:59,421 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:52:00,421 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 0 milliseconds for processing 6 containers.
2022-08-19 21:52:00,897 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:00,959 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #1, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #2, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #3, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #4, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #5, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] WARN  replication.LegacyReplicationManager (LegacyReplicationManager.java:handleUnderReplicatedContainer(1224)) - Cannot replicate container #6, no healthy replica found.
2022-08-19 21:52:01,422 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:processAll(293)) - Replication Monitor Thread took 1 milliseconds for processing 6 containers.
2022-08-19 21:52:01,616 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:52:01,629 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@68a2ff8{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:52:01,630 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@6c368f42{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:52:01,630 [ForkJoinPool.commonPool-worker-0] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:52:01,630 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@1cd734f2{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:52:01,630 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@55006fbb{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:52:01,838 [Mini-Cluster-Provider-Reap] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(580)) - Ozone container server stopped.
2022-08-19 21:52:01,906 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@4f81f670{hddsDatanode,/,null,STOPPED}{jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/hddsDatanode}
2022-08-19 21:52:01,908 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@d6ece65{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:52:01,908 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:52:01,909 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@7fac5770{static,/static,jar:file:/home/runner/.m2/repository/org/apache/ozone/hdds-container-service/1.3.0-SNAPSHOT/hdds-container-service-1.3.0-SNAPSHOT.jar!/webapps/static,STOPPED}
2022-08-19 21:52:01,909 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@2e79675f{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:52:01,914 [Mini-Cluster-Provider-Reap] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(537)) - Stopping the StorageContainerManager
2022-08-19 21:52:01,915 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1552)) - Container Balancer is not running.
2022-08-19 21:52:01,915 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1559)) - Stopping Replication Manager Service.
2022-08-19 21:52:01,915 [Mini-Cluster-Provider-Reap] INFO  replication.ReplicationManager (ReplicationManager.java:stop(239)) - Stopping Replication Monitor Thread.
2022-08-19 21:52:01,956 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1566)) - Stopping the Datanode Admin Monitor.
2022-08-19 21:52:01,957 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1573)) - Stopping Lease Manager of the command watchers
2022-08-19 21:52:01,957 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1580)) - Stopping datanode service RPC server
2022-08-19 21:52:01,957 [Mini-Cluster-Provider-Reap] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(414)) - Stopping the RPC server for DataNodes
2022-08-19 21:52:01,957 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 37571
2022-08-19 21:52:01,957 [ReplicationMonitor] INFO  replication.ReplicationManager (ReplicationManager.java:run(543)) - Replication Monitor Thread is stopped
2022-08-19 21:52:01,969 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 0
2022-08-19 21:52:01,969 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:52:02,029 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(870)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2022-08-19 21:52:02,030 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1588)) - Stopping block service RPC server
2022-08-19 21:52:02,030 [Mini-Cluster-Provider-Reap] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(161)) - Stopping the RPC server for Block Protocol
2022-08-19 21:52:02,030 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 39159
2022-08-19 21:52:02,034 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 0
2022-08-19 21:52:02,070 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1595)) - Stopping the StorageContainerLocationProtocol RPC server
2022-08-19 21:52:02,070 [Mini-Cluster-Provider-Reap] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(179)) - Stopping the RPC server for Client Protocol
2022-08-19 21:52:02,070 [Mini-Cluster-Provider-Reap] INFO  ipc.Server (Server.java:stop(3414)) - Stopping server on 34101
2022-08-19 21:52:02,071 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:52:02,079 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1602)) - Stopping Storage Container Manager HTTP server.
2022-08-19 21:52:02,079 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1376)) - Stopping IPC Server listener on 0
2022-08-19 21:52:02,080 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.w.WebAppContext@31bec348{scm,/,null,STOPPED}{file:/home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/scm}
2022-08-19 21:52:02,080 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1512)) - Stopping IPC Server Responder
2022-08-19 21:52:02,081 [Mini-Cluster-Provider-Reap] INFO  server.AbstractConnector (AbstractConnector.java:doStop(381)) - Stopped ServerConnector@6f28eb2c{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
2022-08-19 21:52:02,081 [Mini-Cluster-Provider-Reap] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2022-08-19 21:52:02,081 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@252acf04{static,/static,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-classes/webapps/static,STOPPED}
2022-08-19 21:52:02,081 [Mini-Cluster-Provider-Reap] INFO  handler.ContextHandler (ContextHandler.java:doStop(1153)) - Stopped o.e.j.s.ServletContextHandler@7eb1fa66{logs,/logs,file:///home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/log,STOPPED}
2022-08-19 21:52:02,086 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1608)) - Stopping SCM LayoutVersionManager Service.
2022-08-19 21:52:02,086 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1616)) - Stopping Block Manager Service.
2022-08-19 21:52:02,086 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2022-08-19 21:52:02,087 [Mini-Cluster-Provider-Reap] INFO  utils.BackgroundService (BackgroundService.java:shutdown(141)) - Shutting down service SCMBlockDeletingService
2022-08-19 21:52:02,087 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1643)) - Stopping SCM Event Queue.
2022-08-19 21:52:02,097 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1654)) - Stopping SCM HA services.
2022-08-19 21:52:02,098 [Mini-Cluster-Provider-Reap] INFO  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:stop(149)) - Stopping RatisPipelineUtilsThread.
2022-08-19 21:52:02,098 [RatisPipelineUtilsThread - 0] WARN  pipeline.BackgroundPipelineCreator (BackgroundPipelineCreator.java:run(180)) - RatisPipelineUtilsThread is interrupted.
2022-08-19 21:52:02,098 [Mini-Cluster-Provider-Reap] INFO  BackgroundPipelineScrubber (BackgroundSCMService.java:stop(131)) - Stopping BackgroundPipelineScrubber Service.
2022-08-19 21:52:02,098 [BackgroundPipelineScrubberThread] WARN  BackgroundPipelineScrubber (BackgroundSCMService.java:run(115)) - BackgroundPipelineScrubber is interrupted, exit
2022-08-19 21:52:02,098 [Mini-Cluster-Provider-Reap] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(1664)) - Stopping SCM MetadataStore.
2022-08-19 21:52:02,100 [Mini-Cluster-Provider-Reap] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(205)) - HddsDatanode metrics system stopped (again)
2022-08-19 21:52:03,637 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:03,658 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:13,061 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:13,069 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:17,031 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:17,036 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:18,361 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:18,368 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:21,457 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:21,461 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:27,467 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:27,477 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:29,155 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:29,175 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:30,898 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:30,959 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:33,637 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:33,659 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:43,062 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:43,069 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:47,031 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:47,036 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:48,361 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:48,369 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:51,458 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:51,461 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:57,467 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:57,478 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:52:59,155 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:52:59,175 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:00,898 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:00,962 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:03,639 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:03,659 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
]]></system-out>
  </testcase>
  <testcase name="testDecommissioningNodesCompleteDecommissionOnSCMRestart" classname="org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance" time="100.002">
    <error type="java.io.IOException"><![CDATA[java.io.IOException: Failed to obtain available cluster in time
	at org.apache.hadoop.ozone.MiniOzoneClusterProvider.provide(MiniOzoneClusterProvider.java:162)
	at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.setUp(TestDecommissionAndMaintenance.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:126)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeEachMethod(TimeoutExtension.java:76)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:506)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeBeforeEachMethodAdapter$21(ClassBasedTestDescriptor.java:491)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeEachMethods$3(TestMethodTestDescriptor.java:171)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeMethodsOrCallbacksUntilExceptionOccurs$6(TestMethodTestDescriptor.java:199)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeMethodsOrCallbacksUntilExceptionOccurs(TestMethodTestDescriptor.java:199)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeEachMethods(TestMethodTestDescriptor.java:168)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
]]></error>
    <system-out><![CDATA[2022-08-19 21:53:13,062 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:13,069 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:17,032 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:17,036 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:18,361 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:18,369 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:21,458 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:21,461 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:27,467 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:27,479 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:29,156 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:29,176 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:30,899 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:30,962 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:33,640 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:33,659 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:43,063 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:43,070 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:47,032 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:47,037 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:48,362 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:48,369 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:51,459 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:51,462 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:57,468 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:57,479 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:53:59,156 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:53:59,176 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:00,899 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:00,962 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:03,640 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:03,659 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:13,063 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:13,070 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:17,032 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:17,038 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:18,362 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:18,369 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:21,459 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:21,462 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:27,468 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:27,479 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:29,157 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:29,177 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:30,899 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:30,962 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:33,642 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:33,660 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:43,064 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:43,070 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:47,033 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:47,038 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
2022-08-19 21:54:48,362 [UnderReplicatedQueueThreadThread] INFO  replication.UnderReplicatedProcessor (UnderReplicatedProcessor.java:processAll(85)) - Processed 0 under replicated containers, failed processing 0
2022-08-19 21:54:48,371 [OverReplicatedQueueThreadThread] INFO  replication.OverReplicatedProcessor (OverReplicatedProcessor.java:processAll(83)) - Processed 0 over replicated containers, failed processing 0
]]></system-out>
  </testcase>
</testsuite>